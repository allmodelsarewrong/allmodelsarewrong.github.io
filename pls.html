<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>14 Partial Least Squares Regression | All Models Are Wrong: Concepts of Statistical Learning</title>
  <meta name="description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  <meta name="generator" content="bookdown 0.16 and GitBook 2.6.7" />

  <meta property="og:title" content="14 Partial Least Squares Regression | All Models Are Wrong: Concepts of Statistical Learning" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://allmodelsarewrong.github.io" />
  
  <meta property="og:description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  <meta name="github-repo" content="allmodelsarewrong/allmodelsarewrong.github.io" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="14 Partial Least Squares Regression | All Models Are Wrong: Concepts of Statistical Learning" />
  
  <meta name="twitter:description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  

<meta name="author" content="Gaston Sanchez Ethan Marzban" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="pcr.html"/>
<link rel="next" href="ridge.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><b>All Models Are Wrong (CSL)</b><br><small>by G. Sanchez & E. Marzban</small></a></li>

<li class="divider"></li>
<li class="part"><span><b>I Welcome</b></span></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="about.html"><a href="about.html"><i class="fa fa-check"></i><b>1</b> About this book</a><ul>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#prerequisites"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="part"><span><b>II Intro</b></span></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#basic-notation"><i class="fa fa-check"></i><b>2.1</b> Basic Notation</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="duality.html"><a href="duality.html"><i class="fa fa-check"></i><b>3</b> Geometric Duality</a><ul>
<li class="chapter" data-level="3.1" data-path="duality.html"><a href="duality.html#rows-space"><i class="fa fa-check"></i><b>3.1</b> Rows Space</a></li>
<li class="chapter" data-level="3.2" data-path="duality.html"><a href="duality.html#columns-space"><i class="fa fa-check"></i><b>3.2</b> Columns Space</a></li>
<li class="chapter" data-level="3.3" data-path="duality.html"><a href="duality.html#cloud-of-individuals"><i class="fa fa-check"></i><b>3.3</b> Cloud of Individuals</a><ul>
<li class="chapter" data-level="3.3.1" data-path="duality.html"><a href="duality.html#average-individual"><i class="fa fa-check"></i><b>3.3.1</b> Average Individual</a></li>
<li class="chapter" data-level="3.3.2" data-path="duality.html"><a href="duality.html#centered-data"><i class="fa fa-check"></i><b>3.3.2</b> Centered Data</a></li>
<li class="chapter" data-level="3.3.3" data-path="duality.html"><a href="duality.html#distance-between-individuals"><i class="fa fa-check"></i><b>3.3.3</b> Distance between individuals</a></li>
<li class="chapter" data-level="3.3.4" data-path="duality.html"><a href="duality.html#distance-to-the-centroid"><i class="fa fa-check"></i><b>3.3.4</b> Distance to the centroid</a></li>
<li class="chapter" data-level="3.3.5" data-path="duality.html"><a href="duality.html#measures-of-dispersion"><i class="fa fa-check"></i><b>3.3.5</b> Measures of Dispersion</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="duality.html"><a href="duality.html#cloud-of-variables"><i class="fa fa-check"></i><b>3.4</b> Cloud of Variables</a><ul>
<li class="chapter" data-level="3.4.1" data-path="duality.html"><a href="duality.html#mean-of-a-variable"><i class="fa fa-check"></i><b>3.4.1</b> Mean of a Variable</a></li>
<li class="chapter" data-level="3.4.2" data-path="duality.html"><a href="duality.html#variance-of-a-variable"><i class="fa fa-check"></i><b>3.4.2</b> Variance of a Variable</a></li>
<li class="chapter" data-level="3.4.3" data-path="duality.html"><a href="duality.html#variance-with-vector-notation"><i class="fa fa-check"></i><b>3.4.3</b> Variance with Vector Notation</a></li>
<li class="chapter" data-level="3.4.4" data-path="duality.html"><a href="duality.html#standard-deviation-as-a-norm"><i class="fa fa-check"></i><b>3.4.4</b> Standard Deviation as a Norm</a></li>
<li class="chapter" data-level="3.4.5" data-path="duality.html"><a href="duality.html#covariance"><i class="fa fa-check"></i><b>3.4.5</b> Covariance</a></li>
<li class="chapter" data-level="3.4.6" data-path="duality.html"><a href="duality.html#correlation"><i class="fa fa-check"></i><b>3.4.6</b> Correlation</a></li>
<li class="chapter" data-level="3.4.7" data-path="duality.html"><a href="duality.html#geometry-of-correlation"><i class="fa fa-check"></i><b>3.4.7</b> Geometry of Correlation</a></li>
<li class="chapter" data-level="3.4.8" data-path="duality.html"><a href="duality.html#orthogonal-projections"><i class="fa fa-check"></i><b>3.4.8</b> Orthogonal Projections</a></li>
<li class="chapter" data-level="3.4.9" data-path="duality.html"><a href="duality.html#the-mean-as-an-orthogonal-projection"><i class="fa fa-check"></i><b>3.4.9</b> The mean as an orthogonal projection</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Unsupervised I: PCA</b></span></li>
<li class="chapter" data-level="4" data-path="pca.html"><a href="pca.html"><i class="fa fa-check"></i><b>4</b> Principal Components Analysis</a><ul>
<li class="chapter" data-level="4.1" data-path="pca.html"><a href="pca.html#low-dimensional-representations"><i class="fa fa-check"></i><b>4.1</b> Low-dimensional Representations</a></li>
<li class="chapter" data-level="4.2" data-path="pca.html"><a href="pca.html#projections"><i class="fa fa-check"></i><b>4.2</b> Projections</a><ul>
<li class="chapter" data-level="4.2.1" data-path="pca.html"><a href="pca.html#vector-and-scalar-projections"><i class="fa fa-check"></i><b>4.2.1</b> Vector and Scalar Projections</a></li>
<li class="chapter" data-level="4.2.2" data-path="pca.html"><a href="pca.html#projected-inertia"><i class="fa fa-check"></i><b>4.2.2</b> Projected Inertia</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="pca.html"><a href="pca.html#maximization-problem"><i class="fa fa-check"></i><b>4.3</b> Maximization Problem</a><ul>
<li class="chapter" data-level="4.3.1" data-path="pca.html"><a href="pca.html#eigenvectors-of-mathbfs"><i class="fa fa-check"></i><b>4.3.1</b> Eigenvectors of <span class="math inline">\(\mathbf{S}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="pca.html"><a href="pca.html#another-perspective-of-pca"><i class="fa fa-check"></i><b>4.4</b> Another Perspective of PCA</a><ul>
<li class="chapter" data-level="4.4.1" data-path="pca.html"><a href="pca.html#finding-principal-components"><i class="fa fa-check"></i><b>4.4.1</b> Finding Principal Components</a></li>
<li class="chapter" data-level="4.4.2" data-path="pca.html"><a href="pca.html#finding-the-first-pc"><i class="fa fa-check"></i><b>4.4.2</b> Finding the first PC</a></li>
<li class="chapter" data-level="4.4.3" data-path="pca.html"><a href="pca.html#finding-the-second-pc"><i class="fa fa-check"></i><b>4.4.3</b> Finding the second PC</a></li>
<li class="chapter" data-level="4.4.4" data-path="pca.html"><a href="pca.html#finding-all-pcs"><i class="fa fa-check"></i><b>4.4.4</b> Finding all PCs</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="pca.html"><a href="pca.html#data-decomposition-model"><i class="fa fa-check"></i><b>4.5</b> Data Decomposition Model</a><ul>
<li class="chapter" data-level="4.5.1" data-path="pca.html"><a href="pca.html#alternative-approaches"><i class="fa fa-check"></i><b>4.5.1</b> Alternative Approaches</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Linear Regression</b></span></li>
<li class="chapter" data-level="5" data-path="ols.html"><a href="ols.html"><i class="fa fa-check"></i><b>5</b> Linear Regression</a><ul>
<li class="chapter" data-level="5.1" data-path="ols.html"><a href="ols.html#motivation"><i class="fa fa-check"></i><b>5.1</b> Motivation</a></li>
<li class="chapter" data-level="5.2" data-path="ols.html"><a href="ols.html#the-ideaintuition-of-regression"><i class="fa fa-check"></i><b>5.2</b> The Idea/Intuition of Regression</a></li>
<li class="chapter" data-level="5.3" data-path="ols.html"><a href="ols.html#the-linear-regression-model"><i class="fa fa-check"></i><b>5.3</b> The Linear Regression Model</a></li>
<li class="chapter" data-level="5.4" data-path="ols.html"><a href="ols.html#the-error-measure"><i class="fa fa-check"></i><b>5.4</b> The Error Measure</a></li>
<li class="chapter" data-level="5.5" data-path="ols.html"><a href="ols.html#the-least-squares-algorithm"><i class="fa fa-check"></i><b>5.5</b> The Least Squares Algorithm</a></li>
<li class="chapter" data-level="5.6" data-path="ols.html"><a href="ols.html#geometries-of-ols"><i class="fa fa-check"></i><b>5.6</b> Geometries of OLS</a><ul>
<li class="chapter" data-level="5.6.1" data-path="ols.html"><a href="ols.html#rows-perspective"><i class="fa fa-check"></i><b>5.6.1</b> Rows Perspective</a></li>
<li class="chapter" data-level="5.6.2" data-path="ols.html"><a href="ols.html#columns-perspective"><i class="fa fa-check"></i><b>5.6.2</b> Columns Perspective</a></li>
<li class="chapter" data-level="5.6.3" data-path="ols.html"><a href="ols.html#parameters-perspective"><i class="fa fa-check"></i><b>5.6.3</b> Parameters Perspective</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="gradient.html"><a href="gradient.html"><i class="fa fa-check"></i><b>6</b> Gradient Descent</a><ul>
<li class="chapter" data-level="6.1" data-path="gradient.html"><a href="gradient.html#error-surface"><i class="fa fa-check"></i><b>6.1</b> Error Surface</a></li>
<li class="chapter" data-level="6.2" data-path="gradient.html"><a href="gradient.html#idea-of-gradient-descent"><i class="fa fa-check"></i><b>6.2</b> Idea of Gradient Descent</a></li>
<li class="chapter" data-level="6.3" data-path="gradient.html"><a href="gradient.html#moving-down-an-error-surface"><i class="fa fa-check"></i><b>6.3</b> Moving Down an Error Surface</a><ul>
<li class="chapter" data-level="6.3.1" data-path="gradient.html"><a href="gradient.html#the-direction-of-mathbfv"><i class="fa fa-check"></i><b>6.3.1</b> The direction of <span class="math inline">\(\mathbf{v}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="gradient.html"><a href="gradient.html#gradient-descent-and-our-model"><i class="fa fa-check"></i><b>6.4</b> Gradient Descent and our Model</a></li>
</ul></li>
<li class="part"><span><b>V Learning Concepts</b></span></li>
<li class="chapter" data-level="7" data-path="learning.html"><a href="learning.html"><i class="fa fa-check"></i><b>7</b> Theoretical Framework</a><ul>
<li class="chapter" data-level="7.1" data-path="learning.html"><a href="learning.html#mental-map"><i class="fa fa-check"></i><b>7.1</b> Mental Map</a></li>
<li class="chapter" data-level="7.2" data-path="learning.html"><a href="learning.html#kinds-of-predictions"><i class="fa fa-check"></i><b>7.2</b> Kinds of Predictions</a><ul>
<li class="chapter" data-level="7.2.1" data-path="learning.html"><a href="learning.html#two-types-of-data"><i class="fa fa-check"></i><b>7.2.1</b> Two Types of Data</a></li>
<li class="chapter" data-level="7.2.2" data-path="learning.html"><a href="learning.html#two-types-of-predictions"><i class="fa fa-check"></i><b>7.2.2</b> Two Types of Predictions</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="learning.html"><a href="learning.html#two-types-of-errors"><i class="fa fa-check"></i><b>7.3</b> Two Types of Errors</a><ul>
<li class="chapter" data-level="7.3.1" data-path="learning.html"><a href="learning.html#individual-errors"><i class="fa fa-check"></i><b>7.3.1</b> Individual Errors</a></li>
<li class="chapter" data-level="7.3.2" data-path="learning.html"><a href="learning.html#overall-errors"><i class="fa fa-check"></i><b>7.3.2</b> Overall Errors</a></li>
<li class="chapter" data-level="7.3.3" data-path="learning.html"><a href="learning.html#auxiliary-technicality"><i class="fa fa-check"></i><b>7.3.3</b> Auxiliary Technicality</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="learning.html"><a href="learning.html#noisy-targets"><i class="fa fa-check"></i><b>7.4</b> Noisy Targets</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="mse.html"><a href="mse.html"><i class="fa fa-check"></i><b>8</b> MSE of Estimator</a><ul>
<li class="chapter" data-level="8.1" data-path="mse.html"><a href="mse.html#mse-of-an-estimator"><i class="fa fa-check"></i><b>8.1</b> MSE of an Estimator</a><ul>
<li class="chapter" data-level="8.1.1" data-path="mse.html"><a href="mse.html#prototypical-cases-of-bias-and-variance"><i class="fa fa-check"></i><b>8.1.1</b> Prototypical Cases of Bias and Variance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="biasvar.html"><a href="biasvar.html"><i class="fa fa-check"></i><b>9</b> Bias-Variance Tradeoff</a><ul>
<li class="chapter" data-level="9.1" data-path="biasvar.html"><a href="biasvar.html#introduction"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="biasvar.html"><a href="biasvar.html#motivation-example"><i class="fa fa-check"></i><b>9.2</b> Motivation Example</a><ul>
<li class="chapter" data-level="9.2.1" data-path="biasvar.html"><a href="biasvar.html#two-hypotheses"><i class="fa fa-check"></i><b>9.2.1</b> Two Hypotheses</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="biasvar.html"><a href="biasvar.html#learning-from-two-points"><i class="fa fa-check"></i><b>9.3</b> Learning from two points</a></li>
<li class="chapter" data-level="9.4" data-path="biasvar.html"><a href="biasvar.html#bias-variance-derivation"><i class="fa fa-check"></i><b>9.4</b> Bias-Variance Derivation</a><ul>
<li class="chapter" data-level="9.4.1" data-path="biasvar.html"><a href="biasvar.html#out-of-sample-predictions"><i class="fa fa-check"></i><b>9.4.1</b> Out-of-Sample Predictions</a></li>
<li class="chapter" data-level="9.4.2" data-path="biasvar.html"><a href="biasvar.html#noisy-target"><i class="fa fa-check"></i><b>9.4.2</b> Noisy Target</a></li>
<li class="chapter" data-level="9.4.3" data-path="biasvar.html"><a href="biasvar.html#types-of-theoretical-mses"><i class="fa fa-check"></i><b>9.4.3</b> Types of Theoretical MSEs</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="biasvar.html"><a href="biasvar.html#the-tradeoff"><i class="fa fa-check"></i><b>9.5</b> The Tradeoff</a><ul>
<li class="chapter" data-level="9.5.1" data-path="biasvar.html"><a href="biasvar.html#bias-variance-tradeoff-picture"><i class="fa fa-check"></i><b>9.5.1</b> Bias-Variance Tradeoff Picture</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="phases.html"><a href="phases.html"><i class="fa fa-check"></i><b>10</b> Learning Phases</a><ul>
<li class="chapter" data-level="10.1" data-path="phases.html"><a href="phases.html#introduction-1"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="phases.html"><a href="phases.html#model-assessment"><i class="fa fa-check"></i><b>10.2</b> Model Assessment</a><ul>
<li class="chapter" data-level="10.2.1" data-path="phases.html"><a href="phases.html#holdout-test-set"><i class="fa fa-check"></i><b>10.2.1</b> Holdout Test Set</a></li>
<li class="chapter" data-level="10.2.2" data-path="phases.html"><a href="phases.html#why-does-a-test-set-work"><i class="fa fa-check"></i><b>10.2.2</b> Why does a test set work?</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="phases.html"><a href="phases.html#model-selection"><i class="fa fa-check"></i><b>10.3</b> Model Selection</a><ul>
<li class="chapter" data-level="10.3.1" data-path="phases.html"><a href="phases.html#three-way-holdout-method"><i class="fa fa-check"></i><b>10.3.1</b> Three-way Holdout Method</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="phases.html"><a href="phases.html#model-training"><i class="fa fa-check"></i><b>10.4</b> Model Training</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="resampling.html"><a href="resampling.html"><i class="fa fa-check"></i><b>11</b> Resample Approaches</a><ul>
<li class="chapter" data-level="11.1" data-path="resampling.html"><a href="resampling.html#general-sampling-blueprint"><i class="fa fa-check"></i><b>11.1</b> General Sampling Blueprint</a></li>
<li class="chapter" data-level="11.2" data-path="resampling.html"><a href="resampling.html#monte-carlo-cross-validation"><i class="fa fa-check"></i><b>11.2</b> Monte Carlo Cross Validation</a></li>
<li class="chapter" data-level="11.3" data-path="resampling.html"><a href="resampling.html#bootstrap-method"><i class="fa fa-check"></i><b>11.3</b> Bootstrap Method</a></li>
<li class="chapter" data-level="11.4" data-path="resampling.html"><a href="resampling.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>11.4</b> <span class="math inline">\(K\)</span>-Fold Cross-Validation</a><ul>
<li class="chapter" data-level="11.4.1" data-path="resampling.html"><a href="resampling.html#leave-one-out-cross-validation-loocv"><i class="fa fa-check"></i><b>11.4.1</b> Leave-One-Out Cross Validation (LOOCV)</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>VI Regularization</b></span></li>
<li class="chapter" data-level="12" data-path="regular.html"><a href="regular.html"><i class="fa fa-check"></i><b>12</b> Regularization Techniques</a><ul>
<li class="chapter" data-level="12.1" data-path="regular.html"><a href="regular.html#multicollinearity-issues"><i class="fa fa-check"></i><b>12.1</b> Multicollinearity Issues</a><ul>
<li class="chapter" data-level="12.1.1" data-path="regular.html"><a href="regular.html#toy-example"><i class="fa fa-check"></i><b>12.1.1</b> Toy Example</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="regular.html"><a href="regular.html#irregular-coefficients"><i class="fa fa-check"></i><b>12.2</b> Irregular Coefficients</a></li>
<li class="chapter" data-level="12.3" data-path="regular.html"><a href="regular.html#connection-to-regularization"><i class="fa fa-check"></i><b>12.3</b> Connection to Regularization</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="pcr.html"><a href="pcr.html"><i class="fa fa-check"></i><b>13</b> Principal Components Regression</a><ul>
<li class="chapter" data-level="13.1" data-path="pcr.html"><a href="pcr.html#motivation-example-1"><i class="fa fa-check"></i><b>13.1</b> Motivation Example</a></li>
<li class="chapter" data-level="13.2" data-path="pcr.html"><a href="pcr.html#the-pcr-model"><i class="fa fa-check"></i><b>13.2</b> The PCR Model</a></li>
<li class="chapter" data-level="13.3" data-path="pcr.html"><a href="pcr.html#how-does-pcr-work"><i class="fa fa-check"></i><b>13.3</b> How does PCR work?</a><ul>
<li class="chapter" data-level="13.3.1" data-path="pcr.html"><a href="pcr.html#transition-formula"><i class="fa fa-check"></i><b>13.3.1</b> Transition Formula</a></li>
<li class="chapter" data-level="13.3.2" data-path="pcr.html"><a href="pcr.html#size-of-coefficients"><i class="fa fa-check"></i><b>13.3.2</b> Size of Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="pcr.html"><a href="pcr.html#selecting-number-of-pcs"><i class="fa fa-check"></i><b>13.4</b> Selecting Number of PCs</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="pls.html"><a href="pls.html"><i class="fa fa-check"></i><b>14</b> Partial Least Squares Regression</a><ul>
<li class="chapter" data-level="14.1" data-path="pls.html"><a href="pls.html#motivation-example-2"><i class="fa fa-check"></i><b>14.1</b> Motivation Example</a></li>
<li class="chapter" data-level="14.2" data-path="pls.html"><a href="pls.html#the-plsr-model"><i class="fa fa-check"></i><b>14.2</b> The PLSR Model</a></li>
<li class="chapter" data-level="14.3" data-path="pls.html"><a href="pls.html#how-does-plsr-work"><i class="fa fa-check"></i><b>14.3</b> How does PLSR work?</a></li>
<li class="chapter" data-level="14.4" data-path="pls.html"><a href="pls.html#plsr-algorithm"><i class="fa fa-check"></i><b>14.4</b> PLSR Algorithm</a><ul>
<li class="chapter" data-level="14.4.1" data-path="pls.html"><a href="pls.html#pls-solution-with-original-variables"><i class="fa fa-check"></i><b>14.4.1</b> PLS Solution with original variables</a></li>
<li class="chapter" data-level="14.4.2" data-path="pls.html"><a href="pls.html#size-of-coefficients-1"><i class="fa fa-check"></i><b>14.4.2</b> Size of Coefficients</a></li>
<li class="chapter" data-level="14.4.3" data-path="pls.html"><a href="pls.html#some-properties"><i class="fa fa-check"></i><b>14.4.3</b> Some Properties</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="pls.html"><a href="pls.html#selecting-number-of-pls-components"><i class="fa fa-check"></i><b>14.5</b> Selecting Number of PLS Components</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="ridge.html"><a href="ridge.html"><i class="fa fa-check"></i><b>15</b> Ridge Regression</a><ul>
<li class="chapter" data-level="15.1" data-path="ridge.html"><a href="ridge.html#a-new-minimization-problem"><i class="fa fa-check"></i><b>15.1</b> A New Minimization Problem</a><ul>
<li class="chapter" data-level="15.1.1" data-path="ridge.html"><a href="ridge.html#constraining-regression-coefficients"><i class="fa fa-check"></i><b>15.1.1</b> Constraining Regression Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="ridge.html"><a href="ridge.html#a-new-minimization-solution"><i class="fa fa-check"></i><b>15.2</b> A New Minimization Solution</a></li>
<li class="chapter" data-level="15.3" data-path="ridge.html"><a href="ridge.html#what-does-rr-accomplish"><i class="fa fa-check"></i><b>15.3</b> What does RR accomplish?</a><ul>
<li class="chapter" data-level="15.3.1" data-path="ridge.html"><a href="ridge.html#how-to-find-lambda"><i class="fa fa-check"></i><b>15.3.1</b> How to find <span class="math inline">\(\lambda\)</span>?</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>VII Extending Linear Regression</b></span></li>
<li class="chapter" data-level="16" data-path="linear-extensions.html"><a href="linear-extensions.html"><i class="fa fa-check"></i><b>16</b> Beyond Linear Regression</a><ul>
<li class="chapter" data-level="16.1" data-path="linear-extensions.html"><a href="linear-extensions.html#introduction-2"><i class="fa fa-check"></i><b>16.1</b> Introduction</a></li>
<li class="chapter" data-level="16.2" data-path="linear-extensions.html"><a href="linear-extensions.html#expanding-the-regression-horizon"><i class="fa fa-check"></i><b>16.2</b> Expanding the Regression Horizon</a><ul>
<li class="chapter" data-level="16.2.1" data-path="linear-extensions.html"><a href="linear-extensions.html#linearity"><i class="fa fa-check"></i><b>16.2.1</b> Linearity</a></li>
<li class="chapter" data-level="16.2.2" data-path="linear-extensions.html"><a href="linear-extensions.html#parametric-and-nonparametric"><i class="fa fa-check"></i><b>16.2.2</b> Parametric and Nonparametric</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="linear-extensions.html"><a href="linear-extensions.html#transforming-features"><i class="fa fa-check"></i><b>16.3</b> Transforming Features</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="nonparametric.html"><a href="nonparametric.html"><i class="fa fa-check"></i><b>17</b> Nonparametric Regression</a><ul>
<li class="chapter" data-level="17.1" data-path="nonparametric.html"><a href="nonparametric.html#conditional-averages"><i class="fa fa-check"></i><b>17.1</b> Conditional Averages</a></li>
<li class="chapter" data-level="17.2" data-path="nonparametric.html"><a href="nonparametric.html#looking-at-the-neighbors"><i class="fa fa-check"></i><b>17.2</b> Looking at the Neighbors</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="knn.html"><a href="knn.html"><i class="fa fa-check"></i><b>18</b> Nearest Neighbor Estimates</a><ul>
<li class="chapter" data-level="18.1" data-path="knn.html"><a href="knn.html#introduction-3"><i class="fa fa-check"></i><b>18.1</b> Introduction</a></li>
<li class="chapter" data-level="18.2" data-path="knn.html"><a href="knn.html#k-nearest-neighbors-knn"><i class="fa fa-check"></i><b>18.2</b> <span class="math inline">\(k\)</span> Nearest Neighbors (KNN)</a><ul>
<li class="chapter" data-level="18.2.1" data-path="knn.html"><a href="knn.html#distance-measures"><i class="fa fa-check"></i><b>18.2.1</b> Distance Measures</a></li>
<li class="chapter" data-level="18.2.2" data-path="knn.html"><a href="knn.html#knn-estimator"><i class="fa fa-check"></i><b>18.2.2</b> KNN Estimator</a></li>
<li class="chapter" data-level="18.2.3" data-path="knn.html"><a href="knn.html#how-to-find-k"><i class="fa fa-check"></i><b>18.2.3</b> How to find <span class="math inline">\(k\)</span>?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html"><i class="fa fa-check"></i><b>19</b> Kernel Smoothers</a><ul>
<li class="chapter" data-level="19.1" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#introduction-4"><i class="fa fa-check"></i><b>19.1</b> Introduction</a></li>
<li class="chapter" data-level="19.2" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#kernel-smoothers-1"><i class="fa fa-check"></i><b>19.2</b> Kernel Smoothers</a><ul>
<li class="chapter" data-level="19.2.1" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#kernel-functions"><i class="fa fa-check"></i><b>19.2.1</b> Kernel Functions</a></li>
<li class="chapter" data-level="19.2.2" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#weights-from-kernels"><i class="fa fa-check"></i><b>19.2.2</b> Weights from Kernels</a></li>
<li class="chapter" data-level="19.2.3" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#kernel-estimator"><i class="fa fa-check"></i><b>19.2.3</b> Kernel Estimator</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#local-polynomial-estimators"><i class="fa fa-check"></i><b>19.3</b> Local Polynomial Estimators</a></li>
</ul></li>
<li class="part"><span><b>VIII Classification</b></span></li>
<li class="chapter" data-level="20" data-path="classif.html"><a href="classif.html"><i class="fa fa-check"></i><b>20</b> Classification</a><ul>
<li class="chapter" data-level="20.1" data-path="classif.html"><a href="classif.html#introduction-5"><i class="fa fa-check"></i><b>20.1</b> Introduction</a><ul>
<li class="chapter" data-level="20.1.1" data-path="classif.html"><a href="classif.html#credit-score-example"><i class="fa fa-check"></i><b>20.1.1</b> Credit Score Example</a></li>
<li class="chapter" data-level="20.1.2" data-path="classif.html"><a href="classif.html#toy-example-1"><i class="fa fa-check"></i><b>20.1.2</b> Toy Example</a></li>
<li class="chapter" data-level="20.1.3" data-path="classif.html"><a href="classif.html#two-class-problem"><i class="fa fa-check"></i><b>20.1.3</b> Two-class Problem</a></li>
<li class="chapter" data-level="20.1.4" data-path="classif.html"><a href="classif.html#bayes-rule-reminder"><i class="fa fa-check"></i><b>20.1.4</b> Bayes’ Rule Reminder</a></li>
</ul></li>
<li class="chapter" data-level="20.2" data-path="classif.html"><a href="classif.html#bayes-classifier"><i class="fa fa-check"></i><b>20.2</b> Bayes Classifier</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="logistic.html"><a href="logistic.html"><i class="fa fa-check"></i><b>21</b> Logistic Regression</a><ul>
<li class="chapter" data-level="21.1" data-path="logistic.html"><a href="logistic.html#motivation-1"><i class="fa fa-check"></i><b>21.1</b> Motivation</a><ul>
<li class="chapter" data-level="21.1.1" data-path="logistic.html"><a href="logistic.html#first-approach-fitting-a-line"><i class="fa fa-check"></i><b>21.1.1</b> First Approach: Fitting a Line</a></li>
<li class="chapter" data-level="21.1.2" data-path="logistic.html"><a href="logistic.html#second-approach-harsh-thresholding"><i class="fa fa-check"></i><b>21.1.2</b> Second Approach: Harsh Thresholding</a></li>
<li class="chapter" data-level="21.1.3" data-path="logistic.html"><a href="logistic.html#third-approach-conditional-means"><i class="fa fa-check"></i><b>21.1.3</b> Third Approach: Conditional Means</a></li>
</ul></li>
<li class="chapter" data-level="21.2" data-path="logistic.html"><a href="logistic.html#logistic-regression-model"><i class="fa fa-check"></i><b>21.2</b> Logistic Regression Model</a><ul>
<li class="chapter" data-level="21.2.1" data-path="logistic.html"><a href="logistic.html#the-criterion-being-optimized"><i class="fa fa-check"></i><b>21.2.1</b> The Criterion Being Optimized</a></li>
<li class="chapter" data-level="21.2.2" data-path="logistic.html"><a href="logistic.html#another-way-to-solve-logistic-regression"><i class="fa fa-check"></i><b>21.2.2</b> Another Way to Solve Logistic Regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="22" data-path="discrim.html"><a href="discrim.html"><i class="fa fa-check"></i><b>22</b> Preamble for Discriminant Analysis</a><ul>
<li class="chapter" data-level="22.1" data-path="discrim.html"><a href="discrim.html#motivation-2"><i class="fa fa-check"></i><b>22.1</b> Motivation</a><ul>
<li class="chapter" data-level="22.1.1" data-path="discrim.html"><a href="discrim.html#distinguishing-species"><i class="fa fa-check"></i><b>22.1.1</b> Distinguishing Species</a></li>
<li class="chapter" data-level="22.1.2" data-path="discrim.html"><a href="discrim.html#sum-of-squares-decomposition"><i class="fa fa-check"></i><b>22.1.2</b> Sum of Squares Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="22.2" data-path="discrim.html"><a href="discrim.html#derived-ratios-from-sum-of-squares"><i class="fa fa-check"></i><b>22.2</b> Derived Ratios from Sum-of-Squares</a><ul>
<li class="chapter" data-level="22.2.1" data-path="discrim.html"><a href="discrim.html#correlation-ratio"><i class="fa fa-check"></i><b>22.2.1</b> Correlation Ratio</a></li>
<li class="chapter" data-level="22.2.2" data-path="discrim.html"><a href="discrim.html#f-ratio"><i class="fa fa-check"></i><b>22.2.2</b> F-Ratio</a></li>
</ul></li>
<li class="chapter" data-level="22.3" data-path="discrim.html"><a href="discrim.html#geometric-perspective"><i class="fa fa-check"></i><b>22.3</b> Geometric Perspective</a><ul>
<li class="chapter" data-level="22.3.1" data-path="discrim.html"><a href="discrim.html#clouds-from-class-structure"><i class="fa fa-check"></i><b>22.3.1</b> Clouds from Class Structure</a></li>
<li class="chapter" data-level="22.3.2" data-path="discrim.html"><a href="discrim.html#dispersion-decomposition"><i class="fa fa-check"></i><b>22.3.2</b> Dispersion Decomposition</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="23" data-path="cda.html"><a href="cda.html"><i class="fa fa-check"></i><b>23</b> Canonical Discriminant Analysis</a><ul>
<li class="chapter" data-level="23.1" data-path="cda.html"><a href="cda.html#cda-semi-supervised-aspect"><i class="fa fa-check"></i><b>23.1</b> CDA: Semi-Supervised Aspect</a></li>
<li class="chapter" data-level="23.2" data-path="cda.html"><a href="cda.html#looking-for-a-discriminant-axis"><i class="fa fa-check"></i><b>23.2</b> Looking for a discriminant axis</a></li>
<li class="chapter" data-level="23.3" data-path="cda.html"><a href="cda.html#looking-for-a-compromise-criterion"><i class="fa fa-check"></i><b>23.3</b> Looking for a Compromise Criterion</a><ul>
<li class="chapter" data-level="23.3.1" data-path="cda.html"><a href="cda.html#correlation-ratio-criterion"><i class="fa fa-check"></i><b>23.3.1</b> Correlation Ratio Criterion</a></li>
<li class="chapter" data-level="23.3.2" data-path="cda.html"><a href="cda.html#f-ratio-criterion"><i class="fa fa-check"></i><b>23.3.2</b> F-ratio Criterion</a></li>
<li class="chapter" data-level="23.3.3" data-path="cda.html"><a href="cda.html#a-special-pca"><i class="fa fa-check"></i><b>23.3.3</b> A Special PCA</a></li>
</ul></li>
<li class="chapter" data-level="23.4" data-path="cda.html"><a href="cda.html#cda-supervised-aspect"><i class="fa fa-check"></i><b>23.4</b> CDA: Supervised Aspect</a><ul>
<li class="chapter" data-level="23.4.1" data-path="cda.html"><a href="cda.html#distance-behind-cda"><i class="fa fa-check"></i><b>23.4.1</b> Distance behind CDA</a></li>
<li class="chapter" data-level="23.4.2" data-path="cda.html"><a href="cda.html#predictive-idea"><i class="fa fa-check"></i><b>23.4.2</b> Predictive Idea</a></li>
<li class="chapter" data-level="23.4.3" data-path="cda.html"><a href="cda.html#cda-classifier"><i class="fa fa-check"></i><b>23.4.3</b> CDA Classifier</a></li>
<li class="chapter" data-level="23.4.4" data-path="cda.html"><a href="cda.html#limitations-of-cda-classifier"><i class="fa fa-check"></i><b>23.4.4</b> Limitations of CDA classifier</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="24" data-path="discanalysis.html"><a href="discanalysis.html"><i class="fa fa-check"></i><b>24</b> Discriminant Analysis</a><ul>
<li class="chapter" data-level="24.1" data-path="discanalysis.html"><a href="discanalysis.html#probabilistic-da"><i class="fa fa-check"></i><b>24.1</b> Probabilistic DA</a><ul>
<li class="chapter" data-level="24.1.1" data-path="discanalysis.html"><a href="discanalysis.html#normal-distributions"><i class="fa fa-check"></i><b>24.1.1</b> Normal Distributions</a></li>
<li class="chapter" data-level="24.1.2" data-path="discanalysis.html"><a href="discanalysis.html#estimating-parameters-of-normal-distributions"><i class="fa fa-check"></i><b>24.1.2</b> Estimating Parameters of Normal Distributions</a></li>
</ul></li>
<li class="chapter" data-level="24.2" data-path="discanalysis.html"><a href="discanalysis.html#discriminant-functions"><i class="fa fa-check"></i><b>24.2</b> Discriminant Functions</a></li>
<li class="chapter" data-level="24.3" data-path="discanalysis.html"><a href="discanalysis.html#quadratic-discriminant-analysis-qda"><i class="fa fa-check"></i><b>24.3</b> Quadratic Discriminant Analysis (QDA)</a></li>
<li class="chapter" data-level="24.4" data-path="discanalysis.html"><a href="discanalysis.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>24.4</b> Linear Discriminant Analysis</a><ul>
<li class="chapter" data-level="24.4.1" data-path="discanalysis.html"><a href="discanalysis.html#canonical-discriminant-analysis"><i class="fa fa-check"></i><b>24.4.1</b> Canonical Discriminant Analysis</a></li>
<li class="chapter" data-level="24.4.2" data-path="discanalysis.html"><a href="discanalysis.html#naive-bayes"><i class="fa fa-check"></i><b>24.4.2</b> Naive Bayes</a></li>
<li class="chapter" data-level="24.4.3" data-path="discanalysis.html"><a href="discanalysis.html#fifth-case"><i class="fa fa-check"></i><b>24.4.3</b> Fifth Case</a></li>
</ul></li>
<li class="chapter" data-level="24.5" data-path="discanalysis.html"><a href="discanalysis.html#comparing-the-cases"><i class="fa fa-check"></i><b>24.5</b> Comparing the Cases</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="classperformance.html"><a href="classperformance.html"><i class="fa fa-check"></i><b>25</b> Performance of Classifiers</a><ul>
<li class="chapter" data-level="25.1" data-path="classperformance.html"><a href="classperformance.html#classification-error-measures"><i class="fa fa-check"></i><b>25.1</b> Classification Error Measures</a><ul>
<li class="chapter" data-level="25.1.1" data-path="classperformance.html"><a href="classperformance.html#errors-for-binary-response"><i class="fa fa-check"></i><b>25.1.1</b> Errors for Binary Response</a></li>
<li class="chapter" data-level="25.1.2" data-path="classperformance.html"><a href="classperformance.html#error-for-categorical-response"><i class="fa fa-check"></i><b>25.1.2</b> Error for Categorical Response</a></li>
</ul></li>
<li class="chapter" data-level="25.2" data-path="classperformance.html"><a href="classperformance.html#confusion-matrices"><i class="fa fa-check"></i><b>25.2</b> Confusion Matrices</a></li>
<li class="chapter" data-level="25.3" data-path="classperformance.html"><a href="classperformance.html#binary-response-example"><i class="fa fa-check"></i><b>25.3</b> Binary Response Example</a><ul>
<li class="chapter" data-level="25.3.1" data-path="classperformance.html"><a href="classperformance.html#application-for-checking-account"><i class="fa fa-check"></i><b>25.3.1</b> Application for Checking Account</a></li>
<li class="chapter" data-level="25.3.2" data-path="classperformance.html"><a href="classperformance.html#application-for-loan"><i class="fa fa-check"></i><b>25.3.2</b> Application for Loan</a></li>
</ul></li>
<li class="chapter" data-level="25.4" data-path="classperformance.html"><a href="classperformance.html#decision-rules-and-errors"><i class="fa fa-check"></i><b>25.4</b> Decision Rules and Errors</a></li>
<li class="chapter" data-level="25.5" data-path="classperformance.html"><a href="classperformance.html#roc-curves"><i class="fa fa-check"></i><b>25.5</b> ROC Curves</a><ul>
<li class="chapter" data-level="25.5.1" data-path="classperformance.html"><a href="classperformance.html#graphing-roc-curves"><i class="fa fa-check"></i><b>25.5.1</b> Graphing ROC curves</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IX Unsupervised II: Clustering</b></span></li>
<li class="chapter" data-level="26" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>26</b> Clustering</a><ul>
<li class="chapter" data-level="26.1" data-path="clustering.html"><a href="clustering.html#about-clustering"><i class="fa fa-check"></i><b>26.1</b> About Clustering</a><ul>
<li class="chapter" data-level="26.1.1" data-path="clustering.html"><a href="clustering.html#types-of-clustering"><i class="fa fa-check"></i><b>26.1.1</b> Types of Clustering</a></li>
<li class="chapter" data-level="26.1.2" data-path="clustering.html"><a href="clustering.html#hard-clustering"><i class="fa fa-check"></i><b>26.1.2</b> Hard Clustering</a></li>
</ul></li>
<li class="chapter" data-level="26.2" data-path="clustering.html"><a href="clustering.html#dispersion-measures"><i class="fa fa-check"></i><b>26.2</b> Dispersion Measures</a></li>
<li class="chapter" data-level="26.3" data-path="clustering.html"><a href="clustering.html#complexity-in-clustering"><i class="fa fa-check"></i><b>26.3</b> Complexity in Clustering</a></li>
</ul></li>
<li class="chapter" data-level="27" data-path="kmeans.html"><a href="kmeans.html"><i class="fa fa-check"></i><b>27</b> K-Means</a><ul>
<li class="chapter" data-level="27.1" data-path="kmeans.html"><a href="kmeans.html#toy-example-2"><i class="fa fa-check"></i><b>27.1</b> Toy Example</a></li>
<li class="chapter" data-level="27.2" data-path="kmeans.html"><a href="kmeans.html#what-does-k-means-do"><i class="fa fa-check"></i><b>27.2</b> What does K-means do?</a></li>
<li class="chapter" data-level="27.3" data-path="kmeans.html"><a href="kmeans.html#k-means-algorithms"><i class="fa fa-check"></i><b>27.3</b> K-Means Algorithms</a><ul>
<li class="chapter" data-level="27.3.1" data-path="kmeans.html"><a href="kmeans.html#classic-version"><i class="fa fa-check"></i><b>27.3.1</b> Classic Version</a></li>
<li class="chapter" data-level="27.3.2" data-path="kmeans.html"><a href="kmeans.html#moving-centers-algorithm"><i class="fa fa-check"></i><b>27.3.2</b> Moving Centers Algorithm</a></li>
<li class="chapter" data-level="27.3.3" data-path="kmeans.html"><a href="kmeans.html#dynamic-clouds"><i class="fa fa-check"></i><b>27.3.3</b> Dynamic Clouds</a></li>
<li class="chapter" data-level="27.3.4" data-path="kmeans.html"><a href="kmeans.html#choosing-k"><i class="fa fa-check"></i><b>27.3.4</b> Choosing <span class="math inline">\(K\)</span></a></li>
<li class="chapter" data-level="27.3.5" data-path="kmeans.html"><a href="kmeans.html#comments"><i class="fa fa-check"></i><b>27.3.5</b> Comments</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="28" data-path="hclus.html"><a href="hclus.html"><i class="fa fa-check"></i><b>28</b> Hierarchical Clustering</a><ul>
<li class="chapter" data-level="28.1" data-path="hclus.html"><a href="hclus.html#agglomerative-methods"><i class="fa fa-check"></i><b>28.1</b> Agglomerative Methods</a></li>
<li class="chapter" data-level="28.2" data-path="hclus.html"><a href="hclus.html#example-single-linkage"><i class="fa fa-check"></i><b>28.2</b> Example: Single Linkage</a><ul>
<li class="chapter" data-level="28.2.1" data-path="hclus.html"><a href="hclus.html#dendrogram"><i class="fa fa-check"></i><b>28.2.1</b> Dendrogram</a></li>
</ul></li>
<li class="chapter" data-level="28.3" data-path="hclus.html"><a href="hclus.html#example-complete-linkage"><i class="fa fa-check"></i><b>28.3</b> Example: Complete Linkage</a><ul>
<li class="chapter" data-level="28.3.1" data-path="hclus.html"><a href="hclus.html#cutting-dendograms"><i class="fa fa-check"></i><b>28.3.1</b> Cutting Dendograms</a></li>
<li class="chapter" data-level="28.3.2" data-path="hclus.html"><a href="hclus.html#pros-and-coons"><i class="fa fa-check"></i><b>28.3.2</b> Pros and COons</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>X Tree-based Methods</b></span></li>
<li class="chapter" data-level="29" data-path="trees.html"><a href="trees.html"><i class="fa fa-check"></i><b>29</b> Intro to Decision Trees</a><ul>
<li class="chapter" data-level="29.1" data-path="trees.html"><a href="trees.html#introduction-6"><i class="fa fa-check"></i><b>29.1</b> Introduction</a></li>
<li class="chapter" data-level="29.2" data-path="trees.html"><a href="trees.html#some-terminology"><i class="fa fa-check"></i><b>29.2</b> Some Terminology</a><ul>
<li class="chapter" data-level="29.2.1" data-path="trees.html"><a href="trees.html#binary-trees"><i class="fa fa-check"></i><b>29.2.1</b> Binary Trees</a></li>
</ul></li>
<li class="chapter" data-level="29.3" data-path="trees.html"><a href="trees.html#space-partitions"><i class="fa fa-check"></i><b>29.3</b> Space Partitions</a><ul>
<li class="chapter" data-level="29.3.1" data-path="trees.html"><a href="trees.html#the-process-of-building-a-tree"><i class="fa fa-check"></i><b>29.3.1</b> The Process of Building a Tree</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="30" data-path="tree-impurities.html"><a href="tree-impurities.html"><i class="fa fa-check"></i><b>30</b> Binary Splits and Impurity</a><ul>
<li class="chapter" data-level="30.1" data-path="tree-impurities.html"><a href="tree-impurities.html#binary-partitions"><i class="fa fa-check"></i><b>30.1</b> Binary Partitions</a><ul>
<li class="chapter" data-level="30.1.1" data-path="tree-impurities.html"><a href="tree-impurities.html#splits-of-binary-variables"><i class="fa fa-check"></i><b>30.1.1</b> Splits of Binary variables</a></li>
<li class="chapter" data-level="30.1.2" data-path="tree-impurities.html"><a href="tree-impurities.html#splits-of-nominal-variables"><i class="fa fa-check"></i><b>30.1.2</b> Splits of Nominal Variables</a></li>
<li class="chapter" data-level="30.1.3" data-path="tree-impurities.html"><a href="tree-impurities.html#splits-of-ordinal-variables"><i class="fa fa-check"></i><b>30.1.3</b> Splits of Ordinal Variables</a></li>
<li class="chapter" data-level="30.1.4" data-path="tree-impurities.html"><a href="tree-impurities.html#splits-continuous-variables"><i class="fa fa-check"></i><b>30.1.4</b> Splits Continuous variables</a></li>
</ul></li>
<li class="chapter" data-level="30.2" data-path="tree-impurities.html"><a href="tree-impurities.html#measures-of-impurity"><i class="fa fa-check"></i><b>30.2</b> Measures of Impurity</a><ul>
<li class="chapter" data-level="30.2.1" data-path="tree-impurities.html"><a href="tree-impurities.html#entropy"><i class="fa fa-check"></i><b>30.2.1</b> Entropy</a></li>
<li class="chapter" data-level="30.2.2" data-path="tree-impurities.html"><a href="tree-impurities.html#the-math-behind-entropy"><i class="fa fa-check"></i><b>30.2.2</b> The Math Behind Entropy</a></li>
<li class="chapter" data-level="30.2.3" data-path="tree-impurities.html"><a href="tree-impurities.html#gini-impurity"><i class="fa fa-check"></i><b>30.2.3</b> Gini Impurity</a></li>
<li class="chapter" data-level="30.2.4" data-path="tree-impurities.html"><a href="tree-impurities.html#variance-based-impurity"><i class="fa fa-check"></i><b>30.2.4</b> Variance-based Impurity</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="31" data-path="tree-splits.html"><a href="tree-splits.html"><i class="fa fa-check"></i><b>31</b> Splitting Nodes</a><ul>
<li class="chapter" data-level="31.1" data-path="tree-splits.html"><a href="tree-splits.html#entropy-based-splits"><i class="fa fa-check"></i><b>31.1</b> Entropy-based Splits</a></li>
<li class="chapter" data-level="31.2" data-path="tree-splits.html"><a href="tree-splits.html#gini-index-based-splits"><i class="fa fa-check"></i><b>31.2</b> Gini-index based Splits</a></li>
<li class="chapter" data-level="31.3" data-path="tree-splits.html"><a href="tree-splits.html#looking-for-the-best-split"><i class="fa fa-check"></i><b>31.3</b> Looking for the best split</a></li>
</ul></li>
<li class="chapter" data-level="32" data-path="tree-basics.html"><a href="tree-basics.html"><i class="fa fa-check"></i><b>32</b> Building Binary Trees</a><ul>
<li class="chapter" data-level="32.1" data-path="tree-basics.html"><a href="tree-basics.html#node-splitting-stopping-criteria"><i class="fa fa-check"></i><b>32.1</b> Node-Splitting Stopping Criteria</a></li>
<li class="chapter" data-level="32.2" data-path="tree-basics.html"><a href="tree-basics.html#issues-with-trees"><i class="fa fa-check"></i><b>32.2</b> Issues with Trees</a><ul>
<li class="chapter" data-level="32.2.1" data-path="tree-basics.html"><a href="tree-basics.html#bias-variance-of-trees"><i class="fa fa-check"></i><b>32.2.1</b> Bias-Variance of Trees</a></li>
</ul></li>
<li class="chapter" data-level="32.3" data-path="tree-basics.html"><a href="tree-basics.html#pruning-a-tree"><i class="fa fa-check"></i><b>32.3</b> Pruning a Tree</a></li>
<li class="chapter" data-level="32.4" data-path="tree-basics.html"><a href="tree-basics.html#pros-and-cons-of-trees"><i class="fa fa-check"></i><b>32.4</b> Pros and Cons of Trees</a><ul>
<li class="chapter" data-level="32.4.1" data-path="tree-basics.html"><a href="tree-basics.html#advantages-of-trees"><i class="fa fa-check"></i><b>32.4.1</b> Advantages of Trees</a></li>
<li class="chapter" data-level="32.4.2" data-path="tree-basics.html"><a href="tree-basics.html#disadvantages-of-trees"><i class="fa fa-check"></i><b>32.4.2</b> Disadvantages of Trees</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="33" data-path="bagging.html"><a href="bagging.html"><i class="fa fa-check"></i><b>33</b> Bagging</a><ul>
<li class="chapter" data-level="33.1" data-path="bagging.html"><a href="bagging.html#introduction-7"><i class="fa fa-check"></i><b>33.1</b> Introduction</a><ul>
<li class="chapter" data-level="33.1.1" data-path="bagging.html"><a href="bagging.html#idea-of-bagging"><i class="fa fa-check"></i><b>33.1.1</b> Idea of Bagging</a></li>
</ul></li>
<li class="chapter" data-level="33.2" data-path="bagging.html"><a href="bagging.html#why-bother-bagging"><i class="fa fa-check"></i><b>33.2</b> Why Bother Bagging?</a></li>
</ul></li>
<li class="chapter" data-level="34" data-path="forest.html"><a href="forest.html"><i class="fa fa-check"></i><b>34</b> Random Forests</a><ul>
<li class="chapter" data-level="34.1" data-path="forest.html"><a href="forest.html#introduction-8"><i class="fa fa-check"></i><b>34.1</b> Introduction</a></li>
<li class="chapter" data-level="34.2" data-path="forest.html"><a href="forest.html#algorithm-1"><i class="fa fa-check"></i><b>34.2</b> Algorithm</a></li>
<li class="chapter" data-level="34.3" data-path="forest.html"><a href="forest.html#two-sources-of-randomness"><i class="fa fa-check"></i><b>34.3</b> Two Sources of Randomness</a><ul>
<li class="chapter" data-level="34.3.1" data-path="forest.html"><a href="forest.html#key-advantage-of-random-forests"><i class="fa fa-check"></i><b>34.3.1</b> Key Advantage of Random Forests</a></li>
<li class="chapter" data-level="34.3.2" data-path="forest.html"><a href="forest.html#errors"><i class="fa fa-check"></i><b>34.3.2</b> Errors</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Made with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">All Models Are Wrong: Concepts of Statistical Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="pls" class="section level1">
<h1><span class="header-section-number">14</span> Partial Least Squares Regression</h1>
<p>Another dimension reduction method that we can use to regularize a model is
Partial Least Squares Regression (PLSR).</p>
<p>Before we dive deep into the nuts and bolts of PLSR, we should let you know that
PLS methods form a very big family of methods. While the regression method is
probably the most popular PLS technique, it is by no means the only one.
And even within PLSR, there’s a wide array of different algorithms that let
you obtain the core solution.</p>
<p>PLS Regression was mainly developed in the early 1980s by Scandinavian
chemometricians Svante Wold and Harald Martens.
The theoretical background was based on the <em>PLS Modeling</em> framework of
Herman Wold (Svante’s father).</p>
<ul>
<li>PLS Regression was developed as an algorithmic solution, with no optimization
criterion explicitly defined.</li>
<li>It was introduced in the fields of chemometrics (where almost immediately became a hit).</li>
<li>It slowly attracted the attention of curious applied statisticians.</li>
<li>It took a couple of years (late 1980s - early 1990s) for applied mathematicians to discover its properties.</li>
<li>Nowadays there are several versions (flavors) of the algorithm to compute a PLS regression.</li>
</ul>
<div id="motivation-example-2" class="section level2">
<h2><span class="header-section-number">14.1</span> Motivation Example</h2>
<p>To introduce PLSR we are using the same data set of the previous chapter: a
subset of the “2004 New Car and Truck Data”. The data set consists of
10 variables measured on 385 cars. Here’s what
the first six rows look like:</p>
<pre><code>#&gt;                               price engine cyl  hp city_mpg
#&gt; Acura 3.5 RL 4dr              43755    3.5   6 225       18
#&gt; Acura 3.5 RL w/Navigation 4dr 46100    3.5   6 225       18
#&gt; Acura MDX                     36945    3.5   6 265       17
#&gt; Acura NSX coupe 2dr manual S  89765    3.2   6 290       17
#&gt; Acura RSX Type S 2dr          23820    2.0   4 200       24
#&gt; Acura TL 4dr                  33195    3.2   6 270       20
#&gt;                               hwy_mpg weight wheel length width
#&gt; Acura 3.5 RL 4dr                   24   3880   115    197    72
#&gt; Acura 3.5 RL w/Navigation 4dr      24   3893   115    197    72
#&gt; Acura MDX                          23   4451   106    189    77
#&gt; Acura NSX coupe 2dr manual S       24   3153   100    174    71
#&gt; Acura RSX Type S 2dr               31   2778   101    172    68
#&gt; Acura TL 4dr                       28   3575   108    186    72</code></pre>
<p>The response variable is <code>price</code>, and there are nine predictors:</p>
<ul>
<li><code>engine</code></li>
<li><code>cyl</code></li>
<li><code>hp</code></li>
<li><code>city_mpg</code></li>
<li><code>hw_mpg</code></li>
<li><code>weight</code></li>
<li><code>wheel</code></li>
<li><code>length</code></li>
<li><code>width</code></li>
</ul>
<p>The regression model involves predicting <code>price</code> in terms of the nine inputs:</p>
<p><span class="math inline">\(\texttt{price} = b_0 + b_1 \texttt{cyl} + b_2 \texttt{hp} + \dots + b_9 \texttt{width} + \boldsymbol{\varepsilon}\)</span></p>
<p>Computing the OLS solution for the regression model of <code>price</code> onto the
other nine predictors we obtain the following coefficients (see first column):</p>
<pre><code>#&gt;               Estimate  Std. Error  t value   Pr(&gt;|t|)
#&gt; (Intercept)  32536.025   17777.488   1.8302  6.802e-02
#&gt; engine       -3273.053    1542.595  -2.1218  3.451e-02
#&gt; cyl           2520.927     896.202   2.8129  5.168e-03
#&gt; hp             246.595      13.201  18.6797  1.621e-55
#&gt; city_mpg      -229.987     332.824  -0.6910  4.900e-01
#&gt; hwy_mpg        979.967     345.558   2.8359  4.817e-03
#&gt; weight           9.937       2.045   4.8584  1.741e-06
#&gt; wheel         -695.392     172.896  -4.0220  6.980e-05
#&gt; length          33.690      89.660   0.3758  7.073e-01
#&gt; width         -635.382     306.344  -2.0741  3.875e-02</code></pre>
</div>
<div id="the-plsr-model" class="section level2">
<h2><span class="header-section-number">14.2</span> The PLSR Model</h2>
<p>In PLS regression, like in PCR, we seek components
<span class="math inline">\(\mathbf{z_1}, \dots, \mathbf{z_k}\)</span>, linear combinations of the inputs
(e.g. <span class="math inline">\(\mathbf{z_1} = \mathbf{Xw_1}\)</span>), such that
they are good predictors for both the response <span class="math inline">\(\mathbf{y}\)</span> as well as the
inputs <span class="math inline">\(\mathbf{x_j}\)</span> for <span class="math inline">\(j = 1,\dots, p\)</span>.</p>
<p>Moreover, there is an implicit assumption in the PLS regression model: both
<span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> are assumed to be functions of a reduced (<span class="math inline">\(k &lt; p\)</span>)
number of components <span class="math inline">\(\mathbf{Z} = [\mathbf{z_1}, \dots, \mathbf{z_k}]\)</span>
that can be used to decompose the inputs and the response:</p>
<p><span class="math display">\[
\mathbf{X} = \mathbf{Z V^\mathsf{T}} + \mathbf{E}
\]</span></p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-137"></span>
<img src="images/pls/pls-inputs-model.svg" alt="Matrix diagram for inputs" width="80%" />
<p class="caption">
Figure 14.1: Matrix diagram for inputs
</p>
</div>
<p>and</p>
<p><span class="math display">\[
\mathbf{y} = \mathbf{Z b} + \mathbf{e}
\]</span></p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-138"></span>
<img src="images/pls/pls-response-model.svg" alt="Matrix diagram for response" width="50%" />
<p class="caption">
Figure 14.2: Matrix diagram for response
</p>
</div>
<p>where:</p>
<ul>
<li><span class="math inline">\(\mathbf{Z}\)</span> is a matrix of PLS components</li>
<li><span class="math inline">\(\mathbf{V}\)</span> is a matrix of PLS loadings</li>
<li><span class="math inline">\(\mathbf{E}\)</span> is a matrix of <span class="math inline">\(X\)</span>-residuals</li>
<li><span class="math inline">\(\mathbf{b}\)</span> is a vector of PLS regression coefficients</li>
<li><span class="math inline">\(\mathbf{e}\)</span> is a vector of <span class="math inline">\(y\)</span>-residuals</li>
</ul>
<p>By taking a few number of components
<span class="math inline">\(\mathbf{Z} = [\mathbf{z_1}, \dots, \mathbf{z_k}]\)</span>, we can have approximations
for the inputs and the response. The model for the <span class="math inline">\(X\)</span>-space is:</p>
<p><span class="math display">\[
\mathbf{\hat{X}} = \mathbf{Z V^{\mathsf{T}}}
\]</span></p>
<p>In turn, the prediction model for <span class="math inline">\(\mathbf{y}\)</span> is given by:</p>
<p><span class="math display">\[
\mathbf{\hat{y}} = \mathbf{Z b}
\]</span></p>
<p>To avoid confusing the PLSR coefficients with the OLS coefficients, we
can be more proper and rewirte the previous expression as:</p>
<p><span class="math display">\[
\mathbf{\hat{y}} = \mathbf{Z} \mathbf{b}_{\text{pls}}
\]</span></p>
<p>to indicate that the obtained <span class="math inline">\(b\)</span>-coefficients are those assocaited to the
PLS components.</p>
</div>
<div id="how-does-plsr-work" class="section level2">
<h2><span class="header-section-number">14.3</span> How does PLSR work?</h2>
<p>So how do we obtain the PLS components, <em>aka</em> PLS scores? We use an iterative
algorithm, obtaining one component at a time. Here’s how to get the first PLS
component <span class="math inline">\(\mathbf{z_1}\)</span>.</p>
<p>Assume that both the inputs and the response are mean-centered (and possibly
standardized).
We compute the covariances between all the input variables and the response: <span class="math inline">\(\mathbf{\tilde{w}_1} = (cov(\mathbf{x_1}, \mathbf{y}), \dots, cov(\mathbf{x_p}, \mathbf{y}))\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-139"></span>
<img src="images/pls/pls-path-diag-z0.svg" alt="Towards the first PLS component." width="40%" />
<p class="caption">
Figure 14.3: Towards the first PLS component.
</p>
</div>
<p>In vector-matrix notation we have:</p>
<p><span class="math display">\[
\mathbf{\tilde{w}_1} = \mathbf{X^\mathsf{T}y}
\]</span></p>
<p>For convenience purposes, we normalize the vector of covariances
<span class="math inline">\(\mathbf{\tilde{w}_1}\)</span>, which gives us a unit-vector <span class="math inline">\(\mathbf{w_1}\)</span>:</p>
<p><span class="math display">\[
\mathbf{w_1} = \frac{\mathbf{\tilde{w}_1}}{\|\mathbf{\tilde{w}_1}\|}
\]</span></p>
<pre><code>#&gt; first PLS weight
#&gt;                  [,1]
#&gt; engine    0.001782118
#&gt; cyl       0.002857956
#&gt; hp        0.171985612
#&gt; city_mpg -0.007484109
#&gt; hwy_mpg  -0.007752089
#&gt; weight    0.984987298
#&gt; wheel     0.004225081
#&gt; length    0.008131684
#&gt; width     0.003089621</code></pre>
<p>We use these weights to compute the first component <span class="math inline">\(\mathbf{z_1}\)</span> as a
linear combination of the inputs:
<span class="math inline">\(\mathbf{z_1} = w_{11} \mathbf{x_1} + \dots + w_{p1} \mathbf{x_p}\)</span>,</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-141"></span>
<img src="images/pls/pls-path-diag-z1.svg" alt="First PLS component." width="40%" />
<p class="caption">
Figure 14.4: First PLS component.
</p>
</div>
<p>or in vector-matrix notation:</p>
<p><span class="math display">\[
\mathbf{z_1} = \mathbf{Xw_1}
\]</span></p>
<p>Here are the first 10 elements of <span class="math inline">\(\mathbf{z_1}\)</span></p>
<pre><code>#&gt; first PLS component (10 elements only)
#&gt;                                     [,1]
#&gt; Acura 3.5 RL 4dr               344.24572
#&gt; Acura 3.5 RL w/Navigation 4dr  357.05055
#&gt; Acura MDX                      913.48050
#&gt; Acura NSX coupe 2dr manual S  -360.90753
#&gt; Acura RSX Type S 2dr          -745.89228
#&gt; Acura TL 4dr                    51.39841
#&gt; Acura TSX 4dr                 -300.53740
#&gt; Audi A4 1.8T 4dr              -284.07748
#&gt; Audi A4 3.0 4dr                -68.58479
#&gt; Audi A4 3.0 convertible 2dr    278.15085</code></pre>
<p>We use this component to regress both the inputs and the response onto it.
The regression coefficient <span class="math inline">\(v_{1j}\)</span> of each simple linear regression between
<span class="math inline">\(\mathbf{z_1}\)</span> and the <span class="math inline">\(j\)</span>-th predictor is called a <strong>PLS loading</strong>. The entire
vector of loadings <span class="math inline">\(\mathbf{v_1}\)</span> associated to first component is:</p>
<p><span class="math display">\[
\mathbf{v_1} = \mathbf{X^\mathsf{T}z_1} / \mathbf{z_{1}^\mathsf{T}z_1}
\]</span></p>
<pre><code>#&gt; first PLS loading
#&gt;                  [,1]
#&gt; engine    0.001176718
#&gt; cyl       0.001561745
#&gt; hp        0.064016991
#&gt; city_mpg -0.005536001
#&gt; hwy_mpg  -0.006343509
#&gt; weight    1.003819205
#&gt; wheel     0.007551534
#&gt; length    0.012276141
#&gt; width     0.003862309</code></pre>
<p>In turn, the PLS regression coefficient <span class="math inline">\(b_1\)</span> is obtained by regressing the
response onto the first component:</p>
<p><span class="math display">\[
b_1 = \mathbf{y^\mathsf{T} z_1} / \mathbf{z_{1}^\mathsf{T}z_1}
\]</span></p>
<p>In our example, the regression coefficient <span class="math inline">\(b_1\)</span> of the first PLS score
<span class="math inline">\(\mathbf{z_1}\)</span> is:</p>
<pre><code>#&gt; first PLS regression coefficient
#&gt; [1] 13.61137</code></pre>
<p>We can get a first one-rank approximation
<span class="math inline">\(\mathbf{\hat{X}} = \mathbf{z_1 v^{\mathsf{T}}_1}\)</span>, and then obtain a residual
matrix <span class="math inline">\(\mathbf{X_1}\)</span> by removing the variation in the inputs captured by
<span class="math inline">\(\mathbf{z_1}\)</span>:</p>
<p><span class="math display">\[
\mathbf{X_1} = \mathbf{X} - \mathbf{\hat{X}} = \mathbf{X} - \mathbf{z_1 v^{\mathsf{T}}_1} \qquad \mathsf{(deflation)}
\]</span></p>
<p>We can also deflate the response:</p>
<p><span class="math display">\[
\mathbf{y_1} = \mathbf{y} - b_1 \mathbf{z_1}
\]</span></p>
<p>We can obtain further PLS components <span class="math inline">\(\mathbf{z_2}\)</span>, <span class="math inline">\(\mathbf{z_3}\)</span>, etc. by
repeating the process described above on the residual data matrices <span class="math inline">\(\mathbf{X_1}\)</span>,
<span class="math inline">\(\mathbf{X_2}\)</span>, etc., and the residual response vectors <span class="math inline">\(\mathbf{y_1}\)</span>,
<span class="math inline">\(\mathbf{y_2}\)</span>, etc. For example, a second PLS score <span class="math inline">\(\mathbf{z_2}\)</span> will be
formed by a linear combination of first <span class="math inline">\(X\)</span>-residuals:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-145"></span>
<img src="images/pls/pls-path-diag-z2.svg" alt="Second PLS component." width="40%" />
<p class="caption">
Figure 14.5: Second PLS component.
</p>
</div>
</div>
<div id="plsr-algorithm" class="section level2">
<h2><span class="header-section-number">14.4</span> PLSR Algorithm</h2>
<p>We will describe what we consider the “standard” or “classic” algorithm.
However, keep in mind that there are a handful of slightly different versions
that may find in other places.</p>
<p>We assume mean-centered and standardized variables <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{y}\)</span>.</p>
<ol start="0" style="list-style-type: decimal">
<li>We start by setting <span class="math inline">\(\mathbf{X_0} = \mathbf{X}\)</span>, and
<span class="math inline">\(\mathbf{y_0} = \mathbf{y}\)</span>.</li>
</ol>
<p>Repeat for <span class="math inline">\(h = 1, \dots, r = \text{rank}(\mathbf{X})\)</span>:</p>
<ol style="list-style-type: decimal">
<li><p>Start with weights <span class="math inline">\(\mathbf{\tilde{w}_h} = \mathbf{X_{h-1}^\mathsf{T}} \mathbf{y_{h-1}} / \mathbf{y_{h-1}}^\mathsf{T} \mathbf{y_{h-1}}\)</span></p></li>
<li><p>Normalize weights: <span class="math inline">\(\mathbf{w_h} = \mathbf{\tilde{w}_h} / \| \mathbf{\tilde{w}_h} \|\)</span></p></li>
<li><p>Compute PLS component: <span class="math inline">\(\mathbf{z_h} = \mathbf{X_{h-1} w_h} / \mathbf{w_{h}^\mathsf{T} w_h}\)</span></p></li>
<li><p>Regress <span class="math inline">\(\mathbf{y_h}\)</span> onto <span class="math inline">\(\mathbf{z_h}\)</span>: <span class="math inline">\(b_h = \mathbf{y_{h}^{\mathsf{T}} z_h / z_{h}^{\mathsf{T}} z_h}\)</span></p></li>
<li><p>Regress <span class="math inline">\(\mathbf{x_j}\)</span> onto <span class="math inline">\(\mathbf{z_h}\)</span>: <span class="math inline">\(\mathbf{v_h} = \mathbf{X_{h-1}}^{\mathsf{T}} \mathbf{z_h} / \mathbf{z_{h}}^{\mathsf{T}} \mathbf{z_h}\)</span></p></li>
<li><p>Deflate (residual) predictors:
<span class="math inline">\(\mathbf{X_h} = \mathbf{X_{h-1}} - \mathbf{z_h} \mathbf{v_{h}^{\mathsf{T}}}\)</span></p></li>
<li><p>Deflate (residual) response: <span class="math inline">\(\mathbf{y_h} = \mathbf{y_{h-1}} - b_h \mathbf{z_h}\)</span></p></li>
</ol>
<p>Notice that steps 1, 3, 4, and 5 involve simple OLS regressions. If you think
about it, what PLS is doing is calculating all the different ingredients
(e.g. <span class="math inline">\(\mathbf{w_h}, \mathbf{z_h}, \mathbf{v_h}, b_h\)</span>) <em>separately</em>, using
<em>least squares</em> regressions. Hence the reason for its name
<strong>partial least squares</strong>.</p>
<p>By default, we can obtain up to <span class="math inline">\(r = rank(\mathbf{X})\)</span> different PLS components.
In the cars example, we can actually obtain nine scores.
The following output shows the regression coefficients of all nine regression
equations:</p>
<pre><code>#&gt;            Z_1:1  Z_1:2   Z_1:3   Z_1:4   Z_1:5   Z_1:6   Z_1:7    Z_1:8
#&gt; engine    0.0243   1.44   -3.34  -15.09  -33.59 -113.70 -284.84 -1148.41
#&gt; cyl       0.0389   3.03    6.87   55.93  166.51  471.47 1056.23  2073.22
#&gt; hp        2.3410 250.04  248.74  262.63  254.81  251.35  243.73   238.81
#&gt; city_mpg -0.1019  -4.69   50.66  368.94  210.79  -69.52 -412.43  -171.42
#&gt; hwy_mpg  -0.1055  -3.49   48.59  464.50  528.56  811.07 1177.28   933.19
#&gt; weight   13.4070  -2.27    1.80    6.44    8.21    9.61    9.77     9.08
#&gt; wheel     0.0575  -7.32 -125.75 -387.68 -797.42 -669.88 -680.47  -676.98
#&gt; length    0.1107  -9.00 -196.71  -90.85   83.57   59.30    2.26    17.07
#&gt; width     0.0421  -1.61  -43.73 -181.27 -427.09 -940.70 -729.49  -725.37
#&gt;             Z_1:9
#&gt; engine   -3273.05
#&gt; cyl       2520.93
#&gt; hp         246.59
#&gt; city_mpg  -229.99
#&gt; hwy_mpg    979.97
#&gt; weight       9.94
#&gt; wheel     -695.39
#&gt; length      33.69
#&gt; width     -635.38</code></pre>
<p>If we keep all components, we get the OLS solution (using standardized data):</p>
<pre><code>#&gt; Regression coefficients using all 9 PLS components
#&gt;      engine         cyl          hp    city_mpg     hwy_mpg      weight 
#&gt; -3273.05304  2520.92691   246.59496  -229.98735   979.96656     9.93652 
#&gt;       wheel      length       width 
#&gt;  -695.39157    33.69009  -635.38224</code></pre>
<p>Compare with the regression coefficients of OLS:</p>
<pre><code>#&gt;    Xengine       Xcyl        Xhp  Xcity_mpg   Xhwy_mpg    Xweight     Xwheel 
#&gt; -3326.7904  3766.1495 17352.2185 -1213.0142  5535.9355  7032.5383 -4937.8106 
#&gt;    Xlength     Xwidth 
#&gt;   446.9752 -2141.8196</code></pre>
<p>Obviously, reatining all PLS scores does not provide an improvement over the
OLS regression. This is because we are not changing anything. Using our
metaphor about “shopping for coefficients”, by keeping all PLS scores we are
spending the same amount of money that we spend in ordinary least-squares
regression.</p>
<p>The dimension reduction idea involves keeping only a few components <span class="math inline">\(k \ll r\)</span>,
such that the fitted model has a better generalization ability than the full
model. The number of components <span class="math inline">\(k\)</span> is called the <strong>tuning parameter</strong>
or <strong>hyperparameter</strong>. How do we determine <span class="math inline">\(k\)</span>? The typical way is to use
cross-validation.</p>
<div id="pls-solution-with-original-variables" class="section level3">
<h3><span class="header-section-number">14.4.1</span> PLS Solution with original variables</h3>
<p>The PLS regression equation is typically expressed in terms of the original
variables <span class="math inline">\(\mathbf{X}\)</span> instead of using the deflated matrices <span class="math inline">\(\mathbf{X_{h-1}}\)</span>.
This re-arrangement is more convenient for prediction purposes.</p>
<p><span class="math display">\[
\mathbf{y = X b}_{\text{pls}} + \mathbf{e} 
\]</span></p>
<p>Note that the PLS <span class="math inline">\(\mathbf{b}_{\text{pls}}\)</span>-coefficients of the regression equation are
not parameters of the PLS regression model. Instead, these are post-hoc calculations
for making things more maneagable.</p>
<div id="transforming-regression-coefficients" class="section level4 unnumbered">
<h4>Transforming Regression Coefficients</h4>
<p>PLS components are obtained as linear combinations of residual matrices <span class="math inline">\(\mathbf{X_h}\)</span>,
but they can also be expressed in terms of the original variables:</p>
<p><span class="math display">\[\begin{align*}
\mathbf{z_1} &amp;= \mathbf{X w_1 = X \overset{*}{w}_{1}} \\
\mathbf{z_2} &amp;= \mathbf{X_1 w_2 = X (I - z_1 v_{1}^{\mathsf{T}}) w_2 = X \overset{*}{w}_{2}} \\
\mathbf{z_3} &amp;= \mathbf{X_2 w_3 = X (I - z_2 v_{2}^{\mathsf{T}}) w_3 = X \overset{*}{w}_{3}} \\
 &amp; \vdots \\
\mathbf{z_h} &amp;= \mathbf{X_{h-1} w_h = X (I - z_{h-1} v_{h-1}^{\mathsf{T}}) w_h  = X \overset{*}{w}_{h}} \\
\end{align*}\]</span></p>
<p>Consequently, <span class="math inline">\(\mathbf{Z_h} = [\mathbf{z_1}, \mathbf{z_2}, \dots, \mathbf{z_h}] = \mathbf{X \overset{*}{W}_{h}}\)</span></p>
<p>We know that <span class="math inline">\(\mathbf{\hat{y}} = b_1 \mathbf{z_1} + \dots + b_h \mathbf{z_h} = \mathbf{Z b}\)</span>.
And because the PLS components can be expressed in terms of the original variables,
we can conveniently reexpress the solution in terms of the original predictors:</p>
<p><span class="math display">\[
\mathbf{\hat{y}} = \mathbf{Z b} = \mathbf{X \overset{*}{W} b} = \mathbf{X \overset{*}{b}}_\text{pls}
\]</span></p>
<p>where <span class="math inline">\(\overset{*}{\mathbf{b}}_\text{pls}\)</span> are the derived PLS-coefficients
(not OLS). Note that these PLS star-coefficients of the regression
equation are NOT parameters of the PLS regression model. Instead, these are
post-hoc calculations for making things more maneagable.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-149"></span>
<img src="images/pls/pls-path-diag-scores.svg" alt="PLS components in terms of original inputs." width="85%" />
<p class="caption">
Figure 14.6: PLS components in terms of original inputs.
</p>
</div>
<p>Interestingly, if you retain all <span class="math inline">\(h = rank(\mathbf{X})\)</span> PLS components, the
<span class="math inline">\(\overset{*}{\mathbf{b}}_\text{pls}\)</span> coefficients will be equal to the OLS
coefficients.</p>
</div>
</div>
<div id="size-of-coefficients-1" class="section level3">
<h3><span class="header-section-number">14.4.2</span> Size of Coefficients</h3>
<p>Let’s look at the <em>evolution</em> of the PLS regression coefficients. This is a
very interesting plot that allows us to see how the size of the coefficients
grow as we add more and more PLS components into the regression equation:</p>
<p><img src="allmodelsarewrong_files/figure-html/unnamed-chunk-150-1.png" width="90%" style="display: block; margin: auto;" /></p>
</div>
<div id="some-properties" class="section level3">
<h3><span class="header-section-number">14.4.3</span> Some Properties</h3>
<p>Some interesting properties of the different elements derived in PLS Regression:</p>
<ul>
<li><span class="math inline">\(\mathbf{z_h^{\mathsf{T}} z_l} = 0, \quad l &gt; h\)</span></li>
<li><span class="math inline">\(\mathbf{w_h^{\mathsf{T}} p_h} = 1\)</span></li>
<li><span class="math inline">\(\mathbf{w_h^{\mathsf{T}} X_{l}^{\mathsf{T}}} = 0, \quad l \geq h\)</span></li>
<li><span class="math inline">\(\mathbf{w_h^{\mathsf{T}} p_l} = 0, \quad l &gt; h\)</span></li>
<li><span class="math inline">\(\mathbf{w_h^{\mathsf{T}} w_l} = 0, \quad l &gt; h\)</span></li>
<li><span class="math inline">\(\mathbf{z_h^{\mathsf{T}} X_l} = 0, \quad l \geq h\)</span></li>
<li><span class="math inline">\(\mathbf{X_h} = \mathbf{X} \prod_{j=1}^{p} (\mathbf{I - w_j v_{j}^{\mathsf{T}}}), \quad h \geq 1\)</span></li>
</ul>
</div>
</div>
<div id="selecting-number-of-pls-components" class="section level2">
<h2><span class="header-section-number">14.5</span> Selecting Number of PLS Components</h2>
<p>The number <span class="math inline">\(q\)</span> of PLS components to use in PLS Regression is a hyperparameter
or tuning parameter. This means that we cannot derive an analytical expression
that tells us what the number <span class="math inline">\(q\)</span> of PCs is the optimal to be used. So how do
we find <span class="math inline">\(q\)</span>? We find <span class="math inline">\(q\)</span> through resampling methods; the most popular resampling
technique being <span class="math inline">\(K\)</span>-fold cross-validation. Here’s a description
of the steps to be carried out:</p>
<p>Assumet that we have a training data set consisting of <span class="math inline">\(n\)</span> data points:
<span class="math inline">\(\mathcal{D}_{train} = (\mathbf{x_1}, y_1), \dots, (\mathbf{x_n}, y_n)\)</span>.
Using <span class="math inline">\(K\)</span>-fold cross-validation, we (randomly) split the data into <span class="math inline">\(K\)</span> folds:</p>
<p><span class="math display">\[
\mathcal{D}_{train} = \mathcal{D}_{fold-1} \cup \mathcal{D}_{fold-2} \dots \cup \mathcal{D}_{fold-K}
\]</span></p>
<p>Each fold set <span class="math inline">\(\mathcal{D}_{fold-k}\)</span> will play the role of an evaluation set <span class="math inline">\(\mathcal{D}_{eval-k}\)</span>.
Having defined the <span class="math inline">\(k\)</span> fold sets, we form the corresponding <span class="math inline">\(K\)</span> retraining sets:</p>
<ul>
<li><span class="math inline">\(\mathcal{D}_{train-1} = \mathcal{D}_{train} \setminus \mathcal{D}_{fold-1}\)</span></li>
<li><span class="math inline">\(\mathcal{D}_{train-2} = \mathcal{D}_{train} \setminus \mathcal{D}_{fold-2}\)</span></li>
<li><span class="math inline">\(\dots\)</span></li>
<li><span class="math inline">\(\mathcal{D}_{train-K} = \mathcal{D}_{train} \setminus \mathcal{D}_{fold-K}\)</span></li>
</ul>
<p>The cross-validation procedure then repeats the following loops:</p>
<ul>
<li>For <span class="math inline">\(q = 1, 2, \dots, r = rank(\mathbf{X})\)</span>
<ul>
<li>For <span class="math inline">\(k = 1, \dots, K\)</span>
<ul>
<li>fit PLSR model <span class="math inline">\(h_{q,k}\)</span> with <span class="math inline">\(q\)</span> PLS-scores on <span class="math inline">\(\mathcal{D}_{train-k}\)</span></li>
<li>compute and store <span class="math inline">\(E_{eval-k} (h_{q,k})\)</span> using <span class="math inline">\(\mathcal{D}_{eval-k}\)</span></li>
</ul></li>
<li>end for <span class="math inline">\(k\)</span></li>
<li>compute and store <span class="math inline">\(E_{cv_{q}} = \frac{1}{K} \sum_k E_{eval-k}(h_{q,k})\)</span></li>
</ul></li>
<li>end for <span class="math inline">\(q\)</span></li>
<li>Compare all cross-validation errors <span class="math inline">\(E_{cv_1}, E_{cv_2}, \dots, E_{cv_r}\)</span> and choose the smallest of them, say <span class="math inline">\(E_{cv_{q^*}}\)</span></li>
<li>Use <span class="math inline">\(q^*\)</span> PLS scores to fit the (finalist) PLSR model:
<span class="math inline">\(\mathbf{\hat{y}} = b_1 \mathbf{z_1} + b_2 \mathbf{z_2} + \dots + b_q^* \mathbf{z_q^*} = \mathbf{Z_{1:q^*}} \mathbf{b_{q^*}}\)</span></li>
<li>Remember that we can reexpress the PLSR model in terms of the original
predictors: <span class="math inline">\(\mathbf{\hat{y}} = (\mathbf{X} \mathbf{\overset{*}{W}_{1:q^*}}) \mathbf{b_{q^*}}\)</span></li>
</ul>
<div id="remarks-1" class="section level4 unnumbered">
<h4>Remarks</h4>
<ul>
<li>PLS regression is somewhat close to Principal Components regression (PCR).</li>
<li>Like PCR, PLSR involves projecting the response onto uncorrelated components (i.e. linear combinations of predictors).</li>
<li>Unlike PCR, the way PLS components are extracted is by taking into account the response variable.</li>
<li>We can conveniently reexpress the solution in terms of the original predictors.</li>
<li>PLSR is not based on any optimization criterion. Rather it is based on an interative algorithm (which converges)</li>
<li>Simplicity in its algorithm: no need to invert any matrix, no need to diagonalize any matrix. All you need to do is compute simple regressions. In other words, you just need inner products.</li>
<li>Missing data is allowed (but you need to modify the algorithm).</li>
<li>Easily extendable to the multivariate case of various responses.</li>
<li>Handles cases where we have more predictors than observations (<span class="math inline">\(p \gg n\)</span>).</li>
</ul>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="pcr.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ridge.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["allmodelsarewrong.pdf", "allmodelsarewrong.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
