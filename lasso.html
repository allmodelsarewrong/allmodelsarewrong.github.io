<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>18 Lasso Regression | All Models Are Wrong: Concepts of Statistical Learning</title>
  <meta name="description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  <meta name="generator" content="bookdown 0.16 and GitBook 2.6.7" />

  <meta property="og:title" content="18 Lasso Regression | All Models Are Wrong: Concepts of Statistical Learning" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://allmodelsarewrong.github.io" />
  
  <meta property="og:description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  <meta name="github-repo" content="allmodelsarewrong/allmodelsarewrong.github.io" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="18 Lasso Regression | All Models Are Wrong: Concepts of Statistical Learning" />
  
  <meta name="twitter:description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  

<meta name="author" content="Gaston Sanchez Ethan Marzban" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ridge.html"/>
<link rel="next" href="linear-extensions.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><b>All Models Are Wrong (CSL)</b><br><small>G. Sanchez & E. Marzban</small></a></li>

<li class="divider"></li>
<li class="part"><span><b>I Welcome</b></span></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="about.html"><a href="about.html"><i class="fa fa-check"></i><b>1</b> About this book</a><ul>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#prerequisites"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="part"><span><b>II Intro</b></span></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#basic-notation"><i class="fa fa-check"></i><b>2.1</b> Basic Notation</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="duality.html"><a href="duality.html"><i class="fa fa-check"></i><b>3</b> Geometric Duality</a><ul>
<li class="chapter" data-level="3.1" data-path="duality.html"><a href="duality.html#rows-space"><i class="fa fa-check"></i><b>3.1</b> Rows Space</a></li>
<li class="chapter" data-level="3.2" data-path="duality.html"><a href="duality.html#columns-space"><i class="fa fa-check"></i><b>3.2</b> Columns Space</a></li>
<li class="chapter" data-level="3.3" data-path="duality.html"><a href="duality.html#cloud-of-individuals"><i class="fa fa-check"></i><b>3.3</b> Cloud of Individuals</a><ul>
<li class="chapter" data-level="3.3.1" data-path="duality.html"><a href="duality.html#average-individual"><i class="fa fa-check"></i><b>3.3.1</b> Average Individual</a></li>
<li class="chapter" data-level="3.3.2" data-path="duality.html"><a href="duality.html#centered-data"><i class="fa fa-check"></i><b>3.3.2</b> Centered Data</a></li>
<li class="chapter" data-level="3.3.3" data-path="duality.html"><a href="duality.html#distance-between-individuals"><i class="fa fa-check"></i><b>3.3.3</b> Distance between individuals</a></li>
<li class="chapter" data-level="3.3.4" data-path="duality.html"><a href="duality.html#distance-to-the-centroid"><i class="fa fa-check"></i><b>3.3.4</b> Distance to the centroid</a></li>
<li class="chapter" data-level="3.3.5" data-path="duality.html"><a href="duality.html#measures-of-dispersion"><i class="fa fa-check"></i><b>3.3.5</b> Measures of Dispersion</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="duality.html"><a href="duality.html#cloud-of-variables"><i class="fa fa-check"></i><b>3.4</b> Cloud of Variables</a><ul>
<li class="chapter" data-level="3.4.1" data-path="duality.html"><a href="duality.html#mean-of-a-variable"><i class="fa fa-check"></i><b>3.4.1</b> Mean of a Variable</a></li>
<li class="chapter" data-level="3.4.2" data-path="duality.html"><a href="duality.html#variance-of-a-variable"><i class="fa fa-check"></i><b>3.4.2</b> Variance of a Variable</a></li>
<li class="chapter" data-level="3.4.3" data-path="duality.html"><a href="duality.html#variance-with-vector-notation"><i class="fa fa-check"></i><b>3.4.3</b> Variance with Vector Notation</a></li>
<li class="chapter" data-level="3.4.4" data-path="duality.html"><a href="duality.html#standard-deviation-as-a-norm"><i class="fa fa-check"></i><b>3.4.4</b> Standard Deviation as a Norm</a></li>
<li class="chapter" data-level="3.4.5" data-path="duality.html"><a href="duality.html#covariance"><i class="fa fa-check"></i><b>3.4.5</b> Covariance</a></li>
<li class="chapter" data-level="3.4.6" data-path="duality.html"><a href="duality.html#correlation"><i class="fa fa-check"></i><b>3.4.6</b> Correlation</a></li>
<li class="chapter" data-level="3.4.7" data-path="duality.html"><a href="duality.html#geometry-of-correlation"><i class="fa fa-check"></i><b>3.4.7</b> Geometry of Correlation</a></li>
<li class="chapter" data-level="3.4.8" data-path="duality.html"><a href="duality.html#orthogonal-projections"><i class="fa fa-check"></i><b>3.4.8</b> Orthogonal Projections</a></li>
<li class="chapter" data-level="3.4.9" data-path="duality.html"><a href="duality.html#the-mean-as-an-orthogonal-projection"><i class="fa fa-check"></i><b>3.4.9</b> The mean as an orthogonal projection</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Unsupervised I: PCA</b></span></li>
<li class="chapter" data-level="4" data-path="pca.html"><a href="pca.html"><i class="fa fa-check"></i><b>4</b> Principal Components Analysis</a><ul>
<li class="chapter" data-level="4.1" data-path="pca.html"><a href="pca.html#low-dimensional-representations"><i class="fa fa-check"></i><b>4.1</b> Low-dimensional Representations</a></li>
<li class="chapter" data-level="4.2" data-path="pca.html"><a href="pca.html#projections"><i class="fa fa-check"></i><b>4.2</b> Projections</a><ul>
<li class="chapter" data-level="4.2.1" data-path="pca.html"><a href="pca.html#vector-and-scalar-projections"><i class="fa fa-check"></i><b>4.2.1</b> Vector and Scalar Projections</a></li>
<li class="chapter" data-level="4.2.2" data-path="pca.html"><a href="pca.html#projected-inertia"><i class="fa fa-check"></i><b>4.2.2</b> Projected Inertia</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="pca.html"><a href="pca.html#maximization-problem"><i class="fa fa-check"></i><b>4.3</b> Maximization Problem</a><ul>
<li class="chapter" data-level="4.3.1" data-path="pca.html"><a href="pca.html#eigenvectors-of-mathbfs"><i class="fa fa-check"></i><b>4.3.1</b> Eigenvectors of <span class="math inline">\(\mathbf{S}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="pca.html"><a href="pca.html#another-perspective-of-pca"><i class="fa fa-check"></i><b>4.4</b> Another Perspective of PCA</a><ul>
<li class="chapter" data-level="4.4.1" data-path="pca.html"><a href="pca.html#finding-principal-components"><i class="fa fa-check"></i><b>4.4.1</b> Finding Principal Components</a></li>
<li class="chapter" data-level="4.4.2" data-path="pca.html"><a href="pca.html#finding-the-first-pc"><i class="fa fa-check"></i><b>4.4.2</b> Finding the first PC</a></li>
<li class="chapter" data-level="4.4.3" data-path="pca.html"><a href="pca.html#finding-the-second-pc"><i class="fa fa-check"></i><b>4.4.3</b> Finding the second PC</a></li>
<li class="chapter" data-level="4.4.4" data-path="pca.html"><a href="pca.html#finding-all-pcs"><i class="fa fa-check"></i><b>4.4.4</b> Finding all PCs</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="pca.html"><a href="pca.html#data-decomposition-model"><i class="fa fa-check"></i><b>4.5</b> Data Decomposition Model</a><ul>
<li class="chapter" data-level="4.5.1" data-path="pca.html"><a href="pca.html#alternative-approaches"><i class="fa fa-check"></i><b>4.5.1</b> Alternative Approaches</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Linear Regression</b></span></li>
<li class="chapter" data-level="5" data-path="ols.html"><a href="ols.html"><i class="fa fa-check"></i><b>5</b> Linear Regression</a><ul>
<li class="chapter" data-level="5.1" data-path="ols.html"><a href="ols.html#motivation"><i class="fa fa-check"></i><b>5.1</b> Motivation</a></li>
<li class="chapter" data-level="5.2" data-path="ols.html"><a href="ols.html#the-ideaintuition-of-regression"><i class="fa fa-check"></i><b>5.2</b> The Idea/Intuition of Regression</a></li>
<li class="chapter" data-level="5.3" data-path="ols.html"><a href="ols.html#the-linear-regression-model"><i class="fa fa-check"></i><b>5.3</b> The Linear Regression Model</a></li>
<li class="chapter" data-level="5.4" data-path="ols.html"><a href="ols.html#the-error-measure"><i class="fa fa-check"></i><b>5.4</b> The Error Measure</a></li>
<li class="chapter" data-level="5.5" data-path="ols.html"><a href="ols.html#the-least-squares-algorithm"><i class="fa fa-check"></i><b>5.5</b> The Least Squares Algorithm</a></li>
<li class="chapter" data-level="5.6" data-path="ols.html"><a href="ols.html#geometries-of-ols"><i class="fa fa-check"></i><b>5.6</b> Geometries of OLS</a><ul>
<li class="chapter" data-level="5.6.1" data-path="ols.html"><a href="ols.html#rows-perspective"><i class="fa fa-check"></i><b>5.6.1</b> Rows Perspective</a></li>
<li class="chapter" data-level="5.6.2" data-path="ols.html"><a href="ols.html#columns-perspective"><i class="fa fa-check"></i><b>5.6.2</b> Columns Perspective</a></li>
<li class="chapter" data-level="5.6.3" data-path="ols.html"><a href="ols.html#parameters-perspective"><i class="fa fa-check"></i><b>5.6.3</b> Parameters Perspective</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="gradient.html"><a href="gradient.html"><i class="fa fa-check"></i><b>6</b> Gradient Descent</a><ul>
<li class="chapter" data-level="6.1" data-path="gradient.html"><a href="gradient.html#error-surface"><i class="fa fa-check"></i><b>6.1</b> Error Surface</a></li>
<li class="chapter" data-level="6.2" data-path="gradient.html"><a href="gradient.html#idea-of-gradient-descent"><i class="fa fa-check"></i><b>6.2</b> Idea of Gradient Descent</a></li>
<li class="chapter" data-level="6.3" data-path="gradient.html"><a href="gradient.html#moving-down-an-error-surface"><i class="fa fa-check"></i><b>6.3</b> Moving Down an Error Surface</a><ul>
<li class="chapter" data-level="6.3.1" data-path="gradient.html"><a href="gradient.html#the-direction-of-mathbfv"><i class="fa fa-check"></i><b>6.3.1</b> The direction of <span class="math inline">\(\mathbf{v}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="gradient.html"><a href="gradient.html#gd-algorithm-for-linear-regression"><i class="fa fa-check"></i><b>6.4</b> GD Algorithm for Linear Regression</a><ul>
<li class="chapter" data-level="6.4.1" data-path="gradient.html"><a href="gradient.html#gd-algorithm-in-vector-matrix-notation"><i class="fa fa-check"></i><b>6.4.1</b> GD Algorithm in vector-matrix notation</a></li>
<li class="chapter" data-level="6.4.2" data-path="gradient.html"><a href="gradient.html#gd-algorithm-in-pointwise-notation"><i class="fa fa-check"></i><b>6.4.2</b> GD algorithm in pointwise notation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="olsml.html"><a href="olsml.html"><i class="fa fa-check"></i><b>7</b> Regression via Maximum Likelihood</a><ul>
<li class="chapter" data-level="7.1" data-path="olsml.html"><a href="olsml.html#linear-regression-reminder"><i class="fa fa-check"></i><b>7.1</b> Linear Regression Reminder</a><ul>
<li class="chapter" data-level="7.1.1" data-path="olsml.html"><a href="olsml.html#maximum-likelihood"><i class="fa fa-check"></i><b>7.1.1</b> Maximum Likelihood</a></li>
<li class="chapter" data-level="7.1.2" data-path="olsml.html"><a href="olsml.html#ml-estimator-of-sigma2"><i class="fa fa-check"></i><b>7.1.2</b> ML Estimator of <span class="math inline">\(\sigma^2\)</span></a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>V Learning Concepts</b></span></li>
<li class="chapter" data-level="8" data-path="learning.html"><a href="learning.html"><i class="fa fa-check"></i><b>8</b> Theoretical Framework</a><ul>
<li class="chapter" data-level="8.1" data-path="learning.html"><a href="learning.html#mental-map"><i class="fa fa-check"></i><b>8.1</b> Mental Map</a></li>
<li class="chapter" data-level="8.2" data-path="learning.html"><a href="learning.html#kinds-of-predictions"><i class="fa fa-check"></i><b>8.2</b> Kinds of Predictions</a><ul>
<li class="chapter" data-level="8.2.1" data-path="learning.html"><a href="learning.html#two-types-of-data"><i class="fa fa-check"></i><b>8.2.1</b> Two Types of Data</a></li>
<li class="chapter" data-level="8.2.2" data-path="learning.html"><a href="learning.html#two-types-of-predictions"><i class="fa fa-check"></i><b>8.2.2</b> Two Types of Predictions</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="learning.html"><a href="learning.html#two-types-of-errors"><i class="fa fa-check"></i><b>8.3</b> Two Types of Errors</a><ul>
<li class="chapter" data-level="8.3.1" data-path="learning.html"><a href="learning.html#individual-errors"><i class="fa fa-check"></i><b>8.3.1</b> Individual Errors</a></li>
<li class="chapter" data-level="8.3.2" data-path="learning.html"><a href="learning.html#overall-errors"><i class="fa fa-check"></i><b>8.3.2</b> Overall Errors</a></li>
<li class="chapter" data-level="8.3.3" data-path="learning.html"><a href="learning.html#probability-as-an-auxiliary-technicality"><i class="fa fa-check"></i><b>8.3.3</b> Probability as an Auxiliary Technicality</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="learning.html"><a href="learning.html#noisy-targets"><i class="fa fa-check"></i><b>8.4</b> Noisy Targets</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="mse.html"><a href="mse.html"><i class="fa fa-check"></i><b>9</b> MSE of Estimator</a><ul>
<li class="chapter" data-level="9.1" data-path="mse.html"><a href="mse.html#mse-of-an-estimator"><i class="fa fa-check"></i><b>9.1</b> MSE of an Estimator</a><ul>
<li class="chapter" data-level="9.1.1" data-path="mse.html"><a href="mse.html#prototypical-cases-of-bias-and-variance"><i class="fa fa-check"></i><b>9.1.1</b> Prototypical Cases of Bias and Variance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="biasvar.html"><a href="biasvar.html"><i class="fa fa-check"></i><b>10</b> Bias-Variance Tradeoff</a><ul>
<li class="chapter" data-level="10.1" data-path="biasvar.html"><a href="biasvar.html#introduction"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="biasvar.html"><a href="biasvar.html#motivation-example"><i class="fa fa-check"></i><b>10.2</b> Motivation Example</a><ul>
<li class="chapter" data-level="10.2.1" data-path="biasvar.html"><a href="biasvar.html#two-hypotheses"><i class="fa fa-check"></i><b>10.2.1</b> Two Hypotheses</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="biasvar.html"><a href="biasvar.html#learning-from-two-points"><i class="fa fa-check"></i><b>10.3</b> Learning from two points</a></li>
<li class="chapter" data-level="10.4" data-path="biasvar.html"><a href="biasvar.html#bias-variance-derivation"><i class="fa fa-check"></i><b>10.4</b> Bias-Variance Derivation</a><ul>
<li class="chapter" data-level="10.4.1" data-path="biasvar.html"><a href="biasvar.html#out-of-sample-predictions"><i class="fa fa-check"></i><b>10.4.1</b> Out-of-Sample Predictions</a></li>
<li class="chapter" data-level="10.4.2" data-path="biasvar.html"><a href="biasvar.html#noisy-target"><i class="fa fa-check"></i><b>10.4.2</b> Noisy Target</a></li>
<li class="chapter" data-level="10.4.3" data-path="biasvar.html"><a href="biasvar.html#types-of-theoretical-mses"><i class="fa fa-check"></i><b>10.4.3</b> Types of Theoretical MSEs</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="biasvar.html"><a href="biasvar.html#the-tradeoff"><i class="fa fa-check"></i><b>10.5</b> The Tradeoff</a><ul>
<li class="chapter" data-level="10.5.1" data-path="biasvar.html"><a href="biasvar.html#bias-variance-tradeoff-picture"><i class="fa fa-check"></i><b>10.5.1</b> Bias-Variance Tradeoff Picture</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="overfit.html"><a href="overfit.html"><i class="fa fa-check"></i><b>11</b> Overfitting</a><ul>
<li class="chapter" data-level="11.1" data-path="overfit.html"><a href="overfit.html#introduction-1"><i class="fa fa-check"></i><b>11.1</b> Introduction</a><ul>
<li class="chapter" data-level="11.1.1" data-path="overfit.html"><a href="overfit.html#bias-variance-reminder-and-pitfalls"><i class="fa fa-check"></i><b>11.1.1</b> Bias-Variance Reminder and Pitfalls</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="overfit.html"><a href="overfit.html#simulation"><i class="fa fa-check"></i><b>11.2</b> Simulation</a><ul>
<li class="chapter" data-level="11.2.1" data-path="overfit.html"><a href="overfit.html#underfitting-overfitting-and-okayfitting"><i class="fa fa-check"></i><b>11.2.1</b> Underfitting, Overfitting and Okayfitting</a></li>
<li class="chapter" data-level="11.2.2" data-path="overfit.html"><a href="overfit.html#more-learning-sets"><i class="fa fa-check"></i><b>11.2.2</b> More learning sets</a></li>
<li class="chapter" data-level="11.2.3" data-path="overfit.html"><a href="overfit.html#when-does-overfitting-occur"><i class="fa fa-check"></i><b>11.2.3</b> When does overfitting occur?</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="overfit.html"><a href="overfit.html#in-summary"><i class="fa fa-check"></i><b>11.3</b> In Summary</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="phases.html"><a href="phases.html"><i class="fa fa-check"></i><b>12</b> Learning Phases</a><ul>
<li class="chapter" data-level="12.1" data-path="phases.html"><a href="phases.html#introduction-2"><i class="fa fa-check"></i><b>12.1</b> Introduction</a></li>
<li class="chapter" data-level="12.2" data-path="phases.html"><a href="phases.html#model-assessment"><i class="fa fa-check"></i><b>12.2</b> Model Assessment</a><ul>
<li class="chapter" data-level="12.2.1" data-path="phases.html"><a href="phases.html#holdout-test-set"><i class="fa fa-check"></i><b>12.2.1</b> Holdout Test Set</a></li>
<li class="chapter" data-level="12.2.2" data-path="phases.html"><a href="phases.html#why-does-a-test-set-work"><i class="fa fa-check"></i><b>12.2.2</b> Why does a test set work?</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="phases.html"><a href="phases.html#model-selection"><i class="fa fa-check"></i><b>12.3</b> Model Selection</a><ul>
<li class="chapter" data-level="12.3.1" data-path="phases.html"><a href="phases.html#three-way-holdout-method"><i class="fa fa-check"></i><b>12.3.1</b> Three-way Holdout Method</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="phases.html"><a href="phases.html#model-training"><i class="fa fa-check"></i><b>12.4</b> Model Training</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="resampling.html"><a href="resampling.html"><i class="fa fa-check"></i><b>13</b> Resampling Approaches</a><ul>
<li class="chapter" data-level="13.1" data-path="resampling.html"><a href="resampling.html#general-sampling-blueprint"><i class="fa fa-check"></i><b>13.1</b> General Sampling Blueprint</a></li>
<li class="chapter" data-level="13.2" data-path="resampling.html"><a href="resampling.html#monte-carlo-cross-validation"><i class="fa fa-check"></i><b>13.2</b> Monte Carlo Cross Validation</a></li>
<li class="chapter" data-level="13.3" data-path="resampling.html"><a href="resampling.html#bootstrap-method"><i class="fa fa-check"></i><b>13.3</b> Bootstrap Method</a></li>
<li class="chapter" data-level="13.4" data-path="resampling.html"><a href="resampling.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>13.4</b> <span class="math inline">\(K\)</span>-Fold Cross-Validation</a><ul>
<li class="chapter" data-level="13.4.1" data-path="resampling.html"><a href="resampling.html#leave-one-out-cross-validation-loocv"><i class="fa fa-check"></i><b>13.4.1</b> Leave-One-Out Cross Validation (LOOCV)</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>VI Regularization</b></span></li>
<li class="chapter" data-level="14" data-path="regular.html"><a href="regular.html"><i class="fa fa-check"></i><b>14</b> Regularization Techniques</a><ul>
<li class="chapter" data-level="14.1" data-path="regular.html"><a href="regular.html#example-regularity-in-models"><i class="fa fa-check"></i><b>14.1</b> Example: Regularity in Models</a></li>
<li class="chapter" data-level="14.2" data-path="regular.html"><a href="regular.html#multicollinearity-issues"><i class="fa fa-check"></i><b>14.2</b> Multicollinearity Issues</a><ul>
<li class="chapter" data-level="14.2.1" data-path="regular.html"><a href="regular.html#toy-example"><i class="fa fa-check"></i><b>14.2.1</b> Toy Example</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="regular.html"><a href="regular.html#irregular-coefficients"><i class="fa fa-check"></i><b>14.3</b> Irregular Coefficients</a></li>
<li class="chapter" data-level="14.4" data-path="regular.html"><a href="regular.html#connection-to-regularization"><i class="fa fa-check"></i><b>14.4</b> Connection to Regularization</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="pcr.html"><a href="pcr.html"><i class="fa fa-check"></i><b>15</b> Principal Components Regression</a><ul>
<li class="chapter" data-level="15.1" data-path="pcr.html"><a href="pcr.html#motivation-example-1"><i class="fa fa-check"></i><b>15.1</b> Motivation Example</a></li>
<li class="chapter" data-level="15.2" data-path="pcr.html"><a href="pcr.html#the-pcr-model"><i class="fa fa-check"></i><b>15.2</b> The PCR Model</a></li>
<li class="chapter" data-level="15.3" data-path="pcr.html"><a href="pcr.html#how-does-pcr-work"><i class="fa fa-check"></i><b>15.3</b> How does PCR work?</a><ul>
<li class="chapter" data-level="15.3.1" data-path="pcr.html"><a href="pcr.html#transition-formula"><i class="fa fa-check"></i><b>15.3.1</b> Transition Formula</a></li>
<li class="chapter" data-level="15.3.2" data-path="pcr.html"><a href="pcr.html#size-of-coefficients"><i class="fa fa-check"></i><b>15.3.2</b> Size of Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="pcr.html"><a href="pcr.html#selecting-number-of-pcs"><i class="fa fa-check"></i><b>15.4</b> Selecting Number of PCs</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="pls.html"><a href="pls.html"><i class="fa fa-check"></i><b>16</b> Partial Least Squares Regression</a><ul>
<li class="chapter" data-level="16.1" data-path="pls.html"><a href="pls.html#motivation-example-2"><i class="fa fa-check"></i><b>16.1</b> Motivation Example</a></li>
<li class="chapter" data-level="16.2" data-path="pls.html"><a href="pls.html#the-plsr-model"><i class="fa fa-check"></i><b>16.2</b> The PLSR Model</a></li>
<li class="chapter" data-level="16.3" data-path="pls.html"><a href="pls.html#how-does-plsr-work"><i class="fa fa-check"></i><b>16.3</b> How does PLSR work?</a></li>
<li class="chapter" data-level="16.4" data-path="pls.html"><a href="pls.html#plsr-algorithm"><i class="fa fa-check"></i><b>16.4</b> PLSR Algorithm</a><ul>
<li class="chapter" data-level="16.4.1" data-path="pls.html"><a href="pls.html#pls-solution-with-original-variables"><i class="fa fa-check"></i><b>16.4.1</b> PLS Solution with original variables</a></li>
<li class="chapter" data-level="16.4.2" data-path="pls.html"><a href="pls.html#some-properties"><i class="fa fa-check"></i><b>16.4.2</b> Some Properties</a></li>
<li class="chapter" data-level="16.4.3" data-path="pls.html"><a href="pls.html#pls-regression-for-price-of-cars"><i class="fa fa-check"></i><b>16.4.3</b> PLS Regression for Price of cars</a></li>
<li class="chapter" data-level="16.4.4" data-path="pls.html"><a href="pls.html#size-of-coefficients-1"><i class="fa fa-check"></i><b>16.4.4</b> Size of Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="16.5" data-path="pls.html"><a href="pls.html#selecting-number-of-pls-components"><i class="fa fa-check"></i><b>16.5</b> Selecting Number of PLS Components</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="ridge.html"><a href="ridge.html"><i class="fa fa-check"></i><b>17</b> Ridge Regression</a><ul>
<li class="chapter" data-level="17.1" data-path="ridge.html"><a href="ridge.html#a-new-minimization-problem"><i class="fa fa-check"></i><b>17.1</b> A New Minimization Problem</a><ul>
<li class="chapter" data-level="17.1.1" data-path="ridge.html"><a href="ridge.html#constraining-regression-coefficients"><i class="fa fa-check"></i><b>17.1.1</b> Constraining Regression Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="ridge.html"><a href="ridge.html#a-new-minimization-solution"><i class="fa fa-check"></i><b>17.2</b> A New Minimization Solution</a></li>
<li class="chapter" data-level="17.3" data-path="ridge.html"><a href="ridge.html#what-does-rr-accomplish"><i class="fa fa-check"></i><b>17.3</b> What does RR accomplish?</a><ul>
<li class="chapter" data-level="17.3.1" data-path="ridge.html"><a href="ridge.html#how-to-find-lambda"><i class="fa fa-check"></i><b>17.3.1</b> How to find <span class="math inline">\(\lambda\)</span>?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="lasso.html"><a href="lasso.html"><i class="fa fa-check"></i><b>18</b> Lasso Regression</a><ul>
<li class="chapter" data-level="18.1" data-path="lasso.html"><a href="lasso.html#mathematical-setup"><i class="fa fa-check"></i><b>18.1</b> Mathematical Setup</a><ul>
<li class="chapter" data-level="18.1.1" data-path="lasso.html"><a href="lasso.html#closed-form"><i class="fa fa-check"></i><b>18.1.1</b> Closed Form?</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="lasso.html"><a href="lasso.html#geometric-visualization"><i class="fa fa-check"></i><b>18.2</b> Geometric Visualization</a><ul>
<li class="chapter" data-level="18.2.1" data-path="lasso.html"><a href="lasso.html#some-more-math-variable-selection-in-action"><i class="fa fa-check"></i><b>18.2.1</b> Some More Math: Variable Selection in Action</a></li>
<li class="chapter" data-level="18.2.2" data-path="lasso.html"><a href="lasso.html#example-with-mtcars"><i class="fa fa-check"></i><b>18.2.2</b> Example with <code>mtcars</code></a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="lasso.html"><a href="lasso.html#going-beyond"><i class="fa fa-check"></i><b>18.3</b> Going Beyond</a></li>
</ul></li>
<li class="part"><span><b>VII Extending Linear Regression</b></span></li>
<li class="chapter" data-level="19" data-path="linear-extensions.html"><a href="linear-extensions.html"><i class="fa fa-check"></i><b>19</b> Beyond Linear Regression</a><ul>
<li class="chapter" data-level="19.1" data-path="linear-extensions.html"><a href="linear-extensions.html#introduction-3"><i class="fa fa-check"></i><b>19.1</b> Introduction</a></li>
<li class="chapter" data-level="19.2" data-path="linear-extensions.html"><a href="linear-extensions.html#expanding-the-regression-horizon"><i class="fa fa-check"></i><b>19.2</b> Expanding the Regression Horizon</a><ul>
<li class="chapter" data-level="19.2.1" data-path="linear-extensions.html"><a href="linear-extensions.html#linearity"><i class="fa fa-check"></i><b>19.2.1</b> Linearity</a></li>
<li class="chapter" data-level="19.2.2" data-path="linear-extensions.html"><a href="linear-extensions.html#parametric-and-nonparametric"><i class="fa fa-check"></i><b>19.2.2</b> Parametric and Nonparametric</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="linear-extensions.html"><a href="linear-extensions.html#transforming-features"><i class="fa fa-check"></i><b>19.3</b> Transforming Features</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="basis.html"><a href="basis.html"><i class="fa fa-check"></i><b>20</b> Basis Expansion</a><ul>
<li class="chapter" data-level="20.1" data-path="basis.html"><a href="basis.html#basis-functions"><i class="fa fa-check"></i><b>20.1</b> Basis Functions</a></li>
<li class="chapter" data-level="20.2" data-path="basis.html"><a href="basis.html#linear-regression"><i class="fa fa-check"></i><b>20.2</b> Linear Regression</a></li>
<li class="chapter" data-level="20.3" data-path="basis.html"><a href="basis.html#polynomial-regression"><i class="fa fa-check"></i><b>20.3</b> Polynomial Regression</a></li>
<li class="chapter" data-level="20.4" data-path="basis.html"><a href="basis.html#gaussian-rbfs"><i class="fa fa-check"></i><b>20.4</b> Gaussian RBF’s</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="nonparametric.html"><a href="nonparametric.html"><i class="fa fa-check"></i><b>21</b> Nonparametric Regression</a><ul>
<li class="chapter" data-level="21.1" data-path="nonparametric.html"><a href="nonparametric.html#conditional-averages"><i class="fa fa-check"></i><b>21.1</b> Conditional Averages</a></li>
<li class="chapter" data-level="21.2" data-path="nonparametric.html"><a href="nonparametric.html#looking-at-the-neighbors"><i class="fa fa-check"></i><b>21.2</b> Looking at the Neighbors</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="knn.html"><a href="knn.html"><i class="fa fa-check"></i><b>22</b> Nearest Neighbor Estimates</a><ul>
<li class="chapter" data-level="22.1" data-path="knn.html"><a href="knn.html#introduction-4"><i class="fa fa-check"></i><b>22.1</b> Introduction</a></li>
<li class="chapter" data-level="22.2" data-path="knn.html"><a href="knn.html#k-nearest-neighbors-knn"><i class="fa fa-check"></i><b>22.2</b> <span class="math inline">\(k\)</span> Nearest Neighbors (KNN)</a><ul>
<li class="chapter" data-level="22.2.1" data-path="knn.html"><a href="knn.html#distance-measures"><i class="fa fa-check"></i><b>22.2.1</b> Distance Measures</a></li>
<li class="chapter" data-level="22.2.2" data-path="knn.html"><a href="knn.html#knn-estimator"><i class="fa fa-check"></i><b>22.2.2</b> KNN Estimator</a></li>
<li class="chapter" data-level="22.2.3" data-path="knn.html"><a href="knn.html#how-to-find-k"><i class="fa fa-check"></i><b>22.2.3</b> How to find <span class="math inline">\(k\)</span>?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="23" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html"><i class="fa fa-check"></i><b>23</b> Kernel Smoothers</a><ul>
<li class="chapter" data-level="23.1" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#introduction-5"><i class="fa fa-check"></i><b>23.1</b> Introduction</a></li>
<li class="chapter" data-level="23.2" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#kernel-smoothers-1"><i class="fa fa-check"></i><b>23.2</b> Kernel Smoothers</a><ul>
<li class="chapter" data-level="23.2.1" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#kernel-functions"><i class="fa fa-check"></i><b>23.2.1</b> Kernel Functions</a></li>
<li class="chapter" data-level="23.2.2" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#weights-from-kernels"><i class="fa fa-check"></i><b>23.2.2</b> Weights from Kernels</a></li>
<li class="chapter" data-level="23.2.3" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#kernel-estimator"><i class="fa fa-check"></i><b>23.2.3</b> Kernel Estimator</a></li>
</ul></li>
<li class="chapter" data-level="23.3" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#local-polynomial-estimators"><i class="fa fa-check"></i><b>23.3</b> Local Polynomial Estimators</a></li>
</ul></li>
<li class="part"><span><b>VIII Classification</b></span></li>
<li class="chapter" data-level="24" data-path="classif.html"><a href="classif.html"><i class="fa fa-check"></i><b>24</b> Classification</a><ul>
<li class="chapter" data-level="24.1" data-path="classif.html"><a href="classif.html#introduction-6"><i class="fa fa-check"></i><b>24.1</b> Introduction</a><ul>
<li class="chapter" data-level="24.1.1" data-path="classif.html"><a href="classif.html#credit-score-example"><i class="fa fa-check"></i><b>24.1.1</b> Credit Score Example</a></li>
<li class="chapter" data-level="24.1.2" data-path="classif.html"><a href="classif.html#toy-example-1"><i class="fa fa-check"></i><b>24.1.2</b> Toy Example</a></li>
<li class="chapter" data-level="24.1.3" data-path="classif.html"><a href="classif.html#two-class-problem"><i class="fa fa-check"></i><b>24.1.3</b> Two-class Problem</a></li>
<li class="chapter" data-level="24.1.4" data-path="classif.html"><a href="classif.html#bayes-rule-reminder"><i class="fa fa-check"></i><b>24.1.4</b> Bayes’ Rule Reminder</a></li>
</ul></li>
<li class="chapter" data-level="24.2" data-path="classif.html"><a href="classif.html#bayes-classifier"><i class="fa fa-check"></i><b>24.2</b> Bayes Classifier</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="logistic.html"><a href="logistic.html"><i class="fa fa-check"></i><b>25</b> Logistic Regression</a><ul>
<li class="chapter" data-level="25.1" data-path="logistic.html"><a href="logistic.html#motivation-1"><i class="fa fa-check"></i><b>25.1</b> Motivation</a><ul>
<li class="chapter" data-level="25.1.1" data-path="logistic.html"><a href="logistic.html#first-approach-fitting-a-line"><i class="fa fa-check"></i><b>25.1.1</b> First Approach: Fitting a Line</a></li>
<li class="chapter" data-level="25.1.2" data-path="logistic.html"><a href="logistic.html#second-approach-harsh-thresholding"><i class="fa fa-check"></i><b>25.1.2</b> Second Approach: Harsh Thresholding</a></li>
<li class="chapter" data-level="25.1.3" data-path="logistic.html"><a href="logistic.html#third-approach-conditional-means"><i class="fa fa-check"></i><b>25.1.3</b> Third Approach: Conditional Means</a></li>
</ul></li>
<li class="chapter" data-level="25.2" data-path="logistic.html"><a href="logistic.html#logistic-regression-model"><i class="fa fa-check"></i><b>25.2</b> Logistic Regression Model</a><ul>
<li class="chapter" data-level="25.2.1" data-path="logistic.html"><a href="logistic.html#the-criterion-being-optimized"><i class="fa fa-check"></i><b>25.2.1</b> The Criterion Being Optimized</a></li>
<li class="chapter" data-level="25.2.2" data-path="logistic.html"><a href="logistic.html#another-way-to-solve-logistic-regression"><i class="fa fa-check"></i><b>25.2.2</b> Another Way to Solve Logistic Regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="26" data-path="discrim.html"><a href="discrim.html"><i class="fa fa-check"></i><b>26</b> Preamble for Discriminant Analysis</a><ul>
<li class="chapter" data-level="26.1" data-path="discrim.html"><a href="discrim.html#motivation-2"><i class="fa fa-check"></i><b>26.1</b> Motivation</a><ul>
<li class="chapter" data-level="26.1.1" data-path="discrim.html"><a href="discrim.html#distinguishing-species"><i class="fa fa-check"></i><b>26.1.1</b> Distinguishing Species</a></li>
<li class="chapter" data-level="26.1.2" data-path="discrim.html"><a href="discrim.html#sum-of-squares-decomposition"><i class="fa fa-check"></i><b>26.1.2</b> Sum of Squares Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="26.2" data-path="discrim.html"><a href="discrim.html#derived-ratios-from-sum-of-squares"><i class="fa fa-check"></i><b>26.2</b> Derived Ratios from Sum-of-Squares</a><ul>
<li class="chapter" data-level="26.2.1" data-path="discrim.html"><a href="discrim.html#correlation-ratio"><i class="fa fa-check"></i><b>26.2.1</b> Correlation Ratio</a></li>
<li class="chapter" data-level="26.2.2" data-path="discrim.html"><a href="discrim.html#f-ratio"><i class="fa fa-check"></i><b>26.2.2</b> F-Ratio</a></li>
</ul></li>
<li class="chapter" data-level="26.3" data-path="discrim.html"><a href="discrim.html#geometric-perspective"><i class="fa fa-check"></i><b>26.3</b> Geometric Perspective</a><ul>
<li class="chapter" data-level="26.3.1" data-path="discrim.html"><a href="discrim.html#clouds-from-class-structure"><i class="fa fa-check"></i><b>26.3.1</b> Clouds from Class Structure</a></li>
<li class="chapter" data-level="26.3.2" data-path="discrim.html"><a href="discrim.html#dispersion-decomposition"><i class="fa fa-check"></i><b>26.3.2</b> Dispersion Decomposition</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="27" data-path="cda.html"><a href="cda.html"><i class="fa fa-check"></i><b>27</b> Canonical Discriminant Analysis</a><ul>
<li class="chapter" data-level="27.1" data-path="cda.html"><a href="cda.html#cda-semi-supervised-aspect"><i class="fa fa-check"></i><b>27.1</b> CDA: Semi-Supervised Aspect</a></li>
<li class="chapter" data-level="27.2" data-path="cda.html"><a href="cda.html#looking-for-a-discriminant-axis"><i class="fa fa-check"></i><b>27.2</b> Looking for a discriminant axis</a></li>
<li class="chapter" data-level="27.3" data-path="cda.html"><a href="cda.html#looking-for-a-compromise-criterion"><i class="fa fa-check"></i><b>27.3</b> Looking for a Compromise Criterion</a><ul>
<li class="chapter" data-level="27.3.1" data-path="cda.html"><a href="cda.html#correlation-ratio-criterion"><i class="fa fa-check"></i><b>27.3.1</b> Correlation Ratio Criterion</a></li>
<li class="chapter" data-level="27.3.2" data-path="cda.html"><a href="cda.html#f-ratio-criterion"><i class="fa fa-check"></i><b>27.3.2</b> F-ratio Criterion</a></li>
<li class="chapter" data-level="27.3.3" data-path="cda.html"><a href="cda.html#a-special-pca"><i class="fa fa-check"></i><b>27.3.3</b> A Special PCA</a></li>
</ul></li>
<li class="chapter" data-level="27.4" data-path="cda.html"><a href="cda.html#cda-supervised-aspect"><i class="fa fa-check"></i><b>27.4</b> CDA: Supervised Aspect</a><ul>
<li class="chapter" data-level="27.4.1" data-path="cda.html"><a href="cda.html#distance-behind-cda"><i class="fa fa-check"></i><b>27.4.1</b> Distance behind CDA</a></li>
<li class="chapter" data-level="27.4.2" data-path="cda.html"><a href="cda.html#predictive-idea"><i class="fa fa-check"></i><b>27.4.2</b> Predictive Idea</a></li>
<li class="chapter" data-level="27.4.3" data-path="cda.html"><a href="cda.html#cda-classifier"><i class="fa fa-check"></i><b>27.4.3</b> CDA Classifier</a></li>
<li class="chapter" data-level="27.4.4" data-path="cda.html"><a href="cda.html#limitations-of-cda-classifier"><i class="fa fa-check"></i><b>27.4.4</b> Limitations of CDA classifier</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="28" data-path="discanalysis.html"><a href="discanalysis.html"><i class="fa fa-check"></i><b>28</b> Discriminant Analysis</a><ul>
<li class="chapter" data-level="28.1" data-path="discanalysis.html"><a href="discanalysis.html#probabilistic-da"><i class="fa fa-check"></i><b>28.1</b> Probabilistic DA</a><ul>
<li class="chapter" data-level="28.1.1" data-path="discanalysis.html"><a href="discanalysis.html#normal-distributions"><i class="fa fa-check"></i><b>28.1.1</b> Normal Distributions</a></li>
<li class="chapter" data-level="28.1.2" data-path="discanalysis.html"><a href="discanalysis.html#estimating-parameters-of-normal-distributions"><i class="fa fa-check"></i><b>28.1.2</b> Estimating Parameters of Normal Distributions</a></li>
</ul></li>
<li class="chapter" data-level="28.2" data-path="discanalysis.html"><a href="discanalysis.html#discriminant-functions"><i class="fa fa-check"></i><b>28.2</b> Discriminant Functions</a></li>
<li class="chapter" data-level="28.3" data-path="discanalysis.html"><a href="discanalysis.html#quadratic-discriminant-analysis-qda"><i class="fa fa-check"></i><b>28.3</b> Quadratic Discriminant Analysis (QDA)</a></li>
<li class="chapter" data-level="28.4" data-path="discanalysis.html"><a href="discanalysis.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>28.4</b> Linear Discriminant Analysis</a><ul>
<li class="chapter" data-level="28.4.1" data-path="discanalysis.html"><a href="discanalysis.html#canonical-discriminant-analysis"><i class="fa fa-check"></i><b>28.4.1</b> Canonical Discriminant Analysis</a></li>
<li class="chapter" data-level="28.4.2" data-path="discanalysis.html"><a href="discanalysis.html#naive-bayes"><i class="fa fa-check"></i><b>28.4.2</b> Naive Bayes</a></li>
<li class="chapter" data-level="28.4.3" data-path="discanalysis.html"><a href="discanalysis.html#fifth-case"><i class="fa fa-check"></i><b>28.4.3</b> Fifth Case</a></li>
</ul></li>
<li class="chapter" data-level="28.5" data-path="discanalysis.html"><a href="discanalysis.html#comparing-the-cases"><i class="fa fa-check"></i><b>28.5</b> Comparing the Cases</a></li>
</ul></li>
<li class="chapter" data-level="29" data-path="classperformance.html"><a href="classperformance.html"><i class="fa fa-check"></i><b>29</b> Performance of Classifiers</a><ul>
<li class="chapter" data-level="29.1" data-path="classperformance.html"><a href="classperformance.html#classification-error-measures"><i class="fa fa-check"></i><b>29.1</b> Classification Error Measures</a><ul>
<li class="chapter" data-level="29.1.1" data-path="classperformance.html"><a href="classperformance.html#errors-for-binary-response"><i class="fa fa-check"></i><b>29.1.1</b> Errors for Binary Response</a></li>
<li class="chapter" data-level="29.1.2" data-path="classperformance.html"><a href="classperformance.html#error-for-categorical-response"><i class="fa fa-check"></i><b>29.1.2</b> Error for Categorical Response</a></li>
</ul></li>
<li class="chapter" data-level="29.2" data-path="classperformance.html"><a href="classperformance.html#confusion-matrices"><i class="fa fa-check"></i><b>29.2</b> Confusion Matrices</a></li>
<li class="chapter" data-level="29.3" data-path="classperformance.html"><a href="classperformance.html#binary-response-example"><i class="fa fa-check"></i><b>29.3</b> Binary Response Example</a><ul>
<li class="chapter" data-level="29.3.1" data-path="classperformance.html"><a href="classperformance.html#application-for-checking-account"><i class="fa fa-check"></i><b>29.3.1</b> Application for Checking Account</a></li>
<li class="chapter" data-level="29.3.2" data-path="classperformance.html"><a href="classperformance.html#application-for-loan"><i class="fa fa-check"></i><b>29.3.2</b> Application for Loan</a></li>
</ul></li>
<li class="chapter" data-level="29.4" data-path="classperformance.html"><a href="classperformance.html#decision-rules-and-errors"><i class="fa fa-check"></i><b>29.4</b> Decision Rules and Errors</a></li>
<li class="chapter" data-level="29.5" data-path="classperformance.html"><a href="classperformance.html#roc-curves"><i class="fa fa-check"></i><b>29.5</b> ROC Curves</a><ul>
<li class="chapter" data-level="29.5.1" data-path="classperformance.html"><a href="classperformance.html#graphing-roc-curves"><i class="fa fa-check"></i><b>29.5.1</b> Graphing ROC curves</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IX Unsupervised II: Clustering</b></span></li>
<li class="chapter" data-level="30" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>30</b> Clustering</a><ul>
<li class="chapter" data-level="30.1" data-path="clustering.html"><a href="clustering.html#about-clustering"><i class="fa fa-check"></i><b>30.1</b> About Clustering</a><ul>
<li class="chapter" data-level="30.1.1" data-path="clustering.html"><a href="clustering.html#types-of-clustering"><i class="fa fa-check"></i><b>30.1.1</b> Types of Clustering</a></li>
<li class="chapter" data-level="30.1.2" data-path="clustering.html"><a href="clustering.html#hard-clustering"><i class="fa fa-check"></i><b>30.1.2</b> Hard Clustering</a></li>
</ul></li>
<li class="chapter" data-level="30.2" data-path="clustering.html"><a href="clustering.html#dispersion-measures"><i class="fa fa-check"></i><b>30.2</b> Dispersion Measures</a></li>
<li class="chapter" data-level="30.3" data-path="clustering.html"><a href="clustering.html#complexity-in-clustering"><i class="fa fa-check"></i><b>30.3</b> Complexity in Clustering</a></li>
</ul></li>
<li class="chapter" data-level="31" data-path="kmeans.html"><a href="kmeans.html"><i class="fa fa-check"></i><b>31</b> K-Means</a><ul>
<li class="chapter" data-level="31.1" data-path="kmeans.html"><a href="kmeans.html#toy-example-2"><i class="fa fa-check"></i><b>31.1</b> Toy Example</a></li>
<li class="chapter" data-level="31.2" data-path="kmeans.html"><a href="kmeans.html#what-does-k-means-do"><i class="fa fa-check"></i><b>31.2</b> What does K-means do?</a></li>
<li class="chapter" data-level="31.3" data-path="kmeans.html"><a href="kmeans.html#k-means-algorithms"><i class="fa fa-check"></i><b>31.3</b> K-Means Algorithms</a><ul>
<li class="chapter" data-level="31.3.1" data-path="kmeans.html"><a href="kmeans.html#classic-version"><i class="fa fa-check"></i><b>31.3.1</b> Classic Version</a></li>
<li class="chapter" data-level="31.3.2" data-path="kmeans.html"><a href="kmeans.html#moving-centers-algorithm"><i class="fa fa-check"></i><b>31.3.2</b> Moving Centers Algorithm</a></li>
<li class="chapter" data-level="31.3.3" data-path="kmeans.html"><a href="kmeans.html#dynamic-clouds"><i class="fa fa-check"></i><b>31.3.3</b> Dynamic Clouds</a></li>
<li class="chapter" data-level="31.3.4" data-path="kmeans.html"><a href="kmeans.html#choosing-k"><i class="fa fa-check"></i><b>31.3.4</b> Choosing <span class="math inline">\(K\)</span></a></li>
<li class="chapter" data-level="31.3.5" data-path="kmeans.html"><a href="kmeans.html#comments"><i class="fa fa-check"></i><b>31.3.5</b> Comments</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="32" data-path="hclus.html"><a href="hclus.html"><i class="fa fa-check"></i><b>32</b> Hierarchical Clustering</a><ul>
<li class="chapter" data-level="32.1" data-path="hclus.html"><a href="hclus.html#agglomerative-methods"><i class="fa fa-check"></i><b>32.1</b> Agglomerative Methods</a></li>
<li class="chapter" data-level="32.2" data-path="hclus.html"><a href="hclus.html#example-single-linkage"><i class="fa fa-check"></i><b>32.2</b> Example: Single Linkage</a><ul>
<li class="chapter" data-level="32.2.1" data-path="hclus.html"><a href="hclus.html#dendrogram"><i class="fa fa-check"></i><b>32.2.1</b> Dendrogram</a></li>
</ul></li>
<li class="chapter" data-level="32.3" data-path="hclus.html"><a href="hclus.html#example-complete-linkage"><i class="fa fa-check"></i><b>32.3</b> Example: Complete Linkage</a><ul>
<li class="chapter" data-level="32.3.1" data-path="hclus.html"><a href="hclus.html#cutting-dendograms"><i class="fa fa-check"></i><b>32.3.1</b> Cutting Dendograms</a></li>
<li class="chapter" data-level="32.3.2" data-path="hclus.html"><a href="hclus.html#pros-and-coons"><i class="fa fa-check"></i><b>32.3.2</b> Pros and COons</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>X Tree-based Methods</b></span></li>
<li class="chapter" data-level="33" data-path="trees.html"><a href="trees.html"><i class="fa fa-check"></i><b>33</b> Intro to Decision Trees</a><ul>
<li class="chapter" data-level="33.1" data-path="trees.html"><a href="trees.html#introduction-7"><i class="fa fa-check"></i><b>33.1</b> Introduction</a></li>
<li class="chapter" data-level="33.2" data-path="trees.html"><a href="trees.html#some-terminology"><i class="fa fa-check"></i><b>33.2</b> Some Terminology</a><ul>
<li class="chapter" data-level="33.2.1" data-path="trees.html"><a href="trees.html#binary-trees"><i class="fa fa-check"></i><b>33.2.1</b> Binary Trees</a></li>
</ul></li>
<li class="chapter" data-level="33.3" data-path="trees.html"><a href="trees.html#space-partitions"><i class="fa fa-check"></i><b>33.3</b> Space Partitions</a><ul>
<li class="chapter" data-level="33.3.1" data-path="trees.html"><a href="trees.html#the-process-of-building-a-tree"><i class="fa fa-check"></i><b>33.3.1</b> The Process of Building a Tree</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="34" data-path="tree-impurities.html"><a href="tree-impurities.html"><i class="fa fa-check"></i><b>34</b> Binary Splits and Impurity</a><ul>
<li class="chapter" data-level="34.1" data-path="tree-impurities.html"><a href="tree-impurities.html#binary-partitions"><i class="fa fa-check"></i><b>34.1</b> Binary Partitions</a><ul>
<li class="chapter" data-level="34.1.1" data-path="tree-impurities.html"><a href="tree-impurities.html#splits-of-binary-variables"><i class="fa fa-check"></i><b>34.1.1</b> Splits of Binary variables</a></li>
<li class="chapter" data-level="34.1.2" data-path="tree-impurities.html"><a href="tree-impurities.html#splits-of-nominal-variables"><i class="fa fa-check"></i><b>34.1.2</b> Splits of Nominal Variables</a></li>
<li class="chapter" data-level="34.1.3" data-path="tree-impurities.html"><a href="tree-impurities.html#splits-of-ordinal-variables"><i class="fa fa-check"></i><b>34.1.3</b> Splits of Ordinal Variables</a></li>
<li class="chapter" data-level="34.1.4" data-path="tree-impurities.html"><a href="tree-impurities.html#splits-continuous-variables"><i class="fa fa-check"></i><b>34.1.4</b> Splits Continuous variables</a></li>
</ul></li>
<li class="chapter" data-level="34.2" data-path="tree-impurities.html"><a href="tree-impurities.html#measures-of-impurity"><i class="fa fa-check"></i><b>34.2</b> Measures of Impurity</a><ul>
<li class="chapter" data-level="34.2.1" data-path="tree-impurities.html"><a href="tree-impurities.html#entropy"><i class="fa fa-check"></i><b>34.2.1</b> Entropy</a></li>
<li class="chapter" data-level="34.2.2" data-path="tree-impurities.html"><a href="tree-impurities.html#the-math-behind-entropy"><i class="fa fa-check"></i><b>34.2.2</b> The Math Behind Entropy</a></li>
<li class="chapter" data-level="34.2.3" data-path="tree-impurities.html"><a href="tree-impurities.html#gini-impurity"><i class="fa fa-check"></i><b>34.2.3</b> Gini Impurity</a></li>
<li class="chapter" data-level="34.2.4" data-path="tree-impurities.html"><a href="tree-impurities.html#variance-based-impurity"><i class="fa fa-check"></i><b>34.2.4</b> Variance-based Impurity</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="35" data-path="tree-splits.html"><a href="tree-splits.html"><i class="fa fa-check"></i><b>35</b> Splitting Nodes</a><ul>
<li class="chapter" data-level="35.1" data-path="tree-splits.html"><a href="tree-splits.html#entropy-based-splits"><i class="fa fa-check"></i><b>35.1</b> Entropy-based Splits</a></li>
<li class="chapter" data-level="35.2" data-path="tree-splits.html"><a href="tree-splits.html#gini-index-based-splits"><i class="fa fa-check"></i><b>35.2</b> Gini-index based Splits</a></li>
<li class="chapter" data-level="35.3" data-path="tree-splits.html"><a href="tree-splits.html#looking-for-the-best-split"><i class="fa fa-check"></i><b>35.3</b> Looking for the best split</a></li>
</ul></li>
<li class="chapter" data-level="36" data-path="tree-basics.html"><a href="tree-basics.html"><i class="fa fa-check"></i><b>36</b> Building Binary Trees</a><ul>
<li class="chapter" data-level="36.1" data-path="tree-basics.html"><a href="tree-basics.html#node-splitting-stopping-criteria"><i class="fa fa-check"></i><b>36.1</b> Node-Splitting Stopping Criteria</a></li>
<li class="chapter" data-level="36.2" data-path="tree-basics.html"><a href="tree-basics.html#issues-with-trees"><i class="fa fa-check"></i><b>36.2</b> Issues with Trees</a><ul>
<li class="chapter" data-level="36.2.1" data-path="tree-basics.html"><a href="tree-basics.html#bias-variance-of-trees"><i class="fa fa-check"></i><b>36.2.1</b> Bias-Variance of Trees</a></li>
</ul></li>
<li class="chapter" data-level="36.3" data-path="tree-basics.html"><a href="tree-basics.html#pruning-a-tree"><i class="fa fa-check"></i><b>36.3</b> Pruning a Tree</a></li>
<li class="chapter" data-level="36.4" data-path="tree-basics.html"><a href="tree-basics.html#pros-and-cons-of-trees"><i class="fa fa-check"></i><b>36.4</b> Pros and Cons of Trees</a><ul>
<li class="chapter" data-level="36.4.1" data-path="tree-basics.html"><a href="tree-basics.html#advantages-of-trees"><i class="fa fa-check"></i><b>36.4.1</b> Advantages of Trees</a></li>
<li class="chapter" data-level="36.4.2" data-path="tree-basics.html"><a href="tree-basics.html#disadvantages-of-trees"><i class="fa fa-check"></i><b>36.4.2</b> Disadvantages of Trees</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="37" data-path="bagging.html"><a href="bagging.html"><i class="fa fa-check"></i><b>37</b> Bagging</a><ul>
<li class="chapter" data-level="37.1" data-path="bagging.html"><a href="bagging.html#introduction-8"><i class="fa fa-check"></i><b>37.1</b> Introduction</a><ul>
<li class="chapter" data-level="37.1.1" data-path="bagging.html"><a href="bagging.html#idea-of-bagging"><i class="fa fa-check"></i><b>37.1.1</b> Idea of Bagging</a></li>
</ul></li>
<li class="chapter" data-level="37.2" data-path="bagging.html"><a href="bagging.html#why-bother-bagging"><i class="fa fa-check"></i><b>37.2</b> Why Bother Bagging?</a></li>
</ul></li>
<li class="chapter" data-level="38" data-path="forest.html"><a href="forest.html"><i class="fa fa-check"></i><b>38</b> Random Forests</a><ul>
<li class="chapter" data-level="38.1" data-path="forest.html"><a href="forest.html#introduction-9"><i class="fa fa-check"></i><b>38.1</b> Introduction</a></li>
<li class="chapter" data-level="38.2" data-path="forest.html"><a href="forest.html#algorithm-2"><i class="fa fa-check"></i><b>38.2</b> Algorithm</a><ul>
<li class="chapter" data-level="38.2.1" data-path="forest.html"><a href="forest.html#two-sources-of-randomness"><i class="fa fa-check"></i><b>38.2.1</b> Two Sources of Randomness</a></li>
<li class="chapter" data-level="38.2.2" data-path="forest.html"><a href="forest.html#regressions-and-classification-forests"><i class="fa fa-check"></i><b>38.2.2</b> Regressions and Classification Forests</a></li>
<li class="chapter" data-level="38.2.3" data-path="forest.html"><a href="forest.html#key-advantage-of-random-forests"><i class="fa fa-check"></i><b>38.2.3</b> Key Advantage of Random Forests</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Made with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">All Models Are Wrong: Concepts of Statistical Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="lasso" class="section level1">
<h1><span class="header-section-number">18</span> Lasso Regression</h1>
<p>In this chapter we describe Lasso.</p>
<p>Though ridge regression has a great many applications and uses, there is one
thing to note: it does not perform variable selection.</p>
<p>Let us parse through this statement in more detail. In the previous chapter,
we saw that the Ridge Regression estimate of <span class="math inline">\(\mathbf{b}_{RR}\)</span> is given by</p>
<p><span class="math display" id="eq:505-01">\[
\mathbf{b}_{RR} = \left( \mathbf{X^\mathsf{T} X} + \lambda \mathbf{I} \right)^{-1} \mathbf{X^\mathsf{T}y}
\tag{18.1}
\]</span></p>
<p>We also encountered the following geometric interpretation of Ridge Regression:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-2"></span>
<img src="images/penalized/ridge-constraint3.svg" alt="Illustration of Ridge Regression" width="70%" />
<p class="caption">
Figure 18.1: Illustration of Ridge Regression
</p>
</div>
<p>Geometrically, we see that ridge coefficients may be set <em>close</em> to zero, but
never exactly equal to zero.</p>
<p>You may ask why we want some coefficients to be equal to zero. To answer this
question, let us return to our general model:</p>
<p><span class="math display" id="eq:505-02">\[
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p
\tag{18.2}
\]</span></p>
<p>where <span class="math inline">\(Y\)</span> denotes our response variable and <span class="math inline">\(X_1, \dots, X_p\)</span> denote the
predictors. If it truly were the case that <span class="math inline">\(\beta_i = 0\)</span>, the entire variable
<span class="math inline">\(X_i\)</span> is actually irrelevant and can be omitted our model completely. As such,
if we can find an estimation technique that sets the coefficients of
“unnecessary” variables equal to 0, we can use this technique to shrink the
space of input variables.</p>
<p>So, we’ve now seen why we want some coefficients to be set equal to 0, and we’ve
seen that Ridge Regression doesn’t actually do this. As such, we posit a new
estimation technique: the <strong>Least Absolute Shrinkage and Selection Operator</strong>
(it even has the word “Selection” in its name!), or LASSO for short.</p>
<div id="mathematical-setup" class="section level2">
<h2><span class="header-section-number">18.1</span> Mathematical Setup</h2>
<p>We begin, as we did in Ridge Regression, with a constrained minimization problem:</p>
<p><span class="math display" id="eq:505-03">\[
\min_{\mathbf{b} \in \mathbb{R}^{p}} \left\{ \frac{1}{n} \left\| \mathbf{y} - \mathbf{Xb} \right\|_2^2 \right\} \hspace{5mm} \text{subject to} \hspace{5mm} \text{norm}\left( \mathbf{b} \right) &lt; c
\tag{18.3}
\]</span></p>
<p>Now, instead of using the <span class="math inline">\(\ell_2\)</span> norm for our constraint, we’ll actually use
an <span class="math inline">\(\ell_1\)</span> norm:</p>
<p><span class="math display" id="eq:505-04">\[
\min_{\mathbf{b} \in \mathbb{R}^{p}} \left\{ \frac{1}{n} \left\| \mathbf{y} - \mathbf{Xb} \right\|_2^2 \right\} \hspace{5mm} \text{subject to} \hspace{5mm} \left\| \mathbf{b} \right\|_1 \leq c
\tag{18.4}
\]</span></p>
<p>In case you’re not familiar with the <span class="math inline">\(\ell_1\)</span> norm, what we mean by this
constraint is</p>
<p><span class="math display" id="eq:505-05">\[
\left\| \mathbf{b} \right\|_1 \leq c \iff \sum_{j=1}^{p} | b_j| \leq c
\tag{18.5}
\]</span></p>
<p>It can be shown that the minimization in equation (1) is exactly equivalent to</p>
<p><span class="math display" id="eq:505-06">\[
\min_{\mathbf{b}} \left\{  \frac{1}{n} \left( \mathbf{y} - \mathbf{Xb} \right)^\mathsf{T}  \left( \mathbf{y} - \mathbf{Xb} \right) + \lambda \sum_{j=1}^{p} |b_j| \right\}
\tag{18.6}
\]</span></p>
<div id="closed-form" class="section level3">
<h3><span class="header-section-number">18.1.1</span> Closed Form?</h3>
<p>Recall that the vector of Ridge Regression coefficients had a simple closed-form
solution:</p>
<p><span class="math display" id="eq:505-07">\[
\mathbf{b}_{RR} = \left( \mathbf{X^\mathsf{T} X} + \lambda \mathbf{I} \right)^{-1} \mathbf{X^\mathsf{T} y}
\tag{18.7}
\]</span></p>
<p>One might ask: do we have a closed-form solution for the LASSO? Unfortunately,
the answer is, in general, no. Even without extensive mathematical background,
you might be able to see why this is: minimization problems involve
differentiation; the LASSO involves an absolute value; differentiation and
absolute values don’t play well together.</p>
<p>Now, there are a few exceptions. In particular, when we have only <span class="math inline">\(p = 1\)</span>
predictors, one can actually find a piecewise-defined (but closed-form) solution
to the LASSO. When the design matrix <span class="math inline">\(\mathbf{X}\)</span> is orthonormal, it can be shown
that the LASSO can be expressed as</p>
<p><span class="math display" id="eq:505-08">\[
\mathrm{sgn}(b_j) \cdot \left( |b_j| - \lambda \right)_{+}
\tag{18.8}
\]</span></p>
<p>where <span class="math inline">\(b_j\)</span> denotes the OLS estimate of <span class="math inline">\(b_j\)</span>, and <span class="math inline">\(x_{+}\)</span> denotes the
positive-part function (i.e. the positive part of <span class="math inline">\(x\)</span>). However, in general,
it is nearly impossible to extract a closed-form solution to the LASSO.</p>
<p>This is not to say we can’t <em>find</em> the coefficients numerically; there exist a
number of extremely effective and efficient algorithms that can solve <span class="math inline">\(\ell_1\)</span>-minimization problems. For example, we could use Gradient Descent
(perhaps, more accurately, sub-gradient descent), or we could use
coordinate descent.</p>
<p>Note, <em>Gradient Descent</em> cannot actually be used here. The problem is that the
<span class="math inline">\(\ell_1\)</span> norm term does not take well to gradients (think about derivatives and
absolute value signs), and as such we don’t have a well-defined update step.</p>
<p>In practice, however, we use built-in functions to do the heavy lifting. For
example, in R, the function <code>glmnet()</code> from the package <code>"glmnet"</code> can compute
both Ridge and LASSO coefficients quite easily.</p>
</div>
</div>
<div id="geometric-visualization" class="section level2">
<h2><span class="header-section-number">18.2</span> Geometric Visualization</h2>
<p>Just because we can’t always find neat closed-form solutions to the LASSO, that
isn’t to say we can gain some intuition behind what’s going on. As we did with
Ridge Regression, we can sketch our constraint region and level sets of constant
error (MSE):</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-3"></span>
<img src="images/penalized/lasso-constraint1.svg" alt="Error contours with constraint region" width="70%" />
<p class="caption">
Figure 18.2: Error contours with constraint region
</p>
</div>
<p>Here’s the kicker: in LASSO, we could have the following situation:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-4"></span>
<img src="images/penalized/lasso-constraint2.svg" alt="Error contours with constraint region" width="70%" />
<p class="caption">
Figure 18.3: Error contours with constraint region
</p>
</div>
<p>Here we see the <strong>variable selection</strong> advertised in the beginning of this
chapter. In this particular scenario, the ideal point has <span class="math inline">\(b_1-\)</span>coordinate equal
to 0, meaning the LASSO has completely zeroed-out <span class="math inline">\(b_1\)</span> in the model, reducing
the number of covariates from <span class="math inline">\(2\)</span> down to <span class="math inline">\(1\)</span>.</p>
<div id="some-more-math-variable-selection-in-action" class="section level3">
<h3><span class="header-section-number">18.2.1</span> Some More Math: Variable Selection in Action</h3>
<p>It may be insightful to see some concrete math behind the idea that the LASSO
performs variable selection, whereas Ridge Regression does not. For now, we make
the simplifying assumption that <span class="math inline">\(\mathbf{X}\)</span> is orthonormal (this ensures a
closed-form solution to the LASSO). As mentioned in <strong>ESL (page 71)</strong>, we have</p>
<p><span class="math display" id="eq:505-09">\[
b_{j-RR} = \frac{b_j}{1 + \lambda} ; \hspace{5mm}  b_{j-LASSO} =  \mathrm{sign}(b_j) (|b_j| - \lambda)_{+}
\tag{18.9}
\]</span></p>
<p>It should be clear now that <span class="math inline">\(\mathbf{b}_{RR} = \mathbf{0}\)</span> only when
<span class="math inline">\(\lambda = \infty\)</span> (which, of course, is a mathematically ill-phrased statement)
which is to say that <span class="math inline">\(\mathbf{b}_{RR}\)</span> is never equal to zero. With LASSO, on
the other hand, setting <span class="math inline">\(\lambda = |b_j|\)</span> results in a <span class="math inline">\(\mathbf{b}_{LASSO}\)</span>
value of exactly zero.</p>
</div>
<div id="example-with-mtcars" class="section level3">
<h3><span class="header-section-number">18.2.2</span> Example with <code>mtcars</code></h3>
<p>As an example, consider the built-in <code>mtcars</code> dataset from R. We may use
<code>glmnet()</code> to compute the LASSO coefficients, for different values of the tuning
parameter <span class="math inline">\(\lambda\)</span> (<span class="math inline">\(\lambda\)</span> was chosen to increase from 0 to 1 in steps of 0.1):</p>
<p><strong>INSERT code for ggplot figure</strong></p>
<p>LASSO coefficients for different <span class="math inline">\(\lambda\)</span> values; as we see, the LASSO
eventually sets coefficients equal to <span class="math inline">\(0\)</span>, thereby eliminating their
corresponding covariate from the original model.</p>
<p>Compare this with the coefficients obtained through Ridge Regression (for the
same <span class="math inline">\(\lambda\)</span> values):</p>
<p><strong>INSERT code for ggplot figure</strong></p>
<p>Ridge coefficients for different <span class="math inline">\(\lambda\)</span> values; as we see, Ridge Regression
does not set coefficients equal to 0.</p>
<p>As with Ridge Regression, there isn’t an analytic expression we can use to
find the optimal value of <span class="math inline">\(\lambda\)</span>. Rather, <span class="math inline">\(\lambda\)</span> should be chosen using
cross-validation.</p>
</div>
</div>
<div id="going-beyond" class="section level2">
<h2><span class="header-section-number">18.3</span> Going Beyond</h2>
<p>So far, we’ve seen <span class="math inline">\(\ell_2\)</span> norms (through Ridge Regression) and <span class="math inline">\(\ell_1\)</span> norms
(through the LASSO). In general, we define the <span class="math inline">\(\ell_p\)</span> norm to be</p>
<p><span class="math display">\[
\left\| \begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \\ \end{bmatrix} \right\|_p = \sum_{j=1}^{n} | v_i |^p
\]</span></p>
<p>One could plausibly replace the <span class="math inline">\(\ell_1\)</span> norm in the LASSO with an <span class="math inline">\(\ell_p\)</span>
norm (for a different choice of <span class="math inline">\(p\)</span>) to yield yet another type of estimator!
One note; for <span class="math inline">\(p &gt; 1\)</span>, the resulting estimator will not perform variable
selection. This is because <span class="math inline">\(|b_j|^p\)</span> <em>is</em> differentiable at <span class="math inline">\(0\)</span> for <span class="math inline">\(p &gt; 1\)</span>.</p>
<p>Another popular penalty (widely regarded as a “compromise” between Ridge
Regression and LASSO) is the <strong>elastic-net penalty</strong> (Zou and Hastie, 2005),
which replaces the <span class="math inline">\(\ell_1\)</span> norm in equation (1) with</p>
<p><span class="math display">\[
\lambda \sum_{j=1}^{p} \left[ \alpha b_j^2 + (1 - \alpha) | b_j | \right]
\]</span></p>
<p>The elastic net does perform variable selection (due to the absolute value term
in its penalty), but also performs shrinkage on the remaining variables
(like Ridge).</p>
<p>The LASSO is just the tip of the iceberg. In fact, we can consider a whole
plethora of models with the form</p>
<p><span class="math display">\[
\min_{\mathbf{b}} \left\{  \frac{1}{n} \left( \mathbf{y} - \mathbf{Xb} \right)^\mathsf{T}  \left( \mathbf{y} - \mathbf{Xb} \right) + \lambda \sum_{j=1}^{p} |b_j| \right\}
\]</span></p>

</div>
</div>



            </section>

          </div>
        </div>
      </div>
<a href="ridge.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="linear-extensions.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["allmodelsarewrong.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
