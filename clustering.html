<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>21 Clustering | All Models Are Wrong</title>
  <meta name="description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  <meta name="generator" content="bookdown 0.16 and GitBook 2.6.7" />

  <meta property="og:title" content="21 Clustering | All Models Are Wrong" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://allmodelsarewrong.github.io" />
  
  <meta property="og:description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  <meta name="github-repo" content="allmodelsarewrong/allmodelsarewrong.github.io" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="21 Clustering | All Models Are Wrong" />
  
  <meta name="twitter:description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  

<meta name="author" content="by Gaston Sanchez, and Ethan Marzban" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="classperformance.html"/>
<link rel="next" href="kmeans.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><b>All Models Are Wrong (CSL)</b><br><small>by G. Sanchez & E. Marzban</small></a></li>

<li class="divider"></li>
<li class="part"><span><b>I Welcome</b></span></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="about.html"><a href="about.html"><i class="fa fa-check"></i><b>1</b> About this book</a><ul>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#prerequisites"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="part"><span><b>II Intro</b></span></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#basic-notation"><i class="fa fa-check"></i><b>2.1</b> Basic Notation</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="duality.html"><a href="duality.html"><i class="fa fa-check"></i><b>3</b> Geometric Duality</a><ul>
<li class="chapter" data-level="3.1" data-path="duality.html"><a href="duality.html#rows-space"><i class="fa fa-check"></i><b>3.1</b> Rows Space</a></li>
<li class="chapter" data-level="3.2" data-path="duality.html"><a href="duality.html#columns-space"><i class="fa fa-check"></i><b>3.2</b> Columns Space</a></li>
<li class="chapter" data-level="3.3" data-path="duality.html"><a href="duality.html#cloud-of-individuals"><i class="fa fa-check"></i><b>3.3</b> Cloud of Individuals</a><ul>
<li class="chapter" data-level="3.3.1" data-path="duality.html"><a href="duality.html#average-individual"><i class="fa fa-check"></i><b>3.3.1</b> Average Individual</a></li>
<li class="chapter" data-level="3.3.2" data-path="duality.html"><a href="duality.html#centered-data"><i class="fa fa-check"></i><b>3.3.2</b> Centered Data</a></li>
<li class="chapter" data-level="3.3.3" data-path="duality.html"><a href="duality.html#distance-between-individuals"><i class="fa fa-check"></i><b>3.3.3</b> Distance between individuals</a></li>
<li class="chapter" data-level="3.3.4" data-path="duality.html"><a href="duality.html#distance-to-the-centroid"><i class="fa fa-check"></i><b>3.3.4</b> Distance to the centroid</a></li>
<li class="chapter" data-level="3.3.5" data-path="duality.html"><a href="duality.html#measures-of-dispersion"><i class="fa fa-check"></i><b>3.3.5</b> Measures of Dispersion</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="duality.html"><a href="duality.html#cloud-of-variables"><i class="fa fa-check"></i><b>3.4</b> Cloud of Variables</a><ul>
<li class="chapter" data-level="3.4.1" data-path="duality.html"><a href="duality.html#mean-of-a-variable"><i class="fa fa-check"></i><b>3.4.1</b> Mean of a Variable</a></li>
<li class="chapter" data-level="3.4.2" data-path="duality.html"><a href="duality.html#variance-of-a-variable"><i class="fa fa-check"></i><b>3.4.2</b> Variance of a Variable</a></li>
<li class="chapter" data-level="3.4.3" data-path="duality.html"><a href="duality.html#variance-with-vector-notation"><i class="fa fa-check"></i><b>3.4.3</b> Variance with Vector Notation</a></li>
<li class="chapter" data-level="3.4.4" data-path="duality.html"><a href="duality.html#standard-deviation-as-a-norm"><i class="fa fa-check"></i><b>3.4.4</b> Standard Deviation as a Norm</a></li>
<li class="chapter" data-level="3.4.5" data-path="duality.html"><a href="duality.html#covariance"><i class="fa fa-check"></i><b>3.4.5</b> Covariance</a></li>
<li class="chapter" data-level="3.4.6" data-path="duality.html"><a href="duality.html#correlation"><i class="fa fa-check"></i><b>3.4.6</b> Correlation</a></li>
<li class="chapter" data-level="3.4.7" data-path="duality.html"><a href="duality.html#geometry-of-correlation"><i class="fa fa-check"></i><b>3.4.7</b> Geometry of Correlation</a></li>
<li class="chapter" data-level="3.4.8" data-path="duality.html"><a href="duality.html#orthogonal-projections"><i class="fa fa-check"></i><b>3.4.8</b> Orthogonal Projections</a></li>
<li class="chapter" data-level="3.4.9" data-path="duality.html"><a href="duality.html#the-mean-as-an-orthogonal-projection"><i class="fa fa-check"></i><b>3.4.9</b> The mean as an orthogonal projection</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Unsupervised I: PCA</b></span></li>
<li class="chapter" data-level="4" data-path="pca.html"><a href="pca.html"><i class="fa fa-check"></i><b>4</b> Principal Components Analysis</a><ul>
<li class="chapter" data-level="4.1" data-path="pca.html"><a href="pca.html#low-dimensional-representations"><i class="fa fa-check"></i><b>4.1</b> Low-dimensional Representations</a></li>
<li class="chapter" data-level="4.2" data-path="pca.html"><a href="pca.html#projections"><i class="fa fa-check"></i><b>4.2</b> Projections</a><ul>
<li class="chapter" data-level="4.2.1" data-path="pca.html"><a href="pca.html#vector-and-scalar-projections"><i class="fa fa-check"></i><b>4.2.1</b> Vector and Scalar Projections</a></li>
<li class="chapter" data-level="4.2.2" data-path="pca.html"><a href="pca.html#projected-inertia"><i class="fa fa-check"></i><b>4.2.2</b> Projected Inertia</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="pca.html"><a href="pca.html#maximization-problem"><i class="fa fa-check"></i><b>4.3</b> Maximization Problem</a><ul>
<li class="chapter" data-level="4.3.1" data-path="pca.html"><a href="pca.html#eigenvectors-of-mathbfs"><i class="fa fa-check"></i><b>4.3.1</b> Eigenvectors of <span class="math inline">\(\mathbf{S}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="pca.html"><a href="pca.html#another-perspective-of-pca"><i class="fa fa-check"></i><b>4.4</b> Another Perspective of PCA</a><ul>
<li class="chapter" data-level="4.4.1" data-path="pca.html"><a href="pca.html#finding-principal-components"><i class="fa fa-check"></i><b>4.4.1</b> Finding Principal Components</a></li>
<li class="chapter" data-level="4.4.2" data-path="pca.html"><a href="pca.html#finding-the-first-pc"><i class="fa fa-check"></i><b>4.4.2</b> Finding the first PC</a></li>
<li class="chapter" data-level="4.4.3" data-path="pca.html"><a href="pca.html#finding-the-second-pc"><i class="fa fa-check"></i><b>4.4.3</b> Finding the second PC</a></li>
<li class="chapter" data-level="4.4.4" data-path="pca.html"><a href="pca.html#finding-all-pcs"><i class="fa fa-check"></i><b>4.4.4</b> Finding all PCs</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="pca.html"><a href="pca.html#data-decomposition-model"><i class="fa fa-check"></i><b>4.5</b> Data Decomposition Model</a><ul>
<li class="chapter" data-level="4.5.1" data-path="pca.html"><a href="pca.html#alternative-approaches"><i class="fa fa-check"></i><b>4.5.1</b> Alternative Approaches</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Linear Regression</b></span></li>
<li class="chapter" data-level="5" data-path="ols.html"><a href="ols.html"><i class="fa fa-check"></i><b>5</b> Linear Regression</a><ul>
<li class="chapter" data-level="5.1" data-path="ols.html"><a href="ols.html#motivation"><i class="fa fa-check"></i><b>5.1</b> Motivation</a></li>
<li class="chapter" data-level="5.2" data-path="ols.html"><a href="ols.html#the-ideaintuition-of-regression"><i class="fa fa-check"></i><b>5.2</b> The Idea/Intuition of Regression</a></li>
<li class="chapter" data-level="5.3" data-path="ols.html"><a href="ols.html#the-linear-regression-model"><i class="fa fa-check"></i><b>5.3</b> The Linear Regression Model</a></li>
<li class="chapter" data-level="5.4" data-path="ols.html"><a href="ols.html#the-error-measure"><i class="fa fa-check"></i><b>5.4</b> The Error Measure</a></li>
<li class="chapter" data-level="5.5" data-path="ols.html"><a href="ols.html#the-least-squares-algorithm"><i class="fa fa-check"></i><b>5.5</b> The Least Squares Algorithm</a></li>
<li class="chapter" data-level="5.6" data-path="ols.html"><a href="ols.html#geometries-of-ols"><i class="fa fa-check"></i><b>5.6</b> Geometries of OLS</a><ul>
<li class="chapter" data-level="5.6.1" data-path="ols.html"><a href="ols.html#rows-perspective"><i class="fa fa-check"></i><b>5.6.1</b> Rows Perspective</a></li>
<li class="chapter" data-level="5.6.2" data-path="ols.html"><a href="ols.html#columns-perspective"><i class="fa fa-check"></i><b>5.6.2</b> Columns Perspective</a></li>
<li class="chapter" data-level="5.6.3" data-path="ols.html"><a href="ols.html#parameters-perspective"><i class="fa fa-check"></i><b>5.6.3</b> Parameters Perspective</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="gradient.html"><a href="gradient.html"><i class="fa fa-check"></i><b>6</b> Gradient Descent</a><ul>
<li class="chapter" data-level="6.1" data-path="gradient.html"><a href="gradient.html#error-surface"><i class="fa fa-check"></i><b>6.1</b> Error Surface</a></li>
<li class="chapter" data-level="6.2" data-path="gradient.html"><a href="gradient.html#idea-of-gradient-descent"><i class="fa fa-check"></i><b>6.2</b> Idea of Gradient Descent</a></li>
<li class="chapter" data-level="6.3" data-path="gradient.html"><a href="gradient.html#moving-down-an-error-surface"><i class="fa fa-check"></i><b>6.3</b> Moving Down an Error Surface</a><ul>
<li class="chapter" data-level="6.3.1" data-path="gradient.html"><a href="gradient.html#the-direction-of-mathbfv"><i class="fa fa-check"></i><b>6.3.1</b> The direction of <span class="math inline">\(\mathbf{v}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="gradient.html"><a href="gradient.html#gradient-descent-and-our-model"><i class="fa fa-check"></i><b>6.4</b> Gradient Descent and our Model</a></li>
</ul></li>
<li class="part"><span><b>V Learning Concepts</b></span></li>
<li class="chapter" data-level="7" data-path="learning.html"><a href="learning.html"><i class="fa fa-check"></i><b>7</b> Theoretical Framework</a><ul>
<li class="chapter" data-level="7.1" data-path="learning.html"><a href="learning.html#mental-map"><i class="fa fa-check"></i><b>7.1</b> Mental Map</a></li>
<li class="chapter" data-level="7.2" data-path="learning.html"><a href="learning.html#kinds-of-predictions"><i class="fa fa-check"></i><b>7.2</b> Kinds of Predictions</a><ul>
<li class="chapter" data-level="7.2.1" data-path="learning.html"><a href="learning.html#two-types-of-data"><i class="fa fa-check"></i><b>7.2.1</b> Two Types of Data</a></li>
<li class="chapter" data-level="7.2.2" data-path="learning.html"><a href="learning.html#two-types-of-predictions"><i class="fa fa-check"></i><b>7.2.2</b> Two Types of Predictions</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="learning.html"><a href="learning.html#two-types-of-errors"><i class="fa fa-check"></i><b>7.3</b> Two Types of Errors</a><ul>
<li class="chapter" data-level="7.3.1" data-path="learning.html"><a href="learning.html#individual-errors"><i class="fa fa-check"></i><b>7.3.1</b> Individual Errors</a></li>
<li class="chapter" data-level="7.3.2" data-path="learning.html"><a href="learning.html#overall-errors"><i class="fa fa-check"></i><b>7.3.2</b> Overall Errors</a></li>
<li class="chapter" data-level="7.3.3" data-path="learning.html"><a href="learning.html#auxiliary-technicality"><i class="fa fa-check"></i><b>7.3.3</b> Auxiliary Technicality</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="learning.html"><a href="learning.html#noisy-targets"><i class="fa fa-check"></i><b>7.4</b> Noisy Targets</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="mse.html"><a href="mse.html"><i class="fa fa-check"></i><b>8</b> MSE of Estimator</a><ul>
<li class="chapter" data-level="8.1" data-path="mse.html"><a href="mse.html#mse-of-an-estimator"><i class="fa fa-check"></i><b>8.1</b> MSE of an Estimator</a><ul>
<li class="chapter" data-level="8.1.1" data-path="mse.html"><a href="mse.html#prototypical-cases-of-bias-and-variance"><i class="fa fa-check"></i><b>8.1.1</b> Prototypical Cases of Bias and Variance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="biasvar.html"><a href="biasvar.html"><i class="fa fa-check"></i><b>9</b> Bias-Variance Tradeoff</a><ul>
<li class="chapter" data-level="9.1" data-path="biasvar.html"><a href="biasvar.html#bias-variance-tradeoff"><i class="fa fa-check"></i><b>9.1</b> Bias-Variance Tradeoff</a></li>
<li class="chapter" data-level="9.2" data-path="biasvar.html"><a href="biasvar.html#motivation-example"><i class="fa fa-check"></i><b>9.2</b> Motivation Example</a><ul>
<li class="chapter" data-level="9.2.1" data-path="biasvar.html"><a href="biasvar.html#two-hypotheses"><i class="fa fa-check"></i><b>9.2.1</b> Two Hypotheses</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="biasvar.html"><a href="biasvar.html#learning-from-two-points"><i class="fa fa-check"></i><b>9.3</b> Learning from two points</a></li>
<li class="chapter" data-level="9.4" data-path="biasvar.html"><a href="biasvar.html#bias-variance-derivation"><i class="fa fa-check"></i><b>9.4</b> Bias-Variance Derivation</a><ul>
<li class="chapter" data-level="9.4.1" data-path="biasvar.html"><a href="biasvar.html#noisy-target"><i class="fa fa-check"></i><b>9.4.1</b> Noisy Target</a></li>
<li class="chapter" data-level="9.4.2" data-path="biasvar.html"><a href="biasvar.html#types-of-theoretical-mses"><i class="fa fa-check"></i><b>9.4.2</b> Types of Theoretical MSEs</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="biasvar.html"><a href="biasvar.html#the-tradeoff"><i class="fa fa-check"></i><b>9.5</b> The Tradeoff</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="validation.html"><a href="validation.html"><i class="fa fa-check"></i><b>10</b> Validation</a><ul>
<li class="chapter" data-level="10.1" data-path="validation.html"><a href="validation.html#model-assessment"><i class="fa fa-check"></i><b>10.1</b> Model Assessment</a></li>
<li class="chapter" data-level="10.2" data-path="validation.html"><a href="validation.html#holdout-method"><i class="fa fa-check"></i><b>10.2</b> Holdout Method</a><ul>
<li class="chapter" data-level="10.2.1" data-path="validation.html"><a href="validation.html#rationale-behind-holdout-method"><i class="fa fa-check"></i><b>10.2.1</b> Rationale Behind Holdout Method</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="validation.html"><a href="validation.html#repeated-holdout-method"><i class="fa fa-check"></i><b>10.3</b> Repeated Holdout Method</a></li>
<li class="chapter" data-level="10.4" data-path="validation.html"><a href="validation.html#bootstrap-method"><i class="fa fa-check"></i><b>10.4</b> Bootstrap Method</a></li>
<li class="chapter" data-level="10.5" data-path="validation.html"><a href="validation.html#model-selection"><i class="fa fa-check"></i><b>10.5</b> Model Selection</a><ul>
<li class="chapter" data-level="10.5.1" data-path="validation.html"><a href="validation.html#three-way-holdout-method"><i class="fa fa-check"></i><b>10.5.1</b> Three-way Holdout Method</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="validation.html"><a href="validation.html#cross-validation"><i class="fa fa-check"></i><b>10.6</b> Cross-Validation</a><ul>
<li class="chapter" data-level="10.6.1" data-path="validation.html"><a href="validation.html#leave-one-out-cross-validation-loocv"><i class="fa fa-check"></i><b>10.6.1</b> Leave-One-Out Cross Validation (LOOCV)</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>VI Regularization</b></span></li>
<li class="chapter" data-level="11" data-path="regular.html"><a href="regular.html"><i class="fa fa-check"></i><b>11</b> Regularization Techniques</a><ul>
<li class="chapter" data-level="11.1" data-path="regular.html"><a href="regular.html#multicollinearity-issues"><i class="fa fa-check"></i><b>11.1</b> Multicollinearity Issues</a><ul>
<li class="chapter" data-level="11.1.1" data-path="regular.html"><a href="regular.html#toy-example"><i class="fa fa-check"></i><b>11.1.1</b> Toy Example</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="regular.html"><a href="regular.html#irregular-coefficients"><i class="fa fa-check"></i><b>11.2</b> Irregular Coefficients</a><ul>
<li class="chapter" data-level="11.2.1" data-path="regular.html"><a href="regular.html#effect-of-multicollinearity"><i class="fa fa-check"></i><b>11.2.1</b> Effect of Multicollinearity</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="regular.html"><a href="regular.html#regularization-metaphor"><i class="fa fa-check"></i><b>11.3</b> Regularization Metaphor</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="pcr.html"><a href="pcr.html"><i class="fa fa-check"></i><b>12</b> Principal Components Regression</a><ul>
<li class="chapter" data-level="12.1" data-path="pcr.html"><a href="pcr.html#motivation-example-1"><i class="fa fa-check"></i><b>12.1</b> Motivation Example</a></li>
<li class="chapter" data-level="12.2" data-path="pcr.html"><a href="pcr.html#the-pcr-model"><i class="fa fa-check"></i><b>12.2</b> The PCR Model</a></li>
<li class="chapter" data-level="12.3" data-path="pcr.html"><a href="pcr.html#how-does-pcr-work"><i class="fa fa-check"></i><b>12.3</b> How does PCR work?</a><ul>
<li class="chapter" data-level="12.3.1" data-path="pcr.html"><a href="pcr.html#transition-formula"><i class="fa fa-check"></i><b>12.3.1</b> Transition Formula</a></li>
<li class="chapter" data-level="12.3.2" data-path="pcr.html"><a href="pcr.html#size-of-coefficients"><i class="fa fa-check"></i><b>12.3.2</b> Size of Coefficients</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="pls.html"><a href="pls.html"><i class="fa fa-check"></i><b>13</b> Partial Least Squares Regression</a><ul>
<li class="chapter" data-level="13.1" data-path="pls.html"><a href="pls.html#motivation-example-2"><i class="fa fa-check"></i><b>13.1</b> Motivation Example</a></li>
<li class="chapter" data-level="13.2" data-path="pls.html"><a href="pls.html#the-plsr-model"><i class="fa fa-check"></i><b>13.2</b> The PLSR Model</a></li>
<li class="chapter" data-level="13.3" data-path="pls.html"><a href="pls.html#how-does-plsr-work"><i class="fa fa-check"></i><b>13.3</b> How does PLSR work?</a></li>
<li class="chapter" data-level="13.4" data-path="pls.html"><a href="pls.html#plsr-algorithm"><i class="fa fa-check"></i><b>13.4</b> PLSR Algorithm</a><ul>
<li class="chapter" data-level="13.4.1" data-path="pls.html"><a href="pls.html#pls-solution-with-original-variables"><i class="fa fa-check"></i><b>13.4.1</b> PLS Solution with original variables</a></li>
<li class="chapter" data-level="13.4.2" data-path="pls.html"><a href="pls.html#size-of-coefficients-1"><i class="fa fa-check"></i><b>13.4.2</b> Size of Coefficients</a></li>
<li class="chapter" data-level="13.4.3" data-path="pls.html"><a href="pls.html#some-properties"><i class="fa fa-check"></i><b>13.4.3</b> Some Properties</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="ridge.html"><a href="ridge.html"><i class="fa fa-check"></i><b>14</b> Ridge Regression</a></li>
<li class="part"><span><b>VII Classification</b></span></li>
<li class="chapter" data-level="15" data-path="classif.html"><a href="classif.html"><i class="fa fa-check"></i><b>15</b> Classification Methods</a></li>
<li class="chapter" data-level="16" data-path="logistic.html"><a href="logistic.html"><i class="fa fa-check"></i><b>16</b> Logistic Regression</a><ul>
<li class="chapter" data-level="16.1" data-path="logistic.html"><a href="logistic.html#motivation-1"><i class="fa fa-check"></i><b>16.1</b> Motivation</a><ul>
<li class="chapter" data-level="16.1.1" data-path="logistic.html"><a href="logistic.html#first-approach-fitting-a-line"><i class="fa fa-check"></i><b>16.1.1</b> First Approach: Fitting a Line</a></li>
<li class="chapter" data-level="16.1.2" data-path="logistic.html"><a href="logistic.html#secodn-approach-harsh-thresholding"><i class="fa fa-check"></i><b>16.1.2</b> Secodn Approach: Harsh Thresholding</a></li>
<li class="chapter" data-level="16.1.3" data-path="logistic.html"><a href="logistic.html#third-approach-conditional-means"><i class="fa fa-check"></i><b>16.1.3</b> Third Approach: Conditional Means</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="logistic.html"><a href="logistic.html#logistic-regression-model"><i class="fa fa-check"></i><b>16.2</b> Logistic Regression Model</a><ul>
<li class="chapter" data-level="16.2.1" data-path="logistic.html"><a href="logistic.html#the-criterion-being-optimized"><i class="fa fa-check"></i><b>16.2.1</b> The Criterion Being Optimized</a></li>
<li class="chapter" data-level="16.2.2" data-path="logistic.html"><a href="logistic.html#another-way-to-solve-logistic-regression"><i class="fa fa-check"></i><b>16.2.2</b> Another Way to Solve Logistic Regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="17" data-path="discrim.html"><a href="discrim.html"><i class="fa fa-check"></i><b>17</b> Preamble for Discriminant Analysis</a><ul>
<li class="chapter" data-level="17.1" data-path="discrim.html"><a href="discrim.html#motivation-2"><i class="fa fa-check"></i><b>17.1</b> Motivation</a><ul>
<li class="chapter" data-level="17.1.1" data-path="discrim.html"><a href="discrim.html#distinguishing-species"><i class="fa fa-check"></i><b>17.1.1</b> Distinguishing Species</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="discrim.html"><a href="discrim.html#derived-ratios-from-sum-of-squares"><i class="fa fa-check"></i><b>17.2</b> Derived Ratios from Sum-of-Squares</a><ul>
<li class="chapter" data-level="17.2.1" data-path="discrim.html"><a href="discrim.html#correlation-ratio"><i class="fa fa-check"></i><b>17.2.1</b> Correlation Ratio</a></li>
<li class="chapter" data-level="17.2.2" data-path="discrim.html"><a href="discrim.html#f-ratio"><i class="fa fa-check"></i><b>17.2.2</b> F-Ratio</a></li>
<li class="chapter" data-level="17.2.3" data-path="discrim.html"><a href="discrim.html#example-with-iris-data"><i class="fa fa-check"></i><b>17.2.3</b> Example with Iris data</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="discrim.html"><a href="discrim.html#geometric-perspective"><i class="fa fa-check"></i><b>17.3</b> Geometric Perspective</a><ul>
<li class="chapter" data-level="17.3.1" data-path="discrim.html"><a href="discrim.html#clouds-from-class-structure"><i class="fa fa-check"></i><b>17.3.1</b> Clouds from Class Structure</a></li>
<li class="chapter" data-level="17.3.2" data-path="discrim.html"><a href="discrim.html#dispersion-decomposition"><i class="fa fa-check"></i><b>17.3.2</b> Dispersion Decomposition</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="cda.html"><a href="cda.html"><i class="fa fa-check"></i><b>18</b> Canonical Discriminant Analysis</a><ul>
<li class="chapter" data-level="18.0.1" data-path="cda.html"><a href="cda.html#looking-for-a-discriminant-axis"><i class="fa fa-check"></i><b>18.0.1</b> Looking for a discriminant axis</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="discanalysis.html"><a href="discanalysis.html"><i class="fa fa-check"></i><b>19</b> Discriminant Analysis</a><ul>
<li class="chapter" data-level="19.1" data-path="discanalysis.html"><a href="discanalysis.html#probabilistic-da"><i class="fa fa-check"></i><b>19.1</b> Probabilistic DA</a></li>
<li class="chapter" data-level="19.2" data-path="discanalysis.html"><a href="discanalysis.html#estimations"><i class="fa fa-check"></i><b>19.2</b> Estimations</a><ul>
<li class="chapter" data-level="19.2.1" data-path="discanalysis.html"><a href="discanalysis.html#univariate-data-boldsymbolp-1"><i class="fa fa-check"></i><b>19.2.1</b> Univariate Data (<span class="math inline">\(\boldsymbol{p = 1}\)</span>)</a></li>
<li class="chapter" data-level="19.2.2" data-path="discanalysis.html"><a href="discanalysis.html#multivariate-data-boldsymbolp-1"><i class="fa fa-check"></i><b>19.2.2</b> Multivariate Data (<span class="math inline">\(\boldsymbol{p &gt; 1}\)</span>)</a></li>
<li class="chapter" data-level="19.2.3" data-path="discanalysis.html"><a href="discanalysis.html#multivariate-estimations"><i class="fa fa-check"></i><b>19.2.3</b> Multivariate Estimations</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="discanalysis.html"><a href="discanalysis.html#discriminant-functions"><i class="fa fa-check"></i><b>19.3</b> Discriminant Functions</a></li>
<li class="chapter" data-level="19.4" data-path="discanalysis.html"><a href="discanalysis.html#quadratic-discriminant-analysis-qda"><i class="fa fa-check"></i><b>19.4</b> Quadratic Discriminant Analysis (QDA)</a></li>
<li class="chapter" data-level="19.5" data-path="discanalysis.html"><a href="discanalysis.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>19.5</b> Linear Discriminant Analysis</a></li>
<li class="chapter" data-level="19.6" data-path="discanalysis.html"><a href="discanalysis.html#canonical-discriminant-analysis"><i class="fa fa-check"></i><b>19.6</b> Canonical Discriminant Analysis</a></li>
<li class="chapter" data-level="19.7" data-path="discanalysis.html"><a href="discanalysis.html#naive-bayes"><i class="fa fa-check"></i><b>19.7</b> Naive Bayes</a></li>
<li class="chapter" data-level="19.8" data-path="discanalysis.html"><a href="discanalysis.html#fifth-case"><i class="fa fa-check"></i><b>19.8</b> Fifth Case</a></li>
<li class="chapter" data-level="19.9" data-path="discanalysis.html"><a href="discanalysis.html#comparing-the-cases"><i class="fa fa-check"></i><b>19.9</b> Comparing the Cases</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="classperformance.html"><a href="classperformance.html"><i class="fa fa-check"></i><b>20</b> Performance of Classifiers</a><ul>
<li class="chapter" data-level="20.1" data-path="classperformance.html"><a href="classperformance.html#error-for-binary-response"><i class="fa fa-check"></i><b>20.1</b> Error for Binary Response</a></li>
<li class="chapter" data-level="20.2" data-path="classperformance.html"><a href="classperformance.html#categorical-response"><i class="fa fa-check"></i><b>20.2</b> Categorical Response</a></li>
<li class="chapter" data-level="20.3" data-path="classperformance.html"><a href="classperformance.html#confusion-matrices"><i class="fa fa-check"></i><b>20.3</b> Confusion Matrices</a></li>
<li class="chapter" data-level="20.4" data-path="classperformance.html"><a href="classperformance.html#binary-response-example"><i class="fa fa-check"></i><b>20.4</b> Binary Response: Example</a><ul>
<li class="chapter" data-level="20.4.1" data-path="classperformance.html"><a href="classperformance.html#types-of-errors-application-for-savings-account"><i class="fa fa-check"></i><b>20.4.1</b> Types of Errors: Application for Savings Account</a></li>
<li class="chapter" data-level="20.4.2" data-path="classperformance.html"><a href="classperformance.html#another-example-loans"><i class="fa fa-check"></i><b>20.4.2</b> Another Example: Loans</a></li>
<li class="chapter" data-level="20.4.3" data-path="classperformance.html"><a href="classperformance.html#the-ideal-situation"><i class="fa fa-check"></i><b>20.4.3</b> The Ideal Situation</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>VIII Clustering</b></span></li>
<li class="chapter" data-level="21" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>21</b> Clustering</a><ul>
<li class="chapter" data-level="21.1" data-path="clustering.html"><a href="clustering.html#about-clustering"><i class="fa fa-check"></i><b>21.1</b> About Clustering</a><ul>
<li class="chapter" data-level="21.1.1" data-path="clustering.html"><a href="clustering.html#types-of-clustering"><i class="fa fa-check"></i><b>21.1.1</b> Types of Clustering</a></li>
<li class="chapter" data-level="21.1.2" data-path="clustering.html"><a href="clustering.html#hard-clustering"><i class="fa fa-check"></i><b>21.1.2</b> Hard Clustering</a></li>
</ul></li>
<li class="chapter" data-level="21.2" data-path="clustering.html"><a href="clustering.html#dispersion-measures"><i class="fa fa-check"></i><b>21.2</b> Dispersion Measures</a></li>
<li class="chapter" data-level="21.3" data-path="clustering.html"><a href="clustering.html#complexity-in-clustering"><i class="fa fa-check"></i><b>21.3</b> Complexity in Clustering</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="kmeans.html"><a href="kmeans.html"><i class="fa fa-check"></i><b>22</b> K-Means</a><ul>
<li class="chapter" data-level="22.1" data-path="kmeans.html"><a href="kmeans.html#toy-example-1"><i class="fa fa-check"></i><b>22.1</b> Toy Example</a></li>
<li class="chapter" data-level="22.2" data-path="kmeans.html"><a href="kmeans.html#what-does-k-means-do"><i class="fa fa-check"></i><b>22.2</b> What does K-means do?</a></li>
<li class="chapter" data-level="22.3" data-path="kmeans.html"><a href="kmeans.html#k-means-algorithms"><i class="fa fa-check"></i><b>22.3</b> K-Means Algorithms</a><ul>
<li class="chapter" data-level="22.3.1" data-path="kmeans.html"><a href="kmeans.html#classic-version"><i class="fa fa-check"></i><b>22.3.1</b> Classic Version</a></li>
<li class="chapter" data-level="22.3.2" data-path="kmeans.html"><a href="kmeans.html#moving-centers-algorithm"><i class="fa fa-check"></i><b>22.3.2</b> Moving Centers Algorithm</a></li>
<li class="chapter" data-level="22.3.3" data-path="kmeans.html"><a href="kmeans.html#dynamic-clouds"><i class="fa fa-check"></i><b>22.3.3</b> Dynamic Clouds</a></li>
<li class="chapter" data-level="22.3.4" data-path="kmeans.html"><a href="kmeans.html#choosing-k"><i class="fa fa-check"></i><b>22.3.4</b> Choosing <span class="math inline">\(k\)</span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="23" data-path="hclus.html"><a href="hclus.html"><i class="fa fa-check"></i><b>23</b> Hierarchical Clustering</a><ul>
<li class="chapter" data-level="23.1" data-path="hclus.html"><a href="hclus.html#agglomerative-methods"><i class="fa fa-check"></i><b>23.1</b> Agglomerative Methods</a></li>
<li class="chapter" data-level="23.2" data-path="hclus.html"><a href="hclus.html#example-single-linkage"><i class="fa fa-check"></i><b>23.2</b> Example: Single Linkage</a><ul>
<li class="chapter" data-level="23.2.1" data-path="hclus.html"><a href="hclus.html#dendrogram"><i class="fa fa-check"></i><b>23.2.1</b> Dendrogram</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Made with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">All Models Are Wrong</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="clustering" class="section level1">
<h1><span class="header-section-number">21</span> Clustering</h1>
<p>In this part of the book, we go back to unsupervised learning methods. In
particular, the so-called clustering methods.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-185"></span>
<img src="images/cluster/four-corners-clustering.svg" alt="Clustering Corner" width="85%" />
<p class="caption">
Figure 21.1: Clustering Corner
</p>
</div>
<p>Clustering refers to a very broad set of techniques for finding groups,
or clusters, in a data set. Here are some examples that have become
famous in the clustering literature:</p>
<ul>
<li><p><strong>Marketing</strong>: discover groups of customers and used them for
targeted marketing.</p></li>
<li><p><strong>Medical Field</strong>: discover groups of patients suitable for
particular treatment protocols.</p></li>
<li><p><strong>Astronomy</strong>: find groups of similar stars and galaxies.</p></li>
<li><p><strong>Genomics</strong>: find groups of genes with similar expressions.</p></li>
</ul>
<p>The main idea of clustering (or cluster analysis) is to find groups of objects
in data. When we say “groups”, it can be groups of individuals, but also
groups of variables. So, more formally, clustering refers to the statistical
process of forming groups of objects (individuals or variables)
into a limited number of groups known as clusters.</p>
<p>Notice from the above diagram that both clustering and classification involve
dealing with a variable of interest that is categorical. However, the difference
between classification and clustering has to to do with whether that feature
of interest is observed or not. In classification we actually observe the class
of the individuals through the response variable <span class="math inline">\(Y\)</span>. In contrast, we <strong>don’t</strong>
have an explicit response in clustering; we want to (in a sense) find a “hidden”
variable that exposes group structure in the data. The groups are not defined
in advanced. In other words, there is no observed variable that imposes a group
structure.</p>
<div id="about-clustering" class="section level2">
<h2><span class="header-section-number">21.1</span> About Clustering</h2>
<p>Clustering has its roots in what used to be called <strong>numerical taxonomy</strong>, later
referred to as <strong>numerical ecology</strong>, which originated in ancient Greece (specifically, Aristotle).</p>
<p>In a more mathematical (numeric) way, the first formal approaches can be traced
back to the late 1950’s and early 1960’s. The main reason: the appearance of
computers during this time. Before computers, how did people find groups? Well,
typically people would generate a scatterplot and visually ascertain whether
or not there are groups. In other words, the humans were the machine learners!</p>
<p>Luckily, the human brain is quite good at seeing patterns, so this approach isn’t
actually that bad. But, of course, computers made this quite a bit easier
(not to mention, for more than 3 variables, visualizing a scatterplot is pretty
much impossible).</p>
<div id="types-of-clustering" class="section level3">
<h3><span class="header-section-number">21.1.1</span> Types of Clustering</h3>
<p>We can divide clustering approaches into two main types: <strong>Hard</strong> (<em>aka</em> crisp),
and <strong>Soft</strong> (<em>aka</em> fuzzy). In this book, we will only focus on hard clustering
(for more about soft clustering, see <a href="https://web.stanford.edu/~hastie/ElemStatLearn/">ESL</a>).</p>
<p>Within hard/crisp clustering, there are two approaches. The first is
<strong>direct partitioning</strong>, e.g. through <span class="math inline">\(k\)</span>-means and <span class="math inline">\(k\)</span>-medoids. The other is
<strong>hierarchical</strong> or embedded partitions, that is, in which partitions may have
sub-partitions within them.</p>
<ul>
<li><p><strong>Direct Partitioning</strong>: Clusters are always separated,
and their number is (usually) defined <em>a priori</em>.</p></li>
<li><p><strong>Hierarchical Partitions</strong>: clusters are either separate or embeded,
defining a hierarchy of partitions.</p></li>
</ul>
<p>You may have what are referred to as “natural” groups; that is, groups that are
somewhat obvious from the data at first glance. Of course, there will be cases
where there are no natural groups; in such a case, we might say we have
“artificial” groups.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-186"></span>
<img src="images/cluster/clustering-structures.svg" alt="Some Typical Clustering Structures" width="70%" />
<p class="caption">
Figure 21.2: Some Typical Clustering Structures
</p>
</div>
</div>
<div id="hard-clustering" class="section level3">
<h3><span class="header-section-number">21.1.2</span> Hard Clustering</h3>
<p>In hard clustering, we want objects in a particular cluster to be relatively
similar to one another (of course, we will need to define what “similar” means).
Additionally, we want there to be some notion of separation/spread; that is,
we want each cluster to be relatively distinguishable from the other cluster(s).
More specifically, we want objects in different clusters to be less similar
than objects within clusters. We give names to these concepts:</p>
<ul>
<li><p><strong>Within-cluster Homogeneity</strong>: Objects in a cluster should be similar enough;
<em>each group is as much homogeneous as possible.</em></p></li>
<li><p><strong>Between-cluster Heterogeneity</strong>: Objects in different clusters should be less
similar than object within each cluster; <em>groups are as distinct as possible among them</em>.</p></li>
</ul>
<p>In other words: we want high within-cluster homogeneity <em>and</em> between-cluster
heterogeneity.</p>
<p>In summary:</p>
<ul>
<li>We seek a partition of the data into distinct groups.</li>
<li>We want the observations within each group to be quite similar to each other.</li>
<li>We must define what it means for two or more observations to be similar or
different.</li>
<li>This is often a domain-specific consideration that must be made
based on knowledge of the data being studied.</li>
</ul>
<p>Of course, to quantify “similar” and “different,” we need some sort of
mathematical notion of <em>closeness</em>; that is, we (once again) need a distance
metric. As usual, we will assume that the rows of the data matrix correspond
to the individuals to be clustered (although you could also cluster variables).</p>
</div>
</div>
<div id="dispersion-measures" class="section level2">
<h2><span class="header-section-number">21.2</span> Dispersion Measures</h2>
<p>Let’s assume that the individuals are embeded in an euclidean space
(e.g. quantitative variables, or output of dimension reduction method). As
usual, suppose that the individuals form a cloud of points in a <span class="math inline">\(p\)</span>-dimensional
space, and that we also have the centroid or average individual, like in the
following figure:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-187"></span>
<img src="images/cluster/clusters-cloud2.svg" alt="Cloud of n points in p-dim space" width="50%" />
<p class="caption">
Figure 21.3: Cloud of n points in p-dim space
</p>
</div>
<p>We can look at the amount of spread or dispersion in the data with respect to
the global centroid. One way to do this is by getting the sum of all squared
distances from the centroid given by:</p>
<p><span class="math display" id="eq:801-1">\[
\text{TSS} = \sum_{i=1}^{n} d^2(\mathbf{x_i}, \mathbf{g})
\tag{21.1}
\]</span></p>
<p>Now suppose that we have some group structure in which objects are divided into
several clusters <span class="math inline">\(C_1, C_2, \dots, C_K\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-188"></span>
<img src="images/cluster/clusters-cloud3.svg" alt="Each cluster forms its own cloud" width="50%" />
<p class="caption">
Figure 21.4: Each cluster forms its own cloud
</p>
</div>
<p>Because each cluster <span class="math inline">\(C_k\)</span> will form its own cloud of points, we can make the
visual representation more pattent by using different shapes for the points.
Likewise, we suppose that each cluster has its own centroid <span class="math inline">\(\mathbf{g_k}\)</span>:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-189"></span>
<img src="images/cluster/clusters-cloud5.svg" alt="Each cluster has a certain within dispersion" width="50%" />
<p class="caption">
Figure 21.5: Each cluster has a certain within dispersion
</p>
</div>
<div id="within-cluster-dispersion" class="section level4 unnumbered">
<h4>Within Cluster Dispersion</h4>
<p>Each cluster will have an associated amount of spread from its centroid, that
is a Cluster Sum of Squared distances (<span class="math inline">\(\text{CSSD}\)</span>):</p>
<p><span class="math display" id="eq:801-2">\[
\text{CSSD} = \sum_{i \in C_k} d^2(\mathbf{x_i}, \mathbf{g_k})
\tag{21.2}
\]</span></p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-190"></span>
<img src="images/cluster/clusters-cloud6.svg" alt="Dispersion within each cluster" width="50%" />
<p class="caption">
Figure 21.6: Dispersion within each cluster
</p>
</div>
<p>We can combine the cluster sum-of-squared distances to obtain the Within-clusters
sum of squared distances <span class="math inline">\(\text{WSSD}\)</span>:</p>
<p><span class="math display" id="eq:801-3">\[
\text{WSSD} = \sum_{k=1}^{K} \text{CSSD}_k = \sum_{k=1}^{K} \sum_{i \in C_k} d^2(\mathbf{x_i}, \mathbf{g_k})
\tag{21.3}
\]</span></p>
</div>
<div id="between-cluster-dispersion" class="section level4 unnumbered">
<h4>Between Cluster Dispersion</h4>
<p>Let’s now turn our attention to the centroids:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-191"></span>
<img src="images/cluster/clusters-cloud7.svg" alt="Focusing on the centroids" width="50%" />
<p class="caption">
Figure 21.7: Focusing on the centroids
</p>
</div>
<p>Focusing on just the centroids, we could examine the distances
between the centroids of the groups and the overall centroid. Let us call this
between-cluster dispersion the <span class="math inline">\(\text{BSSD}\)</span> given by:</p>
<p><span class="math display" id="eq:801-4">\[
\text{BSSD} = \sum_{k=1}^{K} n_k d^2(\mathbf{g_k}, \mathbf{g})
\tag{21.4}
\]</span></p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-192"></span>
<img src="images/cluster/clusters-cloud8.svg" alt="Dispersion between the centroids" width="50%" />
<p class="caption">
Figure 21.8: Dispersion between the centroids
</p>
</div>
</div>
<div id="dispersion-decomposition-1" class="section level4 unnumbered">
<h4>Dispersion Decomposition</h4>
<p>Let’s recap. We have three types of sums-of-squared distances:</p>
<ul>
<li><span class="math inline">\(\text{TSSD}\)</span>: Total sum of squared distances</li>
<li><span class="math inline">\(\text{WSSD}\)</span>: Within-clusters sum of squared distances</li>
<li><span class="math inline">\(\text{BSSD}\)</span>: Between-clusters sum of squared distances</li>
</ul>
<p>It can be shown (Huygens’ theorem) that <span class="math inline">\(\text{TSS} = \text{BSS} + \text{WSS}\)</span>:</p>
<p><span class="math display" id="eq:801-5">\[
\sum_{i=1}^{n} d^2(\mathbf{x_i}, \mathbf{g}) = \sum_{k=1}^{K} n_k d^2(\mathbf{g_k}, \mathbf{g}) + \sum_{k=1}^{K} \sum_{i \in C_k} d^2(\mathbf{x_i}, \mathbf{g_k})
\tag{21.5}
\]</span></p>
<p>There are two criteria for “correct” clustering: <span class="math inline">\(\text{BSSD}\)</span> should be large
and <span class="math inline">\(\text{WSSD}\)</span> should be small. From the decomposition formula, for a fixed
number of clusters <span class="math inline">\(K\)</span>, minimizing <span class="math inline">\(\text{WSSD}\)</span> or maximizing <span class="math inline">\(\text{BSSD}\)</span> are
equivalent.</p>
<p>Notice that in discriminant analysis, we cared a lot about <span class="math inline">\(\text{BSSD}\)</span>. Now,
in clustering, we will want to minimize <span class="math inline">\(\text{WSSD}\)</span>.</p>
</div>
</div>
<div id="complexity-in-clustering" class="section level2">
<h2><span class="header-section-number">21.3</span> Complexity in Clustering</h2>
<p>Consider the following (fictitious) dataset with <span class="math inline">\(n = 5\)</span> observations and
2 variables:</p>
<pre><code>#&gt;      x    y
#&gt; 1  0.7  0.0
#&gt; 2  1.0  0.4
#&gt; 3  0.0  0.7
#&gt; 4  0.3  0.6
#&gt; 5  0.4  1.0</code></pre>
<p>Here’s the scatterplot:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-194"></span>
<img src="images/cluster/aggregation1.svg" alt="Toy data set" width="60%" />
<p class="caption">
Figure 21.9: Toy data set
</p>
</div>
<p>Euclidean squared distance between objects <span class="math inline">\(i\)</span> and <span class="math inline">\(\ell\)</span>
(i.e. <span class="math inline">\(\mathbf{x_i}\)</span> and <span class="math inline">\(\mathbf{x_{\ell}}\)</span>)</p>
<p><span class="math display" id="eq:801-6">\[
d^{2}_{i\ell} = d^2(\mathbf{x_i}, \mathbf{x_{\ell}}) = \sum_{j=1}^{p} (x_{ij} - x_{\ell j})^2 =
\| \mathbf{x_i} - \mathbf{x_{\ell}} \|^{2}_{2}
\tag{21.6}
\]</span></p>
<pre><code>      1     2     3     4     5
1  0.00                        
2  0.25  0.00                  
3  0.98  1.09  0.00            
4  0.52  0.53  0.10  0.00      
5  1.09  0.72  0.25  0.17  0.00</code></pre>
<p>We could consider two possible clustering partitions or configurations:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-196"></span>
<img src="images/cluster/aggregation2.svg" alt="Two possible clustering configurations" width="60%" />
<p class="caption">
Figure 21.10: Two possible clustering configurations
</p>
</div>
<p>Here <span class="math inline">\(n = 5\)</span> and <span class="math inline">\(K = 2\)</span></p>
<p>We could compute the centroids of each group and obtain exact values for
<span class="math inline">\(\mathrm{CSSD}_1\)</span> and <span class="math inline">\(\mathrm{CSSD}_2\)</span> (using the squared-Euclidian distance
as our metric), combine them, and obtain <span class="math inline">\(\mathrm{WSSD}\)</span>.
We would then select the partition that gives the smallest <span class="math inline">\(\mathrm{WSSD}\)</span>.</p>
<p><strong>Red clustering</strong>:</p>
<p><span class="math display">\[
W_{red} = \frac{0.25 + 0.53 + 0.52}{3} + \frac{0.25}{2} = 0.56
\]</span></p>
<p><strong>Blue clustering</strong>:</p>
<p><span class="math display">\[
W_{blue} = \frac{0.25}{2} + \frac{0.10 + 0.17 + 0.25}{3} = 0.30
\]</span></p>
<p>Smaller <span class="math inline">\(W\)</span> is better. This suggests that, to find the partition with smallest
<span class="math inline">\(W\)</span>, we would require trying all possible assignments of the <span class="math inline">\(n\)</span> objects into
<span class="math inline">\(K\)</span> groups. So why don’t we just directly find the clustering
partition <span class="math inline">\(C\)</span> that minimizes <span class="math inline">\(W\)</span>?</p>
<p>The problem is that this would only work for
a small number of individuals, and a small number of partitions. Unfortunately,
the computation cost increases <em>drastically</em> as we add more clusters and more
data points. To see this, consider four objects <span class="math inline">\(a, b, c\)</span> and <span class="math inline">\(d\)</span>. Suppose
we are interested in partitions that produce clusters of different sizes.</p>
<p>The possible (non-overlapping) partitions for <span class="math inline">\(n = 4\)</span> objects are:</p>
<ul>
<li><p>1 partition with 1 cluster:
<span class="math inline">\((abcd)\)</span></p></li>
<li><p>7 partitions with 2 clusters:
<span class="math inline">\((ab,cd), (ac,bd), (ad,bc), (a,bcd), (b,acd), (c,abd) (d,abc)\)</span></p></li>
<li><p>6 partitions with 3 clusters:
<span class="math inline">\((a,b,cd), (a,c,bd), (a,d,bc), (b,c,ad), (b,d,ac), (c,d,ab)\)</span></p></li>
<li><p>1 partition with 4 clusters:
<span class="math inline">\((a,b,c,d)\)</span></p></li>
</ul>
<p>If we wanted only one cluster, we have only <span class="math inline">\((abcd)\)</span>. If we wanted 4 clusters,
we would have <span class="math inline">\(\{(a), (b), (c), (d)\}\)</span>.</p>
<p>In general, how many possible assignments of <span class="math inline">\(n\)</span> objects into <span class="math inline">\(K\)</span> groups?</p>
<p>The number of possible assignments of <span class="math inline">\(n\)</span> objects into <span class="math inline">\(K\)</span> groups is given by:</p>
<p><span class="math display" id="eq:801-7">\[
A(n, K) = \frac{1}{K!} \sum_{k=1}^{K} (-1)^{(K-k)} \binom{K}{k} k^n
\tag{21.7}
\]</span></p>
<p>With four objects, we could form clusters of sizes 1, 2, 3, and 4.
Their corresponding number of assignments are:</p>
<ul>
<li><span class="math inline">\(A(4, 1) = 1\)</span></li>
<li><span class="math inline">\(A(4, 2) = 7\)</span></li>
<li><span class="math inline">\(A(4, 3) = 6\)</span></li>
<li><span class="math inline">\(A(4, 4) = 1\)</span></li>
</ul>
<p>Note that:</p>
<ul>
<li><span class="math inline">\(A(10, 4) = 34105\)</span>, and</li>
<li><span class="math inline">\(A(25, 4) \approx 5 \times 10^{13} \dots\)</span> huge!.</li>
</ul>
<p>Hence, with our toy data set of four objects <span class="math inline">\(a, b, c\)</span> and <span class="math inline">\(d\)</span>, the different
number of (non-overlapping) partitions for different cluster sizes is:</p>
<p><span class="math display">\[
A(4, 1) + A(4, 2) + A(4, 3) + A(4, 4) = 15
\]</span></p>
<p>It turns out that the number of (non-overlapping) partitions for <span class="math inline">\(n\)</span> objects is
the so-called <strong>Bell</strong> number <span class="math inline">\(B_n\)</span>:</p>
<p><span class="math display" id="eq:801-8">\[
B_n = \frac{1}{e} \sum_{k=1}^{\infty} \frac{k^n}{k!}
\tag{21.8}
\]</span></p>
<p>For <span class="math inline">\(n = 4\)</span> objects, we have <span class="math inline">\(B_4 = 15\)</span>. This is for only 4 objects! Imagine
if we had <span class="math inline">\(n = 30\)</span> objects (which is still relatively small), we find
<span class="math inline">\(B_{30} = 8.47 \times 10^{23} = \dots\)</span> a huge number greater than Avogadro’s
number (<span class="math inline">\(6.022 \times 10^{23}\)</span>), which is the number of
molecules in one mole of any gas!</p>
<p>As a general rule, <span class="math inline">\(B_n &gt; \text{exp}(n)\)</span>. which means this is <strong>very</strong>
computationally intractable!</p>
<p>Because of the complexity of clustering problems, we’ll have to settle for an
approximation. Typically, this means that instead of finding the global minimum
we will have to settle for finding a local minimum.</p>
<div id="methodological-considerations" class="section level4 unnumbered">
<h4>Methodological Considerations</h4>
<p>Before moving to the next chapters that cover direct partitioning,
and hierarchical partitions, we want to provide a list of general considerations
that is worth keeping in mind when performing any clustering endevour.</p>
<ul>
<li><p>The definition of natural clusters is a tricky matter.</p></li>
<li><p>Clusters are not always as obvious as presented in textbooks.</p></li>
<li><p>Cluster configurations differ according to the employed algorithm.</p></li>
<li><p>The determination of the “real” number of clusters is emperical
as much as theoretical.</p></li>
<li><p>Practical matters must be taken into account when choosing the number
of clusters.</p></li>
<li><p>Clusters are often required to be readable and interpretable,
rather than theoretically optimal.
e.g. the data are naturally clustered into three groups, but four
clusters are requested.</p></li>
<li><p>In practice, sometimes analysts mix/combine different clustering methods
to obtain <em>consolidations</em>.
e.g. Perform hierarchical clustering on a sample, determine number
of clusters, and then apply direct partitioning on the entire set.</p></li>
</ul>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="classperformance.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="kmeans.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["allmodelsarewrong.pdf", "allmodelsarewrong.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
