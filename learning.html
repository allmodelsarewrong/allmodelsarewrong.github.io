<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>7 Theoretical Framework | All Models Are Wrong</title>
  <meta name="description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  <meta name="generator" content="bookdown 0.14 and GitBook 2.6.7" />

  <meta property="og:title" content="7 Theoretical Framework | All Models Are Wrong" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://allmodelsarewrong.github.io" />
  
  <meta property="og:description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  <meta name="github-repo" content="allmodelsarewrong/allmodelsarewrong.github.io" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="7 Theoretical Framework | All Models Are Wrong" />
  
  <meta name="twitter:description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  

<meta name="author" content="by Gaston Sanchez, and Ethan Marzban" />


<meta name="date" content="2019-11-08" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="gradient.html"/>
<link rel="next" href="mse.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><b>All Models Are Wrong (CSL)</b><br>by G. Sanchez & E. Marzban</a></li>

<li class="divider"></li>
<li class="part"><span><b>I Welcome</b></span></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="about.html"><a href="about.html"><i class="fa fa-check"></i><b>1</b> About this book</a><ul>
<li class="chapter" data-level="1.1" data-path="about.html"><a href="about.html#prerequisites"><i class="fa fa-check"></i><b>1.1</b> Prerequisites</a></li>
<li class="chapter" data-level="1.2" data-path="about.html"><a href="about.html#acknowledgements"><i class="fa fa-check"></i><b>1.2</b> Acknowledgements</a></li>
</ul></li>
<li class="part"><span><b>II Intro</b></span></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#basic-notation"><i class="fa fa-check"></i><b>2.1</b> Basic Notation</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="duality.html"><a href="duality.html"><i class="fa fa-check"></i><b>3</b> Geometric Duality</a><ul>
<li class="chapter" data-level="3.1" data-path="duality.html"><a href="duality.html#rows-space"><i class="fa fa-check"></i><b>3.1</b> Rows Space</a></li>
<li class="chapter" data-level="3.2" data-path="duality.html"><a href="duality.html#columns-space"><i class="fa fa-check"></i><b>3.2</b> Columns Space</a></li>
<li class="chapter" data-level="3.3" data-path="duality.html"><a href="duality.html#cloud-of-individuals"><i class="fa fa-check"></i><b>3.3</b> Cloud of Individuals</a><ul>
<li class="chapter" data-level="3.3.1" data-path="duality.html"><a href="duality.html#average-individual"><i class="fa fa-check"></i><b>3.3.1</b> Average Individual</a></li>
<li class="chapter" data-level="3.3.2" data-path="duality.html"><a href="duality.html#centered-data"><i class="fa fa-check"></i><b>3.3.2</b> Centered Data</a></li>
<li class="chapter" data-level="3.3.3" data-path="duality.html"><a href="duality.html#distance-between-individuals"><i class="fa fa-check"></i><b>3.3.3</b> Distance between individuals</a></li>
<li class="chapter" data-level="3.3.4" data-path="duality.html"><a href="duality.html#distance-to-the-centroid"><i class="fa fa-check"></i><b>3.3.4</b> Distance to the centroid</a></li>
<li class="chapter" data-level="3.3.5" data-path="duality.html"><a href="duality.html#measures-of-dispersion"><i class="fa fa-check"></i><b>3.3.5</b> Measures of Dispersion</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="duality.html"><a href="duality.html#cloud-of-variables"><i class="fa fa-check"></i><b>3.4</b> Cloud of Variables</a><ul>
<li class="chapter" data-level="3.4.1" data-path="duality.html"><a href="duality.html#mean-of-a-variable"><i class="fa fa-check"></i><b>3.4.1</b> Mean of a Variable</a></li>
<li class="chapter" data-level="3.4.2" data-path="duality.html"><a href="duality.html#variance-of-a-variable"><i class="fa fa-check"></i><b>3.4.2</b> Variance of a Variable</a></li>
<li class="chapter" data-level="3.4.3" data-path="duality.html"><a href="duality.html#variance-with-vector-notation"><i class="fa fa-check"></i><b>3.4.3</b> Variance with Vector Notation</a></li>
<li class="chapter" data-level="3.4.4" data-path="duality.html"><a href="duality.html#standard-deviation-as-a-norm"><i class="fa fa-check"></i><b>3.4.4</b> Standard Deviation as a Norm</a></li>
<li class="chapter" data-level="3.4.5" data-path="duality.html"><a href="duality.html#covariance"><i class="fa fa-check"></i><b>3.4.5</b> Covariance</a></li>
<li class="chapter" data-level="3.4.6" data-path="duality.html"><a href="duality.html#correlation"><i class="fa fa-check"></i><b>3.4.6</b> Correlation</a></li>
<li class="chapter" data-level="3.4.7" data-path="duality.html"><a href="duality.html#geometry-of-correlation"><i class="fa fa-check"></i><b>3.4.7</b> Geometry of Correlation</a></li>
<li class="chapter" data-level="3.4.8" data-path="duality.html"><a href="duality.html#orthogonal-projections"><i class="fa fa-check"></i><b>3.4.8</b> Orthogonal Projections</a></li>
<li class="chapter" data-level="3.4.9" data-path="duality.html"><a href="duality.html#the-mean-as-an-orthogonal-projection"><i class="fa fa-check"></i><b>3.4.9</b> The mean as an orthogonal projection</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Unsupervised I: PCA</b></span></li>
<li class="chapter" data-level="4" data-path="pca.html"><a href="pca.html"><i class="fa fa-check"></i><b>4</b> Principal Components Analysis</a><ul>
<li class="chapter" data-level="4.1" data-path="pca.html"><a href="pca.html#low-dimensional-representations"><i class="fa fa-check"></i><b>4.1</b> Low-dimensional Representations</a></li>
<li class="chapter" data-level="4.2" data-path="pca.html"><a href="pca.html#projections"><i class="fa fa-check"></i><b>4.2</b> Projections</a><ul>
<li class="chapter" data-level="4.2.1" data-path="pca.html"><a href="pca.html#vector-and-scalar-projections"><i class="fa fa-check"></i><b>4.2.1</b> Vector and Scalar Projections</a></li>
<li class="chapter" data-level="4.2.2" data-path="pca.html"><a href="pca.html#projected-inertia"><i class="fa fa-check"></i><b>4.2.2</b> Projected Inertia</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="pca.html"><a href="pca.html#maximization-problem"><i class="fa fa-check"></i><b>4.3</b> Maximization Problem</a><ul>
<li class="chapter" data-level="4.3.1" data-path="pca.html"><a href="pca.html#eigenvectors-of-mathbfs"><i class="fa fa-check"></i><b>4.3.1</b> Eigenvectors of <span class="math inline">\(\mathbf{S}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="pca.html"><a href="pca.html#another-perspective-of-pca"><i class="fa fa-check"></i><b>4.4</b> Another Perspective of PCA</a><ul>
<li class="chapter" data-level="4.4.1" data-path="pca.html"><a href="pca.html#finding-principal-components"><i class="fa fa-check"></i><b>4.4.1</b> Finding Principal Components</a></li>
<li class="chapter" data-level="4.4.2" data-path="pca.html"><a href="pca.html#finding-the-first-pc"><i class="fa fa-check"></i><b>4.4.2</b> Finding the first PC</a></li>
<li class="chapter" data-level="4.4.3" data-path="pca.html"><a href="pca.html#finding-the-second-pc"><i class="fa fa-check"></i><b>4.4.3</b> Finding the second PC</a></li>
<li class="chapter" data-level="4.4.4" data-path="pca.html"><a href="pca.html#finding-all-pcs"><i class="fa fa-check"></i><b>4.4.4</b> Finding all PCs</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="pca.html"><a href="pca.html#data-decomposition-model"><i class="fa fa-check"></i><b>4.5</b> Data Decomposition Model</a><ul>
<li class="chapter" data-level="4.5.1" data-path="pca.html"><a href="pca.html#alternative-approaches"><i class="fa fa-check"></i><b>4.5.1</b> Alternative Approaches</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Linear Regression</b></span></li>
<li class="chapter" data-level="5" data-path="ols.html"><a href="ols.html"><i class="fa fa-check"></i><b>5</b> Linear Regression</a><ul>
<li class="chapter" data-level="5.1" data-path="ols.html"><a href="ols.html#motivation"><i class="fa fa-check"></i><b>5.1</b> Motivation</a></li>
<li class="chapter" data-level="5.2" data-path="ols.html"><a href="ols.html#the-ideaintuition-of-regression"><i class="fa fa-check"></i><b>5.2</b> The Idea/Intuition of Regression</a></li>
<li class="chapter" data-level="5.3" data-path="ols.html"><a href="ols.html#the-linear-regression-model"><i class="fa fa-check"></i><b>5.3</b> The Linear Regression Model</a></li>
<li class="chapter" data-level="5.4" data-path="ols.html"><a href="ols.html#the-error-measure"><i class="fa fa-check"></i><b>5.4</b> The Error Measure</a></li>
<li class="chapter" data-level="5.5" data-path="ols.html"><a href="ols.html#the-least-squares-algorithm"><i class="fa fa-check"></i><b>5.5</b> The Least Squares Algorithm</a></li>
<li class="chapter" data-level="5.6" data-path="ols.html"><a href="ols.html#geometries-of-ols"><i class="fa fa-check"></i><b>5.6</b> Geometries of OLS</a><ul>
<li class="chapter" data-level="5.6.1" data-path="ols.html"><a href="ols.html#rows-perspective"><i class="fa fa-check"></i><b>5.6.1</b> Rows Perspective</a></li>
<li class="chapter" data-level="5.6.2" data-path="ols.html"><a href="ols.html#columns-perspective"><i class="fa fa-check"></i><b>5.6.2</b> Columns Perspective</a></li>
<li class="chapter" data-level="5.6.3" data-path="ols.html"><a href="ols.html#parameters-perspective"><i class="fa fa-check"></i><b>5.6.3</b> Parameters Perspective</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="gradient.html"><a href="gradient.html"><i class="fa fa-check"></i><b>6</b> Gradient Descent</a><ul>
<li class="chapter" data-level="6.1" data-path="gradient.html"><a href="gradient.html#error-surface"><i class="fa fa-check"></i><b>6.1</b> Error Surface</a></li>
<li class="chapter" data-level="6.2" data-path="gradient.html"><a href="gradient.html#idea-of-gradient-descent"><i class="fa fa-check"></i><b>6.2</b> Idea of Gradient Descent</a><ul>
<li class="chapter" data-level="6.2.1" data-path="gradient.html"><a href="gradient.html#the-direction-of-mathbfv"><i class="fa fa-check"></i><b>6.2.1</b> The direction of <span class="math inline">\(\mathbf{v}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="gradient.html"><a href="gradient.html#gradient-descent-and-our-model"><i class="fa fa-check"></i><b>6.3</b> Gradient Descent and our Model</a></li>
</ul></li>
<li class="part"><span><b>V Learning Concepts</b></span></li>
<li class="chapter" data-level="7" data-path="learning.html"><a href="learning.html"><i class="fa fa-check"></i><b>7</b> Theoretical Framework</a><ul>
<li class="chapter" data-level="7.1" data-path="learning.html"><a href="learning.html#mental-map"><i class="fa fa-check"></i><b>7.1</b> Mental Map</a></li>
<li class="chapter" data-level="7.2" data-path="learning.html"><a href="learning.html#two-types-of-predictions"><i class="fa fa-check"></i><b>7.2</b> Two Types of Predictions</a></li>
<li class="chapter" data-level="7.3" data-path="learning.html"><a href="learning.html#two-types-of-data"><i class="fa fa-check"></i><b>7.3</b> Two Types of Data</a></li>
<li class="chapter" data-level="7.4" data-path="learning.html"><a href="learning.html#types-of-errors"><i class="fa fa-check"></i><b>7.4</b> Types of Errors</a><ul>
<li class="chapter" data-level="7.4.1" data-path="learning.html"><a href="learning.html#overall-errors"><i class="fa fa-check"></i><b>7.4.1</b> Overall Errors</a></li>
<li class="chapter" data-level="7.4.2" data-path="learning.html"><a href="learning.html#individual-errors"><i class="fa fa-check"></i><b>7.4.2</b> Individual Errors</a></li>
<li class="chapter" data-level="7.4.3" data-path="learning.html"><a href="learning.html#auxiliary-technicality"><i class="fa fa-check"></i><b>7.4.3</b> Auxiliary Technicality</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="learning.html"><a href="learning.html#noisy-targets"><i class="fa fa-check"></i><b>7.5</b> Noisy Targets</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="mse.html"><a href="mse.html"><i class="fa fa-check"></i><b>8</b> MSE of Estimator</a><ul>
<li class="chapter" data-level="8.1" data-path="mse.html"><a href="mse.html#mse-of-an-estimator"><i class="fa fa-check"></i><b>8.1</b> MSE of an Estimator</a><ul>
<li class="chapter" data-level="8.1.1" data-path="mse.html"><a href="mse.html#prototypical-cases-of-bias-and-variance"><i class="fa fa-check"></i><b>8.1.1</b> Prototypical Cases of Bias and Variance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="biasvar.html"><a href="biasvar.html"><i class="fa fa-check"></i><b>9</b> Bias-Variance Tradeoff</a><ul>
<li class="chapter" data-level="9.1" data-path="biasvar.html"><a href="biasvar.html#bias-variance-tradeoff"><i class="fa fa-check"></i><b>9.1</b> Bias-Variance Tradeoff</a></li>
<li class="chapter" data-level="9.2" data-path="biasvar.html"><a href="biasvar.html#motivation-example"><i class="fa fa-check"></i><b>9.2</b> Motivation Example</a><ul>
<li class="chapter" data-level="9.2.1" data-path="biasvar.html"><a href="biasvar.html#two-hypotheses"><i class="fa fa-check"></i><b>9.2.1</b> Two Hypotheses</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="biasvar.html"><a href="biasvar.html#learning-from-two-points"><i class="fa fa-check"></i><b>9.3</b> Learning from two points</a></li>
<li class="chapter" data-level="9.4" data-path="biasvar.html"><a href="biasvar.html#bias-variance-derivation"><i class="fa fa-check"></i><b>9.4</b> Bias-Variance Derivation</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Made with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">All Models Are Wrong</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="learning" class="section level1">
<h1><span class="header-section-number">7</span> Theoretical Framework</h1>
<p>Finally we have arrived to the part of the book in which we provide a
framework for the theory of learning. Well, to be more precise, the framework
is really about the theory of <em>supervised</em> learning.</p>
<p>At the end on the day, in supervised learning, we want a “good” model.
What does “good” model mean?
It means that we want to estimate a model <span class="math inline">\(\hat{f}\)</span> that
gives “good” predictions. What do we mean by “good” predictions? Loosely
speaking, it means that we want to obtain “accurate” predictions. Before
clarifying the notion of accurate predictions, let’s discuss first the concept
of predictions.</p>
<p>Keep in mind that most of what will be covered in this chapter is highly
theoretical. It has to do with the concepts and principles that ideally we
expect to have in a <em>perfect world</em>. Having said, the real world, more often
than not, is far from being ideal. So we will also need to discuss what to do
in practice to overcome the idealistic limitations of our theoretical assumptions.</p>
<div id="mental-map" class="section level2">
<h2><span class="header-section-number">7.1</span> Mental Map</h2>
<p>So far, we have seen an example of Unsupervised Learning (PCA), as well as one
method of Supervised Learning (regression). Now, we begin discussing learning
ideas at an abstract level.</p>
<p>Let’s return to our example of predicting NBA players’ salaries. We have a
series of <em>inputs</em>: height, weight, 2PTS, 3PTS, fouls, etc. From these inputs,
we obtain an <em>output</em>: the salary of a player. We also have a <em>target function</em>
<span class="math inline">\(f : \mathcal{X} \to \mathcal{Y}\)</span>. (i.e. a function mapping from the column spaces
of <span class="math inline">\(\mathbf{X}\)</span> to the output space <span class="math inline">\(\mathbf{y}\)</span>).
Keep in mind that this function is an <em>ideal</em>, and remains unknown throughout
our computations.</p>
<p>Here’s a metaphor that we like to use. Pretend that the target function is
some sort of mythical creature, like a unicorn (or your preferred creature).
We are trying to find this elusive guy.</p>
<p>More generally, we have a dataset
<span class="math inline">\(\mathcal{D}: \{ (\mathbf{x_1}, y_1), ( \mathbf{x_2}, y_2), \dots, (\mathbf{x_n}, y_n) \}\)</span>
(where <span class="math inline">\(\mathbf{x_i}\)</span> represents the vector of observations for the <span class="math inline">\(i\)</span>-th
player/individual).<br />
From this data, we wish to obtain a hypothesis model <span class="math inline">\(\widehat{f}: \mathcal{X} \to \mathcal{Y}\)</span> that is an approximation to the unknown function <span class="math inline">\(f\)</span>.
We can sketch these basic ideas in a sort of “mental map”
We will refer to this picture as the “diagram for unsupervised learning”.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-58"></span>
<img src="images/theory/learning-diag0.png" alt="Supervised Learning Diagram (ver 1)" width="70%" />
<p class="caption">
Figure 7.1: Supervised Learning Diagram (ver 1)
</p>
</div>
<p>The use of orange clouds around certain concepts is intentional; those concepts
appearing in orange are more intangible than those appearing in black.
For instance, we never really “discover” the function <span class="math inline">\(f\)</span> in its entirety;
rather, we just find a good enough approximation to it. As we modify our map,
we will encounter more orange concepts as well of highly theoretical nature.</p>
<p>The goal of supervised learning is to find a “good” model <span class="math inline">\(\widehat{f}\)</span>.
What is a “good” model? Well, a “good” model is a model that gives “good”
(or accurate) predictions. What do we mean by “accurate?” Well, let’s not get
too bogged down with details here.</p>
<p>The magic word here is <strong>predictions</strong>. There are two kinds of predictions:
(1) predictions <span class="math inline">\(\hat{y}_i\)</span> of observed/seen
values <span class="math inline">\(x_i\)</span>, and (2) predicitons <span class="math inline">\(\hat{y}_0\)</span> of unobserved/unseed values <span class="math inline">\(x_0\)</span>.</p>
</div>
<div id="two-types-of-predictions" class="section level2">
<h2><span class="header-section-number">7.2</span> Two Types of Predictions</h2>
<p>Think of a simple linear regression model (e.g. with one predictor). Having a
fitted model <span class="math inline">\(\hat{f}(x)\)</span>, we can use it to make two types of predictions.
On one hand, for an observed point <span class="math inline">\(x_i\)</span>, we can compute <span class="math inline">\(\hat{y}_i = \hat{f}(x_i)\)</span>.
By observed point we mean that <span class="math inline">\(x_i\)</span> was part of the data used to find <span class="math inline">\(\hat{f}()\)</span>.
On the other hand, we can also compute <span class="math inline">\(\hat{y}_0 = \hat{f}(x_0)\)</span> for a point
<span class="math inline">\(x_0\)</span> what was not part of the data used when deriving <span class="math inline">\(\hat{f}()\)</span>.</p>
<p>The two types of predictions refer to: (1) predictions <span class="math inline">\(\hat{y}_i\)</span> of observed/seen
values <span class="math inline">\(x_i\)</span>, and (2) predicitons <span class="math inline">\(\hat{y}_0\)</span> of unobserved/unseed values <span class="math inline">\(x_0\)</span>.</p>
<p>Each type of prediction is associated with a certain behavioral feature of a
model. The predictions of observed data, <span class="math inline">\(\hat{y}_i\)</span>, have to do with the
memorizing aspect (apparent error, resubstitution error). The predictions of
unobserved data, <span class="math inline">\(\hat{y}_0\)</span>, have to do with the generalization aspect
(generalization error, prediction error).</p>
<p>Both kinds of predictions are important, and each of them is interesting in
its own right. However, from the supervised learning standpoint, it is the
second type of predictions that we are ultimately interested in. That is,
we want to find models that are able to give predictions <span class="math inline">\(\hat{y}_0\)</span> as
accurate as possible for the real value <span class="math inline">\(y_0\)</span>.</p>
<p>Don’t get me wrong. Having good predictions <span class="math inline">\(\hat{y}_i\)</span> is important and desirable.
And to a large extent, it is a necessary condition for a good model. However,
it is not a sufficient condition. It is not enough to fit the observed data well,
in order to get a good predictive model. Sometimes, you can have perfect fit
of the observed data, but a terrible performance for unobserved values <span class="math inline">\(x_0\)</span>.</p>
<p>Simply put, in supervised learning we want models with a good generalization
ability.</p>
</div>
<div id="two-types-of-data" class="section level2">
<h2><span class="header-section-number">7.3</span> Two Types of Data</h2>
<p>As you can tell from the previous section, we care about two distinct types of
predictions, which in turn involve two slightly different kinds of data.
The data points <span class="math inline">\(x_i\)</span> that we use to fit a model is what we call training
or learning data. The data points <span class="math inline">\(x_0\)</span> that we use to assess the performance
of a model are points NOT supposed to be part of the training set.</p>
<p>This implies that, at least in theory, we need two finds of data sets:</p>
<ul>
<li><p><strong>In-sample data</strong>, denoted <span class="math inline">\(\mathcal{D}_{in}\)</span>, used to fit a model</p></li>
<li><p><strong>Out-of-sample data</strong>, denoted <span class="math inline">\(\mathcal{D}_{out}\)</span>, used to measure the
predictive quality of a model</p></li>
</ul>
</div>
<div id="types-of-errors" class="section level2">
<h2><span class="header-section-number">7.4</span> Types of Errors</h2>
<p>Given that we’ll have, at least theoretically, two types of data, it shouldn’t
be a surprise that we will also need separate types of errors for each kind
of data.</p>
<p>Correspondingly, we will have (at least in theory) two kinds of errors:</p>
<ul>
<li><p><strong>In-sample Error</strong>, denoted <span class="math inline">\(E_{in}\)</span></p></li>
<li><p><strong>Out-of-sample Error</strong>, denoted <span class="math inline">\(E_{out}\)</span></p></li>
</ul>
<div id="overall-errors" class="section level3">
<h3><span class="header-section-number">7.4.1</span> Overall Errors</h3>
<p>Both errors are <strong>overall measures of error</strong>. This means that we need a function
that summarizes, somehow, the total amount of error. Most overall error measures
are based on the sum of individual errors:</p>
<p><span class="math display">\[
E(\hat{f},f) = \text{measure} \left( \sum err_i (\hat{y}, y) \right)
\]</span></p>
<p>Unless otherwise said, in this book we will use the mean sum of errors are the
default overall error measure. The in-sample error is the average of pointwise
errors:</p>
<p><span class="math display">\[
E_{in} (\hat{f}, f) = \frac{1}{n} \sum_{i} err_i
\]</span></p>
<p>The out-of-sample error is the theoretical mean, or expected value, of the
pointwise errors:</p>
<p><span class="math display">\[
E_{out} (\hat{f}, f) = \mathbb{E}_{\mathcal{X}} \left[ err \left( \hat{f}(x), f(x) \right) \right]
\]</span></p>
<p>The expectation is taken over the input space <span class="math inline">\(\mathcal{X}\)</span>.
The point <span class="math inline">\(x\)</span> denotes a general data point in such space <span class="math inline">\(\mathcal{X}\)</span>.
Notice the theoretical nature of <span class="math inline">\(E_{out}\)</span>. In practice, you will never, never,
be able to compute this quantity.</p>
<p>In the machine learning literature, these overall measures of error are formally
known as <strong>cost functions</strong> or <strong>risks</strong>.</p>
</div>
<div id="individual-errors" class="section level3">
<h3><span class="header-section-number">7.4.2</span> Individual Errors</h3>
<p>What form does the individual error function, <span class="math inline">\(err()\)</span>, take? In theory, they can
take any form you want. This means that you can invent your own individual error
function. However, the most common ones are:</p>
<ul>
<li><p><strong>squared error</strong>: <span class="math inline">\(\quad err(\hat{f}, f) = \left( \hat{y}_i - y_i \right)^2\)</span></p></li>
<li><p><strong>absolute error</strong>: <span class="math inline">\(\quad err(\hat{f}, f) = \left| \hat{y}_i - y_i \right|\)</span></p></li>
<li><p><strong>misclassification error</strong>: <span class="math inline">\(\quad err(\hat{f}, f) = [\![ \hat{y}_i \neq y_i ]\!]\)</span></p></li>
</ul>
<p>In the machine learning literature, these individual errors are formally
known as <strong>loss functions</strong>.</p>
<p>Let’s update our Machine Learning Map to include error measures:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-59"></span>
<img src="images/theory/learning-diag1.png" alt="Supervised Learning Diagram (ver 2)" width="80%" />
<p class="caption">
Figure 7.2: Supervised Learning Diagram (ver 2)
</p>
</div>
</div>
<div id="auxiliary-technicality" class="section level3">
<h3><span class="header-section-number">7.4.3</span> Auxiliary Technicality</h3>
<p>We need to assume some probability distribution <span class="math inline">\(P\)</span> on <span class="math inline">\(\mathcal{X}\)</span>. That is,
we assume our vectors <span class="math inline">\(\mathbf{x_1}, \dots, \mathbf{x_n}\)</span> are independent
identically distributed (i.i.d.) samples from this distribution <span class="math inline">\(P\)</span>.
(Exactly what distribution you pick - normal, chi-squared, <span class="math inline">\(t\)</span>, etc. - is, for the moment, irrelevant).</p>
<p>Recall that out-of-sample data is highly theoretical; we will never be able to
obtain it in its entirety. The best we can do is obtain a subset of the
out-of-sample data (the test data), and estimate the rest of the data.
Our imposition of a distributional structure on <span class="math inline">\(\mathcal{X}\)</span> enables us to link the
in-sample error with the out-of-sample data.</p>
<p>Recall that our ultimate goal is to get a good function <span class="math inline">\(\widehat{f} \approx f\)</span>.
What do we mean by the symbol “<span class="math inline">\(\approx\)</span>”? Technically speaking, we want
<span class="math inline">\(E_{\mathrm{out}}(\widehat{f}) \approx 0\)</span>. If this is the case, we can safely
say that our model has been successfully trained. However, we can never check
if this is the case, since we don’t have access to <span class="math inline">\(E_{\mathrm{out}}\)</span>.</p>
<p>To solve this, we break our goal into two sub-goals:</p>
<p><span class="math display">\[
E_{\mathrm{out}} (\widehat{f}) \approx 0 \ \Rightarrow \begin{cases} E_{\mathrm{in}}(\widehat{f}) \approx 0  &amp; \text{practical result} \\ E_{\mathrm{out}}(\widehat{f}) \approx E_{\mathrm{in}}(\widehat{f}) &amp;  \text{technical/theoretical result}  \\ \end{cases}
\]</span></p>
<p>The first condition is easy to check. How do we check the second? We check the
second condition by invoking our distributional assumption <span class="math inline">\(P\)</span> on <span class="math inline">\(\mathcal{X}\)</span>.
Using our assumption, we can cite various theorems to assert that the second
result indeed holds true. We will later find ways to estimate
<span class="math inline">\(E_{\mathrm{out}}(\widehat{f})\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-60"></span>
<img src="images/theory/learning-diag2.png" alt="Supervised Learning Diagram (ver 3)" width="80%" />
<p class="caption">
Figure 7.3: Supervised Learning Diagram (ver 3)
</p>
</div>
</div>
</div>
<div id="noisy-targets" class="section level2">
<h2><span class="header-section-number">7.5</span> Noisy Targets</h2>
<p>In practice, our function won’t necessarily be a nice (or smooth) function.
Rather, there will be some <strong>noise</strong>. Hence, instead of saying <span class="math inline">\(y = f(x)\)</span> where
<span class="math inline">\(f : \mathcal{X} \to \mathcal{Y}\)</span>, a better statement might be something
like <span class="math inline">\(y = f(x) + \varepsilon\)</span>. But even this notation has some flaws;
for example, we could have multiple inputs mapping to the same output
(which cannot happen if <span class="math inline">\(f\)</span> is a proper “function”). That is, we may have two
individuals with the exact same inputs <span class="math inline">\(\mathbf{x_A} = \mathbf{x_B}\)</span> but with
different response variables <span class="math inline">\(y_A \neq y_B\)</span>. Instead, it makes more sense to
consider some <strong>target distribution</strong> <span class="math inline">\(Prob(y \mid x)\)</span>. In this way,
we can think of our data as forming a joint probability distribution
<span class="math inline">\(Prob(\mathbf{x}, y)\)</span>. That is because
<span class="math inline">\(P(\mathbf{x}, y) = P(\mathbf{x}) P(y \mid \mathbf{x})\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-61"></span>
<img src="images/theory/learning-diag3.png" alt="Supervised Learning Diagram (ver 4)" width="80%" />
<p class="caption">
Figure 7.4: Supervised Learning Diagram (ver 4)
</p>
</div>
<p>In machine learning, we want to learn the conditional distribution
<span class="math inline">\(P(y \mid \mathbf{x})\)</span>. Again, we can think of this probability as
<span class="math inline">\(y = f() + \text{noise}\)</span>. Also, sometimes the Hypothesis Sets and Learning
Algorithm boxes are combined into one, called the <strong>Learning Model</strong>.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="gradient.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="mse.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["allmodelsarewrong.pdf", "allmodelsarewrong.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
