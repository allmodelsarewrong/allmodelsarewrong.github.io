<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>8 Theoretical Framework | All Models Are Wrong: Concepts of Statistical Learning</title>
  <meta name="description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  <meta name="generator" content="bookdown 0.16 and GitBook 2.6.7" />

  <meta property="og:title" content="8 Theoretical Framework | All Models Are Wrong: Concepts of Statistical Learning" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://allmodelsarewrong.github.io" />
  
  <meta property="og:description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  <meta name="github-repo" content="allmodelsarewrong/allmodelsarewrong.github.io" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="8 Theoretical Framework | All Models Are Wrong: Concepts of Statistical Learning" />
  
  <meta name="twitter:description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  

<meta name="author" content="Gaston Sanchez Ethan Marzban" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="olsml.html"/>
<link rel="next" href="mse.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><b>All Models Are Wrong (CSL)</b><br><small>G. Sanchez & E. Marzban</small></a></li>

<li class="divider"></li>
<li class="part"><span><b>I Welcome</b></span></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="about.html"><a href="about.html"><i class="fa fa-check"></i><b>1</b> About this book</a><ul>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#prerequisites"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="part"><span><b>II Intro</b></span></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#basic-notation"><i class="fa fa-check"></i><b>2.1</b> Basic Notation</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="duality.html"><a href="duality.html"><i class="fa fa-check"></i><b>3</b> Geometric Duality</a><ul>
<li class="chapter" data-level="3.1" data-path="duality.html"><a href="duality.html#rows-space"><i class="fa fa-check"></i><b>3.1</b> Rows Space</a></li>
<li class="chapter" data-level="3.2" data-path="duality.html"><a href="duality.html#columns-space"><i class="fa fa-check"></i><b>3.2</b> Columns Space</a></li>
<li class="chapter" data-level="3.3" data-path="duality.html"><a href="duality.html#cloud-of-individuals"><i class="fa fa-check"></i><b>3.3</b> Cloud of Individuals</a><ul>
<li class="chapter" data-level="3.3.1" data-path="duality.html"><a href="duality.html#average-individual"><i class="fa fa-check"></i><b>3.3.1</b> Average Individual</a></li>
<li class="chapter" data-level="3.3.2" data-path="duality.html"><a href="duality.html#centered-data"><i class="fa fa-check"></i><b>3.3.2</b> Centered Data</a></li>
<li class="chapter" data-level="3.3.3" data-path="duality.html"><a href="duality.html#distance-between-individuals"><i class="fa fa-check"></i><b>3.3.3</b> Distance between individuals</a></li>
<li class="chapter" data-level="3.3.4" data-path="duality.html"><a href="duality.html#distance-to-the-centroid"><i class="fa fa-check"></i><b>3.3.4</b> Distance to the centroid</a></li>
<li class="chapter" data-level="3.3.5" data-path="duality.html"><a href="duality.html#measures-of-dispersion"><i class="fa fa-check"></i><b>3.3.5</b> Measures of Dispersion</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="duality.html"><a href="duality.html#cloud-of-variables"><i class="fa fa-check"></i><b>3.4</b> Cloud of Variables</a><ul>
<li class="chapter" data-level="3.4.1" data-path="duality.html"><a href="duality.html#mean-of-a-variable"><i class="fa fa-check"></i><b>3.4.1</b> Mean of a Variable</a></li>
<li class="chapter" data-level="3.4.2" data-path="duality.html"><a href="duality.html#variance-of-a-variable"><i class="fa fa-check"></i><b>3.4.2</b> Variance of a Variable</a></li>
<li class="chapter" data-level="3.4.3" data-path="duality.html"><a href="duality.html#variance-with-vector-notation"><i class="fa fa-check"></i><b>3.4.3</b> Variance with Vector Notation</a></li>
<li class="chapter" data-level="3.4.4" data-path="duality.html"><a href="duality.html#standard-deviation-as-a-norm"><i class="fa fa-check"></i><b>3.4.4</b> Standard Deviation as a Norm</a></li>
<li class="chapter" data-level="3.4.5" data-path="duality.html"><a href="duality.html#covariance"><i class="fa fa-check"></i><b>3.4.5</b> Covariance</a></li>
<li class="chapter" data-level="3.4.6" data-path="duality.html"><a href="duality.html#correlation"><i class="fa fa-check"></i><b>3.4.6</b> Correlation</a></li>
<li class="chapter" data-level="3.4.7" data-path="duality.html"><a href="duality.html#geometry-of-correlation"><i class="fa fa-check"></i><b>3.4.7</b> Geometry of Correlation</a></li>
<li class="chapter" data-level="3.4.8" data-path="duality.html"><a href="duality.html#orthogonal-projections"><i class="fa fa-check"></i><b>3.4.8</b> Orthogonal Projections</a></li>
<li class="chapter" data-level="3.4.9" data-path="duality.html"><a href="duality.html#the-mean-as-an-orthogonal-projection"><i class="fa fa-check"></i><b>3.4.9</b> The mean as an orthogonal projection</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Unsupervised I: PCA</b></span></li>
<li class="chapter" data-level="4" data-path="pca.html"><a href="pca.html"><i class="fa fa-check"></i><b>4</b> Principal Components Analysis</a><ul>
<li class="chapter" data-level="4.1" data-path="pca.html"><a href="pca.html#low-dimensional-representations"><i class="fa fa-check"></i><b>4.1</b> Low-dimensional Representations</a></li>
<li class="chapter" data-level="4.2" data-path="pca.html"><a href="pca.html#projections"><i class="fa fa-check"></i><b>4.2</b> Projections</a><ul>
<li class="chapter" data-level="4.2.1" data-path="pca.html"><a href="pca.html#vector-and-scalar-projections"><i class="fa fa-check"></i><b>4.2.1</b> Vector and Scalar Projections</a></li>
<li class="chapter" data-level="4.2.2" data-path="pca.html"><a href="pca.html#projected-inertia"><i class="fa fa-check"></i><b>4.2.2</b> Projected Inertia</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="pca.html"><a href="pca.html#maximization-problem"><i class="fa fa-check"></i><b>4.3</b> Maximization Problem</a><ul>
<li class="chapter" data-level="4.3.1" data-path="pca.html"><a href="pca.html#eigenvectors-of-mathbfs"><i class="fa fa-check"></i><b>4.3.1</b> Eigenvectors of <span class="math inline">\(\mathbf{S}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="pca.html"><a href="pca.html#another-perspective-of-pca"><i class="fa fa-check"></i><b>4.4</b> Another Perspective of PCA</a><ul>
<li class="chapter" data-level="4.4.1" data-path="pca.html"><a href="pca.html#finding-principal-components"><i class="fa fa-check"></i><b>4.4.1</b> Finding Principal Components</a></li>
<li class="chapter" data-level="4.4.2" data-path="pca.html"><a href="pca.html#finding-the-first-pc"><i class="fa fa-check"></i><b>4.4.2</b> Finding the first PC</a></li>
<li class="chapter" data-level="4.4.3" data-path="pca.html"><a href="pca.html#finding-the-second-pc"><i class="fa fa-check"></i><b>4.4.3</b> Finding the second PC</a></li>
<li class="chapter" data-level="4.4.4" data-path="pca.html"><a href="pca.html#finding-all-pcs"><i class="fa fa-check"></i><b>4.4.4</b> Finding all PCs</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="pca.html"><a href="pca.html#data-decomposition-model"><i class="fa fa-check"></i><b>4.5</b> Data Decomposition Model</a><ul>
<li class="chapter" data-level="4.5.1" data-path="pca.html"><a href="pca.html#alternative-approaches"><i class="fa fa-check"></i><b>4.5.1</b> Alternative Approaches</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Linear Regression</b></span></li>
<li class="chapter" data-level="5" data-path="ols.html"><a href="ols.html"><i class="fa fa-check"></i><b>5</b> Linear Regression</a><ul>
<li class="chapter" data-level="5.1" data-path="ols.html"><a href="ols.html#motivation"><i class="fa fa-check"></i><b>5.1</b> Motivation</a></li>
<li class="chapter" data-level="5.2" data-path="ols.html"><a href="ols.html#the-ideaintuition-of-regression"><i class="fa fa-check"></i><b>5.2</b> The Idea/Intuition of Regression</a></li>
<li class="chapter" data-level="5.3" data-path="ols.html"><a href="ols.html#the-linear-regression-model"><i class="fa fa-check"></i><b>5.3</b> The Linear Regression Model</a></li>
<li class="chapter" data-level="5.4" data-path="ols.html"><a href="ols.html#the-error-measure"><i class="fa fa-check"></i><b>5.4</b> The Error Measure</a></li>
<li class="chapter" data-level="5.5" data-path="ols.html"><a href="ols.html#the-least-squares-algorithm"><i class="fa fa-check"></i><b>5.5</b> The Least Squares Algorithm</a></li>
<li class="chapter" data-level="5.6" data-path="ols.html"><a href="ols.html#geometries-of-ols"><i class="fa fa-check"></i><b>5.6</b> Geometries of OLS</a><ul>
<li class="chapter" data-level="5.6.1" data-path="ols.html"><a href="ols.html#rows-perspective"><i class="fa fa-check"></i><b>5.6.1</b> Rows Perspective</a></li>
<li class="chapter" data-level="5.6.2" data-path="ols.html"><a href="ols.html#columns-perspective"><i class="fa fa-check"></i><b>5.6.2</b> Columns Perspective</a></li>
<li class="chapter" data-level="5.6.3" data-path="ols.html"><a href="ols.html#parameters-perspective"><i class="fa fa-check"></i><b>5.6.3</b> Parameters Perspective</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="gradient.html"><a href="gradient.html"><i class="fa fa-check"></i><b>6</b> Gradient Descent</a><ul>
<li class="chapter" data-level="6.1" data-path="gradient.html"><a href="gradient.html#error-surface"><i class="fa fa-check"></i><b>6.1</b> Error Surface</a></li>
<li class="chapter" data-level="6.2" data-path="gradient.html"><a href="gradient.html#idea-of-gradient-descent"><i class="fa fa-check"></i><b>6.2</b> Idea of Gradient Descent</a></li>
<li class="chapter" data-level="6.3" data-path="gradient.html"><a href="gradient.html#moving-down-an-error-surface"><i class="fa fa-check"></i><b>6.3</b> Moving Down an Error Surface</a><ul>
<li class="chapter" data-level="6.3.1" data-path="gradient.html"><a href="gradient.html#the-direction-of-mathbfv"><i class="fa fa-check"></i><b>6.3.1</b> The direction of <span class="math inline">\(\mathbf{v}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="gradient.html"><a href="gradient.html#gd-algorithm-for-linear-regression"><i class="fa fa-check"></i><b>6.4</b> GD Algorithm for Linear Regression</a><ul>
<li class="chapter" data-level="6.4.1" data-path="gradient.html"><a href="gradient.html#gd-algorithm-in-vector-matrix-notation"><i class="fa fa-check"></i><b>6.4.1</b> GD Algorithm in vector-matrix notation</a></li>
<li class="chapter" data-level="6.4.2" data-path="gradient.html"><a href="gradient.html#gd-algorithm-in-pointwise-notation"><i class="fa fa-check"></i><b>6.4.2</b> GD algorithm in pointwise notation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="olsml.html"><a href="olsml.html"><i class="fa fa-check"></i><b>7</b> Regression via Maximum Likelihood</a><ul>
<li class="chapter" data-level="7.1" data-path="olsml.html"><a href="olsml.html#linear-regression-reminder"><i class="fa fa-check"></i><b>7.1</b> Linear Regression Reminder</a><ul>
<li class="chapter" data-level="7.1.1" data-path="olsml.html"><a href="olsml.html#maximum-likelihood"><i class="fa fa-check"></i><b>7.1.1</b> Maximum Likelihood</a></li>
<li class="chapter" data-level="7.1.2" data-path="olsml.html"><a href="olsml.html#ml-estimator-of-sigma2"><i class="fa fa-check"></i><b>7.1.2</b> ML Estimator of <span class="math inline">\(\sigma^2\)</span></a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>V Learning Concepts</b></span></li>
<li class="chapter" data-level="8" data-path="learning.html"><a href="learning.html"><i class="fa fa-check"></i><b>8</b> Theoretical Framework</a><ul>
<li class="chapter" data-level="8.1" data-path="learning.html"><a href="learning.html#mental-map"><i class="fa fa-check"></i><b>8.1</b> Mental Map</a></li>
<li class="chapter" data-level="8.2" data-path="learning.html"><a href="learning.html#kinds-of-predictions"><i class="fa fa-check"></i><b>8.2</b> Kinds of Predictions</a><ul>
<li class="chapter" data-level="8.2.1" data-path="learning.html"><a href="learning.html#two-types-of-data"><i class="fa fa-check"></i><b>8.2.1</b> Two Types of Data</a></li>
<li class="chapter" data-level="8.2.2" data-path="learning.html"><a href="learning.html#two-types-of-predictions"><i class="fa fa-check"></i><b>8.2.2</b> Two Types of Predictions</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="learning.html"><a href="learning.html#two-types-of-errors"><i class="fa fa-check"></i><b>8.3</b> Two Types of Errors</a><ul>
<li class="chapter" data-level="8.3.1" data-path="learning.html"><a href="learning.html#individual-errors"><i class="fa fa-check"></i><b>8.3.1</b> Individual Errors</a></li>
<li class="chapter" data-level="8.3.2" data-path="learning.html"><a href="learning.html#overall-errors"><i class="fa fa-check"></i><b>8.3.2</b> Overall Errors</a></li>
<li class="chapter" data-level="8.3.3" data-path="learning.html"><a href="learning.html#probability-as-an-auxiliary-technicality"><i class="fa fa-check"></i><b>8.3.3</b> Probability as an Auxiliary Technicality</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="learning.html"><a href="learning.html#noisy-targets"><i class="fa fa-check"></i><b>8.4</b> Noisy Targets</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="mse.html"><a href="mse.html"><i class="fa fa-check"></i><b>9</b> MSE of Estimator</a><ul>
<li class="chapter" data-level="9.1" data-path="mse.html"><a href="mse.html#mse-of-an-estimator"><i class="fa fa-check"></i><b>9.1</b> MSE of an Estimator</a><ul>
<li class="chapter" data-level="9.1.1" data-path="mse.html"><a href="mse.html#prototypical-cases-of-bias-and-variance"><i class="fa fa-check"></i><b>9.1.1</b> Prototypical Cases of Bias and Variance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="biasvar.html"><a href="biasvar.html"><i class="fa fa-check"></i><b>10</b> Bias-Variance Tradeoff</a><ul>
<li class="chapter" data-level="10.1" data-path="biasvar.html"><a href="biasvar.html#introduction"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="biasvar.html"><a href="biasvar.html#motivation-example"><i class="fa fa-check"></i><b>10.2</b> Motivation Example</a><ul>
<li class="chapter" data-level="10.2.1" data-path="biasvar.html"><a href="biasvar.html#two-hypotheses"><i class="fa fa-check"></i><b>10.2.1</b> Two Hypotheses</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="biasvar.html"><a href="biasvar.html#learning-from-two-points"><i class="fa fa-check"></i><b>10.3</b> Learning from two points</a></li>
<li class="chapter" data-level="10.4" data-path="biasvar.html"><a href="biasvar.html#bias-variance-derivation"><i class="fa fa-check"></i><b>10.4</b> Bias-Variance Derivation</a><ul>
<li class="chapter" data-level="10.4.1" data-path="biasvar.html"><a href="biasvar.html#out-of-sample-predictions"><i class="fa fa-check"></i><b>10.4.1</b> Out-of-Sample Predictions</a></li>
<li class="chapter" data-level="10.4.2" data-path="biasvar.html"><a href="biasvar.html#noisy-target"><i class="fa fa-check"></i><b>10.4.2</b> Noisy Target</a></li>
<li class="chapter" data-level="10.4.3" data-path="biasvar.html"><a href="biasvar.html#types-of-theoretical-mses"><i class="fa fa-check"></i><b>10.4.3</b> Types of Theoretical MSEs</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="biasvar.html"><a href="biasvar.html#the-tradeoff"><i class="fa fa-check"></i><b>10.5</b> The Tradeoff</a><ul>
<li class="chapter" data-level="10.5.1" data-path="biasvar.html"><a href="biasvar.html#bias-variance-tradeoff-picture"><i class="fa fa-check"></i><b>10.5.1</b> Bias-Variance Tradeoff Picture</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="overfit.html"><a href="overfit.html"><i class="fa fa-check"></i><b>11</b> Overfitting</a><ul>
<li class="chapter" data-level="11.1" data-path="overfit.html"><a href="overfit.html#introduction-1"><i class="fa fa-check"></i><b>11.1</b> Introduction</a><ul>
<li class="chapter" data-level="11.1.1" data-path="overfit.html"><a href="overfit.html#bias-variance-reminder-and-pitfalls"><i class="fa fa-check"></i><b>11.1.1</b> Bias-Variance Reminder and Pitfalls</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="overfit.html"><a href="overfit.html#simulation"><i class="fa fa-check"></i><b>11.2</b> Simulation</a><ul>
<li class="chapter" data-level="11.2.1" data-path="overfit.html"><a href="overfit.html#underfitting-overfitting-and-okayfitting"><i class="fa fa-check"></i><b>11.2.1</b> Underfitting, Overfitting and Okayfitting</a></li>
<li class="chapter" data-level="11.2.2" data-path="overfit.html"><a href="overfit.html#more-learning-sets"><i class="fa fa-check"></i><b>11.2.2</b> More learning sets</a></li>
<li class="chapter" data-level="11.2.3" data-path="overfit.html"><a href="overfit.html#when-does-overfitting-occurs"><i class="fa fa-check"></i><b>11.2.3</b> When does overfitting occurs?</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="overfit.html"><a href="overfit.html#in-summary"><i class="fa fa-check"></i><b>11.3</b> In Summary</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="phases.html"><a href="phases.html"><i class="fa fa-check"></i><b>12</b> Learning Phases</a><ul>
<li class="chapter" data-level="12.1" data-path="phases.html"><a href="phases.html#introduction-2"><i class="fa fa-check"></i><b>12.1</b> Introduction</a></li>
<li class="chapter" data-level="12.2" data-path="phases.html"><a href="phases.html#model-assessment"><i class="fa fa-check"></i><b>12.2</b> Model Assessment</a><ul>
<li class="chapter" data-level="12.2.1" data-path="phases.html"><a href="phases.html#holdout-test-set"><i class="fa fa-check"></i><b>12.2.1</b> Holdout Test Set</a></li>
<li class="chapter" data-level="12.2.2" data-path="phases.html"><a href="phases.html#why-does-a-test-set-work"><i class="fa fa-check"></i><b>12.2.2</b> Why does a test set work?</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="phases.html"><a href="phases.html#model-selection"><i class="fa fa-check"></i><b>12.3</b> Model Selection</a><ul>
<li class="chapter" data-level="12.3.1" data-path="phases.html"><a href="phases.html#three-way-holdout-method"><i class="fa fa-check"></i><b>12.3.1</b> Three-way Holdout Method</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="phases.html"><a href="phases.html#model-training"><i class="fa fa-check"></i><b>12.4</b> Model Training</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="resampling.html"><a href="resampling.html"><i class="fa fa-check"></i><b>13</b> Resampling Approaches</a><ul>
<li class="chapter" data-level="13.1" data-path="resampling.html"><a href="resampling.html#general-sampling-blueprint"><i class="fa fa-check"></i><b>13.1</b> General Sampling Blueprint</a></li>
<li class="chapter" data-level="13.2" data-path="resampling.html"><a href="resampling.html#monte-carlo-cross-validation"><i class="fa fa-check"></i><b>13.2</b> Monte Carlo Cross Validation</a></li>
<li class="chapter" data-level="13.3" data-path="resampling.html"><a href="resampling.html#bootstrap-method"><i class="fa fa-check"></i><b>13.3</b> Bootstrap Method</a></li>
<li class="chapter" data-level="13.4" data-path="resampling.html"><a href="resampling.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>13.4</b> <span class="math inline">\(K\)</span>-Fold Cross-Validation</a><ul>
<li class="chapter" data-level="13.4.1" data-path="resampling.html"><a href="resampling.html#leave-one-out-cross-validation-loocv"><i class="fa fa-check"></i><b>13.4.1</b> Leave-One-Out Cross Validation (LOOCV)</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>VI Regularization</b></span></li>
<li class="chapter" data-level="14" data-path="regular.html"><a href="regular.html"><i class="fa fa-check"></i><b>14</b> Regularization Techniques</a><ul>
<li class="chapter" data-level="14.1" data-path="regular.html"><a href="regular.html#multicollinearity-issues"><i class="fa fa-check"></i><b>14.1</b> Multicollinearity Issues</a><ul>
<li class="chapter" data-level="14.1.1" data-path="regular.html"><a href="regular.html#toy-example"><i class="fa fa-check"></i><b>14.1.1</b> Toy Example</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="regular.html"><a href="regular.html#irregular-coefficients"><i class="fa fa-check"></i><b>14.2</b> Irregular Coefficients</a></li>
<li class="chapter" data-level="14.3" data-path="regular.html"><a href="regular.html#connection-to-regularization"><i class="fa fa-check"></i><b>14.3</b> Connection to Regularization</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="pcr.html"><a href="pcr.html"><i class="fa fa-check"></i><b>15</b> Principal Components Regression</a><ul>
<li class="chapter" data-level="15.1" data-path="pcr.html"><a href="pcr.html#motivation-example-1"><i class="fa fa-check"></i><b>15.1</b> Motivation Example</a></li>
<li class="chapter" data-level="15.2" data-path="pcr.html"><a href="pcr.html#the-pcr-model"><i class="fa fa-check"></i><b>15.2</b> The PCR Model</a></li>
<li class="chapter" data-level="15.3" data-path="pcr.html"><a href="pcr.html#how-does-pcr-work"><i class="fa fa-check"></i><b>15.3</b> How does PCR work?</a><ul>
<li class="chapter" data-level="15.3.1" data-path="pcr.html"><a href="pcr.html#transition-formula"><i class="fa fa-check"></i><b>15.3.1</b> Transition Formula</a></li>
<li class="chapter" data-level="15.3.2" data-path="pcr.html"><a href="pcr.html#size-of-coefficients"><i class="fa fa-check"></i><b>15.3.2</b> Size of Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="pcr.html"><a href="pcr.html#selecting-number-of-pcs"><i class="fa fa-check"></i><b>15.4</b> Selecting Number of PCs</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="pls.html"><a href="pls.html"><i class="fa fa-check"></i><b>16</b> Partial Least Squares Regression</a><ul>
<li class="chapter" data-level="16.1" data-path="pls.html"><a href="pls.html#motivation-example-2"><i class="fa fa-check"></i><b>16.1</b> Motivation Example</a></li>
<li class="chapter" data-level="16.2" data-path="pls.html"><a href="pls.html#the-plsr-model"><i class="fa fa-check"></i><b>16.2</b> The PLSR Model</a></li>
<li class="chapter" data-level="16.3" data-path="pls.html"><a href="pls.html#how-does-plsr-work"><i class="fa fa-check"></i><b>16.3</b> How does PLSR work?</a></li>
<li class="chapter" data-level="16.4" data-path="pls.html"><a href="pls.html#plsr-algorithm"><i class="fa fa-check"></i><b>16.4</b> PLSR Algorithm</a><ul>
<li class="chapter" data-level="16.4.1" data-path="pls.html"><a href="pls.html#pls-solution-with-original-variables"><i class="fa fa-check"></i><b>16.4.1</b> PLS Solution with original variables</a></li>
<li class="chapter" data-level="16.4.2" data-path="pls.html"><a href="pls.html#some-properties"><i class="fa fa-check"></i><b>16.4.2</b> Some Properties</a></li>
<li class="chapter" data-level="16.4.3" data-path="pls.html"><a href="pls.html#pls-regression-for-price-of-cars"><i class="fa fa-check"></i><b>16.4.3</b> PLS Regression for Price of cars</a></li>
<li class="chapter" data-level="16.4.4" data-path="pls.html"><a href="pls.html#size-of-coefficients-1"><i class="fa fa-check"></i><b>16.4.4</b> Size of Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="16.5" data-path="pls.html"><a href="pls.html#selecting-number-of-pls-components"><i class="fa fa-check"></i><b>16.5</b> Selecting Number of PLS Components</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="ridge.html"><a href="ridge.html"><i class="fa fa-check"></i><b>17</b> Ridge Regression</a><ul>
<li class="chapter" data-level="17.1" data-path="ridge.html"><a href="ridge.html#a-new-minimization-problem"><i class="fa fa-check"></i><b>17.1</b> A New Minimization Problem</a><ul>
<li class="chapter" data-level="17.1.1" data-path="ridge.html"><a href="ridge.html#constraining-regression-coefficients"><i class="fa fa-check"></i><b>17.1.1</b> Constraining Regression Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="ridge.html"><a href="ridge.html#a-new-minimization-solution"><i class="fa fa-check"></i><b>17.2</b> A New Minimization Solution</a></li>
<li class="chapter" data-level="17.3" data-path="ridge.html"><a href="ridge.html#what-does-rr-accomplish"><i class="fa fa-check"></i><b>17.3</b> What does RR accomplish?</a><ul>
<li class="chapter" data-level="17.3.1" data-path="ridge.html"><a href="ridge.html#how-to-find-lambda"><i class="fa fa-check"></i><b>17.3.1</b> How to find <span class="math inline">\(\lambda\)</span>?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="lasso.html"><a href="lasso.html"><i class="fa fa-check"></i><b>18</b> Lasso Regression</a><ul>
<li class="chapter" data-level="18.1" data-path="lasso.html"><a href="lasso.html#mathematical-setup"><i class="fa fa-check"></i><b>18.1</b> Mathematical Setup</a><ul>
<li class="chapter" data-level="18.1.1" data-path="lasso.html"><a href="lasso.html#closed-form"><i class="fa fa-check"></i><b>18.1.1</b> Closed Form?</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="lasso.html"><a href="lasso.html#geometric-visualization"><i class="fa fa-check"></i><b>18.2</b> Geometric Visualization</a><ul>
<li class="chapter" data-level="18.2.1" data-path="lasso.html"><a href="lasso.html#some-more-math-variable-selection-in-action"><i class="fa fa-check"></i><b>18.2.1</b> Some More Math: Variable Selection in Action</a></li>
<li class="chapter" data-level="18.2.2" data-path="lasso.html"><a href="lasso.html#example-with-mtcars"><i class="fa fa-check"></i><b>18.2.2</b> Example with <code>mtcars</code></a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="lasso.html"><a href="lasso.html#going-beyond"><i class="fa fa-check"></i><b>18.3</b> Going Beyond</a></li>
</ul></li>
<li class="part"><span><b>VII Extending Linear Regression</b></span></li>
<li class="chapter" data-level="19" data-path="linear-extensions.html"><a href="linear-extensions.html"><i class="fa fa-check"></i><b>19</b> Beyond Linear Regression</a><ul>
<li class="chapter" data-level="19.1" data-path="linear-extensions.html"><a href="linear-extensions.html#introduction-3"><i class="fa fa-check"></i><b>19.1</b> Introduction</a></li>
<li class="chapter" data-level="19.2" data-path="linear-extensions.html"><a href="linear-extensions.html#expanding-the-regression-horizon"><i class="fa fa-check"></i><b>19.2</b> Expanding the Regression Horizon</a><ul>
<li class="chapter" data-level="19.2.1" data-path="linear-extensions.html"><a href="linear-extensions.html#linearity"><i class="fa fa-check"></i><b>19.2.1</b> Linearity</a></li>
<li class="chapter" data-level="19.2.2" data-path="linear-extensions.html"><a href="linear-extensions.html#parametric-and-nonparametric"><i class="fa fa-check"></i><b>19.2.2</b> Parametric and Nonparametric</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="linear-extensions.html"><a href="linear-extensions.html#transforming-features"><i class="fa fa-check"></i><b>19.3</b> Transforming Features</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="basis.html"><a href="basis.html"><i class="fa fa-check"></i><b>20</b> Basis Expansion</a><ul>
<li class="chapter" data-level="20.1" data-path="basis.html"><a href="basis.html#basis-functions"><i class="fa fa-check"></i><b>20.1</b> Basis Functions</a></li>
<li class="chapter" data-level="20.2" data-path="basis.html"><a href="basis.html#linear-regression"><i class="fa fa-check"></i><b>20.2</b> Linear Regression</a></li>
<li class="chapter" data-level="20.3" data-path="basis.html"><a href="basis.html#polynomial-regression"><i class="fa fa-check"></i><b>20.3</b> Polynomial Regression</a></li>
<li class="chapter" data-level="20.4" data-path="basis.html"><a href="basis.html#gaussian-rbfs"><i class="fa fa-check"></i><b>20.4</b> Gaussian RBF’s</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="nonparametric.html"><a href="nonparametric.html"><i class="fa fa-check"></i><b>21</b> Nonparametric Regression</a><ul>
<li class="chapter" data-level="21.1" data-path="nonparametric.html"><a href="nonparametric.html#conditional-averages"><i class="fa fa-check"></i><b>21.1</b> Conditional Averages</a></li>
<li class="chapter" data-level="21.2" data-path="nonparametric.html"><a href="nonparametric.html#looking-at-the-neighbors"><i class="fa fa-check"></i><b>21.2</b> Looking at the Neighbors</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="knn.html"><a href="knn.html"><i class="fa fa-check"></i><b>22</b> Nearest Neighbor Estimates</a><ul>
<li class="chapter" data-level="22.1" data-path="knn.html"><a href="knn.html#introduction-4"><i class="fa fa-check"></i><b>22.1</b> Introduction</a></li>
<li class="chapter" data-level="22.2" data-path="knn.html"><a href="knn.html#k-nearest-neighbors-knn"><i class="fa fa-check"></i><b>22.2</b> <span class="math inline">\(k\)</span> Nearest Neighbors (KNN)</a><ul>
<li class="chapter" data-level="22.2.1" data-path="knn.html"><a href="knn.html#distance-measures"><i class="fa fa-check"></i><b>22.2.1</b> Distance Measures</a></li>
<li class="chapter" data-level="22.2.2" data-path="knn.html"><a href="knn.html#knn-estimator"><i class="fa fa-check"></i><b>22.2.2</b> KNN Estimator</a></li>
<li class="chapter" data-level="22.2.3" data-path="knn.html"><a href="knn.html#how-to-find-k"><i class="fa fa-check"></i><b>22.2.3</b> How to find <span class="math inline">\(k\)</span>?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="23" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html"><i class="fa fa-check"></i><b>23</b> Kernel Smoothers</a><ul>
<li class="chapter" data-level="23.1" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#introduction-5"><i class="fa fa-check"></i><b>23.1</b> Introduction</a></li>
<li class="chapter" data-level="23.2" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#kernel-smoothers-1"><i class="fa fa-check"></i><b>23.2</b> Kernel Smoothers</a><ul>
<li class="chapter" data-level="23.2.1" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#kernel-functions"><i class="fa fa-check"></i><b>23.2.1</b> Kernel Functions</a></li>
<li class="chapter" data-level="23.2.2" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#weights-from-kernels"><i class="fa fa-check"></i><b>23.2.2</b> Weights from Kernels</a></li>
<li class="chapter" data-level="23.2.3" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#kernel-estimator"><i class="fa fa-check"></i><b>23.2.3</b> Kernel Estimator</a></li>
</ul></li>
<li class="chapter" data-level="23.3" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#local-polynomial-estimators"><i class="fa fa-check"></i><b>23.3</b> Local Polynomial Estimators</a></li>
</ul></li>
<li class="part"><span><b>VIII Classification</b></span></li>
<li class="chapter" data-level="24" data-path="classif.html"><a href="classif.html"><i class="fa fa-check"></i><b>24</b> Classification</a><ul>
<li class="chapter" data-level="24.1" data-path="classif.html"><a href="classif.html#introduction-6"><i class="fa fa-check"></i><b>24.1</b> Introduction</a><ul>
<li class="chapter" data-level="24.1.1" data-path="classif.html"><a href="classif.html#credit-score-example"><i class="fa fa-check"></i><b>24.1.1</b> Credit Score Example</a></li>
<li class="chapter" data-level="24.1.2" data-path="classif.html"><a href="classif.html#toy-example-1"><i class="fa fa-check"></i><b>24.1.2</b> Toy Example</a></li>
<li class="chapter" data-level="24.1.3" data-path="classif.html"><a href="classif.html#two-class-problem"><i class="fa fa-check"></i><b>24.1.3</b> Two-class Problem</a></li>
<li class="chapter" data-level="24.1.4" data-path="classif.html"><a href="classif.html#bayes-rule-reminder"><i class="fa fa-check"></i><b>24.1.4</b> Bayes’ Rule Reminder</a></li>
</ul></li>
<li class="chapter" data-level="24.2" data-path="classif.html"><a href="classif.html#bayes-classifier"><i class="fa fa-check"></i><b>24.2</b> Bayes Classifier</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="logistic.html"><a href="logistic.html"><i class="fa fa-check"></i><b>25</b> Logistic Regression</a><ul>
<li class="chapter" data-level="25.1" data-path="logistic.html"><a href="logistic.html#motivation-1"><i class="fa fa-check"></i><b>25.1</b> Motivation</a><ul>
<li class="chapter" data-level="25.1.1" data-path="logistic.html"><a href="logistic.html#first-approach-fitting-a-line"><i class="fa fa-check"></i><b>25.1.1</b> First Approach: Fitting a Line</a></li>
<li class="chapter" data-level="25.1.2" data-path="logistic.html"><a href="logistic.html#second-approach-harsh-thresholding"><i class="fa fa-check"></i><b>25.1.2</b> Second Approach: Harsh Thresholding</a></li>
<li class="chapter" data-level="25.1.3" data-path="logistic.html"><a href="logistic.html#third-approach-conditional-means"><i class="fa fa-check"></i><b>25.1.3</b> Third Approach: Conditional Means</a></li>
</ul></li>
<li class="chapter" data-level="25.2" data-path="logistic.html"><a href="logistic.html#logistic-regression-model"><i class="fa fa-check"></i><b>25.2</b> Logistic Regression Model</a><ul>
<li class="chapter" data-level="25.2.1" data-path="logistic.html"><a href="logistic.html#the-criterion-being-optimized"><i class="fa fa-check"></i><b>25.2.1</b> The Criterion Being Optimized</a></li>
<li class="chapter" data-level="25.2.2" data-path="logistic.html"><a href="logistic.html#another-way-to-solve-logistic-regression"><i class="fa fa-check"></i><b>25.2.2</b> Another Way to Solve Logistic Regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="26" data-path="discrim.html"><a href="discrim.html"><i class="fa fa-check"></i><b>26</b> Preamble for Discriminant Analysis</a><ul>
<li class="chapter" data-level="26.1" data-path="discrim.html"><a href="discrim.html#motivation-2"><i class="fa fa-check"></i><b>26.1</b> Motivation</a><ul>
<li class="chapter" data-level="26.1.1" data-path="discrim.html"><a href="discrim.html#distinguishing-species"><i class="fa fa-check"></i><b>26.1.1</b> Distinguishing Species</a></li>
<li class="chapter" data-level="26.1.2" data-path="discrim.html"><a href="discrim.html#sum-of-squares-decomposition"><i class="fa fa-check"></i><b>26.1.2</b> Sum of Squares Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="26.2" data-path="discrim.html"><a href="discrim.html#derived-ratios-from-sum-of-squares"><i class="fa fa-check"></i><b>26.2</b> Derived Ratios from Sum-of-Squares</a><ul>
<li class="chapter" data-level="26.2.1" data-path="discrim.html"><a href="discrim.html#correlation-ratio"><i class="fa fa-check"></i><b>26.2.1</b> Correlation Ratio</a></li>
<li class="chapter" data-level="26.2.2" data-path="discrim.html"><a href="discrim.html#f-ratio"><i class="fa fa-check"></i><b>26.2.2</b> F-Ratio</a></li>
</ul></li>
<li class="chapter" data-level="26.3" data-path="discrim.html"><a href="discrim.html#geometric-perspective"><i class="fa fa-check"></i><b>26.3</b> Geometric Perspective</a><ul>
<li class="chapter" data-level="26.3.1" data-path="discrim.html"><a href="discrim.html#clouds-from-class-structure"><i class="fa fa-check"></i><b>26.3.1</b> Clouds from Class Structure</a></li>
<li class="chapter" data-level="26.3.2" data-path="discrim.html"><a href="discrim.html#dispersion-decomposition"><i class="fa fa-check"></i><b>26.3.2</b> Dispersion Decomposition</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="27" data-path="cda.html"><a href="cda.html"><i class="fa fa-check"></i><b>27</b> Canonical Discriminant Analysis</a><ul>
<li class="chapter" data-level="27.1" data-path="cda.html"><a href="cda.html#cda-semi-supervised-aspect"><i class="fa fa-check"></i><b>27.1</b> CDA: Semi-Supervised Aspect</a></li>
<li class="chapter" data-level="27.2" data-path="cda.html"><a href="cda.html#looking-for-a-discriminant-axis"><i class="fa fa-check"></i><b>27.2</b> Looking for a discriminant axis</a></li>
<li class="chapter" data-level="27.3" data-path="cda.html"><a href="cda.html#looking-for-a-compromise-criterion"><i class="fa fa-check"></i><b>27.3</b> Looking for a Compromise Criterion</a><ul>
<li class="chapter" data-level="27.3.1" data-path="cda.html"><a href="cda.html#correlation-ratio-criterion"><i class="fa fa-check"></i><b>27.3.1</b> Correlation Ratio Criterion</a></li>
<li class="chapter" data-level="27.3.2" data-path="cda.html"><a href="cda.html#f-ratio-criterion"><i class="fa fa-check"></i><b>27.3.2</b> F-ratio Criterion</a></li>
<li class="chapter" data-level="27.3.3" data-path="cda.html"><a href="cda.html#a-special-pca"><i class="fa fa-check"></i><b>27.3.3</b> A Special PCA</a></li>
</ul></li>
<li class="chapter" data-level="27.4" data-path="cda.html"><a href="cda.html#cda-supervised-aspect"><i class="fa fa-check"></i><b>27.4</b> CDA: Supervised Aspect</a><ul>
<li class="chapter" data-level="27.4.1" data-path="cda.html"><a href="cda.html#distance-behind-cda"><i class="fa fa-check"></i><b>27.4.1</b> Distance behind CDA</a></li>
<li class="chapter" data-level="27.4.2" data-path="cda.html"><a href="cda.html#predictive-idea"><i class="fa fa-check"></i><b>27.4.2</b> Predictive Idea</a></li>
<li class="chapter" data-level="27.4.3" data-path="cda.html"><a href="cda.html#cda-classifier"><i class="fa fa-check"></i><b>27.4.3</b> CDA Classifier</a></li>
<li class="chapter" data-level="27.4.4" data-path="cda.html"><a href="cda.html#limitations-of-cda-classifier"><i class="fa fa-check"></i><b>27.4.4</b> Limitations of CDA classifier</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="28" data-path="discanalysis.html"><a href="discanalysis.html"><i class="fa fa-check"></i><b>28</b> Discriminant Analysis</a><ul>
<li class="chapter" data-level="28.1" data-path="discanalysis.html"><a href="discanalysis.html#probabilistic-da"><i class="fa fa-check"></i><b>28.1</b> Probabilistic DA</a><ul>
<li class="chapter" data-level="28.1.1" data-path="discanalysis.html"><a href="discanalysis.html#normal-distributions"><i class="fa fa-check"></i><b>28.1.1</b> Normal Distributions</a></li>
<li class="chapter" data-level="28.1.2" data-path="discanalysis.html"><a href="discanalysis.html#estimating-parameters-of-normal-distributions"><i class="fa fa-check"></i><b>28.1.2</b> Estimating Parameters of Normal Distributions</a></li>
</ul></li>
<li class="chapter" data-level="28.2" data-path="discanalysis.html"><a href="discanalysis.html#discriminant-functions"><i class="fa fa-check"></i><b>28.2</b> Discriminant Functions</a></li>
<li class="chapter" data-level="28.3" data-path="discanalysis.html"><a href="discanalysis.html#quadratic-discriminant-analysis-qda"><i class="fa fa-check"></i><b>28.3</b> Quadratic Discriminant Analysis (QDA)</a></li>
<li class="chapter" data-level="28.4" data-path="discanalysis.html"><a href="discanalysis.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>28.4</b> Linear Discriminant Analysis</a><ul>
<li class="chapter" data-level="28.4.1" data-path="discanalysis.html"><a href="discanalysis.html#canonical-discriminant-analysis"><i class="fa fa-check"></i><b>28.4.1</b> Canonical Discriminant Analysis</a></li>
<li class="chapter" data-level="28.4.2" data-path="discanalysis.html"><a href="discanalysis.html#naive-bayes"><i class="fa fa-check"></i><b>28.4.2</b> Naive Bayes</a></li>
<li class="chapter" data-level="28.4.3" data-path="discanalysis.html"><a href="discanalysis.html#fifth-case"><i class="fa fa-check"></i><b>28.4.3</b> Fifth Case</a></li>
</ul></li>
<li class="chapter" data-level="28.5" data-path="discanalysis.html"><a href="discanalysis.html#comparing-the-cases"><i class="fa fa-check"></i><b>28.5</b> Comparing the Cases</a></li>
</ul></li>
<li class="chapter" data-level="29" data-path="classperformance.html"><a href="classperformance.html"><i class="fa fa-check"></i><b>29</b> Performance of Classifiers</a><ul>
<li class="chapter" data-level="29.1" data-path="classperformance.html"><a href="classperformance.html#classification-error-measures"><i class="fa fa-check"></i><b>29.1</b> Classification Error Measures</a><ul>
<li class="chapter" data-level="29.1.1" data-path="classperformance.html"><a href="classperformance.html#errors-for-binary-response"><i class="fa fa-check"></i><b>29.1.1</b> Errors for Binary Response</a></li>
<li class="chapter" data-level="29.1.2" data-path="classperformance.html"><a href="classperformance.html#error-for-categorical-response"><i class="fa fa-check"></i><b>29.1.2</b> Error for Categorical Response</a></li>
</ul></li>
<li class="chapter" data-level="29.2" data-path="classperformance.html"><a href="classperformance.html#confusion-matrices"><i class="fa fa-check"></i><b>29.2</b> Confusion Matrices</a></li>
<li class="chapter" data-level="29.3" data-path="classperformance.html"><a href="classperformance.html#binary-response-example"><i class="fa fa-check"></i><b>29.3</b> Binary Response Example</a><ul>
<li class="chapter" data-level="29.3.1" data-path="classperformance.html"><a href="classperformance.html#application-for-checking-account"><i class="fa fa-check"></i><b>29.3.1</b> Application for Checking Account</a></li>
<li class="chapter" data-level="29.3.2" data-path="classperformance.html"><a href="classperformance.html#application-for-loan"><i class="fa fa-check"></i><b>29.3.2</b> Application for Loan</a></li>
</ul></li>
<li class="chapter" data-level="29.4" data-path="classperformance.html"><a href="classperformance.html#decision-rules-and-errors"><i class="fa fa-check"></i><b>29.4</b> Decision Rules and Errors</a></li>
<li class="chapter" data-level="29.5" data-path="classperformance.html"><a href="classperformance.html#roc-curves"><i class="fa fa-check"></i><b>29.5</b> ROC Curves</a><ul>
<li class="chapter" data-level="29.5.1" data-path="classperformance.html"><a href="classperformance.html#graphing-roc-curves"><i class="fa fa-check"></i><b>29.5.1</b> Graphing ROC curves</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IX Unsupervised II: Clustering</b></span></li>
<li class="chapter" data-level="30" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>30</b> Clustering</a><ul>
<li class="chapter" data-level="30.1" data-path="clustering.html"><a href="clustering.html#about-clustering"><i class="fa fa-check"></i><b>30.1</b> About Clustering</a><ul>
<li class="chapter" data-level="30.1.1" data-path="clustering.html"><a href="clustering.html#types-of-clustering"><i class="fa fa-check"></i><b>30.1.1</b> Types of Clustering</a></li>
<li class="chapter" data-level="30.1.2" data-path="clustering.html"><a href="clustering.html#hard-clustering"><i class="fa fa-check"></i><b>30.1.2</b> Hard Clustering</a></li>
</ul></li>
<li class="chapter" data-level="30.2" data-path="clustering.html"><a href="clustering.html#dispersion-measures"><i class="fa fa-check"></i><b>30.2</b> Dispersion Measures</a></li>
<li class="chapter" data-level="30.3" data-path="clustering.html"><a href="clustering.html#complexity-in-clustering"><i class="fa fa-check"></i><b>30.3</b> Complexity in Clustering</a></li>
</ul></li>
<li class="chapter" data-level="31" data-path="kmeans.html"><a href="kmeans.html"><i class="fa fa-check"></i><b>31</b> K-Means</a><ul>
<li class="chapter" data-level="31.1" data-path="kmeans.html"><a href="kmeans.html#toy-example-2"><i class="fa fa-check"></i><b>31.1</b> Toy Example</a></li>
<li class="chapter" data-level="31.2" data-path="kmeans.html"><a href="kmeans.html#what-does-k-means-do"><i class="fa fa-check"></i><b>31.2</b> What does K-means do?</a></li>
<li class="chapter" data-level="31.3" data-path="kmeans.html"><a href="kmeans.html#k-means-algorithms"><i class="fa fa-check"></i><b>31.3</b> K-Means Algorithms</a><ul>
<li class="chapter" data-level="31.3.1" data-path="kmeans.html"><a href="kmeans.html#classic-version"><i class="fa fa-check"></i><b>31.3.1</b> Classic Version</a></li>
<li class="chapter" data-level="31.3.2" data-path="kmeans.html"><a href="kmeans.html#moving-centers-algorithm"><i class="fa fa-check"></i><b>31.3.2</b> Moving Centers Algorithm</a></li>
<li class="chapter" data-level="31.3.3" data-path="kmeans.html"><a href="kmeans.html#dynamic-clouds"><i class="fa fa-check"></i><b>31.3.3</b> Dynamic Clouds</a></li>
<li class="chapter" data-level="31.3.4" data-path="kmeans.html"><a href="kmeans.html#choosing-k"><i class="fa fa-check"></i><b>31.3.4</b> Choosing <span class="math inline">\(K\)</span></a></li>
<li class="chapter" data-level="31.3.5" data-path="kmeans.html"><a href="kmeans.html#comments"><i class="fa fa-check"></i><b>31.3.5</b> Comments</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="32" data-path="hclus.html"><a href="hclus.html"><i class="fa fa-check"></i><b>32</b> Hierarchical Clustering</a><ul>
<li class="chapter" data-level="32.1" data-path="hclus.html"><a href="hclus.html#agglomerative-methods"><i class="fa fa-check"></i><b>32.1</b> Agglomerative Methods</a></li>
<li class="chapter" data-level="32.2" data-path="hclus.html"><a href="hclus.html#example-single-linkage"><i class="fa fa-check"></i><b>32.2</b> Example: Single Linkage</a><ul>
<li class="chapter" data-level="32.2.1" data-path="hclus.html"><a href="hclus.html#dendrogram"><i class="fa fa-check"></i><b>32.2.1</b> Dendrogram</a></li>
</ul></li>
<li class="chapter" data-level="32.3" data-path="hclus.html"><a href="hclus.html#example-complete-linkage"><i class="fa fa-check"></i><b>32.3</b> Example: Complete Linkage</a><ul>
<li class="chapter" data-level="32.3.1" data-path="hclus.html"><a href="hclus.html#cutting-dendograms"><i class="fa fa-check"></i><b>32.3.1</b> Cutting Dendograms</a></li>
<li class="chapter" data-level="32.3.2" data-path="hclus.html"><a href="hclus.html#pros-and-coons"><i class="fa fa-check"></i><b>32.3.2</b> Pros and COons</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>X Tree-based Methods</b></span></li>
<li class="chapter" data-level="33" data-path="trees.html"><a href="trees.html"><i class="fa fa-check"></i><b>33</b> Intro to Decision Trees</a><ul>
<li class="chapter" data-level="33.1" data-path="trees.html"><a href="trees.html#introduction-7"><i class="fa fa-check"></i><b>33.1</b> Introduction</a></li>
<li class="chapter" data-level="33.2" data-path="trees.html"><a href="trees.html#some-terminology"><i class="fa fa-check"></i><b>33.2</b> Some Terminology</a><ul>
<li class="chapter" data-level="33.2.1" data-path="trees.html"><a href="trees.html#binary-trees"><i class="fa fa-check"></i><b>33.2.1</b> Binary Trees</a></li>
</ul></li>
<li class="chapter" data-level="33.3" data-path="trees.html"><a href="trees.html#space-partitions"><i class="fa fa-check"></i><b>33.3</b> Space Partitions</a><ul>
<li class="chapter" data-level="33.3.1" data-path="trees.html"><a href="trees.html#the-process-of-building-a-tree"><i class="fa fa-check"></i><b>33.3.1</b> The Process of Building a Tree</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="34" data-path="tree-impurities.html"><a href="tree-impurities.html"><i class="fa fa-check"></i><b>34</b> Binary Splits and Impurity</a><ul>
<li class="chapter" data-level="34.1" data-path="tree-impurities.html"><a href="tree-impurities.html#binary-partitions"><i class="fa fa-check"></i><b>34.1</b> Binary Partitions</a><ul>
<li class="chapter" data-level="34.1.1" data-path="tree-impurities.html"><a href="tree-impurities.html#splits-of-binary-variables"><i class="fa fa-check"></i><b>34.1.1</b> Splits of Binary variables</a></li>
<li class="chapter" data-level="34.1.2" data-path="tree-impurities.html"><a href="tree-impurities.html#splits-of-nominal-variables"><i class="fa fa-check"></i><b>34.1.2</b> Splits of Nominal Variables</a></li>
<li class="chapter" data-level="34.1.3" data-path="tree-impurities.html"><a href="tree-impurities.html#splits-of-ordinal-variables"><i class="fa fa-check"></i><b>34.1.3</b> Splits of Ordinal Variables</a></li>
<li class="chapter" data-level="34.1.4" data-path="tree-impurities.html"><a href="tree-impurities.html#splits-continuous-variables"><i class="fa fa-check"></i><b>34.1.4</b> Splits Continuous variables</a></li>
</ul></li>
<li class="chapter" data-level="34.2" data-path="tree-impurities.html"><a href="tree-impurities.html#measures-of-impurity"><i class="fa fa-check"></i><b>34.2</b> Measures of Impurity</a><ul>
<li class="chapter" data-level="34.2.1" data-path="tree-impurities.html"><a href="tree-impurities.html#entropy"><i class="fa fa-check"></i><b>34.2.1</b> Entropy</a></li>
<li class="chapter" data-level="34.2.2" data-path="tree-impurities.html"><a href="tree-impurities.html#the-math-behind-entropy"><i class="fa fa-check"></i><b>34.2.2</b> The Math Behind Entropy</a></li>
<li class="chapter" data-level="34.2.3" data-path="tree-impurities.html"><a href="tree-impurities.html#gini-impurity"><i class="fa fa-check"></i><b>34.2.3</b> Gini Impurity</a></li>
<li class="chapter" data-level="34.2.4" data-path="tree-impurities.html"><a href="tree-impurities.html#variance-based-impurity"><i class="fa fa-check"></i><b>34.2.4</b> Variance-based Impurity</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="35" data-path="tree-splits.html"><a href="tree-splits.html"><i class="fa fa-check"></i><b>35</b> Splitting Nodes</a><ul>
<li class="chapter" data-level="35.1" data-path="tree-splits.html"><a href="tree-splits.html#entropy-based-splits"><i class="fa fa-check"></i><b>35.1</b> Entropy-based Splits</a></li>
<li class="chapter" data-level="35.2" data-path="tree-splits.html"><a href="tree-splits.html#gini-index-based-splits"><i class="fa fa-check"></i><b>35.2</b> Gini-index based Splits</a></li>
<li class="chapter" data-level="35.3" data-path="tree-splits.html"><a href="tree-splits.html#looking-for-the-best-split"><i class="fa fa-check"></i><b>35.3</b> Looking for the best split</a></li>
</ul></li>
<li class="chapter" data-level="36" data-path="tree-basics.html"><a href="tree-basics.html"><i class="fa fa-check"></i><b>36</b> Building Binary Trees</a><ul>
<li class="chapter" data-level="36.1" data-path="tree-basics.html"><a href="tree-basics.html#node-splitting-stopping-criteria"><i class="fa fa-check"></i><b>36.1</b> Node-Splitting Stopping Criteria</a></li>
<li class="chapter" data-level="36.2" data-path="tree-basics.html"><a href="tree-basics.html#issues-with-trees"><i class="fa fa-check"></i><b>36.2</b> Issues with Trees</a><ul>
<li class="chapter" data-level="36.2.1" data-path="tree-basics.html"><a href="tree-basics.html#bias-variance-of-trees"><i class="fa fa-check"></i><b>36.2.1</b> Bias-Variance of Trees</a></li>
</ul></li>
<li class="chapter" data-level="36.3" data-path="tree-basics.html"><a href="tree-basics.html#pruning-a-tree"><i class="fa fa-check"></i><b>36.3</b> Pruning a Tree</a></li>
<li class="chapter" data-level="36.4" data-path="tree-basics.html"><a href="tree-basics.html#pros-and-cons-of-trees"><i class="fa fa-check"></i><b>36.4</b> Pros and Cons of Trees</a><ul>
<li class="chapter" data-level="36.4.1" data-path="tree-basics.html"><a href="tree-basics.html#advantages-of-trees"><i class="fa fa-check"></i><b>36.4.1</b> Advantages of Trees</a></li>
<li class="chapter" data-level="36.4.2" data-path="tree-basics.html"><a href="tree-basics.html#disadvantages-of-trees"><i class="fa fa-check"></i><b>36.4.2</b> Disadvantages of Trees</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="37" data-path="bagging.html"><a href="bagging.html"><i class="fa fa-check"></i><b>37</b> Bagging</a><ul>
<li class="chapter" data-level="37.1" data-path="bagging.html"><a href="bagging.html#introduction-8"><i class="fa fa-check"></i><b>37.1</b> Introduction</a><ul>
<li class="chapter" data-level="37.1.1" data-path="bagging.html"><a href="bagging.html#idea-of-bagging"><i class="fa fa-check"></i><b>37.1.1</b> Idea of Bagging</a></li>
</ul></li>
<li class="chapter" data-level="37.2" data-path="bagging.html"><a href="bagging.html#why-bother-bagging"><i class="fa fa-check"></i><b>37.2</b> Why Bother Bagging?</a></li>
</ul></li>
<li class="chapter" data-level="38" data-path="forest.html"><a href="forest.html"><i class="fa fa-check"></i><b>38</b> Random Forests</a><ul>
<li class="chapter" data-level="38.1" data-path="forest.html"><a href="forest.html#introduction-9"><i class="fa fa-check"></i><b>38.1</b> Introduction</a></li>
<li class="chapter" data-level="38.2" data-path="forest.html"><a href="forest.html#algorithm-2"><i class="fa fa-check"></i><b>38.2</b> Algorithm</a><ul>
<li class="chapter" data-level="38.2.1" data-path="forest.html"><a href="forest.html#two-sources-of-randomness"><i class="fa fa-check"></i><b>38.2.1</b> Two Sources of Randomness</a></li>
<li class="chapter" data-level="38.2.2" data-path="forest.html"><a href="forest.html#regressions-and-classification-forests"><i class="fa fa-check"></i><b>38.2.2</b> Regressions and Classification Forests</a></li>
<li class="chapter" data-level="38.2.3" data-path="forest.html"><a href="forest.html#key-advantage-of-random-forests"><i class="fa fa-check"></i><b>38.2.3</b> Key Advantage of Random Forests</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Made with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">All Models Are Wrong: Concepts of Statistical Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="learning" class="section level1">
<h1><span class="header-section-number">8</span> Theoretical Framework</h1>
<p>Finally we have arrived to the part of the book in which we provide a
framework for the theory of learning. Well, to be more precise, the framework
is really about the theory of <em>supervised</em> learning. The purpose of this chapter
is to give you a <em>mental map</em> of the conceptual elements that are present
in a supervised learning problem.</p>
<p>Keep in mind that most of what is covered in this chapter is highly theoretical.
It has to do with the concepts and principles that ideally we
expect to find in a prediction task (e.g. regression, classification).
Having said, we will also need to discuss what to do in practice in order to
handle most of these theoretical elements (in the upcoming chapters).</p>
<div id="mental-map" class="section level2">
<h2><span class="header-section-number">8.1</span> Mental Map</h2>
<p>So far, we have seen an example of Unsupervised Learning (PCA), as well as one
method of Supervised Learning (linear regression). Now, we begin discussing
learning ideas at an abstract level.</p>
<p>Let’s return to our example of predicting NBA players’ salaries. Suppose we have
data of NBA players: player’s height, player’s weight, player’s years of
professional experience, player’s number of 2 points, player’s number of 3
points, etc. And assume also that we have data about the players’ salaries.</p>
<table>
<thead>
<tr class="header">
<th align="center">Player</th>
<th align="center">Height</th>
<th align="center">Weight</th>
<th align="center">Yrs Expr</th>
<th align="center">2-Pts</th>
<th align="center">3-Pts</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="center"><span class="math inline">\(\bigcirc\)</span></td>
<td align="center"><span class="math inline">\(\bigcirc\)</span></td>
<td align="center"><span class="math inline">\(\bigcirc\)</span></td>
<td align="center"><span class="math inline">\(\bigcirc\)</span></td>
<td align="center"><span class="math inline">\(\bigcirc\)</span></td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="center"><span class="math inline">\(\bigcirc\)</span></td>
<td align="center"><span class="math inline">\(\bigcirc\)</span></td>
<td align="center"><span class="math inline">\(\bigcirc\)</span></td>
<td align="center"><span class="math inline">\(\bigcirc\)</span></td>
<td align="center"><span class="math inline">\(\bigcirc\)</span></td>
</tr>
<tr class="odd">
<td align="center">…</td>
<td align="center">…</td>
<td align="center">…</td>
<td align="center">…</td>
<td align="center">…</td>
<td align="center">…</td>
</tr>
<tr class="even">
<td align="center">n</td>
<td align="center"><span class="math inline">\(\bigcirc\)</span></td>
<td align="center"><span class="math inline">\(\bigcirc\)</span></td>
<td align="center"><span class="math inline">\(\bigcirc\)</span></td>
<td align="center"><span class="math inline">\(\bigcirc\)</span></td>
<td align="center"><span class="math inline">\(\bigcirc\)</span></td>
</tr>
</tbody>
</table>
<p>We are interested in predicting the salary of players, which is the <strong>output</strong>
variable, denoted as <span class="math inline">\(Y\)</span>. The rest of the variables (e.g. height, weight,
experience, 2PTS, 3PTS) are the <strong>inputs</strong> denoted as <span class="math inline">\(X_1, \dots, X_p\)</span>.</p>
<p>Likewise, we assume the existence of a <strong>target function</strong> <span class="math inline">\(f()\)</span>:</p>
<p><span class="math display" id="eq:401-1">\[
\textsf{target function} \qquad f : \mathcal{X} \to \mathcal{Y}
\tag{8.1}
\]</span></p>
<p>which is a function mapping from the inputs’ space <span class="math inline">\(\mathcal{X}\)</span> to the output
space <span class="math inline">\(\mathcal{y}\)</span>. Keep in mind that this function is an <em>ideal</em>, and remains
unknown throughout our computations. Here’s a metaphor that we like to use.
Pretend that the target function is some sort of mythical creature, like a
unicorn (or your preferred creature). We are trying to find this elusive guy.</p>
<p>More generally, we have a <strong>dataset</strong> <span class="math inline">\(\mathcal{D}\)</span>:</p>
<p><span class="math display" id="eq:401-2">\[
\textsf{dataset} \qquad \mathcal{D} = \large \{ (\mathbf{x_1}, y_1), ( \mathbf{x_2}, y_2), \dots, (\mathbf{x_n}, y_n) \large \}
\tag{8.2}
\]</span></p>
<p>where <span class="math inline">\(\mathbf{x_i}\)</span> represents the vector of features for the <span class="math inline">\(i\)</span>-th
player, and <span class="math inline">\(y_i\)</span> represents his salary.</p>
<p>From this data, we wish to obtain a fitted model, formally known as
a <strong>hypothesis model</strong> <span class="math inline">\(\widehat{f}()\)</span>:</p>
<p><span class="math display" id="eq:401-3">\[
\textsf{hypothesis model} \qquad \widehat{f}: \mathcal{X} \to \mathcal{Y}
\tag{8.3}
\]</span></p>
<p>and then use <span class="math inline">\(\widehat{f}\)</span> to approximate the unknown target function <span class="math inline">\(f\)</span>.</p>
<p>In order to find <span class="math inline">\(\widehat{f}()\)</span>, we typically consider a set of candidate
models, also known as a <em>hypothesis set</em>,
<span class="math inline">\(\mathcal{H} = \{ h_1, h_2, \dots, h_m \}\)</span>. The selected hypothesized model
<span class="math inline">\(h^*_m\)</span> will be the one used as our <em>final</em> model <span class="math inline">\(\widehat{f}\)</span>.</p>
<p>We can sketch these basic ideas in a sort of mental map; we will refer to the
following picture as the “diagram for supervised learning”.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-2"></span>
<img src="images/theory/learning-diag0.svg" alt="Supervised Learning Diagram (ver 1)" width="85%" />
<p class="caption">
Figure 8.1: Supervised Learning Diagram (ver 1)
</p>
</div>
<p>Here’s how to read this diagram. We are using orange clouds around those concepts
that are more intangible than those appearing within rectangular or oval shapes.
One of these orange clouds is the unknown target function <span class="math inline">\(f\)</span>, which as its
name indicates it is <strong>unknown</strong>. This implies that we never really “discover”
the target function <span class="math inline">\(f\)</span> in its entirety; rather, we just find a good enough
approximation to it by estimating <span class="math inline">\(\widehat{f}\)</span>. Now, as you can tell from the
mental map, the other orange clound has to do with precisely this idea of
<em>good approximation</em>: <span class="math inline">\(\widehat{f} \approx f\)</span>. It is also theoretical because of
what we just said: that we don’t know <span class="math inline">\(f\)</span>. We should let you know that as we
modify our diagram, we will encounter more orange concepts as well of highly
theoretical nature.</p>
<p>Now let’s turn our attention to the blue rectangular elements. One of them
is the data set <span class="math inline">\(\mathcal{D}\)</span> which is influenced by the unknown target function.
The other blue rectangle has to do with a set of candidate models <span class="math inline">\(\mathcal{H}\)</span>,
which is sometimes referred to as the <em>hypothesis set</em>. Both the data set and
the set of models are tangible ingredients. Moreover, the set of candidate
models is totally under our control. We get to decide what type of models
we want try out (e.g. linear model, polynomial models, non-parametric models).</p>
<p>Then we go to the yellow oval shape which right now is simply labeled as the
<em>learning algorithm</em> <span class="math inline">\(\mathcal{A}\)</span>. This corresponds to the set of instructions
and steps to be carried out when learning from data. It is also the stage of
the diagram in which most computations take place.</p>
<p>Finally, we arrive at the yellow rectangle containing the final model
<span class="math inline">\(\widehat{f}\)</span>. This is supposed to be the selected model by the learning
algorithm from the set of hypothesis models. Ideally, this model is the one
that provides a good approximation for the target function <span class="math inline">\(f\)</span>.</p>
<p>Going back to the holy grail of supervised learning, our goal is to find a
model <span class="math inline">\(\widehat{f}\)</span> that gives “good” or accurate predictions. Before discussing
what exactly do we mean by “accurate”, we first need to talk about predictions.</p>
</div>
<div id="kinds-of-predictions" class="section level2">
<h2><span class="header-section-number">8.2</span> Kinds of Predictions</h2>
<p>What is the ultimate goal in supervised learning? Quick answer, we want a
“good” model. What does “good” model mean? Simply put, it means that we want to
estimate an unknown model <span class="math inline">\(f\)</span> with some model <span class="math inline">\(\widehat{f}\)</span> that gives “good”
predictions. What do we mean by “good” predictions? Loosely speaking, it means
that we want to obtain “accurate” predictions. Before clarifying the notion of
<em>accurate predictions</em>, let’s discuss first the concept of <strong>predictions</strong>.</p>
<p>Think of a simple linear regression model (e.g. with one predictor). Having a
fitted model <span class="math inline">\(\widehat{f}(x)\)</span>, we can use it to make two types of predictions.
On one hand, for an observed point <span class="math inline">\(x_i\)</span>, we can compute <span class="math inline">\(\hat{y}_i = \widehat{f}(x_i)\)</span>.
By observed point we mean that <span class="math inline">\(x_i\)</span> was part of the data used to find <span class="math inline">\(\widehat{f}\)</span>.
On the other hand, we can also compute <span class="math inline">\(\hat{y}_0 = \widehat{f}(x_0)\)</span> for a point
<span class="math inline">\(x_0\)</span> what was not part of the data used when deriving <span class="math inline">\(\widehat{f}\)</span>.</p>
<div id="two-types-of-data" class="section level3">
<h3><span class="header-section-number">8.2.1</span> Two Types of Data</h3>
<p>The two distinct types of predictions involve two slightly different kinds of
data. The data points <span class="math inline">\(x_i\)</span> that we use to fit a model is what most authors
call learning data. The data points <span class="math inline">\(x_0\)</span> that we use to assess the performance
of a model are points NOT supposed to be part of the learning set. In this
book we are going to give special names to these data points. We will use the
name <em>In-sample data</em> to refer to the learning data, and <em>Out-of-sample data</em>
to refer to those data points not used to learn a model.</p>
<p>To summarize, we have at least in theory, two kinds of data sets:</p>
<ul>
<li><p><strong>In-sample data</strong>, denoted <span class="math inline">\(\mathcal{D}_{in}\)</span>, used to fit a model</p></li>
<li><p><strong>Out-of-sample data</strong>, denoted <span class="math inline">\(\mathcal{D}_{out}\)</span>, used to measure the
predictive quality of a model</p></li>
</ul>
</div>
<div id="two-types-of-predictions" class="section level3">
<h3><span class="header-section-number">8.2.2</span> Two Types of Predictions</h3>
<p>Given the two kinds of data points, we have two corresponding types of
predictions:</p>
<ol style="list-style-type: decimal">
<li><p>predictions <span class="math inline">\(\hat{y}_i\)</span> of observed/seen values <span class="math inline">\(x_i\)</span></p></li>
<li><p>predicitons <span class="math inline">\(\hat{y}_0\)</span> of unobserved/unseed values <span class="math inline">\(x_0\)</span></p></li>
</ol>
<p>Each type of prediction is associated with a certain behavioral feature of a
model. The predictions of observed data, <span class="math inline">\(\hat{y}_i\)</span>, have to do with the
memorizing aspect (i.e. apparent error, resubstitution error). The predictions
of unobserved data, <span class="math inline">\(\hat{y}_0\)</span>, have to do with the generalization aspect
(i.e. generalization error, prediction error).</p>
<p>Both kinds of predictions are important, and each of them is interesting in
its own right. However, from the supervised learning standpoint, it is the
second type of predictions that we are ultimately interested in. That is,
we want to find models that are able to give predictions <span class="math inline">\(\hat{y}_0\)</span> as
accurate as possible for the real value <span class="math inline">\(y_0\)</span>.</p>
<p>Don’t get us wrong. Having good predictions <span class="math inline">\(\hat{y}_i\)</span> of observed values
is important and desirable. And to a large extent, it is a necessary condition
for a good model. However, it is not a sufficient condition. It is not enough
to fit the observed data well,
in order to get a good predictive model. Sometimes, you can perfectly fit
the observed data, but have a terrible performance for unobserved values <span class="math inline">\(x_0\)</span>.</p>
</div>
</div>
<div id="two-types-of-errors" class="section level2">
<h2><span class="header-section-number">8.3</span> Two Types of Errors</h2>
<p>In theory, we are dealing with two types of predictions, each of which is
associated to certain types of data points.</p>
<p>Because we are interested in obtaining models that give accurate predictions,
we need a way to measure the accuracy of such predictions. At the conceptual
level we need some mechanism to quantify how different the fitted model is
from the target function <span class="math inline">\(f\)</span>:</p>
<p><span class="math display">\[
\widehat{f} \text{ -vs- } f
\]</span></p>
<p>It would be nice to have some measure of how much discrepancy exists between
the estimated model and the target model. This means that we need a function
that summarizes, somehow, the total amount of error. We will denote such term as
an <em>Overall Measure of Error</em>:</p>
<p><span class="math display" id="eq:401-4">\[
\text{Overall Measure of Error:} \quad E(\widehat{f},f) 
\tag{8.4}
\]</span></p>
<p>The typical way in which an overall measure of error is defined is in terms
of individual or pointwise errors <span class="math inline">\(err_i(\hat{y}_i, y_i)\)</span> that quantify the
difference between an observed value <span class="math inline">\(y_i\)</span> and its predicted value <span class="math inline">\(\hat{y}_i\)</span>.
As a matter of fact, most overall errors focus on the addition of the pointwise
errors:</p>
<p><span class="math display" id="eq:401-5">\[
E(\widehat{f},f) = \text{measure} \left( \sum err_i(\hat{y}_i, y_i) \right )
\tag{8.5}
\]</span></p>
<p>Unless otherwise said, in this book we will use the mean sum of errors as the
default overall error measure:</p>
<p><span class="math display" id="eq:401-6">\[
E(\widehat{f},f) = \frac{1}{n} \left( \sum_i err_i (\hat{y}_i, y_i) \right)
\tag{8.6}
\]</span></p>
<div id="individual-errors" class="section level3">
<h3><span class="header-section-number">8.3.1</span> Individual Errors</h3>
<p>What form does the individual error function, <span class="math inline">\(err()\)</span>, take? In theory, they can
take any form you want. This means that you can invent your own individual error
function. However, the most common ones are:</p>
<ul>
<li><p><strong>squared error</strong>: <span class="math inline">\(\quad err(\widehat{f}, f) = \left( \hat{y}_i - y_i \right)^2\)</span></p></li>
<li><p><strong>absolute error</strong>: <span class="math inline">\(\quad err(\widehat{f}, f) = \left| \hat{y}_i - y_i \right|\)</span></p></li>
<li><p><strong>misclassification error</strong>: <span class="math inline">\(\quad err(\widehat{f}, f) = [\![ \hat{y}_i \neq y_i ]\!]\)</span></p></li>
</ul>
<p>In the machine learning literature, these individual errors are formally
known as <strong>loss functions</strong>.</p>
</div>
<div id="overall-errors" class="section level3">
<h3><span class="header-section-number">8.3.2</span> Overall Errors</h3>
<p>As you can imagine, there are actually two types of overall error measures,
based on the type of data that is used to assess the individual errors:</p>
<ul>
<li><p><strong>In-sample Error</strong>, denoted <span class="math inline">\(E_{in}\)</span></p></li>
<li><p><strong>Out-of-sample Error</strong>, denoted <span class="math inline">\(E_{out}\)</span></p></li>
</ul>
<p>The in-sample error is typically defined as the average of pointwise errors
from data points of the in-sample data <span class="math inline">\(\mathcal{D}_{in}\)</span>:</p>
<p><span class="math display" id="eq:401-7">\[
E_{in} (\widehat{f}, f) = \frac{1}{n} \sum_{i} err_i
\tag{8.7}
\]</span></p>
<p>The out-of-sample error is the theoretical mean, or expected value, of the
pointwise errors over the entire input space:</p>
<p><span class="math display" id="eq:401-8">\[
E_{out} (\widehat{f}, f) = \mathbb{E}_{\mathcal{X}} \left[ err \left( \widehat{f}(x), f(x) \right) \right]
\tag{8.8}
\]</span></p>
<p>The point <span class="math inline">\(x\)</span> denotes a general data point in the input space <span class="math inline">\(\mathcal{X}\)</span>.
And as we said, the expectation is taken over the input space <span class="math inline">\(\mathcal{X}\)</span>.
Which means that the nature of <span class="math inline">\(E_{out}\)</span> is highly theoretical. In practice,
you will never, never, be able to compute this quantity.</p>
<p>In the machine learning literature, these overall measures of error tend to
be formally known as <strong>cost functions</strong> or <strong>risks</strong>.</p>
<p>Let’s update our supervised learning diagram to include error measures (see
figure below). We add a new box (in blue) involving an overall error measure
as well as some pointwise error function.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-3"></span>
<img src="images/theory/learning-diag1.svg" alt="Supervised Learning Diagram (ver 2)" width="85%" />
<p class="caption">
Figure 8.2: Supervised Learning Diagram (ver 2)
</p>
</div>
<p>Notice the connections of the error elements to both the learning algortihm
<span class="math inline">\(\mathcal{A}\)</span>, and the final model <span class="math inline">\(\widehat{f}\)</span>. Why is this? As we will learn in
the upcoming chapters, learning algorithms use—implicitly or explicitly—a
pointwise error function <span class="math inline">\(err()\)</span>. In turn, in order to determine which candidate
model <span class="math inline">\(h()\)</span> is the best approximation to the target model <span class="math inline">\(f()\)</span>, we need to
use an overall measure of error <span class="math inline">\(E()\)</span>.</p>
</div>
<div id="probability-as-an-auxiliary-technicality" class="section level3">
<h3><span class="header-section-number">8.3.3</span> Probability as an Auxiliary Technicality</h3>
<p>For theoretical reasons, we need to assume some probability distribution over
the input space: <span class="math inline">\(P\)</span> on <span class="math inline">\(\mathcal{X}\)</span>. That is, we assume our vectors
<span class="math inline">\(\mathbf{x_1}, \dots, \mathbf{x_n}\)</span> are independent identically distributed
(i.i.d.) samples from this distribution <span class="math inline">\(P\)</span>. (Exactly what distribution you
pick—normal, chi-squared, <span class="math inline">\(t\)</span>, etc.—is, for the moment, irrelevant).</p>
<p>Recall that out-of-sample data is highly theoretical; we will never be able to
obtain it in its entirety. The best we can do is to obtain a subset of the
out-of-sample data (called the test data). Our imposition of a distributional
structure on <span class="math inline">\(\mathcal{X}\)</span> enables us to link the in-sample error with the
out-of-sample data.</p>
<p>Our ultimate goal is to get a good function <span class="math inline">\(\widehat{f} \approx f\)</span>.
What do we mean by the symbol “<span class="math inline">\(\approx\)</span>”? Technically speaking, we want
<span class="math inline">\(E_{\mathrm{out}}(\widehat{f}) \approx 0\)</span>. If this is the case, we can safely
say that our model has been successfully trained. However, we can never check
if this is the case, since we don’t have access to <span class="math inline">\(E_{\mathrm{out}}\)</span>.</p>
<p>To solve this, we break our goal into two sub-goals:</p>
<p><span class="math display">\[
E_{\mathrm{out}} (\widehat{f}) \approx 0 \ \Rightarrow \begin{cases} E_{\mathrm{in}}(\widehat{f}) \approx 0  &amp; \text{practical result} \\
&amp; \\
E_{\mathrm{out}}(\widehat{f}) \approx E_{\mathrm{in}}(\widehat{f}) &amp;  \text{technical/theoretical result}  \\ \end{cases}
\]</span></p>
<p>The first condition is easy to check because we do have access to the in-sample
data. How do we check the second condition? We check the second condition by
invoking our distributional assumption <span class="math inline">\(P\)</span> on <span class="math inline">\(\mathcal{X}\)</span>.
Using our assumption, we can cite various theorems to assert that the second
result indeed holds true (this part is out of this book’s scope). We will later
find ways to estimate <span class="math inline">\(E_{\mathrm{out}}(\widehat{f})\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-4"></span>
<img src="images/theory/learning-diag2.svg" alt="Supervised Learning Diagram (ver 3)" width="85%" />
<p class="caption">
Figure 8.3: Supervised Learning Diagram (ver 3)
</p>
</div>
</div>
</div>
<div id="noisy-targets" class="section level2">
<h2><span class="header-section-number">8.4</span> Noisy Targets</h2>
<p>In practice, the target function won’t necessarily be a nice (or smooth)
function. Rather, there will be some <strong>noise</strong>. Hence, instead of saying
<span class="math inline">\(y = f(x)\)</span> where <span class="math inline">\(f : \mathcal{X} \to \mathcal{Y}\)</span>, a better statement might be
something like <span class="math inline">\(y = f(x) + \varepsilon\)</span>. But even this notation has some flaws;
for example, we could have multiple inputs mapping to the same output
(which cannot happen if <span class="math inline">\(f\)</span> is a proper “function”). That is, we may have two
individuals with the exact same inputs <span class="math inline">\(\mathbf{x_A} = \mathbf{x_B}\)</span> but with
different response variables <span class="math inline">\(y_A \neq y_B\)</span>. Instead, it makes more sense to
consider some <strong>target conditional distribution</strong> <span class="math inline">\(P(y \mid x)\)</span>. In this way,
we can think of our data as forming a joint probability distribution
<span class="math inline">\(P(\mathbf{x}, y)\)</span>. That is because:</p>
<p><span class="math display" id="eq:401-9">\[
P(\mathbf{x}, y) = P(\mathbf{x})  P(y \mid \mathbf{x})
\tag{8.9}
\]</span></p>
<p>Here’s the updated version of the supervised learning diagram with a modified
orange cloud containing the unknown target distribution, and the noisy function.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-5"></span>
<img src="images/theory/learning-diag3.svg" alt="Supervised Learning Diagram (ver 4)" width="85%" />
<p class="caption">
Figure 8.4: Supervised Learning Diagram (ver 4)
</p>
</div>
<p>In supervised learning, we want to learn the conditional distribution
<span class="math inline">\(P(y \mid \mathbf{x})\)</span>. Again, we can think of this probability in terms of
<span class="math inline">\(y = f() + \text{noise}\)</span>. Also, sometimes the Hypothesis Set and the Learning
Algorithm boxes are combined into one, called the <strong>Learning Model</strong>.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="olsml.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="mse.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["allmodelsarewrong.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
