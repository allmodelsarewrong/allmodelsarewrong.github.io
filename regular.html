<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>11 Regularization Techniques | All Models Are Wrong</title>
  <meta name="description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  <meta name="generator" content="bookdown 0.16 and GitBook 2.6.7" />

  <meta property="og:title" content="11 Regularization Techniques | All Models Are Wrong" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://allmodelsarewrong.github.io" />
  
  <meta property="og:description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  <meta name="github-repo" content="allmodelsarewrong/allmodelsarewrong.github.io" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="11 Regularization Techniques | All Models Are Wrong" />
  
  <meta name="twitter:description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  

<meta name="author" content="by Gaston Sanchez, and Ethan Marzban" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="validation.html"/>
<link rel="next" href="pcr.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><b>All Models Are Wrong (CSL)</b><br><small>by G. Sanchez & E. Marzban</small></a></li>

<li class="divider"></li>
<li class="part"><span><b>I Welcome</b></span></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="about.html"><a href="about.html"><i class="fa fa-check"></i><b>1</b> About this book</a><ul>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#prerequisites"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="part"><span><b>II Intro</b></span></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#basic-notation"><i class="fa fa-check"></i><b>2.1</b> Basic Notation</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="duality.html"><a href="duality.html"><i class="fa fa-check"></i><b>3</b> Geometric Duality</a><ul>
<li class="chapter" data-level="3.1" data-path="duality.html"><a href="duality.html#rows-space"><i class="fa fa-check"></i><b>3.1</b> Rows Space</a></li>
<li class="chapter" data-level="3.2" data-path="duality.html"><a href="duality.html#columns-space"><i class="fa fa-check"></i><b>3.2</b> Columns Space</a></li>
<li class="chapter" data-level="3.3" data-path="duality.html"><a href="duality.html#cloud-of-individuals"><i class="fa fa-check"></i><b>3.3</b> Cloud of Individuals</a><ul>
<li class="chapter" data-level="3.3.1" data-path="duality.html"><a href="duality.html#average-individual"><i class="fa fa-check"></i><b>3.3.1</b> Average Individual</a></li>
<li class="chapter" data-level="3.3.2" data-path="duality.html"><a href="duality.html#centered-data"><i class="fa fa-check"></i><b>3.3.2</b> Centered Data</a></li>
<li class="chapter" data-level="3.3.3" data-path="duality.html"><a href="duality.html#distance-between-individuals"><i class="fa fa-check"></i><b>3.3.3</b> Distance between individuals</a></li>
<li class="chapter" data-level="3.3.4" data-path="duality.html"><a href="duality.html#distance-to-the-centroid"><i class="fa fa-check"></i><b>3.3.4</b> Distance to the centroid</a></li>
<li class="chapter" data-level="3.3.5" data-path="duality.html"><a href="duality.html#measures-of-dispersion"><i class="fa fa-check"></i><b>3.3.5</b> Measures of Dispersion</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="duality.html"><a href="duality.html#cloud-of-variables"><i class="fa fa-check"></i><b>3.4</b> Cloud of Variables</a><ul>
<li class="chapter" data-level="3.4.1" data-path="duality.html"><a href="duality.html#mean-of-a-variable"><i class="fa fa-check"></i><b>3.4.1</b> Mean of a Variable</a></li>
<li class="chapter" data-level="3.4.2" data-path="duality.html"><a href="duality.html#variance-of-a-variable"><i class="fa fa-check"></i><b>3.4.2</b> Variance of a Variable</a></li>
<li class="chapter" data-level="3.4.3" data-path="duality.html"><a href="duality.html#variance-with-vector-notation"><i class="fa fa-check"></i><b>3.4.3</b> Variance with Vector Notation</a></li>
<li class="chapter" data-level="3.4.4" data-path="duality.html"><a href="duality.html#standard-deviation-as-a-norm"><i class="fa fa-check"></i><b>3.4.4</b> Standard Deviation as a Norm</a></li>
<li class="chapter" data-level="3.4.5" data-path="duality.html"><a href="duality.html#covariance"><i class="fa fa-check"></i><b>3.4.5</b> Covariance</a></li>
<li class="chapter" data-level="3.4.6" data-path="duality.html"><a href="duality.html#correlation"><i class="fa fa-check"></i><b>3.4.6</b> Correlation</a></li>
<li class="chapter" data-level="3.4.7" data-path="duality.html"><a href="duality.html#geometry-of-correlation"><i class="fa fa-check"></i><b>3.4.7</b> Geometry of Correlation</a></li>
<li class="chapter" data-level="3.4.8" data-path="duality.html"><a href="duality.html#orthogonal-projections"><i class="fa fa-check"></i><b>3.4.8</b> Orthogonal Projections</a></li>
<li class="chapter" data-level="3.4.9" data-path="duality.html"><a href="duality.html#the-mean-as-an-orthogonal-projection"><i class="fa fa-check"></i><b>3.4.9</b> The mean as an orthogonal projection</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Unsupervised I: PCA</b></span></li>
<li class="chapter" data-level="4" data-path="pca.html"><a href="pca.html"><i class="fa fa-check"></i><b>4</b> Principal Components Analysis</a><ul>
<li class="chapter" data-level="4.1" data-path="pca.html"><a href="pca.html#low-dimensional-representations"><i class="fa fa-check"></i><b>4.1</b> Low-dimensional Representations</a></li>
<li class="chapter" data-level="4.2" data-path="pca.html"><a href="pca.html#projections"><i class="fa fa-check"></i><b>4.2</b> Projections</a><ul>
<li class="chapter" data-level="4.2.1" data-path="pca.html"><a href="pca.html#vector-and-scalar-projections"><i class="fa fa-check"></i><b>4.2.1</b> Vector and Scalar Projections</a></li>
<li class="chapter" data-level="4.2.2" data-path="pca.html"><a href="pca.html#projected-inertia"><i class="fa fa-check"></i><b>4.2.2</b> Projected Inertia</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="pca.html"><a href="pca.html#maximization-problem"><i class="fa fa-check"></i><b>4.3</b> Maximization Problem</a><ul>
<li class="chapter" data-level="4.3.1" data-path="pca.html"><a href="pca.html#eigenvectors-of-mathbfs"><i class="fa fa-check"></i><b>4.3.1</b> Eigenvectors of <span class="math inline">\(\mathbf{S}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="pca.html"><a href="pca.html#another-perspective-of-pca"><i class="fa fa-check"></i><b>4.4</b> Another Perspective of PCA</a><ul>
<li class="chapter" data-level="4.4.1" data-path="pca.html"><a href="pca.html#finding-principal-components"><i class="fa fa-check"></i><b>4.4.1</b> Finding Principal Components</a></li>
<li class="chapter" data-level="4.4.2" data-path="pca.html"><a href="pca.html#finding-the-first-pc"><i class="fa fa-check"></i><b>4.4.2</b> Finding the first PC</a></li>
<li class="chapter" data-level="4.4.3" data-path="pca.html"><a href="pca.html#finding-the-second-pc"><i class="fa fa-check"></i><b>4.4.3</b> Finding the second PC</a></li>
<li class="chapter" data-level="4.4.4" data-path="pca.html"><a href="pca.html#finding-all-pcs"><i class="fa fa-check"></i><b>4.4.4</b> Finding all PCs</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="pca.html"><a href="pca.html#data-decomposition-model"><i class="fa fa-check"></i><b>4.5</b> Data Decomposition Model</a><ul>
<li class="chapter" data-level="4.5.1" data-path="pca.html"><a href="pca.html#alternative-approaches"><i class="fa fa-check"></i><b>4.5.1</b> Alternative Approaches</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Linear Regression</b></span></li>
<li class="chapter" data-level="5" data-path="ols.html"><a href="ols.html"><i class="fa fa-check"></i><b>5</b> Linear Regression</a><ul>
<li class="chapter" data-level="5.1" data-path="ols.html"><a href="ols.html#motivation"><i class="fa fa-check"></i><b>5.1</b> Motivation</a></li>
<li class="chapter" data-level="5.2" data-path="ols.html"><a href="ols.html#the-ideaintuition-of-regression"><i class="fa fa-check"></i><b>5.2</b> The Idea/Intuition of Regression</a></li>
<li class="chapter" data-level="5.3" data-path="ols.html"><a href="ols.html#the-linear-regression-model"><i class="fa fa-check"></i><b>5.3</b> The Linear Regression Model</a></li>
<li class="chapter" data-level="5.4" data-path="ols.html"><a href="ols.html#the-error-measure"><i class="fa fa-check"></i><b>5.4</b> The Error Measure</a></li>
<li class="chapter" data-level="5.5" data-path="ols.html"><a href="ols.html#the-least-squares-algorithm"><i class="fa fa-check"></i><b>5.5</b> The Least Squares Algorithm</a></li>
<li class="chapter" data-level="5.6" data-path="ols.html"><a href="ols.html#geometries-of-ols"><i class="fa fa-check"></i><b>5.6</b> Geometries of OLS</a><ul>
<li class="chapter" data-level="5.6.1" data-path="ols.html"><a href="ols.html#rows-perspective"><i class="fa fa-check"></i><b>5.6.1</b> Rows Perspective</a></li>
<li class="chapter" data-level="5.6.2" data-path="ols.html"><a href="ols.html#columns-perspective"><i class="fa fa-check"></i><b>5.6.2</b> Columns Perspective</a></li>
<li class="chapter" data-level="5.6.3" data-path="ols.html"><a href="ols.html#parameters-perspective"><i class="fa fa-check"></i><b>5.6.3</b> Parameters Perspective</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="gradient.html"><a href="gradient.html"><i class="fa fa-check"></i><b>6</b> Gradient Descent</a><ul>
<li class="chapter" data-level="6.1" data-path="gradient.html"><a href="gradient.html#error-surface"><i class="fa fa-check"></i><b>6.1</b> Error Surface</a></li>
<li class="chapter" data-level="6.2" data-path="gradient.html"><a href="gradient.html#idea-of-gradient-descent"><i class="fa fa-check"></i><b>6.2</b> Idea of Gradient Descent</a></li>
<li class="chapter" data-level="6.3" data-path="gradient.html"><a href="gradient.html#moving-down-an-error-surface"><i class="fa fa-check"></i><b>6.3</b> Moving Down an Error Surface</a><ul>
<li class="chapter" data-level="6.3.1" data-path="gradient.html"><a href="gradient.html#the-direction-of-mathbfv"><i class="fa fa-check"></i><b>6.3.1</b> The direction of <span class="math inline">\(\mathbf{v}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="gradient.html"><a href="gradient.html#gradient-descent-and-our-model"><i class="fa fa-check"></i><b>6.4</b> Gradient Descent and our Model</a></li>
</ul></li>
<li class="part"><span><b>V Learning Concepts</b></span></li>
<li class="chapter" data-level="7" data-path="learning.html"><a href="learning.html"><i class="fa fa-check"></i><b>7</b> Theoretical Framework</a><ul>
<li class="chapter" data-level="7.1" data-path="learning.html"><a href="learning.html#mental-map"><i class="fa fa-check"></i><b>7.1</b> Mental Map</a></li>
<li class="chapter" data-level="7.2" data-path="learning.html"><a href="learning.html#kinds-of-predictions"><i class="fa fa-check"></i><b>7.2</b> Kinds of Predictions</a><ul>
<li class="chapter" data-level="7.2.1" data-path="learning.html"><a href="learning.html#two-types-of-data"><i class="fa fa-check"></i><b>7.2.1</b> Two Types of Data</a></li>
<li class="chapter" data-level="7.2.2" data-path="learning.html"><a href="learning.html#two-types-of-predictions"><i class="fa fa-check"></i><b>7.2.2</b> Two Types of Predictions</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="learning.html"><a href="learning.html#two-types-of-errors"><i class="fa fa-check"></i><b>7.3</b> Two Types of Errors</a><ul>
<li class="chapter" data-level="7.3.1" data-path="learning.html"><a href="learning.html#individual-errors"><i class="fa fa-check"></i><b>7.3.1</b> Individual Errors</a></li>
<li class="chapter" data-level="7.3.2" data-path="learning.html"><a href="learning.html#overall-errors"><i class="fa fa-check"></i><b>7.3.2</b> Overall Errors</a></li>
<li class="chapter" data-level="7.3.3" data-path="learning.html"><a href="learning.html#auxiliary-technicality"><i class="fa fa-check"></i><b>7.3.3</b> Auxiliary Technicality</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="learning.html"><a href="learning.html#noisy-targets"><i class="fa fa-check"></i><b>7.4</b> Noisy Targets</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="mse.html"><a href="mse.html"><i class="fa fa-check"></i><b>8</b> MSE of Estimator</a><ul>
<li class="chapter" data-level="8.1" data-path="mse.html"><a href="mse.html#mse-of-an-estimator"><i class="fa fa-check"></i><b>8.1</b> MSE of an Estimator</a><ul>
<li class="chapter" data-level="8.1.1" data-path="mse.html"><a href="mse.html#prototypical-cases-of-bias-and-variance"><i class="fa fa-check"></i><b>8.1.1</b> Prototypical Cases of Bias and Variance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="biasvar.html"><a href="biasvar.html"><i class="fa fa-check"></i><b>9</b> Bias-Variance Tradeoff</a><ul>
<li class="chapter" data-level="9.1" data-path="biasvar.html"><a href="biasvar.html#bias-variance-tradeoff"><i class="fa fa-check"></i><b>9.1</b> Bias-Variance Tradeoff</a></li>
<li class="chapter" data-level="9.2" data-path="biasvar.html"><a href="biasvar.html#motivation-example"><i class="fa fa-check"></i><b>9.2</b> Motivation Example</a><ul>
<li class="chapter" data-level="9.2.1" data-path="biasvar.html"><a href="biasvar.html#two-hypotheses"><i class="fa fa-check"></i><b>9.2.1</b> Two Hypotheses</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="biasvar.html"><a href="biasvar.html#learning-from-two-points"><i class="fa fa-check"></i><b>9.3</b> Learning from two points</a></li>
<li class="chapter" data-level="9.4" data-path="biasvar.html"><a href="biasvar.html#bias-variance-derivation"><i class="fa fa-check"></i><b>9.4</b> Bias-Variance Derivation</a><ul>
<li class="chapter" data-level="9.4.1" data-path="biasvar.html"><a href="biasvar.html#noisy-target"><i class="fa fa-check"></i><b>9.4.1</b> Noisy Target</a></li>
<li class="chapter" data-level="9.4.2" data-path="biasvar.html"><a href="biasvar.html#types-of-theoretical-mses"><i class="fa fa-check"></i><b>9.4.2</b> Types of Theoretical MSEs</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="biasvar.html"><a href="biasvar.html#the-tradeoff"><i class="fa fa-check"></i><b>9.5</b> The Tradeoff</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="validation.html"><a href="validation.html"><i class="fa fa-check"></i><b>10</b> Validation</a><ul>
<li class="chapter" data-level="10.1" data-path="validation.html"><a href="validation.html#model-assessment"><i class="fa fa-check"></i><b>10.1</b> Model Assessment</a></li>
<li class="chapter" data-level="10.2" data-path="validation.html"><a href="validation.html#holdout-method"><i class="fa fa-check"></i><b>10.2</b> Holdout Method</a><ul>
<li class="chapter" data-level="10.2.1" data-path="validation.html"><a href="validation.html#rationale-behind-holdout-method"><i class="fa fa-check"></i><b>10.2.1</b> Rationale Behind Holdout Method</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="validation.html"><a href="validation.html#repeated-holdout-method"><i class="fa fa-check"></i><b>10.3</b> Repeated Holdout Method</a></li>
<li class="chapter" data-level="10.4" data-path="validation.html"><a href="validation.html#bootstrap-method"><i class="fa fa-check"></i><b>10.4</b> Bootstrap Method</a></li>
<li class="chapter" data-level="10.5" data-path="validation.html"><a href="validation.html#model-selection"><i class="fa fa-check"></i><b>10.5</b> Model Selection</a><ul>
<li class="chapter" data-level="10.5.1" data-path="validation.html"><a href="validation.html#three-way-holdout-method"><i class="fa fa-check"></i><b>10.5.1</b> Three-way Holdout Method</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="validation.html"><a href="validation.html#cross-validation"><i class="fa fa-check"></i><b>10.6</b> Cross-Validation</a><ul>
<li class="chapter" data-level="10.6.1" data-path="validation.html"><a href="validation.html#leave-one-out-cross-validation-loocv"><i class="fa fa-check"></i><b>10.6.1</b> Leave-One-Out Cross Validation (LOOCV)</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>VI Regularization</b></span></li>
<li class="chapter" data-level="11" data-path="regular.html"><a href="regular.html"><i class="fa fa-check"></i><b>11</b> Regularization Techniques</a><ul>
<li class="chapter" data-level="11.1" data-path="regular.html"><a href="regular.html#multicollinearity-issues"><i class="fa fa-check"></i><b>11.1</b> Multicollinearity Issues</a><ul>
<li class="chapter" data-level="11.1.1" data-path="regular.html"><a href="regular.html#toy-example"><i class="fa fa-check"></i><b>11.1.1</b> Toy Example</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="regular.html"><a href="regular.html#irregular-coefficients"><i class="fa fa-check"></i><b>11.2</b> Irregular Coefficients</a><ul>
<li class="chapter" data-level="11.2.1" data-path="regular.html"><a href="regular.html#effect-of-multicollinearity"><i class="fa fa-check"></i><b>11.2.1</b> Effect of Multicollinearity</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="regular.html"><a href="regular.html#regularization-metaphor"><i class="fa fa-check"></i><b>11.3</b> Regularization Metaphor</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="pcr.html"><a href="pcr.html"><i class="fa fa-check"></i><b>12</b> Principal Components Regression</a><ul>
<li class="chapter" data-level="12.1" data-path="pcr.html"><a href="pcr.html#motivation-example-1"><i class="fa fa-check"></i><b>12.1</b> Motivation Example</a></li>
<li class="chapter" data-level="12.2" data-path="pcr.html"><a href="pcr.html#the-pcr-model"><i class="fa fa-check"></i><b>12.2</b> The PCR Model</a></li>
<li class="chapter" data-level="12.3" data-path="pcr.html"><a href="pcr.html#how-does-pcr-work"><i class="fa fa-check"></i><b>12.3</b> How does PCR work?</a><ul>
<li class="chapter" data-level="12.3.1" data-path="pcr.html"><a href="pcr.html#transition-formula"><i class="fa fa-check"></i><b>12.3.1</b> Transition Formula</a></li>
<li class="chapter" data-level="12.3.2" data-path="pcr.html"><a href="pcr.html#size-of-coefficients"><i class="fa fa-check"></i><b>12.3.2</b> Size of Coefficients</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="pls.html"><a href="pls.html"><i class="fa fa-check"></i><b>13</b> Partial Least Squares Regression</a><ul>
<li class="chapter" data-level="13.1" data-path="pls.html"><a href="pls.html#motivation-example-2"><i class="fa fa-check"></i><b>13.1</b> Motivation Example</a></li>
<li class="chapter" data-level="13.2" data-path="pls.html"><a href="pls.html#the-plsr-model"><i class="fa fa-check"></i><b>13.2</b> The PLSR Model</a></li>
<li class="chapter" data-level="13.3" data-path="pls.html"><a href="pls.html#how-does-plsr-work"><i class="fa fa-check"></i><b>13.3</b> How does PLSR work?</a></li>
<li class="chapter" data-level="13.4" data-path="pls.html"><a href="pls.html#plsr-algorithm"><i class="fa fa-check"></i><b>13.4</b> PLSR Algorithm</a><ul>
<li class="chapter" data-level="13.4.1" data-path="pls.html"><a href="pls.html#pls-solution-with-original-variables"><i class="fa fa-check"></i><b>13.4.1</b> PLS Solution with original variables</a></li>
<li class="chapter" data-level="13.4.2" data-path="pls.html"><a href="pls.html#size-of-coefficients-1"><i class="fa fa-check"></i><b>13.4.2</b> Size of Coefficients</a></li>
<li class="chapter" data-level="13.4.3" data-path="pls.html"><a href="pls.html#some-properties"><i class="fa fa-check"></i><b>13.4.3</b> Some Properties</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="ridge.html"><a href="ridge.html"><i class="fa fa-check"></i><b>14</b> Ridge Regression</a></li>
<li class="part"><span><b>VII Classification</b></span></li>
<li class="chapter" data-level="15" data-path="classif.html"><a href="classif.html"><i class="fa fa-check"></i><b>15</b> Classification Methods</a></li>
<li class="chapter" data-level="16" data-path="logistic.html"><a href="logistic.html"><i class="fa fa-check"></i><b>16</b> Logistic Regression</a><ul>
<li class="chapter" data-level="16.1" data-path="logistic.html"><a href="logistic.html#motivation-1"><i class="fa fa-check"></i><b>16.1</b> Motivation</a><ul>
<li class="chapter" data-level="16.1.1" data-path="logistic.html"><a href="logistic.html#first-approach-fitting-a-line"><i class="fa fa-check"></i><b>16.1.1</b> First Approach: Fitting a Line</a></li>
<li class="chapter" data-level="16.1.2" data-path="logistic.html"><a href="logistic.html#secodn-approach-harsh-thresholding"><i class="fa fa-check"></i><b>16.1.2</b> Secodn Approach: Harsh Thresholding</a></li>
<li class="chapter" data-level="16.1.3" data-path="logistic.html"><a href="logistic.html#third-approach-conditional-means"><i class="fa fa-check"></i><b>16.1.3</b> Third Approach: Conditional Means</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="logistic.html"><a href="logistic.html#logistic-regression-model"><i class="fa fa-check"></i><b>16.2</b> Logistic Regression Model</a><ul>
<li class="chapter" data-level="16.2.1" data-path="logistic.html"><a href="logistic.html#the-criterion-being-optimized"><i class="fa fa-check"></i><b>16.2.1</b> The Criterion Being Optimized</a></li>
<li class="chapter" data-level="16.2.2" data-path="logistic.html"><a href="logistic.html#another-way-to-solve-logistic-regression"><i class="fa fa-check"></i><b>16.2.2</b> Another Way to Solve Logistic Regression</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>VIII Clustering</b></span></li>
<li class="chapter" data-level="17" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>17</b> Clustering</a><ul>
<li class="chapter" data-level="17.1" data-path="clustering.html"><a href="clustering.html#about-clustering"><i class="fa fa-check"></i><b>17.1</b> About Clustering</a><ul>
<li class="chapter" data-level="17.1.1" data-path="clustering.html"><a href="clustering.html#types-of-clustering"><i class="fa fa-check"></i><b>17.1.1</b> Types of Clustering</a></li>
<li class="chapter" data-level="17.1.2" data-path="clustering.html"><a href="clustering.html#hard-clustering"><i class="fa fa-check"></i><b>17.1.2</b> Hard Clustering</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="clustering.html"><a href="clustering.html#dispersion-measures"><i class="fa fa-check"></i><b>17.2</b> Dispersion Measures</a></li>
<li class="chapter" data-level="17.3" data-path="clustering.html"><a href="clustering.html#complexity-in-clustering"><i class="fa fa-check"></i><b>17.3</b> Complexity in Clustering</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="kmeans.html"><a href="kmeans.html"><i class="fa fa-check"></i><b>18</b> K-Means</a><ul>
<li class="chapter" data-level="18.1" data-path="kmeans.html"><a href="kmeans.html#toy-example-1"><i class="fa fa-check"></i><b>18.1</b> Toy Example</a></li>
<li class="chapter" data-level="18.2" data-path="kmeans.html"><a href="kmeans.html#what-does-k-means-do"><i class="fa fa-check"></i><b>18.2</b> What does K-means do?</a></li>
<li class="chapter" data-level="18.3" data-path="kmeans.html"><a href="kmeans.html#k-means-algorithms"><i class="fa fa-check"></i><b>18.3</b> K-Means Algorithms</a><ul>
<li class="chapter" data-level="18.3.1" data-path="kmeans.html"><a href="kmeans.html#classic-version"><i class="fa fa-check"></i><b>18.3.1</b> Classic Version</a></li>
<li class="chapter" data-level="18.3.2" data-path="kmeans.html"><a href="kmeans.html#moving-centers-algorithm"><i class="fa fa-check"></i><b>18.3.2</b> Moving Centers Algorithm</a></li>
<li class="chapter" data-level="18.3.3" data-path="kmeans.html"><a href="kmeans.html#dynamic-clouds"><i class="fa fa-check"></i><b>18.3.3</b> Dynamic Clouds</a></li>
<li class="chapter" data-level="18.3.4" data-path="kmeans.html"><a href="kmeans.html#choosing-k"><i class="fa fa-check"></i><b>18.3.4</b> Choosing <span class="math inline">\(k\)</span></a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Made with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">All Models Are Wrong</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regular" class="section level1">
<h1><span class="header-section-number">11</span> Regularization Techniques</h1>
<p>In this part of the book we will talk about the notion of regularization
(what is regularization, what is the purpose of regularization, what
approaches are used for regularization) all of this within the context of
linear models. However, keep in mind that you can also use regularization
in non-linear contexts.</p>
<p>So what is regularization? If you look at the dictionary definition of the term
<em>regularization</em> you should find something like this:</p>
<blockquote>
<p><strong>regularization</strong>; the act of bringing to uniformity; make (something) regular</p>
</blockquote>
<p>Simply put: regularization has to do with “making things regular”.</p>
<p>But what about <em>regularization</em> in the context of supervised learning? What are
the “irregular” things that need to be made “regular”? In order to answer this
question, we will first motivate the discussion by sWe will answer this
questions by discussing the effects of having predictors with high multicollinearity.</p>
<p>means reducing the variability
(and therefore the size) of the vector of
predicted coefficients. What we have seen is that if there is multicollinearity,
some of the elements of <span class="math inline">\(\mathbf{b}_{\text{ols}}\)</span> will have high variance.
In other words, <span class="math inline">\(\| \mathbf{b}_{\text{ols}} \|_{\ell_p}\)</span> will be very large
(in this case, we say that the coefficients are <strong>irregular</strong>).
In regularization, we try to mitigate the effects of these irregular coefficients.</p>
<p>We will discuss two main types of approaches to achieve regularization:</p>
<ul>
<li><p><strong>Dimension Reduction</strong>: Principal Components Regression (PCR), Partial Least Squares regression (PLSR)</p></li>
<li><p><strong>Penalized Methods</strong>: Ridge Regression, Lasso regression.</p></li>
</ul>
<div id="multicollinearity-issues" class="section level2">
<h2><span class="header-section-number">11.1</span> Multicollinearity Issues</h2>
<p>One of the issues when fitting regression models is due to multicollinearity:
the condition that arises when two or more predictors are highly correlated.</p>
<p>How does this affect OLS regression?</p>
<p>When one or more predictors are linear combinations of other predictors, then
<span class="math inline">\(\mathbf{X^\mathsf{T} X}\)</span> is singular. This is known as <em>exact collinearity</em>.
When this happends, there is no unique LS estimate <span class="math inline">\(\mathbf{b}\)</span>.</p>
<p>A more challenging problem arises when <span class="math inline">\(\mathbf{X^\mathsf{T} X}\)</span> is close to
singular but not exactly.
This is usually referred to as <em>near perfect collinearity</em> or simply
<em>multicollinearity</em>. Multicollinearity leads to imprecise (unstable)
estimates <span class="math inline">\(\mathbf{b}\)</span>.</p>
<p>What are the typical causes of multicollinearity?</p>
<ul>
<li>One or more predictors are linear combinations of other predictors</li>
<li>One or more predictors are almost perfect linear combinations of other predictors</li>
<li>More predictors than observations <span class="math inline">\(p &gt; n\)</span></li>
</ul>
<div id="toy-example" class="section level3">
<h3><span class="header-section-number">11.1.1</span> Toy Example</h3>
<p>Let’s play with <code>mtcars</code>, one of the built-in data sets in R.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="regular.html#cb1-1"></a>mtcars[<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>, ]</span>
<span id="cb1-2"><a href="regular.html#cb1-2"></a><span class="co">#&gt;                    mpg cyl  disp  hp drat    wt  qsec vs am gear carb</span></span>
<span id="cb1-3"><a href="regular.html#cb1-3"></a><span class="co">#&gt; Mazda RX4         21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4</span></span>
<span id="cb1-4"><a href="regular.html#cb1-4"></a><span class="co">#&gt; Mazda RX4 Wag     21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4</span></span>
<span id="cb1-5"><a href="regular.html#cb1-5"></a><span class="co">#&gt; Datsun 710        22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1</span></span>
<span id="cb1-6"><a href="regular.html#cb1-6"></a><span class="co">#&gt; Hornet 4 Drive    21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1</span></span>
<span id="cb1-7"><a href="regular.html#cb1-7"></a><span class="co">#&gt; Hornet Sportabout 18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2</span></span>
<span id="cb1-8"><a href="regular.html#cb1-8"></a><span class="co">#&gt; Valiant           18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1</span></span>
<span id="cb1-9"><a href="regular.html#cb1-9"></a><span class="co">#&gt; Duster 360        14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4</span></span>
<span id="cb1-10"><a href="regular.html#cb1-10"></a><span class="co">#&gt; Merc 240D         24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2</span></span>
<span id="cb1-11"><a href="regular.html#cb1-11"></a><span class="co">#&gt; Merc 230          22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2</span></span>
<span id="cb1-12"><a href="regular.html#cb1-12"></a><span class="co">#&gt; Merc 280          19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4</span></span></code></pre></div>
<p>Let’s use <code>mpg</code> as response, and <code>disp</code>, <code>hp</code>, and <code>wt</code> as predictors.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="regular.html#cb2-1"></a><span class="co"># response</span></span>
<span id="cb2-2"><a href="regular.html#cb2-2"></a>mpg &lt;-<span class="st"> </span>mtcars<span class="op">$</span>mpg</span>
<span id="cb2-3"><a href="regular.html#cb2-3"></a></span>
<span id="cb2-4"><a href="regular.html#cb2-4"></a><span class="co"># predictors</span></span>
<span id="cb2-5"><a href="regular.html#cb2-5"></a>disp &lt;-<span class="st"> </span>mtcars<span class="op">$</span>disp</span>
<span id="cb2-6"><a href="regular.html#cb2-6"></a>hp &lt;-<span class="st"> </span>mtcars<span class="op">$</span>hp</span>
<span id="cb2-7"><a href="regular.html#cb2-7"></a>wt &lt;-<span class="st"> </span>mtcars<span class="op">$</span>wt</span>
<span id="cb2-8"><a href="regular.html#cb2-8"></a></span>
<span id="cb2-9"><a href="regular.html#cb2-9"></a><span class="co"># standardized predictors</span></span>
<span id="cb2-10"><a href="regular.html#cb2-10"></a>X &lt;-<span class="st"> </span><span class="kw">scale</span>(<span class="kw">cbind</span>(disp, hp, wt))</span></code></pre></div>
<p>Let’s inspect the correlation matrix:</p>
<p><span class="math display">\[
\mathbf{R} = \frac{1}{n-1} \mathbf{X^\mathsf{T} X}
\]</span></p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="regular.html#cb3-1"></a><span class="co"># correlation matrix</span></span>
<span id="cb3-2"><a href="regular.html#cb3-2"></a><span class="kw">cor</span>(X)</span>
<span id="cb3-3"><a href="regular.html#cb3-3"></a><span class="co">#&gt;           disp        hp        wt</span></span>
<span id="cb3-4"><a href="regular.html#cb3-4"></a><span class="co">#&gt; disp 1.0000000 0.7909486 0.8879799</span></span>
<span id="cb3-5"><a href="regular.html#cb3-5"></a><span class="co">#&gt; hp   0.7909486 1.0000000 0.6587479</span></span>
<span id="cb3-6"><a href="regular.html#cb3-6"></a><span class="co">#&gt; wt   0.8879799 0.6587479 1.0000000</span></span></code></pre></div>
<p>If we look at the circle of correlations from a principal components analysis,
we see the following pattern:</p>
<p><img src="allmodelsarewrong_files/figure-html/unnamed-chunk-84-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Carrying out an ordinary least squares regression:</p>
<pre><code>#&gt; 
#&gt; Call:
#&gt; lm(formula = mpg ~ disp + hp + wt)
#&gt; 
#&gt; Coefficients:
#&gt; (Intercept)         disp           hp           wt  
#&gt;   37.105505    -0.000937    -0.031157    -3.800891
#&gt; 
#&gt; Call:
#&gt; lm(formula = mpg ~ disp + hp + wt)
#&gt; 
#&gt; Residuals:
#&gt;    Min     1Q Median     3Q    Max 
#&gt; -3.891 -1.640 -0.172  1.061  5.861 
#&gt; 
#&gt; Coefficients:
#&gt;              Estimate Std. Error t value Pr(&gt;|t|)    
#&gt; (Intercept) 37.105505   2.110815  17.579  &lt; 2e-16 ***
#&gt; disp        -0.000937   0.010350  -0.091  0.92851    
#&gt; hp          -0.031157   0.011436  -2.724  0.01097 *  
#&gt; wt          -3.800891   1.066191  -3.565  0.00133 ** 
#&gt; ---
#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
#&gt; 
#&gt; Residual standard error: 2.639 on 28 degrees of freedom
#&gt; Multiple R-squared:  0.8268, Adjusted R-squared:  0.8083 
#&gt; F-statistic: 44.57 on 3 and 28 DF,  p-value: 8.65e-11</code></pre>
<p>What about <span class="math inline">\(\mathbf{(X^\mathsf{T} X)^{-1}}\)</span></p>
<pre><code>#&gt;             disp          hp          wt
#&gt; disp  0.23627475 -0.08598357 -0.15316573
#&gt; hp   -0.08598357  0.08827847  0.01819843
#&gt; wt   -0.15316573  0.01819843  0.15627798</code></pre>
<p>Let’s introduce exact collinearity. For example, let’s create a fourth variable
<code>disp1</code> that is a multiple of <code>disp</code>, and then let’s add this variable to the data
matrix <code>X</code>. What happens if we try to compute the inverse of <span class="math inline">\(\mathbf{X^\mathsf{T}X}\)</span>?</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="regular.html#cb6-1"></a>disp1 &lt;-<span class="st"> </span><span class="dv">10</span> <span class="op">*</span><span class="st"> </span>disp</span>
<span id="cb6-2"><a href="regular.html#cb6-2"></a></span>
<span id="cb6-3"><a href="regular.html#cb6-3"></a>X1 &lt;-<span class="st"> </span><span class="kw">scale</span>(<span class="kw">cbind</span>(disp, disp1, hp, wt))</span>
<span id="cb6-4"><a href="regular.html#cb6-4"></a></span>
<span id="cb6-5"><a href="regular.html#cb6-5"></a><span class="kw">solve</span>(<span class="kw">t</span>(X1) <span class="op">%*%</span><span class="st"> </span>X1)</span>
<span id="cb6-6"><a href="regular.html#cb6-6"></a><span class="co">#&gt; Error in solve.default(t(X1) %*% X1): system is computationally singular: reciprocal condition number = 1.55757e-17</span></span></code></pre></div>
<p>As you can tell, R detected perfect collinearity, and the computation of the
inverse shows that the matrix if singular.</p>
<p>Let’s introduce near-exact collinearity</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="regular.html#cb7-1"></a><span class="kw">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb7-2"><a href="regular.html#cb7-2"></a>disp2 &lt;-<span class="st"> </span>disp <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="kw">length</span>(disp))</span>
<span id="cb7-3"><a href="regular.html#cb7-3"></a></span>
<span id="cb7-4"><a href="regular.html#cb7-4"></a>X2 &lt;-<span class="st"> </span><span class="kw">scale</span>(<span class="kw">cbind</span>(disp, disp2, hp, wt))</span>
<span id="cb7-5"><a href="regular.html#cb7-5"></a></span>
<span id="cb7-6"><a href="regular.html#cb7-6"></a><span class="kw">solve</span>(<span class="kw">t</span>(X2) <span class="op">%*%</span><span class="st"> </span>X2)</span>
<span id="cb7-7"><a href="regular.html#cb7-7"></a><span class="co">#&gt;              disp       disp2          hp          wt</span></span>
<span id="cb7-8"><a href="regular.html#cb7-8"></a><span class="co">#&gt; disp   588.167214 -590.721826  1.01055902  1.99383316</span></span>
<span id="cb7-9"><a href="regular.html#cb7-9"></a><span class="co">#&gt; disp2 -590.721826  593.525960 -1.10174784 -2.15719062</span></span>
<span id="cb7-10"><a href="regular.html#cb7-10"></a><span class="co">#&gt; hp       1.010559   -1.101748  0.09032362  0.02220277</span></span>
<span id="cb7-11"><a href="regular.html#cb7-11"></a><span class="co">#&gt; wt       1.993833   -2.157191  0.02220277  0.16411837</span></span></code></pre></div>
<p>Let’s make things even more extreme!</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="regular.html#cb8-1"></a><span class="kw">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb8-2"><a href="regular.html#cb8-2"></a>disp3 &lt;-<span class="st"> </span>disp <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="kw">length</span>(disp), <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="fl">0.1</span>)</span>
<span id="cb8-3"><a href="regular.html#cb8-3"></a></span>
<span id="cb8-4"><a href="regular.html#cb8-4"></a>X3 &lt;-<span class="st"> </span><span class="kw">scale</span>(<span class="kw">cbind</span>(disp, disp3, hp, wt))</span>
<span id="cb8-5"><a href="regular.html#cb8-5"></a></span>
<span id="cb8-6"><a href="regular.html#cb8-6"></a><span class="kw">cor</span>(disp, disp3)</span>
<span id="cb8-7"><a href="regular.html#cb8-7"></a><span class="co">#&gt; [1] 0.9999997</span></span></code></pre></div>
<p>And let’s introduce a minor, tiny modification, that in theory, would hardly
have any effect:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="regular.html#cb9-1"></a><span class="co"># small changes may have a &quot;butterfly&quot; effect</span></span>
<span id="cb9-2"><a href="regular.html#cb9-2"></a>disp31 &lt;-<span class="st"> </span>disp3</span>
<span id="cb9-3"><a href="regular.html#cb9-3"></a></span>
<span id="cb9-4"><a href="regular.html#cb9-4"></a><span class="co"># change just one observation</span></span>
<span id="cb9-5"><a href="regular.html#cb9-5"></a>disp31[<span class="dv">1</span>] &lt;-<span class="st"> </span>disp3[<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span><span class="fl">1.01</span></span>
<span id="cb9-6"><a href="regular.html#cb9-6"></a></span>
<span id="cb9-7"><a href="regular.html#cb9-7"></a>X31 &lt;-<span class="st"> </span><span class="kw">scale</span>(<span class="kw">cbind</span>(disp, disp31, hp, wt))</span>
<span id="cb9-8"><a href="regular.html#cb9-8"></a></span>
<span id="cb9-9"><a href="regular.html#cb9-9"></a><span class="kw">cor</span>(disp, disp31)</span>
<span id="cb9-10"><a href="regular.html#cb9-10"></a><span class="co">#&gt; [1] 0.9999973</span></span></code></pre></div>
<p>When we calculate the inverse of <span class="math inline">\(\mathbf{X}_{31}^\mathsf{T} \mathbf{X}_{31}\)</span>,
something weird happens:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="regular.html#cb10-1"></a><span class="kw">solve</span>(<span class="kw">t</span>(X3) <span class="op">%*%</span><span class="st"> </span>X3)</span>
<span id="cb10-2"><a href="regular.html#cb10-2"></a><span class="co">#&gt;               disp        disp3           hp           wt</span></span>
<span id="cb10-3"><a href="regular.html#cb10-3"></a><span class="co">#&gt; disp   59175.36325 -59202.97211  10.91501090  21.38646548</span></span>
<span id="cb10-4"><a href="regular.html#cb10-4"></a><span class="co">#&gt; disp3 -59202.97211  59230.83035 -11.00617104 -21.54976679</span></span>
<span id="cb10-5"><a href="regular.html#cb10-5"></a><span class="co">#&gt; hp        10.91501    -11.00617   0.09032362   0.02220277</span></span>
<span id="cb10-6"><a href="regular.html#cb10-6"></a><span class="co">#&gt; wt        21.38647    -21.54977   0.02220277   0.16411837</span></span>
<span id="cb10-7"><a href="regular.html#cb10-7"></a></span>
<span id="cb10-8"><a href="regular.html#cb10-8"></a><span class="kw">solve</span>(<span class="kw">t</span>(X31) <span class="op">%*%</span><span class="st"> </span>X31)</span>
<span id="cb10-9"><a href="regular.html#cb10-9"></a><span class="co">#&gt;                 disp        disp31          hp          wt</span></span>
<span id="cb10-10"><a href="regular.html#cb10-10"></a><span class="co">#&gt; disp    5941.5946101 -5942.3977358  0.30661752  0.64947961</span></span>
<span id="cb10-11"><a href="regular.html#cb10-11"></a><span class="co">#&gt; disp31 -5942.3977358  5943.4373181 -0.39266978 -0.80278577</span></span>
<span id="cb10-12"><a href="regular.html#cb10-12"></a><span class="co">#&gt; hp         0.3066175    -0.3926698  0.08830442  0.01825147</span></span>
<span id="cb10-13"><a href="regular.html#cb10-13"></a><span class="co">#&gt; wt         0.6494796    -0.8027858  0.01825147  0.15638642</span></span></code></pre></div>
<p>This is what we may call a “butterfly effect.” By modifying just one cell in
<span class="math inline">\(\mathbf{X}_{31}\)</span> by adding a little amount of random noise, the inverses</p>
<p><span class="math inline">\((\mathbf{X}_{3}^\mathsf{T} \mathbf{X}_{3})^{-1}\)</span> and <span class="math inline">\(\mathbf{X}_{31}^\mathsf{T} \mathbf{X}_{31}\)</span> have changed dramatically.</p>
</div>
</div>
<div id="irregular-coefficients" class="section level2">
<h2><span class="header-section-number">11.2</span> Irregular Coefficients</h2>
<p>In OLS regression, the theoretical variance-covariance matrix of the regression
coefficients is given by:</p>
<p><span class="math display">\[
Var(\mathbf{\boldsymbol{\hat{\beta}}}) = 
\
\begin{bmatrix} 
Var(\hat{\beta}_1) &amp; Cov(\hat{\beta}_1, \hat{\beta}_2) &amp; \cdots &amp; Cov(\hat{\beta}_1, \hat{\beta}_p) \\
Cov(\hat{\beta}_2, \hat{\beta}_1) &amp; Var(\hat{\beta}_2) &amp; \cdots &amp; Cov(\hat{\beta}_2, \hat{\beta}_p) \\
\vdots &amp;  &amp; \ddots &amp; \vdots \\
Cov(\hat{\beta}_p, \hat{\beta}_1) &amp; Cov(\hat{\beta}_p, \hat{\beta}_2) &amp; \cdots &amp; Var(\hat{\beta}_p) \\
\end{bmatrix}
\]</span></p>
<p><span class="math display">\[
Var(\mathbf{\boldsymbol{\hat{\beta}}}) = \sigma^2 (\mathbf{X^\mathsf{T} X})^{-1}
\]</span></p>
<p>Doing some algebra,</p>
<p><span class="math display">\[
Var(\boldsymbol{\hat{\beta}}) = \sigma^2 (\mathbf{X^\mathsf{T} X})^{-1}
\]</span></p>
<p>The variance of a particular coefficient <span class="math inline">\(\hat{\beta}_j\)</span> is given by:</p>
<p><span class="math display">\[
Var(\hat{\beta}_j) = \sigma^2 \left [ (\mathbf{X^\mathsf{T} X})^{-1} \right ]_{jj} 
\]</span></p>
<p>where <span class="math inline">\(\left [ (\mathbf{X^\mathsf{T} X})^{-1} \right ]_{jj}\)</span> is the <span class="math inline">\(j\)</span>-th
diagonal element of <span class="math inline">\((\mathbf{X^\mathsf{T} X})^{-1}\)</span></p>
<p>A couple of remarks:</p>
<ul>
<li>Recall again that we don’t know <span class="math inline">\(\sigma^2\)</span>. How can we find an estimator
<span class="math inline">\(\hat{\sigma}^2\)</span>?</li>
<li>We don’t observe the error terms <span class="math inline">\(\boldsymbol{\varepsilon}\)</span> but we do have
the residuals <span class="math inline">\(\mathbf{e = y - \hat{y}}\)</span></li>
<li>As well as the Residual Sum of Squares (RSS)</li>
</ul>
<p><span class="math display">\[
\text{RSS} = \sum_{i=1}^{n} e_{i}^{2} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
\]</span></p>
<div id="effect-of-multicollinearity" class="section level3">
<h3><span class="header-section-number">11.2.1</span> Effect of Multicollinearity</h3>
<p>Assuming standardized variables, <span class="math inline">\(\mathbf{X^\mathsf{T} X} = n \mathbf{R}\)</span>
It can be shown that:</p>
<p><span class="math display">\[
Var(\boldsymbol{\hat{\beta}}) = \sigma^2 \left ( \frac{\mathbf{R}^{-1}}{n} \right )
\]</span></p>
<p>and <span class="math inline">\(Var(\hat{\beta}_j)\)</span> can then be expressed as:</p>
<p><span class="math display">\[
Var(\hat{\beta}_j) = \frac{\sigma^2}{n} [\mathbf{R}^{-1}]_{jj}
\]</span></p>
<p>It turns out that:</p>
<p><span class="math display">\[
[\mathbf{R}^{-1}]_{jj} = \frac{1}{1 - R_{j}^{2}}
\]</span></p>
<p>is known as the <strong>Variance Inflation Factor</strong> or VIF. If <span class="math inline">\(R_{j}^{2}\)</span> is close
to 1, then VIF will be large, and so <span class="math inline">\(Var(\hat{\beta})\)</span> will also be large.</p>
<p>If we write the eigenvalue decomposition of <span class="math inline">\(\mathbf{R}\)</span> as:</p>
<p><span class="math display">\[
\mathbf{R = V \boldsymbol{\Lambda} V^\mathsf{T}}
\]</span></p>
<p>then the inverse of <span class="math inline">\(\mathbf{R}\)</span> becomes:</p>
<p><span class="math display">\[
\mathbf{R^{-1} = V \boldsymbol{\Lambda}^{-1} V^\mathsf{T}}
\]</span></p>
<p>It can be shown that:</p>
<p><span class="math display">\[
Var(\hat{\beta}_j) = \left ( \frac{\sigma^2}{n} \right )  \sum_{l=1}^{p} \frac{v^{2}_{jl}}{\lambda_l}
\]</span></p>
<p>As you can tell, the variance of the estimators depends on the inverses of the
eigenvalues of <span class="math inline">\(\mathbf{R}\)</span>.
With very small eigenvalues, the larger the variance of the estimates.</p>
<p>In summary:</p>
<ul>
<li>the standard errors of <span class="math inline">\(\hat{\beta}_j\)</span> are inflated.</li>
<li>the fit is unstable, and becomes very sensitive to small perturbations.</li>
<li>small changes in <span class="math inline">\(Y\)</span> can lead to large changes in the coefficients.</li>
</ul>
<p>What would you do to overcome multicollinearity?</p>
<ul>
<li>Reduce number of predictors</li>
<li>If <span class="math inline">\(p &gt; n\)</span>, then try to get more observations (increase <span class="math inline">\(n\)</span>)</li>
<li>Find an orthogonal basis for the predictors</li>
<li>Impose constraints on the estimated coefficients</li>
<li>A mix of some or all of the above?</li>
<li><strong>Other ideas?</strong></li>
</ul>
</div>
</div>
<div id="regularization-metaphor" class="section level2">
<h2><span class="header-section-number">11.3</span> Regularization Metaphor</h2>
<p>In OLS, we want to find parameters that give us a linear combination of our
response variables. Think of this as if you go “shopping” for parameters.
We assume (initially) that we have a blank check, and can pick whatever
parameters we want. In regularization, we impose some sort of restriction
on what coefficients we can buy. We could use penalized methods or we could use
a slightly more lenient method: dimension reduction.</p>
<p>In dimension reduction, there are no explicit penalties, but we still impose a
“cost” on the regressors that are highly collinear. We reduce the number of
parameters that we can buy.</p>
<p>In penalized methods, we explicitly impose a budget on how much money we can
spend when buying parameters.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="validation.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="pcr.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["allmodelsarewrong.pdf", "allmodelsarewrong.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
