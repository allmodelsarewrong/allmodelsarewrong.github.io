<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>20 Kernel Smoothers | All Models Are Wrong: Concepts of Statistical Learning</title>
  <meta name="description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  <meta name="generator" content="bookdown 0.16 and GitBook 2.6.7" />

  <meta property="og:title" content="20 Kernel Smoothers | All Models Are Wrong: Concepts of Statistical Learning" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://allmodelsarewrong.github.io" />
  
  <meta property="og:description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  <meta name="github-repo" content="allmodelsarewrong/allmodelsarewrong.github.io" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="20 Kernel Smoothers | All Models Are Wrong: Concepts of Statistical Learning" />
  
  <meta name="twitter:description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  

<meta name="author" content="Gaston Sanchez Ethan Marzban" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="knn.html"/>
<link rel="next" href="classif.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><b>All Models Are Wrong (CSL)</b><br><small>G. Sanchez & E. Marzban</small></a></li>

<li class="divider"></li>
<li class="part"><span><b>I Welcome</b></span></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="about.html"><a href="about.html"><i class="fa fa-check"></i><b>1</b> About this book</a><ul>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#prerequisites"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="part"><span><b>II Intro</b></span></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#basic-notation"><i class="fa fa-check"></i><b>2.1</b> Basic Notation</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="duality.html"><a href="duality.html"><i class="fa fa-check"></i><b>3</b> Geometric Duality</a><ul>
<li class="chapter" data-level="3.1" data-path="duality.html"><a href="duality.html#rows-space"><i class="fa fa-check"></i><b>3.1</b> Rows Space</a></li>
<li class="chapter" data-level="3.2" data-path="duality.html"><a href="duality.html#columns-space"><i class="fa fa-check"></i><b>3.2</b> Columns Space</a></li>
<li class="chapter" data-level="3.3" data-path="duality.html"><a href="duality.html#cloud-of-individuals"><i class="fa fa-check"></i><b>3.3</b> Cloud of Individuals</a><ul>
<li class="chapter" data-level="3.3.1" data-path="duality.html"><a href="duality.html#average-individual"><i class="fa fa-check"></i><b>3.3.1</b> Average Individual</a></li>
<li class="chapter" data-level="3.3.2" data-path="duality.html"><a href="duality.html#centered-data"><i class="fa fa-check"></i><b>3.3.2</b> Centered Data</a></li>
<li class="chapter" data-level="3.3.3" data-path="duality.html"><a href="duality.html#distance-between-individuals"><i class="fa fa-check"></i><b>3.3.3</b> Distance between individuals</a></li>
<li class="chapter" data-level="3.3.4" data-path="duality.html"><a href="duality.html#distance-to-the-centroid"><i class="fa fa-check"></i><b>3.3.4</b> Distance to the centroid</a></li>
<li class="chapter" data-level="3.3.5" data-path="duality.html"><a href="duality.html#measures-of-dispersion"><i class="fa fa-check"></i><b>3.3.5</b> Measures of Dispersion</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="duality.html"><a href="duality.html#cloud-of-variables"><i class="fa fa-check"></i><b>3.4</b> Cloud of Variables</a><ul>
<li class="chapter" data-level="3.4.1" data-path="duality.html"><a href="duality.html#mean-of-a-variable"><i class="fa fa-check"></i><b>3.4.1</b> Mean of a Variable</a></li>
<li class="chapter" data-level="3.4.2" data-path="duality.html"><a href="duality.html#variance-of-a-variable"><i class="fa fa-check"></i><b>3.4.2</b> Variance of a Variable</a></li>
<li class="chapter" data-level="3.4.3" data-path="duality.html"><a href="duality.html#variance-with-vector-notation"><i class="fa fa-check"></i><b>3.4.3</b> Variance with Vector Notation</a></li>
<li class="chapter" data-level="3.4.4" data-path="duality.html"><a href="duality.html#standard-deviation-as-a-norm"><i class="fa fa-check"></i><b>3.4.4</b> Standard Deviation as a Norm</a></li>
<li class="chapter" data-level="3.4.5" data-path="duality.html"><a href="duality.html#covariance"><i class="fa fa-check"></i><b>3.4.5</b> Covariance</a></li>
<li class="chapter" data-level="3.4.6" data-path="duality.html"><a href="duality.html#correlation"><i class="fa fa-check"></i><b>3.4.6</b> Correlation</a></li>
<li class="chapter" data-level="3.4.7" data-path="duality.html"><a href="duality.html#geometry-of-correlation"><i class="fa fa-check"></i><b>3.4.7</b> Geometry of Correlation</a></li>
<li class="chapter" data-level="3.4.8" data-path="duality.html"><a href="duality.html#orthogonal-projections"><i class="fa fa-check"></i><b>3.4.8</b> Orthogonal Projections</a></li>
<li class="chapter" data-level="3.4.9" data-path="duality.html"><a href="duality.html#the-mean-as-an-orthogonal-projection"><i class="fa fa-check"></i><b>3.4.9</b> The mean as an orthogonal projection</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Unsupervised I: PCA</b></span></li>
<li class="chapter" data-level="4" data-path="pca.html"><a href="pca.html"><i class="fa fa-check"></i><b>4</b> Principal Components Analysis</a><ul>
<li class="chapter" data-level="4.1" data-path="pca.html"><a href="pca.html#low-dimensional-representations"><i class="fa fa-check"></i><b>4.1</b> Low-dimensional Representations</a></li>
<li class="chapter" data-level="4.2" data-path="pca.html"><a href="pca.html#projections"><i class="fa fa-check"></i><b>4.2</b> Projections</a><ul>
<li class="chapter" data-level="4.2.1" data-path="pca.html"><a href="pca.html#vector-and-scalar-projections"><i class="fa fa-check"></i><b>4.2.1</b> Vector and Scalar Projections</a></li>
<li class="chapter" data-level="4.2.2" data-path="pca.html"><a href="pca.html#projected-inertia"><i class="fa fa-check"></i><b>4.2.2</b> Projected Inertia</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="pca.html"><a href="pca.html#maximization-problem"><i class="fa fa-check"></i><b>4.3</b> Maximization Problem</a><ul>
<li class="chapter" data-level="4.3.1" data-path="pca.html"><a href="pca.html#eigenvectors-of-mathbfs"><i class="fa fa-check"></i><b>4.3.1</b> Eigenvectors of <span class="math inline">\(\mathbf{S}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="pca.html"><a href="pca.html#another-perspective-of-pca"><i class="fa fa-check"></i><b>4.4</b> Another Perspective of PCA</a><ul>
<li class="chapter" data-level="4.4.1" data-path="pca.html"><a href="pca.html#finding-principal-components"><i class="fa fa-check"></i><b>4.4.1</b> Finding Principal Components</a></li>
<li class="chapter" data-level="4.4.2" data-path="pca.html"><a href="pca.html#finding-the-first-pc"><i class="fa fa-check"></i><b>4.4.2</b> Finding the first PC</a></li>
<li class="chapter" data-level="4.4.3" data-path="pca.html"><a href="pca.html#finding-the-second-pc"><i class="fa fa-check"></i><b>4.4.3</b> Finding the second PC</a></li>
<li class="chapter" data-level="4.4.4" data-path="pca.html"><a href="pca.html#finding-all-pcs"><i class="fa fa-check"></i><b>4.4.4</b> Finding all PCs</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="pca.html"><a href="pca.html#data-decomposition-model"><i class="fa fa-check"></i><b>4.5</b> Data Decomposition Model</a><ul>
<li class="chapter" data-level="4.5.1" data-path="pca.html"><a href="pca.html#alternative-approaches"><i class="fa fa-check"></i><b>4.5.1</b> Alternative Approaches</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Linear Regression</b></span></li>
<li class="chapter" data-level="5" data-path="ols.html"><a href="ols.html"><i class="fa fa-check"></i><b>5</b> Linear Regression</a><ul>
<li class="chapter" data-level="5.1" data-path="ols.html"><a href="ols.html#motivation"><i class="fa fa-check"></i><b>5.1</b> Motivation</a></li>
<li class="chapter" data-level="5.2" data-path="ols.html"><a href="ols.html#the-ideaintuition-of-regression"><i class="fa fa-check"></i><b>5.2</b> The Idea/Intuition of Regression</a></li>
<li class="chapter" data-level="5.3" data-path="ols.html"><a href="ols.html#the-linear-regression-model"><i class="fa fa-check"></i><b>5.3</b> The Linear Regression Model</a></li>
<li class="chapter" data-level="5.4" data-path="ols.html"><a href="ols.html#the-error-measure"><i class="fa fa-check"></i><b>5.4</b> The Error Measure</a></li>
<li class="chapter" data-level="5.5" data-path="ols.html"><a href="ols.html#the-least-squares-algorithm"><i class="fa fa-check"></i><b>5.5</b> The Least Squares Algorithm</a></li>
<li class="chapter" data-level="5.6" data-path="ols.html"><a href="ols.html#geometries-of-ols"><i class="fa fa-check"></i><b>5.6</b> Geometries of OLS</a><ul>
<li class="chapter" data-level="5.6.1" data-path="ols.html"><a href="ols.html#rows-perspective"><i class="fa fa-check"></i><b>5.6.1</b> Rows Perspective</a></li>
<li class="chapter" data-level="5.6.2" data-path="ols.html"><a href="ols.html#columns-perspective"><i class="fa fa-check"></i><b>5.6.2</b> Columns Perspective</a></li>
<li class="chapter" data-level="5.6.3" data-path="ols.html"><a href="ols.html#parameters-perspective"><i class="fa fa-check"></i><b>5.6.3</b> Parameters Perspective</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="gradient.html"><a href="gradient.html"><i class="fa fa-check"></i><b>6</b> Gradient Descent</a><ul>
<li class="chapter" data-level="6.1" data-path="gradient.html"><a href="gradient.html#error-surface"><i class="fa fa-check"></i><b>6.1</b> Error Surface</a></li>
<li class="chapter" data-level="6.2" data-path="gradient.html"><a href="gradient.html#idea-of-gradient-descent"><i class="fa fa-check"></i><b>6.2</b> Idea of Gradient Descent</a></li>
<li class="chapter" data-level="6.3" data-path="gradient.html"><a href="gradient.html#moving-down-an-error-surface"><i class="fa fa-check"></i><b>6.3</b> Moving Down an Error Surface</a><ul>
<li class="chapter" data-level="6.3.1" data-path="gradient.html"><a href="gradient.html#the-direction-of-mathbfv"><i class="fa fa-check"></i><b>6.3.1</b> The direction of <span class="math inline">\(\mathbf{v}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="gradient.html"><a href="gradient.html#gradient-descent-and-our-model"><i class="fa fa-check"></i><b>6.4</b> Gradient Descent and our Model</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="olsml.html"><a href="olsml.html"><i class="fa fa-check"></i><b>7</b> Regression via Maximum Likelihood</a><ul>
<li class="chapter" data-level="7.1" data-path="olsml.html"><a href="olsml.html#linear-regression-reminder"><i class="fa fa-check"></i><b>7.1</b> Linear Regression Reminder</a><ul>
<li class="chapter" data-level="7.1.1" data-path="olsml.html"><a href="olsml.html#maximum-likelihood"><i class="fa fa-check"></i><b>7.1.1</b> Maximum Likelihood</a></li>
<li class="chapter" data-level="7.1.2" data-path="olsml.html"><a href="olsml.html#ml-estimator-of-sigma2"><i class="fa fa-check"></i><b>7.1.2</b> ML Estimator of <span class="math inline">\(\sigma^2\)</span></a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>V Learning Concepts</b></span></li>
<li class="chapter" data-level="8" data-path="learning.html"><a href="learning.html"><i class="fa fa-check"></i><b>8</b> Theoretical Framework</a><ul>
<li class="chapter" data-level="8.1" data-path="learning.html"><a href="learning.html#mental-map"><i class="fa fa-check"></i><b>8.1</b> Mental Map</a></li>
<li class="chapter" data-level="8.2" data-path="learning.html"><a href="learning.html#kinds-of-predictions"><i class="fa fa-check"></i><b>8.2</b> Kinds of Predictions</a><ul>
<li class="chapter" data-level="8.2.1" data-path="learning.html"><a href="learning.html#two-types-of-data"><i class="fa fa-check"></i><b>8.2.1</b> Two Types of Data</a></li>
<li class="chapter" data-level="8.2.2" data-path="learning.html"><a href="learning.html#two-types-of-predictions"><i class="fa fa-check"></i><b>8.2.2</b> Two Types of Predictions</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="learning.html"><a href="learning.html#two-types-of-errors"><i class="fa fa-check"></i><b>8.3</b> Two Types of Errors</a><ul>
<li class="chapter" data-level="8.3.1" data-path="learning.html"><a href="learning.html#individual-errors"><i class="fa fa-check"></i><b>8.3.1</b> Individual Errors</a></li>
<li class="chapter" data-level="8.3.2" data-path="learning.html"><a href="learning.html#overall-errors"><i class="fa fa-check"></i><b>8.3.2</b> Overall Errors</a></li>
<li class="chapter" data-level="8.3.3" data-path="learning.html"><a href="learning.html#auxiliary-technicality"><i class="fa fa-check"></i><b>8.3.3</b> Auxiliary Technicality</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="learning.html"><a href="learning.html#noisy-targets"><i class="fa fa-check"></i><b>8.4</b> Noisy Targets</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="mse.html"><a href="mse.html"><i class="fa fa-check"></i><b>9</b> MSE of Estimator</a><ul>
<li class="chapter" data-level="9.1" data-path="mse.html"><a href="mse.html#mse-of-an-estimator"><i class="fa fa-check"></i><b>9.1</b> MSE of an Estimator</a><ul>
<li class="chapter" data-level="9.1.1" data-path="mse.html"><a href="mse.html#prototypical-cases-of-bias-and-variance"><i class="fa fa-check"></i><b>9.1.1</b> Prototypical Cases of Bias and Variance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="biasvar.html"><a href="biasvar.html"><i class="fa fa-check"></i><b>10</b> Bias-Variance Tradeoff</a><ul>
<li class="chapter" data-level="10.1" data-path="biasvar.html"><a href="biasvar.html#introduction"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="biasvar.html"><a href="biasvar.html#motivation-example"><i class="fa fa-check"></i><b>10.2</b> Motivation Example</a><ul>
<li class="chapter" data-level="10.2.1" data-path="biasvar.html"><a href="biasvar.html#two-hypotheses"><i class="fa fa-check"></i><b>10.2.1</b> Two Hypotheses</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="biasvar.html"><a href="biasvar.html#learning-from-two-points"><i class="fa fa-check"></i><b>10.3</b> Learning from two points</a></li>
<li class="chapter" data-level="10.4" data-path="biasvar.html"><a href="biasvar.html#bias-variance-derivation"><i class="fa fa-check"></i><b>10.4</b> Bias-Variance Derivation</a><ul>
<li class="chapter" data-level="10.4.1" data-path="biasvar.html"><a href="biasvar.html#out-of-sample-predictions"><i class="fa fa-check"></i><b>10.4.1</b> Out-of-Sample Predictions</a></li>
<li class="chapter" data-level="10.4.2" data-path="biasvar.html"><a href="biasvar.html#noisy-target"><i class="fa fa-check"></i><b>10.4.2</b> Noisy Target</a></li>
<li class="chapter" data-level="10.4.3" data-path="biasvar.html"><a href="biasvar.html#types-of-theoretical-mses"><i class="fa fa-check"></i><b>10.4.3</b> Types of Theoretical MSEs</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="biasvar.html"><a href="biasvar.html#the-tradeoff"><i class="fa fa-check"></i><b>10.5</b> The Tradeoff</a><ul>
<li class="chapter" data-level="10.5.1" data-path="biasvar.html"><a href="biasvar.html#bias-variance-tradeoff-picture"><i class="fa fa-check"></i><b>10.5.1</b> Bias-Variance Tradeoff Picture</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="phases.html"><a href="phases.html"><i class="fa fa-check"></i><b>11</b> Learning Phases</a><ul>
<li class="chapter" data-level="11.1" data-path="phases.html"><a href="phases.html#introduction-1"><i class="fa fa-check"></i><b>11.1</b> Introduction</a></li>
<li class="chapter" data-level="11.2" data-path="phases.html"><a href="phases.html#model-assessment"><i class="fa fa-check"></i><b>11.2</b> Model Assessment</a><ul>
<li class="chapter" data-level="11.2.1" data-path="phases.html"><a href="phases.html#holdout-test-set"><i class="fa fa-check"></i><b>11.2.1</b> Holdout Test Set</a></li>
<li class="chapter" data-level="11.2.2" data-path="phases.html"><a href="phases.html#why-does-a-test-set-work"><i class="fa fa-check"></i><b>11.2.2</b> Why does a test set work?</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="phases.html"><a href="phases.html#model-selection"><i class="fa fa-check"></i><b>11.3</b> Model Selection</a><ul>
<li class="chapter" data-level="11.3.1" data-path="phases.html"><a href="phases.html#three-way-holdout-method"><i class="fa fa-check"></i><b>11.3.1</b> Three-way Holdout Method</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="phases.html"><a href="phases.html#model-training"><i class="fa fa-check"></i><b>11.4</b> Model Training</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="resampling.html"><a href="resampling.html"><i class="fa fa-check"></i><b>12</b> Resample Approaches</a><ul>
<li class="chapter" data-level="12.1" data-path="resampling.html"><a href="resampling.html#general-sampling-blueprint"><i class="fa fa-check"></i><b>12.1</b> General Sampling Blueprint</a></li>
<li class="chapter" data-level="12.2" data-path="resampling.html"><a href="resampling.html#monte-carlo-cross-validation"><i class="fa fa-check"></i><b>12.2</b> Monte Carlo Cross Validation</a></li>
<li class="chapter" data-level="12.3" data-path="resampling.html"><a href="resampling.html#bootstrap-method"><i class="fa fa-check"></i><b>12.3</b> Bootstrap Method</a></li>
<li class="chapter" data-level="12.4" data-path="resampling.html"><a href="resampling.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>12.4</b> <span class="math inline">\(K\)</span>-Fold Cross-Validation</a><ul>
<li class="chapter" data-level="12.4.1" data-path="resampling.html"><a href="resampling.html#leave-one-out-cross-validation-loocv"><i class="fa fa-check"></i><b>12.4.1</b> Leave-One-Out Cross Validation (LOOCV)</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>VI Regularization</b></span></li>
<li class="chapter" data-level="13" data-path="regular.html"><a href="regular.html"><i class="fa fa-check"></i><b>13</b> Regularization Techniques</a><ul>
<li class="chapter" data-level="13.1" data-path="regular.html"><a href="regular.html#multicollinearity-issues"><i class="fa fa-check"></i><b>13.1</b> Multicollinearity Issues</a><ul>
<li class="chapter" data-level="13.1.1" data-path="regular.html"><a href="regular.html#toy-example"><i class="fa fa-check"></i><b>13.1.1</b> Toy Example</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="regular.html"><a href="regular.html#irregular-coefficients"><i class="fa fa-check"></i><b>13.2</b> Irregular Coefficients</a></li>
<li class="chapter" data-level="13.3" data-path="regular.html"><a href="regular.html#connection-to-regularization"><i class="fa fa-check"></i><b>13.3</b> Connection to Regularization</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="pcr.html"><a href="pcr.html"><i class="fa fa-check"></i><b>14</b> Principal Components Regression</a><ul>
<li class="chapter" data-level="14.1" data-path="pcr.html"><a href="pcr.html#motivation-example-1"><i class="fa fa-check"></i><b>14.1</b> Motivation Example</a></li>
<li class="chapter" data-level="14.2" data-path="pcr.html"><a href="pcr.html#the-pcr-model"><i class="fa fa-check"></i><b>14.2</b> The PCR Model</a></li>
<li class="chapter" data-level="14.3" data-path="pcr.html"><a href="pcr.html#how-does-pcr-work"><i class="fa fa-check"></i><b>14.3</b> How does PCR work?</a><ul>
<li class="chapter" data-level="14.3.1" data-path="pcr.html"><a href="pcr.html#transition-formula"><i class="fa fa-check"></i><b>14.3.1</b> Transition Formula</a></li>
<li class="chapter" data-level="14.3.2" data-path="pcr.html"><a href="pcr.html#size-of-coefficients"><i class="fa fa-check"></i><b>14.3.2</b> Size of Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="pcr.html"><a href="pcr.html#selecting-number-of-pcs"><i class="fa fa-check"></i><b>14.4</b> Selecting Number of PCs</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="pls.html"><a href="pls.html"><i class="fa fa-check"></i><b>15</b> Partial Least Squares Regression</a><ul>
<li class="chapter" data-level="15.1" data-path="pls.html"><a href="pls.html#motivation-example-2"><i class="fa fa-check"></i><b>15.1</b> Motivation Example</a></li>
<li class="chapter" data-level="15.2" data-path="pls.html"><a href="pls.html#the-plsr-model"><i class="fa fa-check"></i><b>15.2</b> The PLSR Model</a></li>
<li class="chapter" data-level="15.3" data-path="pls.html"><a href="pls.html#how-does-plsr-work"><i class="fa fa-check"></i><b>15.3</b> How does PLSR work?</a></li>
<li class="chapter" data-level="15.4" data-path="pls.html"><a href="pls.html#plsr-algorithm"><i class="fa fa-check"></i><b>15.4</b> PLSR Algorithm</a><ul>
<li class="chapter" data-level="15.4.1" data-path="pls.html"><a href="pls.html#pls-solution-with-original-variables"><i class="fa fa-check"></i><b>15.4.1</b> PLS Solution with original variables</a></li>
<li class="chapter" data-level="15.4.2" data-path="pls.html"><a href="pls.html#size-of-coefficients-1"><i class="fa fa-check"></i><b>15.4.2</b> Size of Coefficients</a></li>
<li class="chapter" data-level="15.4.3" data-path="pls.html"><a href="pls.html#some-properties"><i class="fa fa-check"></i><b>15.4.3</b> Some Properties</a></li>
</ul></li>
<li class="chapter" data-level="15.5" data-path="pls.html"><a href="pls.html#selecting-number-of-pls-components"><i class="fa fa-check"></i><b>15.5</b> Selecting Number of PLS Components</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="ridge.html"><a href="ridge.html"><i class="fa fa-check"></i><b>16</b> Ridge Regression</a><ul>
<li class="chapter" data-level="16.1" data-path="ridge.html"><a href="ridge.html#a-new-minimization-problem"><i class="fa fa-check"></i><b>16.1</b> A New Minimization Problem</a><ul>
<li class="chapter" data-level="16.1.1" data-path="ridge.html"><a href="ridge.html#constraining-regression-coefficients"><i class="fa fa-check"></i><b>16.1.1</b> Constraining Regression Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="ridge.html"><a href="ridge.html#a-new-minimization-solution"><i class="fa fa-check"></i><b>16.2</b> A New Minimization Solution</a></li>
<li class="chapter" data-level="16.3" data-path="ridge.html"><a href="ridge.html#what-does-rr-accomplish"><i class="fa fa-check"></i><b>16.3</b> What does RR accomplish?</a><ul>
<li class="chapter" data-level="16.3.1" data-path="ridge.html"><a href="ridge.html#how-to-find-lambda"><i class="fa fa-check"></i><b>16.3.1</b> How to find <span class="math inline">\(\lambda\)</span>?</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>VII Extending Linear Regression</b></span></li>
<li class="chapter" data-level="17" data-path="linear-extensions.html"><a href="linear-extensions.html"><i class="fa fa-check"></i><b>17</b> Beyond Linear Regression</a><ul>
<li class="chapter" data-level="17.1" data-path="linear-extensions.html"><a href="linear-extensions.html#introduction-2"><i class="fa fa-check"></i><b>17.1</b> Introduction</a></li>
<li class="chapter" data-level="17.2" data-path="linear-extensions.html"><a href="linear-extensions.html#expanding-the-regression-horizon"><i class="fa fa-check"></i><b>17.2</b> Expanding the Regression Horizon</a><ul>
<li class="chapter" data-level="17.2.1" data-path="linear-extensions.html"><a href="linear-extensions.html#linearity"><i class="fa fa-check"></i><b>17.2.1</b> Linearity</a></li>
<li class="chapter" data-level="17.2.2" data-path="linear-extensions.html"><a href="linear-extensions.html#parametric-and-nonparametric"><i class="fa fa-check"></i><b>17.2.2</b> Parametric and Nonparametric</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="linear-extensions.html"><a href="linear-extensions.html#transforming-features"><i class="fa fa-check"></i><b>17.3</b> Transforming Features</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="nonparametric.html"><a href="nonparametric.html"><i class="fa fa-check"></i><b>18</b> Nonparametric Regression</a><ul>
<li class="chapter" data-level="18.1" data-path="nonparametric.html"><a href="nonparametric.html#conditional-averages"><i class="fa fa-check"></i><b>18.1</b> Conditional Averages</a></li>
<li class="chapter" data-level="18.2" data-path="nonparametric.html"><a href="nonparametric.html#looking-at-the-neighbors"><i class="fa fa-check"></i><b>18.2</b> Looking at the Neighbors</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="knn.html"><a href="knn.html"><i class="fa fa-check"></i><b>19</b> Nearest Neighbor Estimates</a><ul>
<li class="chapter" data-level="19.1" data-path="knn.html"><a href="knn.html#introduction-3"><i class="fa fa-check"></i><b>19.1</b> Introduction</a></li>
<li class="chapter" data-level="19.2" data-path="knn.html"><a href="knn.html#k-nearest-neighbors-knn"><i class="fa fa-check"></i><b>19.2</b> <span class="math inline">\(k\)</span> Nearest Neighbors (KNN)</a><ul>
<li class="chapter" data-level="19.2.1" data-path="knn.html"><a href="knn.html#distance-measures"><i class="fa fa-check"></i><b>19.2.1</b> Distance Measures</a></li>
<li class="chapter" data-level="19.2.2" data-path="knn.html"><a href="knn.html#knn-estimator"><i class="fa fa-check"></i><b>19.2.2</b> KNN Estimator</a></li>
<li class="chapter" data-level="19.2.3" data-path="knn.html"><a href="knn.html#how-to-find-k"><i class="fa fa-check"></i><b>19.2.3</b> How to find <span class="math inline">\(k\)</span>?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="20" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html"><i class="fa fa-check"></i><b>20</b> Kernel Smoothers</a><ul>
<li class="chapter" data-level="20.1" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#introduction-4"><i class="fa fa-check"></i><b>20.1</b> Introduction</a></li>
<li class="chapter" data-level="20.2" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#kernel-smoothers-1"><i class="fa fa-check"></i><b>20.2</b> Kernel Smoothers</a><ul>
<li class="chapter" data-level="20.2.1" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#kernel-functions"><i class="fa fa-check"></i><b>20.2.1</b> Kernel Functions</a></li>
<li class="chapter" data-level="20.2.2" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#weights-from-kernels"><i class="fa fa-check"></i><b>20.2.2</b> Weights from Kernels</a></li>
<li class="chapter" data-level="20.2.3" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#kernel-estimator"><i class="fa fa-check"></i><b>20.2.3</b> Kernel Estimator</a></li>
</ul></li>
<li class="chapter" data-level="20.3" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#local-polynomial-estimators"><i class="fa fa-check"></i><b>20.3</b> Local Polynomial Estimators</a></li>
</ul></li>
<li class="part"><span><b>VIII Classification</b></span></li>
<li class="chapter" data-level="21" data-path="classif.html"><a href="classif.html"><i class="fa fa-check"></i><b>21</b> Classification</a><ul>
<li class="chapter" data-level="21.1" data-path="classif.html"><a href="classif.html#introduction-5"><i class="fa fa-check"></i><b>21.1</b> Introduction</a><ul>
<li class="chapter" data-level="21.1.1" data-path="classif.html"><a href="classif.html#credit-score-example"><i class="fa fa-check"></i><b>21.1.1</b> Credit Score Example</a></li>
<li class="chapter" data-level="21.1.2" data-path="classif.html"><a href="classif.html#toy-example-1"><i class="fa fa-check"></i><b>21.1.2</b> Toy Example</a></li>
<li class="chapter" data-level="21.1.3" data-path="classif.html"><a href="classif.html#two-class-problem"><i class="fa fa-check"></i><b>21.1.3</b> Two-class Problem</a></li>
<li class="chapter" data-level="21.1.4" data-path="classif.html"><a href="classif.html#bayes-rule-reminder"><i class="fa fa-check"></i><b>21.1.4</b> Bayesâ€™ Rule Reminder</a></li>
</ul></li>
<li class="chapter" data-level="21.2" data-path="classif.html"><a href="classif.html#bayes-classifier"><i class="fa fa-check"></i><b>21.2</b> Bayes Classifier</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="logistic.html"><a href="logistic.html"><i class="fa fa-check"></i><b>22</b> Logistic Regression</a><ul>
<li class="chapter" data-level="22.1" data-path="logistic.html"><a href="logistic.html#motivation-1"><i class="fa fa-check"></i><b>22.1</b> Motivation</a><ul>
<li class="chapter" data-level="22.1.1" data-path="logistic.html"><a href="logistic.html#first-approach-fitting-a-line"><i class="fa fa-check"></i><b>22.1.1</b> First Approach: Fitting a Line</a></li>
<li class="chapter" data-level="22.1.2" data-path="logistic.html"><a href="logistic.html#second-approach-harsh-thresholding"><i class="fa fa-check"></i><b>22.1.2</b> Second Approach: Harsh Thresholding</a></li>
<li class="chapter" data-level="22.1.3" data-path="logistic.html"><a href="logistic.html#third-approach-conditional-means"><i class="fa fa-check"></i><b>22.1.3</b> Third Approach: Conditional Means</a></li>
</ul></li>
<li class="chapter" data-level="22.2" data-path="logistic.html"><a href="logistic.html#logistic-regression-model"><i class="fa fa-check"></i><b>22.2</b> Logistic Regression Model</a><ul>
<li class="chapter" data-level="22.2.1" data-path="logistic.html"><a href="logistic.html#the-criterion-being-optimized"><i class="fa fa-check"></i><b>22.2.1</b> The Criterion Being Optimized</a></li>
<li class="chapter" data-level="22.2.2" data-path="logistic.html"><a href="logistic.html#another-way-to-solve-logistic-regression"><i class="fa fa-check"></i><b>22.2.2</b> Another Way to Solve Logistic Regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="23" data-path="discrim.html"><a href="discrim.html"><i class="fa fa-check"></i><b>23</b> Preamble for Discriminant Analysis</a><ul>
<li class="chapter" data-level="23.1" data-path="discrim.html"><a href="discrim.html#motivation-2"><i class="fa fa-check"></i><b>23.1</b> Motivation</a><ul>
<li class="chapter" data-level="23.1.1" data-path="discrim.html"><a href="discrim.html#distinguishing-species"><i class="fa fa-check"></i><b>23.1.1</b> Distinguishing Species</a></li>
<li class="chapter" data-level="23.1.2" data-path="discrim.html"><a href="discrim.html#sum-of-squares-decomposition"><i class="fa fa-check"></i><b>23.1.2</b> Sum of Squares Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="23.2" data-path="discrim.html"><a href="discrim.html#derived-ratios-from-sum-of-squares"><i class="fa fa-check"></i><b>23.2</b> Derived Ratios from Sum-of-Squares</a><ul>
<li class="chapter" data-level="23.2.1" data-path="discrim.html"><a href="discrim.html#correlation-ratio"><i class="fa fa-check"></i><b>23.2.1</b> Correlation Ratio</a></li>
<li class="chapter" data-level="23.2.2" data-path="discrim.html"><a href="discrim.html#f-ratio"><i class="fa fa-check"></i><b>23.2.2</b> F-Ratio</a></li>
</ul></li>
<li class="chapter" data-level="23.3" data-path="discrim.html"><a href="discrim.html#geometric-perspective"><i class="fa fa-check"></i><b>23.3</b> Geometric Perspective</a><ul>
<li class="chapter" data-level="23.3.1" data-path="discrim.html"><a href="discrim.html#clouds-from-class-structure"><i class="fa fa-check"></i><b>23.3.1</b> Clouds from Class Structure</a></li>
<li class="chapter" data-level="23.3.2" data-path="discrim.html"><a href="discrim.html#dispersion-decomposition"><i class="fa fa-check"></i><b>23.3.2</b> Dispersion Decomposition</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="24" data-path="cda.html"><a href="cda.html"><i class="fa fa-check"></i><b>24</b> Canonical Discriminant Analysis</a><ul>
<li class="chapter" data-level="24.1" data-path="cda.html"><a href="cda.html#cda-semi-supervised-aspect"><i class="fa fa-check"></i><b>24.1</b> CDA: Semi-Supervised Aspect</a></li>
<li class="chapter" data-level="24.2" data-path="cda.html"><a href="cda.html#looking-for-a-discriminant-axis"><i class="fa fa-check"></i><b>24.2</b> Looking for a discriminant axis</a></li>
<li class="chapter" data-level="24.3" data-path="cda.html"><a href="cda.html#looking-for-a-compromise-criterion"><i class="fa fa-check"></i><b>24.3</b> Looking for a Compromise Criterion</a><ul>
<li class="chapter" data-level="24.3.1" data-path="cda.html"><a href="cda.html#correlation-ratio-criterion"><i class="fa fa-check"></i><b>24.3.1</b> Correlation Ratio Criterion</a></li>
<li class="chapter" data-level="24.3.2" data-path="cda.html"><a href="cda.html#f-ratio-criterion"><i class="fa fa-check"></i><b>24.3.2</b> F-ratio Criterion</a></li>
<li class="chapter" data-level="24.3.3" data-path="cda.html"><a href="cda.html#a-special-pca"><i class="fa fa-check"></i><b>24.3.3</b> A Special PCA</a></li>
</ul></li>
<li class="chapter" data-level="24.4" data-path="cda.html"><a href="cda.html#cda-supervised-aspect"><i class="fa fa-check"></i><b>24.4</b> CDA: Supervised Aspect</a><ul>
<li class="chapter" data-level="24.4.1" data-path="cda.html"><a href="cda.html#distance-behind-cda"><i class="fa fa-check"></i><b>24.4.1</b> Distance behind CDA</a></li>
<li class="chapter" data-level="24.4.2" data-path="cda.html"><a href="cda.html#predictive-idea"><i class="fa fa-check"></i><b>24.4.2</b> Predictive Idea</a></li>
<li class="chapter" data-level="24.4.3" data-path="cda.html"><a href="cda.html#cda-classifier"><i class="fa fa-check"></i><b>24.4.3</b> CDA Classifier</a></li>
<li class="chapter" data-level="24.4.4" data-path="cda.html"><a href="cda.html#limitations-of-cda-classifier"><i class="fa fa-check"></i><b>24.4.4</b> Limitations of CDA classifier</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="25" data-path="discanalysis.html"><a href="discanalysis.html"><i class="fa fa-check"></i><b>25</b> Discriminant Analysis</a><ul>
<li class="chapter" data-level="25.1" data-path="discanalysis.html"><a href="discanalysis.html#probabilistic-da"><i class="fa fa-check"></i><b>25.1</b> Probabilistic DA</a><ul>
<li class="chapter" data-level="25.1.1" data-path="discanalysis.html"><a href="discanalysis.html#normal-distributions"><i class="fa fa-check"></i><b>25.1.1</b> Normal Distributions</a></li>
<li class="chapter" data-level="25.1.2" data-path="discanalysis.html"><a href="discanalysis.html#estimating-parameters-of-normal-distributions"><i class="fa fa-check"></i><b>25.1.2</b> Estimating Parameters of Normal Distributions</a></li>
</ul></li>
<li class="chapter" data-level="25.2" data-path="discanalysis.html"><a href="discanalysis.html#discriminant-functions"><i class="fa fa-check"></i><b>25.2</b> Discriminant Functions</a></li>
<li class="chapter" data-level="25.3" data-path="discanalysis.html"><a href="discanalysis.html#quadratic-discriminant-analysis-qda"><i class="fa fa-check"></i><b>25.3</b> Quadratic Discriminant Analysis (QDA)</a></li>
<li class="chapter" data-level="25.4" data-path="discanalysis.html"><a href="discanalysis.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>25.4</b> Linear Discriminant Analysis</a><ul>
<li class="chapter" data-level="25.4.1" data-path="discanalysis.html"><a href="discanalysis.html#canonical-discriminant-analysis"><i class="fa fa-check"></i><b>25.4.1</b> Canonical Discriminant Analysis</a></li>
<li class="chapter" data-level="25.4.2" data-path="discanalysis.html"><a href="discanalysis.html#naive-bayes"><i class="fa fa-check"></i><b>25.4.2</b> Naive Bayes</a></li>
<li class="chapter" data-level="25.4.3" data-path="discanalysis.html"><a href="discanalysis.html#fifth-case"><i class="fa fa-check"></i><b>25.4.3</b> Fifth Case</a></li>
</ul></li>
<li class="chapter" data-level="25.5" data-path="discanalysis.html"><a href="discanalysis.html#comparing-the-cases"><i class="fa fa-check"></i><b>25.5</b> Comparing the Cases</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="classperformance.html"><a href="classperformance.html"><i class="fa fa-check"></i><b>26</b> Performance of Classifiers</a><ul>
<li class="chapter" data-level="26.1" data-path="classperformance.html"><a href="classperformance.html#classification-error-measures"><i class="fa fa-check"></i><b>26.1</b> Classification Error Measures</a><ul>
<li class="chapter" data-level="26.1.1" data-path="classperformance.html"><a href="classperformance.html#errors-for-binary-response"><i class="fa fa-check"></i><b>26.1.1</b> Errors for Binary Response</a></li>
<li class="chapter" data-level="26.1.2" data-path="classperformance.html"><a href="classperformance.html#error-for-categorical-response"><i class="fa fa-check"></i><b>26.1.2</b> Error for Categorical Response</a></li>
</ul></li>
<li class="chapter" data-level="26.2" data-path="classperformance.html"><a href="classperformance.html#confusion-matrices"><i class="fa fa-check"></i><b>26.2</b> Confusion Matrices</a></li>
<li class="chapter" data-level="26.3" data-path="classperformance.html"><a href="classperformance.html#binary-response-example"><i class="fa fa-check"></i><b>26.3</b> Binary Response Example</a><ul>
<li class="chapter" data-level="26.3.1" data-path="classperformance.html"><a href="classperformance.html#application-for-checking-account"><i class="fa fa-check"></i><b>26.3.1</b> Application for Checking Account</a></li>
<li class="chapter" data-level="26.3.2" data-path="classperformance.html"><a href="classperformance.html#application-for-loan"><i class="fa fa-check"></i><b>26.3.2</b> Application for Loan</a></li>
</ul></li>
<li class="chapter" data-level="26.4" data-path="classperformance.html"><a href="classperformance.html#decision-rules-and-errors"><i class="fa fa-check"></i><b>26.4</b> Decision Rules and Errors</a></li>
<li class="chapter" data-level="26.5" data-path="classperformance.html"><a href="classperformance.html#roc-curves"><i class="fa fa-check"></i><b>26.5</b> ROC Curves</a><ul>
<li class="chapter" data-level="26.5.1" data-path="classperformance.html"><a href="classperformance.html#graphing-roc-curves"><i class="fa fa-check"></i><b>26.5.1</b> Graphing ROC curves</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IX Unsupervised II: Clustering</b></span></li>
<li class="chapter" data-level="27" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>27</b> Clustering</a><ul>
<li class="chapter" data-level="27.1" data-path="clustering.html"><a href="clustering.html#about-clustering"><i class="fa fa-check"></i><b>27.1</b> About Clustering</a><ul>
<li class="chapter" data-level="27.1.1" data-path="clustering.html"><a href="clustering.html#types-of-clustering"><i class="fa fa-check"></i><b>27.1.1</b> Types of Clustering</a></li>
<li class="chapter" data-level="27.1.2" data-path="clustering.html"><a href="clustering.html#hard-clustering"><i class="fa fa-check"></i><b>27.1.2</b> Hard Clustering</a></li>
</ul></li>
<li class="chapter" data-level="27.2" data-path="clustering.html"><a href="clustering.html#dispersion-measures"><i class="fa fa-check"></i><b>27.2</b> Dispersion Measures</a></li>
<li class="chapter" data-level="27.3" data-path="clustering.html"><a href="clustering.html#complexity-in-clustering"><i class="fa fa-check"></i><b>27.3</b> Complexity in Clustering</a></li>
</ul></li>
<li class="chapter" data-level="28" data-path="kmeans.html"><a href="kmeans.html"><i class="fa fa-check"></i><b>28</b> K-Means</a><ul>
<li class="chapter" data-level="28.1" data-path="kmeans.html"><a href="kmeans.html#toy-example-2"><i class="fa fa-check"></i><b>28.1</b> Toy Example</a></li>
<li class="chapter" data-level="28.2" data-path="kmeans.html"><a href="kmeans.html#what-does-k-means-do"><i class="fa fa-check"></i><b>28.2</b> What does K-means do?</a></li>
<li class="chapter" data-level="28.3" data-path="kmeans.html"><a href="kmeans.html#k-means-algorithms"><i class="fa fa-check"></i><b>28.3</b> K-Means Algorithms</a><ul>
<li class="chapter" data-level="28.3.1" data-path="kmeans.html"><a href="kmeans.html#classic-version"><i class="fa fa-check"></i><b>28.3.1</b> Classic Version</a></li>
<li class="chapter" data-level="28.3.2" data-path="kmeans.html"><a href="kmeans.html#moving-centers-algorithm"><i class="fa fa-check"></i><b>28.3.2</b> Moving Centers Algorithm</a></li>
<li class="chapter" data-level="28.3.3" data-path="kmeans.html"><a href="kmeans.html#dynamic-clouds"><i class="fa fa-check"></i><b>28.3.3</b> Dynamic Clouds</a></li>
<li class="chapter" data-level="28.3.4" data-path="kmeans.html"><a href="kmeans.html#choosing-k"><i class="fa fa-check"></i><b>28.3.4</b> Choosing <span class="math inline">\(K\)</span></a></li>
<li class="chapter" data-level="28.3.5" data-path="kmeans.html"><a href="kmeans.html#comments"><i class="fa fa-check"></i><b>28.3.5</b> Comments</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="29" data-path="hclus.html"><a href="hclus.html"><i class="fa fa-check"></i><b>29</b> Hierarchical Clustering</a><ul>
<li class="chapter" data-level="29.1" data-path="hclus.html"><a href="hclus.html#agglomerative-methods"><i class="fa fa-check"></i><b>29.1</b> Agglomerative Methods</a></li>
<li class="chapter" data-level="29.2" data-path="hclus.html"><a href="hclus.html#example-single-linkage"><i class="fa fa-check"></i><b>29.2</b> Example: Single Linkage</a><ul>
<li class="chapter" data-level="29.2.1" data-path="hclus.html"><a href="hclus.html#dendrogram"><i class="fa fa-check"></i><b>29.2.1</b> Dendrogram</a></li>
</ul></li>
<li class="chapter" data-level="29.3" data-path="hclus.html"><a href="hclus.html#example-complete-linkage"><i class="fa fa-check"></i><b>29.3</b> Example: Complete Linkage</a><ul>
<li class="chapter" data-level="29.3.1" data-path="hclus.html"><a href="hclus.html#cutting-dendograms"><i class="fa fa-check"></i><b>29.3.1</b> Cutting Dendograms</a></li>
<li class="chapter" data-level="29.3.2" data-path="hclus.html"><a href="hclus.html#pros-and-coons"><i class="fa fa-check"></i><b>29.3.2</b> Pros and COons</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>X Tree-based Methods</b></span></li>
<li class="chapter" data-level="30" data-path="trees.html"><a href="trees.html"><i class="fa fa-check"></i><b>30</b> Intro to Decision Trees</a><ul>
<li class="chapter" data-level="30.1" data-path="trees.html"><a href="trees.html#introduction-6"><i class="fa fa-check"></i><b>30.1</b> Introduction</a></li>
<li class="chapter" data-level="30.2" data-path="trees.html"><a href="trees.html#some-terminology"><i class="fa fa-check"></i><b>30.2</b> Some Terminology</a><ul>
<li class="chapter" data-level="30.2.1" data-path="trees.html"><a href="trees.html#binary-trees"><i class="fa fa-check"></i><b>30.2.1</b> Binary Trees</a></li>
</ul></li>
<li class="chapter" data-level="30.3" data-path="trees.html"><a href="trees.html#space-partitions"><i class="fa fa-check"></i><b>30.3</b> Space Partitions</a><ul>
<li class="chapter" data-level="30.3.1" data-path="trees.html"><a href="trees.html#the-process-of-building-a-tree"><i class="fa fa-check"></i><b>30.3.1</b> The Process of Building a Tree</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="31" data-path="tree-impurities.html"><a href="tree-impurities.html"><i class="fa fa-check"></i><b>31</b> Binary Splits and Impurity</a><ul>
<li class="chapter" data-level="31.1" data-path="tree-impurities.html"><a href="tree-impurities.html#binary-partitions"><i class="fa fa-check"></i><b>31.1</b> Binary Partitions</a><ul>
<li class="chapter" data-level="31.1.1" data-path="tree-impurities.html"><a href="tree-impurities.html#splits-of-binary-variables"><i class="fa fa-check"></i><b>31.1.1</b> Splits of Binary variables</a></li>
<li class="chapter" data-level="31.1.2" data-path="tree-impurities.html"><a href="tree-impurities.html#splits-of-nominal-variables"><i class="fa fa-check"></i><b>31.1.2</b> Splits of Nominal Variables</a></li>
<li class="chapter" data-level="31.1.3" data-path="tree-impurities.html"><a href="tree-impurities.html#splits-of-ordinal-variables"><i class="fa fa-check"></i><b>31.1.3</b> Splits of Ordinal Variables</a></li>
<li class="chapter" data-level="31.1.4" data-path="tree-impurities.html"><a href="tree-impurities.html#splits-continuous-variables"><i class="fa fa-check"></i><b>31.1.4</b> Splits Continuous variables</a></li>
</ul></li>
<li class="chapter" data-level="31.2" data-path="tree-impurities.html"><a href="tree-impurities.html#measures-of-impurity"><i class="fa fa-check"></i><b>31.2</b> Measures of Impurity</a><ul>
<li class="chapter" data-level="31.2.1" data-path="tree-impurities.html"><a href="tree-impurities.html#entropy"><i class="fa fa-check"></i><b>31.2.1</b> Entropy</a></li>
<li class="chapter" data-level="31.2.2" data-path="tree-impurities.html"><a href="tree-impurities.html#the-math-behind-entropy"><i class="fa fa-check"></i><b>31.2.2</b> The Math Behind Entropy</a></li>
<li class="chapter" data-level="31.2.3" data-path="tree-impurities.html"><a href="tree-impurities.html#gini-impurity"><i class="fa fa-check"></i><b>31.2.3</b> Gini Impurity</a></li>
<li class="chapter" data-level="31.2.4" data-path="tree-impurities.html"><a href="tree-impurities.html#variance-based-impurity"><i class="fa fa-check"></i><b>31.2.4</b> Variance-based Impurity</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="32" data-path="tree-splits.html"><a href="tree-splits.html"><i class="fa fa-check"></i><b>32</b> Splitting Nodes</a><ul>
<li class="chapter" data-level="32.1" data-path="tree-splits.html"><a href="tree-splits.html#entropy-based-splits"><i class="fa fa-check"></i><b>32.1</b> Entropy-based Splits</a></li>
<li class="chapter" data-level="32.2" data-path="tree-splits.html"><a href="tree-splits.html#gini-index-based-splits"><i class="fa fa-check"></i><b>32.2</b> Gini-index based Splits</a></li>
<li class="chapter" data-level="32.3" data-path="tree-splits.html"><a href="tree-splits.html#looking-for-the-best-split"><i class="fa fa-check"></i><b>32.3</b> Looking for the best split</a></li>
</ul></li>
<li class="chapter" data-level="33" data-path="tree-basics.html"><a href="tree-basics.html"><i class="fa fa-check"></i><b>33</b> Building Binary Trees</a><ul>
<li class="chapter" data-level="33.1" data-path="tree-basics.html"><a href="tree-basics.html#node-splitting-stopping-criteria"><i class="fa fa-check"></i><b>33.1</b> Node-Splitting Stopping Criteria</a></li>
<li class="chapter" data-level="33.2" data-path="tree-basics.html"><a href="tree-basics.html#issues-with-trees"><i class="fa fa-check"></i><b>33.2</b> Issues with Trees</a><ul>
<li class="chapter" data-level="33.2.1" data-path="tree-basics.html"><a href="tree-basics.html#bias-variance-of-trees"><i class="fa fa-check"></i><b>33.2.1</b> Bias-Variance of Trees</a></li>
</ul></li>
<li class="chapter" data-level="33.3" data-path="tree-basics.html"><a href="tree-basics.html#pruning-a-tree"><i class="fa fa-check"></i><b>33.3</b> Pruning a Tree</a></li>
<li class="chapter" data-level="33.4" data-path="tree-basics.html"><a href="tree-basics.html#pros-and-cons-of-trees"><i class="fa fa-check"></i><b>33.4</b> Pros and Cons of Trees</a><ul>
<li class="chapter" data-level="33.4.1" data-path="tree-basics.html"><a href="tree-basics.html#advantages-of-trees"><i class="fa fa-check"></i><b>33.4.1</b> Advantages of Trees</a></li>
<li class="chapter" data-level="33.4.2" data-path="tree-basics.html"><a href="tree-basics.html#disadvantages-of-trees"><i class="fa fa-check"></i><b>33.4.2</b> Disadvantages of Trees</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="34" data-path="bagging.html"><a href="bagging.html"><i class="fa fa-check"></i><b>34</b> Bagging</a><ul>
<li class="chapter" data-level="34.1" data-path="bagging.html"><a href="bagging.html#introduction-7"><i class="fa fa-check"></i><b>34.1</b> Introduction</a><ul>
<li class="chapter" data-level="34.1.1" data-path="bagging.html"><a href="bagging.html#idea-of-bagging"><i class="fa fa-check"></i><b>34.1.1</b> Idea of Bagging</a></li>
</ul></li>
<li class="chapter" data-level="34.2" data-path="bagging.html"><a href="bagging.html#why-bother-bagging"><i class="fa fa-check"></i><b>34.2</b> Why Bother Bagging?</a></li>
</ul></li>
<li class="chapter" data-level="35" data-path="forest.html"><a href="forest.html"><i class="fa fa-check"></i><b>35</b> Random Forests</a><ul>
<li class="chapter" data-level="35.1" data-path="forest.html"><a href="forest.html#introduction-8"><i class="fa fa-check"></i><b>35.1</b> Introduction</a></li>
<li class="chapter" data-level="35.2" data-path="forest.html"><a href="forest.html#algorithm-1"><i class="fa fa-check"></i><b>35.2</b> Algorithm</a><ul>
<li class="chapter" data-level="35.2.1" data-path="forest.html"><a href="forest.html#two-sources-of-randomness"><i class="fa fa-check"></i><b>35.2.1</b> Two Sources of Randomness</a></li>
<li class="chapter" data-level="35.2.2" data-path="forest.html"><a href="forest.html#regressions-and-classification-forests"><i class="fa fa-check"></i><b>35.2.2</b> Regressions and Classification Forests</a></li>
<li class="chapter" data-level="35.2.3" data-path="forest.html"><a href="forest.html#key-advantage-of-random-forests"><i class="fa fa-check"></i><b>35.2.3</b> Key Advantage of Random Forests</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Made with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">All Models Are Wrong: Concepts of Statistical Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="kernel-smoothers" class="section level1">
<h1><span class="header-section-number">20</span> Kernel Smoothers</h1>
<div id="introduction-4" class="section level2">
<h2><span class="header-section-number">20.1</span> Introduction</h2>
<p>In the previous chapter, we described the KNN estimator as the arithmetic mean
or average of the <span class="math inline">\(y_i\)</span> values associated to the <span class="math inline">\(k\)</span> closest <span class="math inline">\(x_i\)</span> observations
to the query point <span class="math inline">\(x_0\)</span>:</p>
<p><span class="math display">\[
\hat{f}(x_0) = \frac{1}{k} \sum_{i \in \mathcal{N}_0} y_i \quad \longrightarrow \quad \hat{f}(x_0) = \sum_{i \in \mathcal{N}_0} w_i(x_0) \hspace{1mm} y_i
\]</span></p>
<p>As you can tell, such estimate can be expressed in a generic form of a weighted
average of <span class="math inline">\(y_i\)</span> (with <span class="math inline">\(i \in \mathcal{N}_0\)</span>), in which the weights are all
constant <span class="math inline">\(w_i(x_0) = 1/k\)</span>.</p>
<p>This way of handling the weights <span class="math inline">\(w_i(x_0)\)</span> gives the same importance to the
<span class="math inline">\(k\)</span> nearest neighbors of <span class="math inline">\(x_0\)</span>, regardless of how close or how far they are
from the query point. Which sometimes could be problematic.</p>
<p>Consider the figure below. Suppose that we are interested in predicting
<span class="math inline">\(y_0\)</span> using <span class="math inline">\(k\)</span> closest neighbors.
The local information of <span class="math inline">\(x_i\)</span> is concentrated around the actual observed
<span class="math inline">\(x_0\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-176"></span>
<img src="images/nonparam/knn.svg" alt="k = 10 Closest Neighbors" width="70%" />
<p class="caption">
Figure 20.1: k = 10 Closest Neighbors
</p>
</div>
<p>However, if we consider more neighbors <span class="math inline">\(k\)</span>, now some of the <span class="math inline">\(x_i\)</span> values are
considerably farther from the query point <span class="math inline">\(x_0\)</span>:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-177"></span>
<img src="images/nonparam/knn2.svg" alt="k = 20 Closest Neighbors" width="70%" />
<p class="caption">
Figure 20.2: k = 20 Closest Neighbors
</p>
</div>
<p>Instead of giving the same importance to all the neighbors, we could instead
give them variable importances depending on their proximity to <span class="math inline">\(x_0\)</span>. Those
values <span class="math inline">\(x_i\)</span> that are closer to <span class="math inline">\(x_0\)</span> should receive more importance via a
bigger weight. Those values <span class="math inline">\(x_i\)</span> that are farther from <span class="math inline">\(x_0\)</span> should receive
less importance vis a smaller weight.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-178"></span>
<img src="images/nonparam/knn-weighted.svg" alt="k = 20 Closest Neighbors, Weighted" width="70%" />
<p class="caption">
Figure 20.3: k = 20 Closest Neighbors, Weighted
</p>
</div>
<p>Consequently, we can modify the formula of the KNN estimator that uses a
constant average, and replace the constant weights with variable weights.
This modification will result in an estimator that computes <span class="math inline">\(\hat{f}(x_0)\)</span>
as a weighted average.</p>
</div>
<div id="kernel-smoothers-1" class="section level2">
<h2><span class="header-section-number">20.2</span> Kernel Smoothers</h2>
<p>We are looking for a weighted average estimator</p>
<p><span class="math display">\[
\hat{f}(x_0) = \sum_{i \in N_0} w_i(x_0) \hspace{1mm} y_i
\]</span></p>
<p>in which the weights <span class="math inline">\(w_i(x_0)\)</span> take into account the proximity of <span class="math inline">\(x_i\)</span> to
<span class="math inline">\(x_0\)</span>.</p>
<p>One very interesting option for determining weights is with <strong>Kernel functions</strong>.
These are functions that take into account the density of <span class="math inline">\(X\)</span>-points.</p>
<div id="kernel-functions" class="section level3">
<h3><span class="header-section-number">20.2.1</span> Kernel Functions</h3>
<p>What is a kernel? The term â€œkernelâ€ is somewhat of an umbrella term in the
stattistical and machine world. You will often hear people talking about
kernel functions, kernel methods, kernel regression, kernel classifiers, just
to mention a few of them. With respect to nonparametric regression, and in
particular with linear smoothers, a <em>kernel</em> is a function that has special
properties. Before looking at those properties, we should say that the type of
kernel we are referring to in this chapter, is different (although related)
from terms such as Mercer kernels, and the so-called kernel trick.</p>
<p>A one-dimensional smoothing kernel is a symmetric function <span class="math inline">\(K(u) : \mathbb{R} \rightarrow \mathbb{R}\)</span> with the following properties:</p>
<ul>
<li><p>finite support: <span class="math inline">\(K(u) = 0\)</span> for <span class="math inline">\(|u| \geq 1\)</span></p></li>
<li><p>symmetry: <span class="math inline">\(K(u) = K(-u)\)</span></p></li>
<li><p>positive values: <span class="math inline">\(K(u) &gt; 0\)</span> for <span class="math inline">\(|u| &lt; 1\)</span></p></li>
<li><p>normalization: <span class="math inline">\(\int K(x) dx = 1\)</span></p></li>
<li><p>zero-midpoint: <span class="math inline">\(\int x K(x) dx = 0\)</span></p></li>
<li><p><span class="math inline">\(\int x^2 K(x) dx &gt; 0\)</span></p></li>
</ul>
<p>Some of the common Kernel functions are:</p>
<p><span class="math display">\[
\begin{aligned}
  \text{Uniform} &amp; \qquad K(u) = \frac{1}{2}, \quad \text{for } |u| \leq 1 \\
  &amp; \\
  \text{Gaussian} &amp; \qquad K(u) = \frac{1}{\sqrt{2 \pi}} exp(-u^2 / 2) \\
  &amp; \\
  \text{Epanechnikov} &amp; \qquad K(u) = \frac{3}{4} (1 - u^2), \quad \text{for } |u| \leq 1\\
  &amp; \\
  \text{Biweight} &amp; \qquad K(u) = \frac{15}{16} (1 - u^2)^2, \quad \text{for } |u| \leq 1 \\
  &amp; \\
  \text{Triweight} &amp; \qquad K(u) = \frac{35}{32} (1 - u^2)^3, \quad \text{for } |u| \leq 1 \\
\end{aligned}
\]</span></p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-179"></span>
<img src="images/nonparam/Kernels.svg" alt="Various Kernels (source: wikipedia)" width="70%" />
<p class="caption">
Figure 20.4: Various Kernels (source: wikipedia)
</p>
</div>
<p>In case you are curious, Wikipediaâ€™s entry about kernel functions has many more
options: <a href="https://en.wikipedia.org/wiki/Kernel_(statistics)" class="uri">https://en.wikipedia.org/wiki/Kernel_(statistics)</a></p>
</div>
<div id="weights-from-kernels" class="section level3">
<h3><span class="header-section-number">20.2.2</span> Weights from Kernels</h3>
<p>Choosing any type of one-dimensional kernel, we can construct weights</p>
<p><span class="math display">\[
w_i (x_0) = \frac{K \big(\frac{x_i - x_0}{h} \big)}{\sum_{i}^{n} K \big( \frac{x_i - x_0}{h} \big)}
\]</span></p>
<p>Weights obtained in this way have the property that they add up to 1:</p>
<p><span class="math display">\[
\sum_{i}^{n} w_i (x_0) = 1
\]</span></p>
</div>
<div id="kernel-estimator" class="section level3">
<h3><span class="header-section-number">20.2.3</span> Kernel Estimator</h3>
<p>The kernel estimator is</p>
<p><span class="math display">\[
\hat{f}(x_0) = \frac{\sum_{i=1}^{n} y_i K \big(\frac{x_i - x_0}{h} \big)}{\sum_{i=1}^{n} K \big(\frac{x_i - x_0}{h} \big)} = \sum_{i=1}^{n} w_i(x_0) y_i
\]</span></p>
<p>The kernel estimator minimizes a localized squared error</p>
<p><span class="math display">\[
\sum_{i=1}^{n} K \left( \frac{x_i - x_0}{h} \right) (y_i - \theta)^2
\]</span></p>
<p>It can be shown that the estimator <span class="math inline">\(\theta = \hat{f}()\)</span> is given by:</p>
<p><span class="math display">\[
\hat{f}(x_0) = \frac{\sum_{i=1}^{n} y_i K \big(\frac{x_i - x_0}{h} \big)}{\sum_{i=1}^{n} K \big(\frac{x_i - x_0}{h} \big)} = \sum_{i=1}^{n} w_i(x_0) y_i
\]</span></p>
<p>This estimator is also known as the <strong>Nadaraya-Watson</strong> (NW) estimator, and it
is a local constant estimator.</p>
<p>Why do we say <span class="math inline">\(\hat{f}()\)</span> is a local constant estimator? Because it is a local
weighted average of the <span class="math inline">\(y_i\)</span>â€™s.</p>
<p>If we use the Epanechnikov kernel, it turns out that the NW estimator is
optimal. But this is not something to get obsessed about. In practice, you can
use any flavor of kernel function, and be able to obtain similar results.</p>
</div>
</div>
<div id="local-polynomial-estimators" class="section level2">
<h2><span class="header-section-number">20.3</span> Local Polynomial Estimators</h2>
<p>Letâ€™s bring back the kernel estimator which minimizes a localized squared
error function:</p>
<p><span class="math display">\[
\sum_{i=1}^{n} K \left( \frac{x_i - x_0}{h} \right) (y_i - \theta)^2
\]</span></p>
<p>Instead of using a local weighted average of <span class="math inline">\(y_i\)</span>â€™s, which is basically a
local constant fit, why not use a more flexible fit such as a local linear
fit, or more general, a local polynomial fit?</p>
<p>This is precisely what local polynomial estimators are for.</p>
<p>If we use a polynomial of degree 1, that is a line, the local minimization
problem becomes:</p>
<p><span class="math display">\[
\sum_{i=1}^{n} K \left( \frac{x_i - x_0}{h} \right) \big( y_i - b_{0,x_0} - b_{1,x_0} (x_i - x_0) \big)^2
\]</span></p>
<p>The estimated <span class="math inline">\(\hat{f}(x_0) \approx b_{0,x_0} + b_{1,x_0} (x - x_0)\)</span>.</p>
<p>If we arrange the terms in an <span class="math inline">\(n \times 2\)</span> matrix <span class="math inline">\(\mathbf{X_0}\)</span>, and a
<span class="math inline">\(n \times n\)</span> matrix of weights <span class="math inline">\(\mathbf{W}\)</span></p>
<p><span class="math display">\[
\mathbf{X_0} = 
\
\begin{bmatrix} 
1 &amp; x_{1} - x_0 \\
1 &amp; x_{2} - x_0 \\
\vdots &amp; \vdots \\
1 &amp; x_{n} - x_0 \\
\end{bmatrix},
\\
\quad
\\
\mathbf{W_0} = 
\begin{bmatrix} 
K_h (x_1 - x_0) &amp; 0 &amp; \dots &amp; 0\\
0 &amp; K_h (x_2 - x_0) &amp; \dots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \dots &amp; K_h (x_n - x_0) \\
\end{bmatrix}
\]</span></p>
<p>then:</p>
<p><span class="math display">\[
\mathbf{b}_{x_0} = (\mathbf{X_0}^\mathsf{T} \mathbf{W_0 X_0})^{-1} \mathbf{X_0}^\mathsf{T} \mathbf{W_0 y}
\]</span></p>

</div>
</div>



            </section>

          </div>
        </div>
      </div>
<a href="knn.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="classif.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["allmodelsarewrong.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
