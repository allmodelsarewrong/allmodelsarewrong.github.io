<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>6 Gradient Descent | All Models Are Wrong</title>
  <meta name="description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  <meta name="generator" content="bookdown 0.14 and GitBook 2.6.7" />

  <meta property="og:title" content="6 Gradient Descent | All Models Are Wrong" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://allmodelsarewrong.github.io" />
  
  <meta property="og:description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  <meta name="github-repo" content="allmodelsarewrong/allmodelsarewrong.github.io" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="6 Gradient Descent | All Models Are Wrong" />
  
  <meta name="twitter:description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  

<meta name="author" content="by Gaston Sanchez, and Ethan Marzban" />


<meta name="date" content="2019-11-03" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ols.html"/>

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><b>All Models Are Wrong (CSL)</b><br>by G. Sanchez & E. Marzban</a></li>

<li class="divider"></li>
<li class="part"><span><b>I Welcome</b></span></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="about.html"><a href="about.html"><i class="fa fa-check"></i><b>1</b> About this book</a><ul>
<li class="chapter" data-level="1.1" data-path="about.html"><a href="about.html#prerequisites"><i class="fa fa-check"></i><b>1.1</b> Prerequisites</a></li>
<li class="chapter" data-level="1.2" data-path="about.html"><a href="about.html#acknowledgements"><i class="fa fa-check"></i><b>1.2</b> Acknowledgements</a></li>
</ul></li>
<li class="part"><span><b>II Intro</b></span></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#basic-notation"><i class="fa fa-check"></i><b>2.1</b> Basic Notation</a></li>
<li class="chapter" data-level="2.2" data-path="intro.html"><a href="intro.html#about-statistical-learning"><i class="fa fa-check"></i><b>2.2</b> About Statistical Learning</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="duality.html"><a href="duality.html"><i class="fa fa-check"></i><b>3</b> Geometric Duality</a><ul>
<li class="chapter" data-level="3.1" data-path="duality.html"><a href="duality.html#rows-space"><i class="fa fa-check"></i><b>3.1</b> Rows Space</a></li>
<li class="chapter" data-level="3.2" data-path="duality.html"><a href="duality.html#columns-space"><i class="fa fa-check"></i><b>3.2</b> Columns Space</a></li>
<li class="chapter" data-level="3.3" data-path="duality.html"><a href="duality.html#cloud-of-individuals"><i class="fa fa-check"></i><b>3.3</b> Cloud of Individuals</a><ul>
<li class="chapter" data-level="3.3.1" data-path="duality.html"><a href="duality.html#average-individual"><i class="fa fa-check"></i><b>3.3.1</b> Average Individual</a></li>
<li class="chapter" data-level="3.3.2" data-path="duality.html"><a href="duality.html#centered-data"><i class="fa fa-check"></i><b>3.3.2</b> Centered Data</a></li>
<li class="chapter" data-level="3.3.3" data-path="duality.html"><a href="duality.html#distance-between-individuals"><i class="fa fa-check"></i><b>3.3.3</b> Distance between individuals</a></li>
<li class="chapter" data-level="3.3.4" data-path="duality.html"><a href="duality.html#distance-to-the-centroid"><i class="fa fa-check"></i><b>3.3.4</b> Distance to the centroid</a></li>
<li class="chapter" data-level="3.3.5" data-path="duality.html"><a href="duality.html#measures-of-dispersion"><i class="fa fa-check"></i><b>3.3.5</b> Measures of Dispersion</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="duality.html"><a href="duality.html#cloud-of-variables"><i class="fa fa-check"></i><b>3.4</b> Cloud of Variables</a><ul>
<li class="chapter" data-level="3.4.1" data-path="duality.html"><a href="duality.html#mean-of-a-variable"><i class="fa fa-check"></i><b>3.4.1</b> Mean of a Variable</a></li>
<li class="chapter" data-level="3.4.2" data-path="duality.html"><a href="duality.html#variance-of-a-variable"><i class="fa fa-check"></i><b>3.4.2</b> Variance of a Variable</a></li>
<li class="chapter" data-level="3.4.3" data-path="duality.html"><a href="duality.html#variance-with-vector-notation"><i class="fa fa-check"></i><b>3.4.3</b> Variance with Vector Notation</a></li>
<li class="chapter" data-level="3.4.4" data-path="duality.html"><a href="duality.html#standard-deviation-as-a-norm"><i class="fa fa-check"></i><b>3.4.4</b> Standard Deviation as a Norm</a></li>
<li class="chapter" data-level="3.4.5" data-path="duality.html"><a href="duality.html#covariance"><i class="fa fa-check"></i><b>3.4.5</b> Covariance</a></li>
<li class="chapter" data-level="3.4.6" data-path="duality.html"><a href="duality.html#correlation"><i class="fa fa-check"></i><b>3.4.6</b> Correlation</a></li>
<li class="chapter" data-level="3.4.7" data-path="duality.html"><a href="duality.html#geometry-of-correlation"><i class="fa fa-check"></i><b>3.4.7</b> Geometry of Correlation</a></li>
<li class="chapter" data-level="3.4.8" data-path="duality.html"><a href="duality.html#orthogonal-projections"><i class="fa fa-check"></i><b>3.4.8</b> Orthogonal Projections</a></li>
<li class="chapter" data-level="3.4.9" data-path="duality.html"><a href="duality.html#the-mean-as-an-orthogonal-projection"><i class="fa fa-check"></i><b>3.4.9</b> The mean as an orthogonal projection</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Unsupervised I: PCA</b></span></li>
<li class="chapter" data-level="4" data-path="pca.html"><a href="pca.html"><i class="fa fa-check"></i><b>4</b> Principal Components Analysis</a><ul>
<li class="chapter" data-level="4.1" data-path="pca.html"><a href="pca.html#low-dimensional-representations"><i class="fa fa-check"></i><b>4.1</b> Low-dimensional Representations</a></li>
<li class="chapter" data-level="4.2" data-path="pca.html"><a href="pca.html#pca-idea"><i class="fa fa-check"></i><b>4.2</b> PCA Idea</a><ul>
<li class="chapter" data-level="4.2.1" data-path="pca.html"><a href="pca.html#finding-principal-components"><i class="fa fa-check"></i><b>4.2.1</b> Finding Principal Components</a></li>
<li class="chapter" data-level="4.2.2" data-path="pca.html"><a href="pca.html#finding-the-first-pc"><i class="fa fa-check"></i><b>4.2.2</b> Finding the first PC</a></li>
<li class="chapter" data-level="4.2.3" data-path="pca.html"><a href="pca.html#finding-the-second-pc"><i class="fa fa-check"></i><b>4.2.3</b> Finding the second PC</a></li>
<li class="chapter" data-level="4.2.4" data-path="pca.html"><a href="pca.html#finding-all-pcs"><i class="fa fa-check"></i><b>4.2.4</b> Finding all PCs</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="pca.html"><a href="pca.html#pca-model"><i class="fa fa-check"></i><b>4.3</b> PCA Model</a></li>
<li class="chapter" data-level="4.4" data-path="pca.html"><a href="pca.html#another-perspective"><i class="fa fa-check"></i><b>4.4</b> Another Perspective</a></li>
</ul></li>
<li class="part"><span><b>IV Linear Regression</b></span></li>
<li class="chapter" data-level="5" data-path="ols.html"><a href="ols.html"><i class="fa fa-check"></i><b>5</b> Linear Regression</a><ul>
<li class="chapter" data-level="5.1" data-path="ols.html"><a href="ols.html#motivation"><i class="fa fa-check"></i><b>5.1</b> Motivation</a></li>
<li class="chapter" data-level="5.2" data-path="ols.html"><a href="ols.html#the-model"><i class="fa fa-check"></i><b>5.2</b> The Model</a></li>
<li class="chapter" data-level="5.3" data-path="ols.html"><a href="ols.html#the-error-measure"><i class="fa fa-check"></i><b>5.3</b> The Error Measure</a></li>
<li class="chapter" data-level="5.4" data-path="ols.html"><a href="ols.html#the-algorithm"><i class="fa fa-check"></i><b>5.4</b> The Algorithm</a></li>
<li class="chapter" data-level="5.5" data-path="ols.html"><a href="ols.html#geometries-of-ols"><i class="fa fa-check"></i><b>5.5</b> Geometries of OLS</a><ul>
<li class="chapter" data-level="5.5.1" data-path="ols.html"><a href="ols.html#rows-perspective"><i class="fa fa-check"></i><b>5.5.1</b> Rows Perspective</a></li>
<li class="chapter" data-level="5.5.2" data-path="ols.html"><a href="ols.html#columns-perspective"><i class="fa fa-check"></i><b>5.5.2</b> Columns Perspective</a></li>
<li class="chapter" data-level="5.5.3" data-path="ols.html"><a href="ols.html#parameters-perspective"><i class="fa fa-check"></i><b>5.5.3</b> Parameters Perspective</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="gradient.html"><a href="gradient.html"><i class="fa fa-check"></i><b>6</b> Gradient Descent</a><ul>
<li class="chapter" data-level="6.1" data-path="gradient.html"><a href="gradient.html#error-surface"><i class="fa fa-check"></i><b>6.1</b> Error Surface</a></li>
<li class="chapter" data-level="6.2" data-path="gradient.html"><a href="gradient.html#idea-of-gradient-descent"><i class="fa fa-check"></i><b>6.2</b> Idea of Gradient Descent</a><ul>
<li class="chapter" data-level="6.2.1" data-path="gradient.html"><a href="gradient.html#the-direction-of-mathbfv"><i class="fa fa-check"></i><b>6.2.1</b> The direction of <span class="math inline">\(\mathbf{v}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="gradient.html"><a href="gradient.html#gradient-descent-and-our-model"><i class="fa fa-check"></i><b>6.3</b> Gradient Descent and our Model</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Made with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">All Models Are Wrong</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="gradient" class="section level1">
<h1><span class="header-section-number">6</span> Gradient Descent</h1>
<p>Before moving to the next part of the book that deals with the theory of learning,
we want to introduce a very popular optimization technique that is commonly
used in many statistical learning methods: the famous <strong>gradient descent</strong>
algorithm.</p>
<div id="error-surface" class="section level2">
<h2><span class="header-section-number">6.1</span> Error Surface</h2>
<p>Consider again the error surface:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-2"></span>
<img src="images/ols/error-surface.png" alt="Error Surface" width="70%" />
<p class="caption">
Figure 6.1: Error Surface
</p>
</div>
<p>We previously saw that the vector <span class="math inline">\(\mathbf{w}^{*}\)</span> that minimizes the error surface
is exactly our OLS estimate for <span class="math inline">\(\mathbf{w}\)</span>, our vector of regression coefficients.
In the previous chapter, we used direct methods (i.e. computing the derivative of the
error function and setting it equal to <span class="math inline">\(\mathbf{0}\)</span>) to find this minimum point.</p>
<p>Interestingly, we can also use iterative methods to compute this minimum.
The iterative method that we will discuss in this chapter is called
<strong>gradient descent</strong>.</p>
</div>
<div id="idea-of-gradient-descent" class="section level2">
<h2><span class="header-section-number">6.2</span> Idea of Gradient Descent</h2>
<p>The main idea of gradient descent is as follows: we start with an arbitrary point
<span class="math inline">\(\mathbf{w}^{(0)} = ( w_0^{(0)}, w_1^{(0)}, \dots, w_p^{(0)} )\)</span> somewhere on the
error surface. We then “move down” the surface to obtain a new vector
<span class="math inline">\(\mathbf{w}^{(1)}\)</span>, closer to the minimizing point <span class="math inline">\(\mathbf{w}^{*}\)</span>.
For illustrative purposes, consider a <span class="math inline">\(1-\)</span>dimensional error “surface”:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-3"></span>
<img src="images/ols/one-dim-error-surface.png" alt="One dimensional error surface" width="70%" />
<p class="caption">
Figure 6.2: One dimensional error surface
</p>
</div>
<p>What do we mean by “moving down?” Well, mathematically, this means we generate
the new vector <span class="math inline">\(\mathbf{w}^{(1)}\)</span> using</p>
<p><span class="math display">\[
\mathbf{w}^{(1)} = \mathbf{w}^{(0)} + \alpha \mathbf{v}
\]</span></p>
<p>We call <span class="math inline">\(\alpha\)</span> the <strong>step size</strong> (intuitively, <span class="math inline">\(\alpha\)</span> tells us how far down
the surface we are moving) and we set <span class="math inline">\(\mathbf{v}\)</span> to be a <strong>unit</strong> vector,
in some direction. We will discuss how to find the direction of <span class="math inline">\(\mathbf{v}\)</span>
in a little bit. Right now let’s just focus on generating new vectors in this
manner:</p>
<p><span class="math display">\[\begin{align*}
\mathbf{w}^{(2)} &amp; = \mathbf{w}^{(1)} + \alpha \mathbf{v}^{(1)} \\ 
\mathbf{w}^{(3)} &amp; = \mathbf{w}^{(2)} + \alpha \mathbf{v}^{(2)} \\ 
\vdots &amp; \hspace{10mm} \vdots \\
\mathbf{w}^{(s+1)} &amp; = \mathbf{w}^{(s)} + \alpha \mathbf{v}^{(s)}
\end{align*}\]</span></p>
<p>Note that we are assuming a constant step size; that is, note that <span class="math inline">\(\alpha\)</span> remains
the same at each iteration. There are more sophisticated versions of gradient
descent that allow a <em>variable</em> step size, however we will not consider that case.
As we can see in the <span class="math inline">\(1-\)</span>D example above, the direction in which we travel will
change at each step of the process. We will also see this mathematically in the
next subsection.</p>
<div id="the-direction-of-mathbfv" class="section level3">
<h3><span class="header-section-number">6.2.1</span> The direction of <span class="math inline">\(\mathbf{v}\)</span></h3>
<p>How do we find the direction of <span class="math inline">\(\mathbf{v}\)</span>? Consider the gradient of our error
function. The gradient always points in the direction of steepest <em>ascent</em>
(i.e. largest positive change). Hence, we want to travel in the exact opposite
direction of the gradient.</p>
<p>Let’s “prove” this mathematically. In terms of the error function itself, what
does it mean for our vectors <span class="math inline">\(\mathbf{w}^{(s+1)}\)</span> to be “getting closer” to the
minimizing point? Well, it means that the error at point <span class="math inline">\(\mathbf{w}^{(s+1)}\)</span>
is less than the error at point <span class="math inline">\(\mathbf{w}^{(s)}\)</span>. Hence, we examine the
difference between the errors at these two points:</p>
<p><span class="math display">\[\begin{align*} 
\Delta E_{\mathbf{w}}  &amp; := E\left( \mathbf{w}^{(s + 1)} \right) - E\left( \mathbf{w}^{(s)} \right) \\
&amp; = E \left( \mathbf{w}^{(s)} + \alpha \mathbf{v} \right) - E\left( \mathbf{w}^{(s)} \right) 
\end{align*}\]</span></p>
<p>In order for our vector <span class="math inline">\(\mathbf{w}^{(s+1)}\)</span> to be closer to the minimizing
point that <span class="math inline">\(\mathbf{w}^{(s)}\)</span>, we want this quantity to be as negative as possible.
To find the <span class="math inline">\(\mathbf{v}\)</span> that makes this true, we need to use a trick:
Taylor Expand <span class="math inline">\(E(\mathbf{w}^{(s)} + \alpha \mathbf{v})\)</span>. Doing so, we obtain:</p>
<p><span class="math display">\[\begin{align*}
\Delta E_{\mathbf{w}} &amp; = E\left( \mathbf{w}^{(s)} \right) + \nabla E\left( \mathbf{w}^{(s)} \right)^{\mathrm{T}} \left(\alpha \mathbf{v} \right) + O \left( \alpha^2 \right) - E\left( \mathbf{w}^{(s)} \right) \\
&amp; = \alpha \nabla E \left( \mathbf{w}^{(s)} \right)^{\mathrm{T}} \mathbf{v} + O(\alpha^2) 
\end{align*}\]</span></p>
<p>Hence, we must examine <span class="math inline">\([ \nabla E(\mathbf{w}{s})]^{\mathrm{T}} \mathbf{v}\)</span>.
For notational convenience, let us (temporarily) define
<span class="math inline">\(\mathbf{u} : = \nabla E ( \mathbf{w}^{(s)} )\)</span></p>
<p>With respect to the orientation of <span class="math inline">\(\mathbf{v}\)</span> and <span class="math inline">\(\mathbf{u}\)</span>, we can consider
three prototypical cases: <span class="math inline">\(\mathbf{u}\)</span> and <span class="math inline">\(\mathbf{v}\)</span> are either antiparallel, parallel, or orthogonal.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-4"></span>
<img src="images/ols/three-prototypical-cases.png" alt="Three prototypical cases" width="70%" />
<p class="caption">
Figure 6.3: Three prototypical cases
</p>
</div>
<p>When <span class="math inline">\(\mathbf{u} \parallel \mathbf{v}\)</span> (i.e. the second case above), then <span class="math inline">\(\mathbf{u}^\mathsf{T}\mathbf{v} = \| \mathbf{u} \|\)</span>.
When <span class="math inline">\(\mathbf{u} \not\parallel \mathbf{v}\)</span> (i.e. the first case above), then <span class="math inline">\(\mathbf{u}^\mathsf{T} = - \| \mathbf{u} \|\)</span>.
And, of course, if <span class="math inline">\(\mathbf{u} \perp \mathbf{v}\)</span> then
<span class="math inline">\(\mathbf{u}^\mathsf{T} \mathbf{v} = 0\)</span>.
Therefore, in any of the three cases, we have that</p>
<p><span class="math display">\[
\mathbf{u}^\mathsf{T} \mathbf{v} \geq - \left\| \mathbf{u} \right\|
\]</span></p>
<p>Hence, recalling that <span class="math inline">\(\mathbf{u} := \nabla E(\mathbf{w}^{(s)})\)</span>, we can plug
this result into our error computation to obtain:</p>
<p><span class="math display">\[\begin{align*} 
\Delta E_{\mathbf{w}}  &amp; = \alpha \nabla E \left( \mathbf{w}^{(s)} \right)^{\mathrm{T}} \mathbf{v} + O(\alpha^2)  \\
&amp; \geq - \alpha \left\| \nabla E\left( \mathbf{w}^{(s)} \right) \right\|  \mathbf{v}
\end{align*}\]</span></p>
<p>Thus, to make <span class="math inline">\(\Delta E_{\mathbf{w}}\)</span> as negative as possible, we should take
<span class="math inline">\(\mathbf{v}\)</span> parallel to <span class="math inline">\(( - \| \nabla E(\mathbf{w}^{(s)}) \| )\)</span>.</p>
<p><strong>The Moral:</strong> We want the following:</p>
<p><span class="math display">\[
\mathbf{v} = - \frac{\nabla E(\mathbf{w}^{(s)}) }{\left\| \nabla E(\mathbf{w}^{(s)})  \right\| }
\]</span></p>
<p>which means we want to move in the direction opposite to that of the gradient.
Keep in mind that we divided by the norm because we previously defined
<span class="math inline">\(\mathbf{v}\)</span> to be a <strong>unit} vector</strong>.</p>
<p>This also reveals the meaning behind the name <strong>gradient descent</strong>; we are
<em>descending</em> in the direction opposite to the <em>gradient</em> of the error function.</p>
</div>
</div>
<div id="gradient-descent-and-our-model" class="section level2">
<h2><span class="header-section-number">6.3</span> Gradient Descent and our Model</h2>
<p>Before we posit the full algorithm for gradient descent in the context of regression,
let us investigate the actual gradient further. Since we have a formula for
<span class="math inline">\(E_{\mathbf{w}}\)</span>, we can find a closed form for its gradient:</p>
<p><span class="math display">\[\begin{align*}
E(\mathbf{w}) &amp; = \frac{1}{n} \left( \mathbf{y} - \mathbf{X} \mathbf{w} \right)^{\mathrm{T}} \left( \mathbf{y} - \mathbf{X} \mathbf{w} \right) \\
\\
&amp; = \frac{1}{n} \left( \mathbf{w}^\mathsf{T} \mathbf{X}^\mathsf{T} \mathbf{X} \mathbf{w} - 2 \mathbf{w}^\mathsf{T} \mathbf{X}^\mathsf{T} \mathbf{y} + \mathbf{y}^\mathsf{T} \mathbf{y} \right) \\
\\
\nabla E \left( \mathbf{w} \right) &amp; = \frac{1}{n} \left( 2 \mathbf{X}^\mathsf{T} \mathbf{X} \mathbf{w} - 2 \mathbf{X}^\mathsf{T} \mathbf{y} \right) 
\end{align*}\]</span></p>
<p>This was from the columns point of view; we can also find a different formula
from the row’s perspective:</p>
<p><span class="math display">\[\begin{align*}
E(\mathbf{w}) &amp; = \frac{1}{n} \sum_{i = 1}^{n} \left( y_i - \mathbf{w}^\mathsf{T} \mathbf{x_i} \right)^2 = \frac{1}{n} \sum_{i=1}^{n} \left( y_i - \hat{y}_i \right) \\
&amp; = \frac{1}{n} \sum_{i=1}^{n} \left( y_i - w_0 x_{i0} - w_1 x_{i1} - \dots - w_{j} x_{ij} - \dots - w_{p} x_{ip} \right)^2 \\
\frac{\partial}{\partial w_j} E(\mathbf{w}) &amp; = - \frac{2}{n} \sum_{i=1}^{n}  \left( y_i - w_0 x_{i0} - w_1 x_{i1} - \dots - w_{j} x_{ij} - \dots - w_{p} x_{ip} \right) x_{ij} \\
\frac{\partial}{\partial w_j} E(\mathbf{w}^{(0)}) &amp; = - \frac{2}{n} \sum_{i=1}^{n}  \left( y_i - w_0 x_{i0} - w_1 x_{i1} - \dots - w_{j} x_{ij} - \dots - w_{p} x_{ip} \right) x_{ij} \\
&amp; = - \frac{2}{n} \sum_{i=1}^{n} \left( y_i - \left[ \mathbf{w}^{(0)} \right]^{\mathsf{T}} \mathbf{x_i} \right) x_{ij} 
\end{align*}\]</span></p>
<p>This will be a better formula to use in our iterative algorithm, posited below.</p>
<p><strong>Algorithm</strong></p>
<p>Initialize a vector <span class="math inline">\(\mathbf{w}^{(0)} = \left( w_{0}^{(0)}, w_{1}^{(0)}, \dots, w_{p}^{(0)} \right)\)</span></p>
<p>Repeat the following over <span class="math inline">\(s\)</span> (starting with <span class="math inline">\(s = 0\)</span>), until convergence:</p>
<p><span class="math inline">\(w_{j}^{(s+1)}:= w_{j}^{(s)} + \alpha \cdot \frac{2}{n} \sum_{i=1}^{n} \left( y_i - \left[\mathbf{w}^{(s)}\right]^{\mathsf{T}} \mathbf{x_i} \right) x_{ij}\)</span> for all <span class="math inline">\(j = 0, \dots, p\)</span> simultaneously. In more compact notation,</p>
<p><span class="math display">\[
w_{j}^{(s+1)} = w_{j}^{(s)} + \alpha \cdot \frac{\partial }{\partial w_j} E(\mathbf{w}^{(s)})
\]</span></p>
<p>Store these elements in the vector <span class="math inline">\(\mathbf{w}^{(s+1)} = \left( w_{0}^{(s+1)} , w_{1}^{(s+1)} , \dots, w_{p}^{(s+1)} \right)\)</span></p>
<p>When there is little change between <span class="math inline">\(\mathbf{w}^{(k + 1)}\)</span> and <span class="math inline">\(\mathbf{w}^{(k)}\)</span>
(for some integer <span class="math inline">\(k\)</span>), the algorithm will have converged and
<span class="math inline">\(\mathbf{w}^{*} = \mathbf{w}^{(k+1)}\)</span>.</p>
<p>Note that in the algorithm, we have dropped the condition that <span class="math inline">\(\mathbf{v}\)</span> is
a unit vector. Indeed, if you look on the Wikipedia page for “Gradient Descent,”
the algorithm listed there also omits the unit-length condition.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ols.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["allmodelsarewrong.pdf", "allmodelsarewrong.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
