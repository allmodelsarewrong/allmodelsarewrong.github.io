<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>6 Gradient Descent | All Models Are Wrong</title>
  <meta name="description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  <meta name="generator" content="bookdown 0.16 and GitBook 2.6.7" />

  <meta property="og:title" content="6 Gradient Descent | All Models Are Wrong" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://allmodelsarewrong.github.io" />
  
  <meta property="og:description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  <meta name="github-repo" content="allmodelsarewrong/allmodelsarewrong.github.io" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="6 Gradient Descent | All Models Are Wrong" />
  
  <meta name="twitter:description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  

<meta name="author" content="by Gaston Sanchez, and Ethan Marzban" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ols.html"/>
<link rel="next" href="learning.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><b>All Models Are Wrong (CSL)</b><br><small>by G. Sanchez & E. Marzban</small></a></li>

<li class="divider"></li>
<li class="part"><span><b>I Welcome</b></span></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="about.html"><a href="about.html"><i class="fa fa-check"></i><b>1</b> About this book</a><ul>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#prerequisites"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="part"><span><b>II Intro</b></span></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#basic-notation"><i class="fa fa-check"></i><b>2.1</b> Basic Notation</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="duality.html"><a href="duality.html"><i class="fa fa-check"></i><b>3</b> Geometric Duality</a><ul>
<li class="chapter" data-level="3.1" data-path="duality.html"><a href="duality.html#rows-space"><i class="fa fa-check"></i><b>3.1</b> Rows Space</a></li>
<li class="chapter" data-level="3.2" data-path="duality.html"><a href="duality.html#columns-space"><i class="fa fa-check"></i><b>3.2</b> Columns Space</a></li>
<li class="chapter" data-level="3.3" data-path="duality.html"><a href="duality.html#cloud-of-individuals"><i class="fa fa-check"></i><b>3.3</b> Cloud of Individuals</a><ul>
<li class="chapter" data-level="3.3.1" data-path="duality.html"><a href="duality.html#average-individual"><i class="fa fa-check"></i><b>3.3.1</b> Average Individual</a></li>
<li class="chapter" data-level="3.3.2" data-path="duality.html"><a href="duality.html#centered-data"><i class="fa fa-check"></i><b>3.3.2</b> Centered Data</a></li>
<li class="chapter" data-level="3.3.3" data-path="duality.html"><a href="duality.html#distance-between-individuals"><i class="fa fa-check"></i><b>3.3.3</b> Distance between individuals</a></li>
<li class="chapter" data-level="3.3.4" data-path="duality.html"><a href="duality.html#distance-to-the-centroid"><i class="fa fa-check"></i><b>3.3.4</b> Distance to the centroid</a></li>
<li class="chapter" data-level="3.3.5" data-path="duality.html"><a href="duality.html#measures-of-dispersion"><i class="fa fa-check"></i><b>3.3.5</b> Measures of Dispersion</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="duality.html"><a href="duality.html#cloud-of-variables"><i class="fa fa-check"></i><b>3.4</b> Cloud of Variables</a><ul>
<li class="chapter" data-level="3.4.1" data-path="duality.html"><a href="duality.html#mean-of-a-variable"><i class="fa fa-check"></i><b>3.4.1</b> Mean of a Variable</a></li>
<li class="chapter" data-level="3.4.2" data-path="duality.html"><a href="duality.html#variance-of-a-variable"><i class="fa fa-check"></i><b>3.4.2</b> Variance of a Variable</a></li>
<li class="chapter" data-level="3.4.3" data-path="duality.html"><a href="duality.html#variance-with-vector-notation"><i class="fa fa-check"></i><b>3.4.3</b> Variance with Vector Notation</a></li>
<li class="chapter" data-level="3.4.4" data-path="duality.html"><a href="duality.html#standard-deviation-as-a-norm"><i class="fa fa-check"></i><b>3.4.4</b> Standard Deviation as a Norm</a></li>
<li class="chapter" data-level="3.4.5" data-path="duality.html"><a href="duality.html#covariance"><i class="fa fa-check"></i><b>3.4.5</b> Covariance</a></li>
<li class="chapter" data-level="3.4.6" data-path="duality.html"><a href="duality.html#correlation"><i class="fa fa-check"></i><b>3.4.6</b> Correlation</a></li>
<li class="chapter" data-level="3.4.7" data-path="duality.html"><a href="duality.html#geometry-of-correlation"><i class="fa fa-check"></i><b>3.4.7</b> Geometry of Correlation</a></li>
<li class="chapter" data-level="3.4.8" data-path="duality.html"><a href="duality.html#orthogonal-projections"><i class="fa fa-check"></i><b>3.4.8</b> Orthogonal Projections</a></li>
<li class="chapter" data-level="3.4.9" data-path="duality.html"><a href="duality.html#the-mean-as-an-orthogonal-projection"><i class="fa fa-check"></i><b>3.4.9</b> The mean as an orthogonal projection</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Unsupervised I: PCA</b></span></li>
<li class="chapter" data-level="4" data-path="pca.html"><a href="pca.html"><i class="fa fa-check"></i><b>4</b> Principal Components Analysis</a><ul>
<li class="chapter" data-level="4.1" data-path="pca.html"><a href="pca.html#low-dimensional-representations"><i class="fa fa-check"></i><b>4.1</b> Low-dimensional Representations</a></li>
<li class="chapter" data-level="4.2" data-path="pca.html"><a href="pca.html#projections"><i class="fa fa-check"></i><b>4.2</b> Projections</a><ul>
<li class="chapter" data-level="4.2.1" data-path="pca.html"><a href="pca.html#vector-and-scalar-projections"><i class="fa fa-check"></i><b>4.2.1</b> Vector and Scalar Projections</a></li>
<li class="chapter" data-level="4.2.2" data-path="pca.html"><a href="pca.html#projected-inertia"><i class="fa fa-check"></i><b>4.2.2</b> Projected Inertia</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="pca.html"><a href="pca.html#maximization-problem"><i class="fa fa-check"></i><b>4.3</b> Maximization Problem</a><ul>
<li class="chapter" data-level="4.3.1" data-path="pca.html"><a href="pca.html#eigenvectors-of-mathbfs"><i class="fa fa-check"></i><b>4.3.1</b> Eigenvectors of <span class="math inline">\(\mathbf{S}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="pca.html"><a href="pca.html#another-perspective-of-pca"><i class="fa fa-check"></i><b>4.4</b> Another Perspective of PCA</a><ul>
<li class="chapter" data-level="4.4.1" data-path="pca.html"><a href="pca.html#finding-principal-components"><i class="fa fa-check"></i><b>4.4.1</b> Finding Principal Components</a></li>
<li class="chapter" data-level="4.4.2" data-path="pca.html"><a href="pca.html#finding-the-first-pc"><i class="fa fa-check"></i><b>4.4.2</b> Finding the first PC</a></li>
<li class="chapter" data-level="4.4.3" data-path="pca.html"><a href="pca.html#finding-the-second-pc"><i class="fa fa-check"></i><b>4.4.3</b> Finding the second PC</a></li>
<li class="chapter" data-level="4.4.4" data-path="pca.html"><a href="pca.html#finding-all-pcs"><i class="fa fa-check"></i><b>4.4.4</b> Finding all PCs</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="pca.html"><a href="pca.html#data-decomposition-model"><i class="fa fa-check"></i><b>4.5</b> Data Decomposition Model</a><ul>
<li class="chapter" data-level="4.5.1" data-path="pca.html"><a href="pca.html#alternative-approaches"><i class="fa fa-check"></i><b>4.5.1</b> Alternative Approaches</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Linear Regression</b></span></li>
<li class="chapter" data-level="5" data-path="ols.html"><a href="ols.html"><i class="fa fa-check"></i><b>5</b> Linear Regression</a><ul>
<li class="chapter" data-level="5.1" data-path="ols.html"><a href="ols.html#motivation"><i class="fa fa-check"></i><b>5.1</b> Motivation</a></li>
<li class="chapter" data-level="5.2" data-path="ols.html"><a href="ols.html#the-ideaintuition-of-regression"><i class="fa fa-check"></i><b>5.2</b> The Idea/Intuition of Regression</a></li>
<li class="chapter" data-level="5.3" data-path="ols.html"><a href="ols.html#the-linear-regression-model"><i class="fa fa-check"></i><b>5.3</b> The Linear Regression Model</a></li>
<li class="chapter" data-level="5.4" data-path="ols.html"><a href="ols.html#the-error-measure"><i class="fa fa-check"></i><b>5.4</b> The Error Measure</a></li>
<li class="chapter" data-level="5.5" data-path="ols.html"><a href="ols.html#the-least-squares-algorithm"><i class="fa fa-check"></i><b>5.5</b> The Least Squares Algorithm</a></li>
<li class="chapter" data-level="5.6" data-path="ols.html"><a href="ols.html#geometries-of-ols"><i class="fa fa-check"></i><b>5.6</b> Geometries of OLS</a><ul>
<li class="chapter" data-level="5.6.1" data-path="ols.html"><a href="ols.html#rows-perspective"><i class="fa fa-check"></i><b>5.6.1</b> Rows Perspective</a></li>
<li class="chapter" data-level="5.6.2" data-path="ols.html"><a href="ols.html#columns-perspective"><i class="fa fa-check"></i><b>5.6.2</b> Columns Perspective</a></li>
<li class="chapter" data-level="5.6.3" data-path="ols.html"><a href="ols.html#parameters-perspective"><i class="fa fa-check"></i><b>5.6.3</b> Parameters Perspective</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="gradient.html"><a href="gradient.html"><i class="fa fa-check"></i><b>6</b> Gradient Descent</a><ul>
<li class="chapter" data-level="6.1" data-path="gradient.html"><a href="gradient.html#error-surface"><i class="fa fa-check"></i><b>6.1</b> Error Surface</a></li>
<li class="chapter" data-level="6.2" data-path="gradient.html"><a href="gradient.html#idea-of-gradient-descent"><i class="fa fa-check"></i><b>6.2</b> Idea of Gradient Descent</a></li>
<li class="chapter" data-level="6.3" data-path="gradient.html"><a href="gradient.html#moving-down-an-error-surface"><i class="fa fa-check"></i><b>6.3</b> Moving Down an Error Surface</a><ul>
<li class="chapter" data-level="6.3.1" data-path="gradient.html"><a href="gradient.html#the-direction-of-mathbfv"><i class="fa fa-check"></i><b>6.3.1</b> The direction of <span class="math inline">\(\mathbf{v}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="gradient.html"><a href="gradient.html#gradient-descent-and-our-model"><i class="fa fa-check"></i><b>6.4</b> Gradient Descent and our Model</a></li>
</ul></li>
<li class="part"><span><b>V Learning Concepts</b></span></li>
<li class="chapter" data-level="7" data-path="learning.html"><a href="learning.html"><i class="fa fa-check"></i><b>7</b> Theoretical Framework</a><ul>
<li class="chapter" data-level="7.1" data-path="learning.html"><a href="learning.html#mental-map"><i class="fa fa-check"></i><b>7.1</b> Mental Map</a></li>
<li class="chapter" data-level="7.2" data-path="learning.html"><a href="learning.html#kinds-of-predictions"><i class="fa fa-check"></i><b>7.2</b> Kinds of Predictions</a><ul>
<li class="chapter" data-level="7.2.1" data-path="learning.html"><a href="learning.html#two-types-of-data"><i class="fa fa-check"></i><b>7.2.1</b> Two Types of Data</a></li>
<li class="chapter" data-level="7.2.2" data-path="learning.html"><a href="learning.html#two-types-of-predictions"><i class="fa fa-check"></i><b>7.2.2</b> Two Types of Predictions</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="learning.html"><a href="learning.html#two-types-of-errors"><i class="fa fa-check"></i><b>7.3</b> Two Types of Errors</a><ul>
<li class="chapter" data-level="7.3.1" data-path="learning.html"><a href="learning.html#individual-errors"><i class="fa fa-check"></i><b>7.3.1</b> Individual Errors</a></li>
<li class="chapter" data-level="7.3.2" data-path="learning.html"><a href="learning.html#overall-errors"><i class="fa fa-check"></i><b>7.3.2</b> Overall Errors</a></li>
<li class="chapter" data-level="7.3.3" data-path="learning.html"><a href="learning.html#auxiliary-technicality"><i class="fa fa-check"></i><b>7.3.3</b> Auxiliary Technicality</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="learning.html"><a href="learning.html#noisy-targets"><i class="fa fa-check"></i><b>7.4</b> Noisy Targets</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="mse.html"><a href="mse.html"><i class="fa fa-check"></i><b>8</b> MSE of Estimator</a><ul>
<li class="chapter" data-level="8.1" data-path="mse.html"><a href="mse.html#mse-of-an-estimator"><i class="fa fa-check"></i><b>8.1</b> MSE of an Estimator</a><ul>
<li class="chapter" data-level="8.1.1" data-path="mse.html"><a href="mse.html#prototypical-cases-of-bias-and-variance"><i class="fa fa-check"></i><b>8.1.1</b> Prototypical Cases of Bias and Variance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="biasvar.html"><a href="biasvar.html"><i class="fa fa-check"></i><b>9</b> Bias-Variance Tradeoff</a><ul>
<li class="chapter" data-level="9.1" data-path="biasvar.html"><a href="biasvar.html#bias-variance-tradeoff"><i class="fa fa-check"></i><b>9.1</b> Bias-Variance Tradeoff</a></li>
<li class="chapter" data-level="9.2" data-path="biasvar.html"><a href="biasvar.html#motivation-example"><i class="fa fa-check"></i><b>9.2</b> Motivation Example</a><ul>
<li class="chapter" data-level="9.2.1" data-path="biasvar.html"><a href="biasvar.html#two-hypotheses"><i class="fa fa-check"></i><b>9.2.1</b> Two Hypotheses</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="biasvar.html"><a href="biasvar.html#learning-from-two-points"><i class="fa fa-check"></i><b>9.3</b> Learning from two points</a></li>
<li class="chapter" data-level="9.4" data-path="biasvar.html"><a href="biasvar.html#bias-variance-derivation"><i class="fa fa-check"></i><b>9.4</b> Bias-Variance Derivation</a><ul>
<li class="chapter" data-level="9.4.1" data-path="biasvar.html"><a href="biasvar.html#noisy-target"><i class="fa fa-check"></i><b>9.4.1</b> Noisy Target</a></li>
<li class="chapter" data-level="9.4.2" data-path="biasvar.html"><a href="biasvar.html#types-of-theoretical-mses"><i class="fa fa-check"></i><b>9.4.2</b> Types of Theoretical MSEs</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="biasvar.html"><a href="biasvar.html#the-tradeoff"><i class="fa fa-check"></i><b>9.5</b> The Tradeoff</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="validation.html"><a href="validation.html"><i class="fa fa-check"></i><b>10</b> Validation</a><ul>
<li class="chapter" data-level="10.1" data-path="validation.html"><a href="validation.html#model-assessment"><i class="fa fa-check"></i><b>10.1</b> Model Assessment</a></li>
<li class="chapter" data-level="10.2" data-path="validation.html"><a href="validation.html#holdout-method"><i class="fa fa-check"></i><b>10.2</b> Holdout Method</a><ul>
<li class="chapter" data-level="10.2.1" data-path="validation.html"><a href="validation.html#rationale-behind-holdout-method"><i class="fa fa-check"></i><b>10.2.1</b> Rationale Behind Holdout Method</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="validation.html"><a href="validation.html#repeated-holdout-method"><i class="fa fa-check"></i><b>10.3</b> Repeated Holdout Method</a></li>
<li class="chapter" data-level="10.4" data-path="validation.html"><a href="validation.html#bootstrap-method"><i class="fa fa-check"></i><b>10.4</b> Bootstrap Method</a></li>
<li class="chapter" data-level="10.5" data-path="validation.html"><a href="validation.html#model-selection"><i class="fa fa-check"></i><b>10.5</b> Model Selection</a><ul>
<li class="chapter" data-level="10.5.1" data-path="validation.html"><a href="validation.html#three-way-holdout-method"><i class="fa fa-check"></i><b>10.5.1</b> Three-way Holdout Method</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="validation.html"><a href="validation.html#cross-validation"><i class="fa fa-check"></i><b>10.6</b> Cross-Validation</a><ul>
<li class="chapter" data-level="10.6.1" data-path="validation.html"><a href="validation.html#leave-one-out-cross-validation-loocv"><i class="fa fa-check"></i><b>10.6.1</b> Leave-One-Out Cross Validation (LOOCV)</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>VI Regularization</b></span></li>
<li class="chapter" data-level="11" data-path="regular.html"><a href="regular.html"><i class="fa fa-check"></i><b>11</b> Regularization Techniques</a><ul>
<li class="chapter" data-level="11.1" data-path="regular.html"><a href="regular.html#multicollinearity-issues"><i class="fa fa-check"></i><b>11.1</b> Multicollinearity Issues</a><ul>
<li class="chapter" data-level="11.1.1" data-path="regular.html"><a href="regular.html#toy-example"><i class="fa fa-check"></i><b>11.1.1</b> Toy Example</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="regular.html"><a href="regular.html#irregular-coefficients"><i class="fa fa-check"></i><b>11.2</b> Irregular Coefficients</a><ul>
<li class="chapter" data-level="11.2.1" data-path="regular.html"><a href="regular.html#effect-of-multicollinearity"><i class="fa fa-check"></i><b>11.2.1</b> Effect of Multicollinearity</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="regular.html"><a href="regular.html#regularization-metaphor"><i class="fa fa-check"></i><b>11.3</b> Regularization Metaphor</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="pcr.html"><a href="pcr.html"><i class="fa fa-check"></i><b>12</b> Principal Components Regression</a><ul>
<li class="chapter" data-level="12.1" data-path="pcr.html"><a href="pcr.html#motivation-example-1"><i class="fa fa-check"></i><b>12.1</b> Motivation Example</a></li>
<li class="chapter" data-level="12.2" data-path="pcr.html"><a href="pcr.html#the-pcr-model"><i class="fa fa-check"></i><b>12.2</b> The PCR Model</a></li>
<li class="chapter" data-level="12.3" data-path="pcr.html"><a href="pcr.html#how-does-pcr-work"><i class="fa fa-check"></i><b>12.3</b> How does PCR work?</a><ul>
<li class="chapter" data-level="12.3.1" data-path="pcr.html"><a href="pcr.html#transition-formula"><i class="fa fa-check"></i><b>12.3.1</b> Transition Formula</a></li>
<li class="chapter" data-level="12.3.2" data-path="pcr.html"><a href="pcr.html#size-of-coefficients"><i class="fa fa-check"></i><b>12.3.2</b> Size of Coefficients</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="pls.html"><a href="pls.html"><i class="fa fa-check"></i><b>13</b> Partial Least Squares Regression</a><ul>
<li class="chapter" data-level="13.1" data-path="pls.html"><a href="pls.html#motivation-example-2"><i class="fa fa-check"></i><b>13.1</b> Motivation Example</a></li>
<li class="chapter" data-level="13.2" data-path="pls.html"><a href="pls.html#the-plsr-model"><i class="fa fa-check"></i><b>13.2</b> The PLSR Model</a></li>
<li class="chapter" data-level="13.3" data-path="pls.html"><a href="pls.html#how-does-plsr-work"><i class="fa fa-check"></i><b>13.3</b> How does PLSR work?</a></li>
<li class="chapter" data-level="13.4" data-path="pls.html"><a href="pls.html#plsr-algorithm"><i class="fa fa-check"></i><b>13.4</b> PLSR Algorithm</a><ul>
<li class="chapter" data-level="13.4.1" data-path="pls.html"><a href="pls.html#pls-solution-with-original-variables"><i class="fa fa-check"></i><b>13.4.1</b> PLS Solution with original variables</a></li>
<li class="chapter" data-level="13.4.2" data-path="pls.html"><a href="pls.html#size-of-coefficients-1"><i class="fa fa-check"></i><b>13.4.2</b> Size of Coefficients</a></li>
<li class="chapter" data-level="13.4.3" data-path="pls.html"><a href="pls.html#some-properties"><i class="fa fa-check"></i><b>13.4.3</b> Some Properties</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="ridge.html"><a href="ridge.html"><i class="fa fa-check"></i><b>14</b> Ridge Regression</a></li>
<li class="part"><span><b>VII Classification</b></span></li>
<li class="chapter" data-level="15" data-path="classif.html"><a href="classif.html"><i class="fa fa-check"></i><b>15</b> Classification Methods</a></li>
<li class="chapter" data-level="16" data-path="logistic.html"><a href="logistic.html"><i class="fa fa-check"></i><b>16</b> Logistic Regression</a><ul>
<li class="chapter" data-level="16.1" data-path="logistic.html"><a href="logistic.html#motivation-1"><i class="fa fa-check"></i><b>16.1</b> Motivation</a><ul>
<li class="chapter" data-level="16.1.1" data-path="logistic.html"><a href="logistic.html#first-approach-fitting-a-line"><i class="fa fa-check"></i><b>16.1.1</b> First Approach: Fitting a Line</a></li>
<li class="chapter" data-level="16.1.2" data-path="logistic.html"><a href="logistic.html#secodn-approach-harsh-thresholding"><i class="fa fa-check"></i><b>16.1.2</b> Secodn Approach: Harsh Thresholding</a></li>
<li class="chapter" data-level="16.1.3" data-path="logistic.html"><a href="logistic.html#third-approach-conditional-means"><i class="fa fa-check"></i><b>16.1.3</b> Third Approach: Conditional Means</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="logistic.html"><a href="logistic.html#logistic-regression-model"><i class="fa fa-check"></i><b>16.2</b> Logistic Regression Model</a><ul>
<li class="chapter" data-level="16.2.1" data-path="logistic.html"><a href="logistic.html#the-criterion-being-optimized"><i class="fa fa-check"></i><b>16.2.1</b> The Criterion Being Optimized</a></li>
<li class="chapter" data-level="16.2.2" data-path="logistic.html"><a href="logistic.html#another-way-to-solve-logistic-regression"><i class="fa fa-check"></i><b>16.2.2</b> Another Way to Solve Logistic Regression</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>VIII Clustering</b></span></li>
<li class="chapter" data-level="17" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>17</b> Clustering</a><ul>
<li class="chapter" data-level="17.1" data-path="clustering.html"><a href="clustering.html#about-clustering"><i class="fa fa-check"></i><b>17.1</b> About Clustering</a><ul>
<li class="chapter" data-level="17.1.1" data-path="clustering.html"><a href="clustering.html#types-of-clustering"><i class="fa fa-check"></i><b>17.1.1</b> Types of Clustering</a></li>
<li class="chapter" data-level="17.1.2" data-path="clustering.html"><a href="clustering.html#hard-clustering"><i class="fa fa-check"></i><b>17.1.2</b> Hard Clustering</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="clustering.html"><a href="clustering.html#dispersion-measures"><i class="fa fa-check"></i><b>17.2</b> Dispersion Measures</a></li>
<li class="chapter" data-level="17.3" data-path="clustering.html"><a href="clustering.html#complexity-in-clustering"><i class="fa fa-check"></i><b>17.3</b> Complexity in Clustering</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="kmeans.html"><a href="kmeans.html"><i class="fa fa-check"></i><b>18</b> K-Means</a><ul>
<li class="chapter" data-level="18.1" data-path="kmeans.html"><a href="kmeans.html#toy-example-1"><i class="fa fa-check"></i><b>18.1</b> Toy Example</a></li>
<li class="chapter" data-level="18.2" data-path="kmeans.html"><a href="kmeans.html#what-does-k-means-do"><i class="fa fa-check"></i><b>18.2</b> What does K-means do?</a></li>
<li class="chapter" data-level="18.3" data-path="kmeans.html"><a href="kmeans.html#k-means-algorithms"><i class="fa fa-check"></i><b>18.3</b> K-Means Algorithms</a><ul>
<li class="chapter" data-level="18.3.1" data-path="kmeans.html"><a href="kmeans.html#classic-version"><i class="fa fa-check"></i><b>18.3.1</b> Classic Version</a></li>
<li class="chapter" data-level="18.3.2" data-path="kmeans.html"><a href="kmeans.html#moving-centers-algorithm"><i class="fa fa-check"></i><b>18.3.2</b> Moving Centers Algorithm</a></li>
<li class="chapter" data-level="18.3.3" data-path="kmeans.html"><a href="kmeans.html#dynamic-clouds"><i class="fa fa-check"></i><b>18.3.3</b> Dynamic Clouds</a></li>
<li class="chapter" data-level="18.3.4" data-path="kmeans.html"><a href="kmeans.html#choosing-k"><i class="fa fa-check"></i><b>18.3.4</b> Choosing <span class="math inline">\(k\)</span></a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Made with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">All Models Are Wrong</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="gradient" class="section level1">
<h1><span class="header-section-number">6</span> Gradient Descent</h1>
<p>Before moving to the next part of the book which deals with the theory of learning,
we want to introduce a very popular optimization technique that is commonly
used in many statistical learning methods: the famous <strong>gradient descent</strong>
algorithm.</p>
<div id="error-surface" class="section level2">
<h2><span class="header-section-number">6.1</span> Error Surface</h2>
<p>Consider the overall error measure of a linear regression problem, for example
the mean squared error (<span class="math inline">\(\text{MSE}\)</span>)—or if you prefer the sum of squared errors, which is simply <span class="math inline">\(\text{SSE} = n \text{MSE}\)</span>).</p>
<p><span class="math display" id="eq:302-1">\[
E(\mathbf{y}, \mathbf{\hat{y}}) = \frac{1}{n} \left( \mathbf{b^\mathsf{T} X^\mathsf{T}} \mathbf{X b} - 2 \mathbf{b^\mathsf{T} X^\mathsf{T} y} + \mathbf{y^\mathsf{T}y} \right)
\tag{6.1}
\]</span></p>
<p>As we saw in the <a href="ols.html#ols">previous chapter</a>, we can look at such error measure
from the perspective of the parameters (i.e. the regression coefficients).
From this perspective, we denote this error function as <span class="math inline">\(E(\mathbf{b})\)</span>,
making explicit its dependency on the vector of coefficients <span class="math inline">\(\mathbf{b}\)</span>.</p>
<p><span class="math display" id="eq:302-2">\[
E(\mathbf{b}) = \frac{1}{n} ( \underbrace{\mathbf{b^\mathsf{T} X^\mathsf{T}} \mathbf{X b}}_{\text{Quadratic Form}} - \underbrace{ 2 \mathbf{b^\mathsf{T} X^\mathsf{T} y}}_{\text{Linear}} + \underbrace{ \mathbf{y^\mathsf{T}y}}_{\text{Constant}} )
\tag{6.2}
\]</span></p>
<p>As you can tell, <span class="math inline">\(E(\mathbf{b})\)</span> is a quadratic function with respect to
<span class="math inline">\(\mathbf{b}\)</span>. Moreover, <span class="math inline">\(E(\mathbf{b})\)</span> is a positive semidefinite quadratic
form which implies that it is a <strong>convex</strong> function.</p>
<p>What does this all mean?</p>
<p>For illustration purposes, let’s consider again a linear regression with two
inputs <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>, and assume that there is no constant term. In this case,
the error function <span class="math inline">\(E(\mathbf{b})\)</span> will generate a convex error surface with the
shape of a bowl:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-54"></span>
<img src="images/ols/ols-error-surface0.svg" alt="Error Surface" width="70%" />
<p class="caption">
Figure 6.1: Error Surface
</p>
</div>
<p>In general, with a convex error function, we know that there is a minimum,<br />
and the challenge is to find such value.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-55"></span>
<img src="images/ols/ols-error-surface2.svg" alt="Error Surface" width="70%" />
<p class="caption">
Figure 6.2: Error Surface
</p>
</div>
<p>With OLS we can use direct methods to obtain the minimum. All we need to do is
to compute the derivative of the error function <span class="math inline">\(E(\mathbf{b}\)</span>), set it equal
to zero, and find this minimum point <span class="math inline">\(\mathbf{\overset{*}{b}}\)</span>.
As you know, assuming that the matrix <span class="math inline">\(\mathbf{X^\mathsf{T} X}\)</span> is invertible,
the OLS minimum is easily calculated as:
<span class="math inline">\(\mathbf{b} = (\mathbf{X^\mathsf{T}X})^{-1} \mathbf{X^\mathsf{T} y}\)</span>.</p>
<p>Interestingly, we can also use iterative methods to compute this minimum.
The iterative method that we will discuss here is called
<strong>gradient descent</strong>.</p>
</div>
<div id="idea-of-gradient-descent" class="section level2">
<h2><span class="header-section-number">6.2</span> Idea of Gradient Descent</h2>
<p>The main idea of gradient descent is as follows: we start with an arbitrary
point <span class="math inline">\(\mathbf{b}^{(0)} = ( b_1^{(0)}, b_2^{(0)} )\)</span> of model parameters,
and we evaluate the error function at this point: <span class="math inline">\(E(\mathbf{b}^{(0)})\)</span>.
This gives us a location somewhere on the error surface. See figure below.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-56"></span>
<img src="images/ols/gradient-error-surface1.svg" alt="Starting position on error surface" width="70%" />
<p class="caption">
Figure 6.3: Starting position on error surface
</p>
</div>
<p>We then get a new vector <span class="math inline">\(\mathbf{b}^{(1)}\)</span> in a way that we “move down” the
surface to obtain a new position <span class="math inline">\(E(\mathbf{b}^{(1)})\)</span>:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-57"></span>
<img src="images/ols/gradient-error-surface2.svg" alt="Begin to move down on error surface" width="70%" />
<p class="caption">
Figure 6.4: Begin to move down on error surface
</p>
</div>
<p>We keep “moving down” the surface, obtaining at each step <span class="math inline">\(s\)</span> subsequent new
vectors <span class="math inline">\(\mathbf{b}^{(s)}\)</span> that result in an error <span class="math inline">\(E(\mathbf{b}^{(s)})\)</span> that
is closer to the minimum of <span class="math inline">\(E(\mathbf{b})\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-58"></span>
<img src="images/ols/gradient-error-surface3.svg" alt="Keep moving down on error surface" width="70%" />
<p class="caption">
Figure 6.5: Keep moving down on error surface
</p>
</div>
<p>Eventually, we should get very, very close to the minimizing point, and
ideally, arrive at the minimum <span class="math inline">\(\mathbf{\overset{*}{b}}\)</span>.</p>
<p>One important thing to always keep in mind when dealing with minimization
(and maximization) problems is that, in practice, we don’t get to see the
surface. Instead, we only have local information at the current point we
evaluate the error function.</p>
<p>Here’s a useful metaphor: Imagine that you are on the top a mountain
(or some similar landscape) and your goal is to get to the bottom of the valley.
The issue is that it is a foggy day (or night), and the visibility conditions
are so poor that you only get to see/feel what is very near you
(a couple of inches around you). What would you do to get to the bottom of the
valley? Well, you start touching your surroundings trying to feel in which
direction the slope of the terrain goes down. The key is to identify the
direction in which the slope gives you the steepest descent. This is the
conceptual idea behind optimization algorithms like gradient descent.</p>
</div>
<div id="moving-down-an-error-surface" class="section level2">
<h2><span class="header-section-number">6.3</span> Moving Down an Error Surface</h2>
<p>What do we mean by “moving down the error surface”? Well, mathematically, this
means we generate the new vector <span class="math inline">\(\mathbf{b}^{(1)}\)</span> from the initial point
<span class="math inline">\(\mathbf{b}^{(0)}\)</span>, using the following formula:</p>
<p><span class="math display" id="eq:302-3">\[
\mathbf{b}^{(1)} = \mathbf{b}^{(0)} + \alpha \mathbf{v}
\tag{6.3}
\]</span></p>
<p>As you can tell, in addition to the values of parameter vectors <span class="math inline">\(\mathbf{b}^{(0)}\)</span>
and <span class="math inline">\(\mathbf{b}^{(1)}\)</span>, we have two extra ingredients: <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\mathbf{v}\)</span>.</p>
<p>We call <span class="math inline">\(\alpha\)</span> the <strong>step size</strong>. Intuitively, <span class="math inline">\(\alpha\)</span> tells us how far down
the surface we are moving. In turn, <span class="math inline">\(\mathbf{v}\)</span> is the vector indicating the
direction in which we need to move. Because we are interested in this direction,
we can simply consider <span class="math inline">\(\mathbf{v}\)</span> to be a <strong>unit vector</strong>. We will discuss
how to find the direction of <span class="math inline">\(\mathbf{v}\)</span> in a little bit. Right now let’s just
focus on generating new vectors in this manner:</p>
<p><span class="math display" id="eq:302-4">\[\begin{align*}
\tag{6.4}
\mathbf{b}^{(2)} &amp; = \mathbf{b}^{(1)} + \alpha \mathbf{v}^{(1)} \\ 
\mathbf{b}^{(3)} &amp; = \mathbf{b}^{(2)} + \alpha \mathbf{v}^{(2)} \\ 
\vdots &amp; \hspace{10mm} \vdots \\
\mathbf{b}^{(s+1)} &amp; = \mathbf{b}^{(s)} + \alpha \mathbf{v}^{(s)}
\end{align*}\]</span></p>
<p>Note that we are assuming a constant step size <span class="math inline">\(\alpha\)</span>; that is, note that
<span class="math inline">\(\alpha\)</span> remains the same at each iteration. We should say that there are more
sophisticated versions of gradient descent that allow a <em>variable</em> step size,
however we will not consider that case.
As we can see in the series of figures above, the direction in which we travel
will change at each step of the process. We will also see this mathematically
in the next subsection.</p>
<div id="the-direction-of-mathbfv" class="section level3">
<h3><span class="header-section-number">6.3.1</span> The direction of <span class="math inline">\(\mathbf{v}\)</span></h3>
<p>How do we find the direction of <span class="math inline">\(\mathbf{v}\)</span>? Consider the gradient of our error
function. The gradient always points in the direction of steepest <em>ascent</em>
(i.e. largest positive change). Hence, we want to travel in the exact opposite
direction of the gradient.</p>
<p>Let’s “prove” this mathematically. In terms of the error function itself, what
does it mean for our vectors <span class="math inline">\(\mathbf{b}^{(s+1)}\)</span> to be “getting closer” to the
minimizing point? Well, it means that the error at point <span class="math inline">\(\mathbf{b}^{(s+1)}\)</span>
is less than the error at point <span class="math inline">\(\mathbf{b}^{(s)}\)</span>. Hence, we examine
<span class="math inline">\(\Delta E_{\mathbf{b}}\)</span>, the difference between the errors at these two points:</p>
<p><span class="math display" id="eq:302-5">\[\begin{align*} 
\Delta E_{\mathbf{b}}  &amp; := E\big( \mathbf{b}^{(s + 1)} \big) - E\big( \mathbf{b}^{(s)} \big) \\
&amp; = E \big( \mathbf{b}^{(s)} + \alpha \mathbf{v} \big) - E\big( \mathbf{b}^{(s)} \big) 
\tag{6.5}
\end{align*}\]</span></p>
<p>In order for our vector <span class="math inline">\(\mathbf{b}^{(s+1)}\)</span> to be closer to the minimizing
point that <span class="math inline">\(\mathbf{b}^{(s)}\)</span>, we want this quantity to be as negative as possible.
To find the <span class="math inline">\(\mathbf{v}\)</span> that makes this true, we need to use a trick:
get the Taylor series expansion of <span class="math inline">\(E(\mathbf{b}^{(s)} + \alpha \mathbf{v})\)</span>. Doing so, we obtain:</p>
<p><span class="math display" id="eq:302-6">\[\begin{align*}
\Delta E_{\mathbf{b}} &amp;= E\big( \mathbf{b}^{(s)} \big) + \nabla E\big( \mathbf{b}^{(s)} \big)^{\mathsf{T}} \big(\alpha \mathbf{v} \big) + O \big( \alpha^2 \big) - E\big( \mathbf{b}^{(s)} \big) \\
&amp; = \alpha \hspace{1mm} \nabla E \big( \mathbf{b}^{(s)} \big)^{\mathsf{T}} \mathbf{v} + O(\alpha^2) 
\tag{6.6}
\end{align*}\]</span></p>
<p>where <span class="math inline">\(O(\alpha^2)\)</span> denotes those terms of order 2 or greater in the series expansion.</p>
<p>We will focus our attention on the series expansion of just one term, and ignore
the higher order terms comprised by <span class="math inline">\(O(\alpha^2)\)</span>. In other words, we will
focus on the term:
<span class="math inline">\(\alpha \hspace{1mm} \nabla E \big( \mathbf{b}^{(s)} \big)^{\mathsf{T}} \mathbf{v}\)</span>.
Notice that this term involves the inner product between the gradient and
the unit vector <span class="math inline">\(\mathbf{v}\)</span>, that is:
<span class="math inline">\([ \nabla E(\mathbf{b}^{(s)})]^{\mathsf{T}} \mathbf{v}\)</span>.</p>
<p>For notational convenience, let us (temporarily) define
<span class="math inline">\(\mathbf{u} : = \nabla E ( \mathbf{b}^{(s)} )\)</span>. In this way, we need to examine
the inner product: <span class="math inline">\(\mathbf{u^\mathsf{T}v}\)</span>.
We can consider three prototypical cases with respect to the orientation of <span class="math inline">\(\mathbf{v}\)</span> and <span class="math inline">\(\mathbf{u}\)</span>: either antiparallel, parallel, or orthogonal.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-59"></span>
<img src="images/ols/three-prototypical-cases.png" alt="Three prototypical cases" width="80%" />
<p class="caption">
Figure 6.6: Three prototypical cases
</p>
</div>
<ul>
<li><p>When both vectors are parallel, <span class="math inline">\(\mathbf{u} \parallel \mathbf{v}\)</span> (i.e. the
second case above), then <span class="math inline">\(\mathbf{u}^\mathsf{T}\mathbf{v} = \| \mathbf{u} \|\)</span>.
Recall that we are assuming <span class="math inline">\(\mathbf{v}\)</span> to be a unit vector.</p></li>
<li><p>When both vectors are antiparallel, <span class="math inline">\(\mathbf{u} \not\parallel \mathbf{v}\)</span>
(i.e. the first case above), then <span class="math inline">\(\mathbf{u}^\mathsf{T} \mathbf{v} = - \| \mathbf{u} \|\)</span>.</p></li>
<li><p>When both vectors are orthogonal, <span class="math inline">\(\mathbf{u} \perp \mathbf{v}\)</span>, then
<span class="math inline">\(\mathbf{u}^\mathsf{T} \mathbf{v} = 0\)</span>.</p></li>
</ul>
<p>Therefore, in any of the three cases, we have that</p>
<p><span class="math display" id="eq:302-7">\[
\mathbf{u}^\mathsf{T} \mathbf{v} \geq - \left\| \mathbf{u} \right\|
\tag{6.7}
\]</span></p>
<p>This means that the least we can get is <span class="math inline">\(- \left\| \mathbf{u} \right\|\)</span>, which
occurs when <span class="math inline">\(\mathbf{v}\)</span> is in the opposite direction of <span class="math inline">\(\mathbf{u}\)</span>.</p>
<p>Hence, recalling that <span class="math inline">\(\mathbf{u} := \nabla E(\mathbf{b}^{(s)})\)</span>, we can plug
this result into our error computation to obtain:</p>
<p><span class="math display" id="eq:302-8">\[\begin{align*} 
\Delta E_{\mathbf{b}}  &amp; := E\big( \mathbf{b}^{(s + 1)} \big) - E\big( \mathbf{b}^{(s)} \big) \\
&amp; = E \big( \mathbf{b}^{(s)} + \alpha \mathbf{v} \big) - E\big( \mathbf{b}^{(s)} \big) \\
&amp; = \alpha \hspace{1mm} \nabla E \big( \mathbf{b}^{(s)} \big)^{\mathsf{T}} \mathbf{v} + O(\alpha^2)  \\
&amp; \geq - \alpha \left\| \nabla E\big( \mathbf{b}^{(s)} \big) \right\|
\tag{6.8}
\end{align*}\]</span></p>
<p>Thus, to make <span class="math inline">\(\Delta E_{\mathbf{b}}\)</span> as negative as possible, we should take
<span class="math inline">\(\mathbf{v}\)</span> parallel to the opposite direction of the gradient.</p>
<p><strong>The Moral:</strong> We want the following:</p>
<p><span class="math display" id="eq:302-9">\[
\mathbf{v} = - \frac{\nabla E(\mathbf{b}^{(s)}) }{\left\| \nabla E(\mathbf{b}^{(s)})  \right\| }
\tag{6.9}
\]</span></p>
<p>which means we want to move in the direction opposite to that of the gradient.
Keep in mind that we divided by the norm because we previously defined
<span class="math inline">\(\mathbf{v}\)</span> to be a <strong>unit vector</strong>.</p>
<p>This also reveals the meaning behind the name <strong>gradient descent</strong>; we are
<em>descending</em> in the direction opposite to the <em>gradient</em> of the error function.</p>
</div>
</div>
<div id="gradient-descent-and-our-model" class="section level2">
<h2><span class="header-section-number">6.4</span> Gradient Descent and our Model</h2>
<p>Before we present the full algorithm for gradient descent in the context of regression,
let us investigate the actual gradient further. Since we have a formula for
<span class="math inline">\(E(\mathbf{b})\)</span>, we can find a closed form for its gradient
<span class="math inline">\(\nabla E(\mathbf{b})\)</span>:</p>
<p><span class="math display" id="eq:302-10">\[\begin{align*}
E(\mathbf{b}) &amp; = \frac{1}{n} \left( \mathbf{y} - \mathbf{X} \mathbf{b} \right)^{\mathsf{T}} \left( \mathbf{y} - \mathbf{X} \mathbf{b} \right) \\
\\
&amp; = \frac{1}{n} \left( \mathbf{b}^\mathsf{T} \mathbf{X}^\mathsf{T} \mathbf{X} \mathbf{b} - 2 \mathbf{b}^\mathsf{T} \mathbf{X}^\mathsf{T} \mathbf{y} + \mathbf{y}^\mathsf{T} \mathbf{y} \right) \\
\\
\nabla E(\mathbf{b}) &amp; = \frac{1}{n} \left( 2 \mathbf{X}^\mathsf{T} \mathbf{X} \mathbf{b} - 2 \mathbf{X}^\mathsf{T} \mathbf{y} \right) 
\tag{6.10}
\end{align*}\]</span></p>
<p>This was from the columns point of view; we can also find a different formula
from the row’s perspective in pointwise notation:</p>
<p><span class="math display" id="eq:302-11">\[\begin{align*}
E(\mathbf{b}) &amp; = \frac{1}{n} \sum_{i = 1}^{n} \left( y_i - \mathbf{b}^\mathsf{T} \mathbf{x_i} \right)^2 = \frac{1}{n} \sum_{i=1}^{n} \left( y_i - \hat{y}_i \right) \\
&amp; = \frac{1}{n} \sum_{i=1}^{n} \left( y_i - b_0 x_{i0} - b_1 x_{i1} - \dots - b_{j} x_{ij} - \dots - b_{p} x_{ip} \right)^2 \\
\frac{\partial}{\partial b_j} E(\mathbf{b}) &amp; = - \frac{2}{n} \sum_{i=1}^{n}  \left( y_i - b_0 x_{i0} - b_1 x_{i1} - \dots - b_{j} x_{ij} - \dots - b_{p} x_{ip} \right) x_{ij} \\
\frac{\partial}{\partial b_j} E(\mathbf{b}^{(0)}) &amp; = - \frac{2}{n} \sum_{i=1}^{n}  \left( y_i - b_0 x_{i0} - b_1 x_{i1} - \dots - b_{j} x_{ij} - \dots - b_{p} x_{ip} \right) x_{ij} \\
&amp; = - \frac{2}{n} \sum_{i=1}^{n} \left( y_i - \left[ \mathbf{b}^{(0)} \right]^{\mathsf{T}} \mathbf{x_i} \right) x_{ij} 
\tag{6.11}
\end{align*}\]</span></p>
<p>This will be a better formula to use in our iterative algorithm, described below.</p>
<div id="algorithm" class="section level4 unnumbered">
<h4>Algorithm</h4>
<p>Initialize a vector <span class="math inline">\(\mathbf{b}^{(0)} = \left( b_{0}^{(0)}, b_{1}^{(0)}, \dots, b_{p}^{(0)} \right)\)</span></p>
<p>Repeat the following over <span class="math inline">\(s\)</span> (starting with <span class="math inline">\(s = 0\)</span>), until convergence:</p>
<p><span class="math inline">\(b_{j}^{(s+1)}:= b_{j}^{(s)} + \alpha \cdot \frac{2}{n} \sum_{i=1}^{n} \left( y_i - \left[\mathbf{b}^{(s)}\right]^{\mathsf{T}} \mathbf{x_i} \right) x_{ij}\)</span> for all <span class="math inline">\(j = 0, \dots, p\)</span> simultaneously.</p>
<p>In more compact notation,</p>
<p><span class="math display">\[
b_{j}^{(s+1)} = b_{j}^{(s)} + \alpha \cdot \frac{\partial }{\partial b_j} E(\mathbf{b}^{(s)})
\]</span></p>
<p>Store these elements in the vector <span class="math inline">\(\mathbf{b}^{(s+1)} = \left( b_{0}^{(s+1)} , b_{1}^{(s+1)} , \dots, b_{p}^{(s+1)} \right)\)</span></p>
<p>When there is little change between <span class="math inline">\(\mathbf{b}^{(k + 1)}\)</span> and <span class="math inline">\(\mathbf{b}^{(k)}\)</span>
(for some integer <span class="math inline">\(k\)</span>), the algorithm will have converged and
<span class="math inline">\(\mathbf{b}^{*} = \mathbf{b}^{(k+1)}\)</span>.</p>
<p>Note that in the algorithm, we have dropped the condition that <span class="math inline">\(\mathbf{v}\)</span> is
a unit vector. Indeed, if you look on the Wikipedia page for
<a href="https://en.wikipedia.org/wiki/Gradient_descent">Gradient Descent</a>,
the algorithm listed there also omits the unit-length condition.</p>

</div>
</div>
</div>



            </section>

          </div>
        </div>
      </div>
<a href="ols.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="learning.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["allmodelsarewrong.pdf", "allmodelsarewrong.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
