<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5 Linear Regression | All Models Are Wrong</title>
  <meta name="description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  <meta name="generator" content="bookdown 0.16 and GitBook 2.6.7" />

  <meta property="og:title" content="5 Linear Regression | All Models Are Wrong" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://allmodelsarewrong.github.io" />
  
  <meta property="og:description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  <meta name="github-repo" content="allmodelsarewrong/allmodelsarewrong.github.io" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5 Linear Regression | All Models Are Wrong" />
  
  <meta name="twitter:description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  

<meta name="author" content="by Gaston Sanchez, and Ethan Marzban" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="pca.html"/>
<link rel="next" href="gradient.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><b>All Models Are Wrong (CSL)</b><br><small>by G. Sanchez & E. Marzban</small></a></li>

<li class="divider"></li>
<li class="part"><span><b>I Welcome</b></span></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="about.html"><a href="about.html"><i class="fa fa-check"></i><b>1</b> About this book</a><ul>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#prerequisites"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="part"><span><b>II Intro</b></span></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#basic-notation"><i class="fa fa-check"></i><b>2.1</b> Basic Notation</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="duality.html"><a href="duality.html"><i class="fa fa-check"></i><b>3</b> Geometric Duality</a><ul>
<li class="chapter" data-level="3.1" data-path="duality.html"><a href="duality.html#rows-space"><i class="fa fa-check"></i><b>3.1</b> Rows Space</a></li>
<li class="chapter" data-level="3.2" data-path="duality.html"><a href="duality.html#columns-space"><i class="fa fa-check"></i><b>3.2</b> Columns Space</a></li>
<li class="chapter" data-level="3.3" data-path="duality.html"><a href="duality.html#cloud-of-individuals"><i class="fa fa-check"></i><b>3.3</b> Cloud of Individuals</a><ul>
<li class="chapter" data-level="3.3.1" data-path="duality.html"><a href="duality.html#average-individual"><i class="fa fa-check"></i><b>3.3.1</b> Average Individual</a></li>
<li class="chapter" data-level="3.3.2" data-path="duality.html"><a href="duality.html#centered-data"><i class="fa fa-check"></i><b>3.3.2</b> Centered Data</a></li>
<li class="chapter" data-level="3.3.3" data-path="duality.html"><a href="duality.html#distance-between-individuals"><i class="fa fa-check"></i><b>3.3.3</b> Distance between individuals</a></li>
<li class="chapter" data-level="3.3.4" data-path="duality.html"><a href="duality.html#distance-to-the-centroid"><i class="fa fa-check"></i><b>3.3.4</b> Distance to the centroid</a></li>
<li class="chapter" data-level="3.3.5" data-path="duality.html"><a href="duality.html#measures-of-dispersion"><i class="fa fa-check"></i><b>3.3.5</b> Measures of Dispersion</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="duality.html"><a href="duality.html#cloud-of-variables"><i class="fa fa-check"></i><b>3.4</b> Cloud of Variables</a><ul>
<li class="chapter" data-level="3.4.1" data-path="duality.html"><a href="duality.html#mean-of-a-variable"><i class="fa fa-check"></i><b>3.4.1</b> Mean of a Variable</a></li>
<li class="chapter" data-level="3.4.2" data-path="duality.html"><a href="duality.html#variance-of-a-variable"><i class="fa fa-check"></i><b>3.4.2</b> Variance of a Variable</a></li>
<li class="chapter" data-level="3.4.3" data-path="duality.html"><a href="duality.html#variance-with-vector-notation"><i class="fa fa-check"></i><b>3.4.3</b> Variance with Vector Notation</a></li>
<li class="chapter" data-level="3.4.4" data-path="duality.html"><a href="duality.html#standard-deviation-as-a-norm"><i class="fa fa-check"></i><b>3.4.4</b> Standard Deviation as a Norm</a></li>
<li class="chapter" data-level="3.4.5" data-path="duality.html"><a href="duality.html#covariance"><i class="fa fa-check"></i><b>3.4.5</b> Covariance</a></li>
<li class="chapter" data-level="3.4.6" data-path="duality.html"><a href="duality.html#correlation"><i class="fa fa-check"></i><b>3.4.6</b> Correlation</a></li>
<li class="chapter" data-level="3.4.7" data-path="duality.html"><a href="duality.html#geometry-of-correlation"><i class="fa fa-check"></i><b>3.4.7</b> Geometry of Correlation</a></li>
<li class="chapter" data-level="3.4.8" data-path="duality.html"><a href="duality.html#orthogonal-projections"><i class="fa fa-check"></i><b>3.4.8</b> Orthogonal Projections</a></li>
<li class="chapter" data-level="3.4.9" data-path="duality.html"><a href="duality.html#the-mean-as-an-orthogonal-projection"><i class="fa fa-check"></i><b>3.4.9</b> The mean as an orthogonal projection</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Unsupervised I: PCA</b></span></li>
<li class="chapter" data-level="4" data-path="pca.html"><a href="pca.html"><i class="fa fa-check"></i><b>4</b> Principal Components Analysis</a><ul>
<li class="chapter" data-level="4.1" data-path="pca.html"><a href="pca.html#low-dimensional-representations"><i class="fa fa-check"></i><b>4.1</b> Low-dimensional Representations</a></li>
<li class="chapter" data-level="4.2" data-path="pca.html"><a href="pca.html#projections"><i class="fa fa-check"></i><b>4.2</b> Projections</a><ul>
<li class="chapter" data-level="4.2.1" data-path="pca.html"><a href="pca.html#vector-and-scalar-projections"><i class="fa fa-check"></i><b>4.2.1</b> Vector and Scalar Projections</a></li>
<li class="chapter" data-level="4.2.2" data-path="pca.html"><a href="pca.html#projected-inertia"><i class="fa fa-check"></i><b>4.2.2</b> Projected Inertia</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="pca.html"><a href="pca.html#maximization-problem"><i class="fa fa-check"></i><b>4.3</b> Maximization Problem</a><ul>
<li class="chapter" data-level="4.3.1" data-path="pca.html"><a href="pca.html#eigenvectors-of-mathbfs"><i class="fa fa-check"></i><b>4.3.1</b> Eigenvectors of <span class="math inline">\(\mathbf{S}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="pca.html"><a href="pca.html#another-perspective-of-pca"><i class="fa fa-check"></i><b>4.4</b> Another Perspective of PCA</a><ul>
<li class="chapter" data-level="4.4.1" data-path="pca.html"><a href="pca.html#finding-principal-components"><i class="fa fa-check"></i><b>4.4.1</b> Finding Principal Components</a></li>
<li class="chapter" data-level="4.4.2" data-path="pca.html"><a href="pca.html#finding-the-first-pc"><i class="fa fa-check"></i><b>4.4.2</b> Finding the first PC</a></li>
<li class="chapter" data-level="4.4.3" data-path="pca.html"><a href="pca.html#finding-the-second-pc"><i class="fa fa-check"></i><b>4.4.3</b> Finding the second PC</a></li>
<li class="chapter" data-level="4.4.4" data-path="pca.html"><a href="pca.html#finding-all-pcs"><i class="fa fa-check"></i><b>4.4.4</b> Finding all PCs</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="pca.html"><a href="pca.html#data-decomposition-model"><i class="fa fa-check"></i><b>4.5</b> Data Decomposition Model</a><ul>
<li class="chapter" data-level="4.5.1" data-path="pca.html"><a href="pca.html#alternative-approaches"><i class="fa fa-check"></i><b>4.5.1</b> Alternative Approaches</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Linear Regression</b></span></li>
<li class="chapter" data-level="5" data-path="ols.html"><a href="ols.html"><i class="fa fa-check"></i><b>5</b> Linear Regression</a><ul>
<li class="chapter" data-level="5.1" data-path="ols.html"><a href="ols.html#motivation"><i class="fa fa-check"></i><b>5.1</b> Motivation</a></li>
<li class="chapter" data-level="5.2" data-path="ols.html"><a href="ols.html#the-ideaintuition-of-regression"><i class="fa fa-check"></i><b>5.2</b> The Idea/Intuition of Regression</a></li>
<li class="chapter" data-level="5.3" data-path="ols.html"><a href="ols.html#the-linear-regression-model"><i class="fa fa-check"></i><b>5.3</b> The Linear Regression Model</a></li>
<li class="chapter" data-level="5.4" data-path="ols.html"><a href="ols.html#the-error-measure"><i class="fa fa-check"></i><b>5.4</b> The Error Measure</a></li>
<li class="chapter" data-level="5.5" data-path="ols.html"><a href="ols.html#the-least-squares-algorithm"><i class="fa fa-check"></i><b>5.5</b> The Least Squares Algorithm</a></li>
<li class="chapter" data-level="5.6" data-path="ols.html"><a href="ols.html#geometries-of-ols"><i class="fa fa-check"></i><b>5.6</b> Geometries of OLS</a><ul>
<li class="chapter" data-level="5.6.1" data-path="ols.html"><a href="ols.html#rows-perspective"><i class="fa fa-check"></i><b>5.6.1</b> Rows Perspective</a></li>
<li class="chapter" data-level="5.6.2" data-path="ols.html"><a href="ols.html#columns-perspective"><i class="fa fa-check"></i><b>5.6.2</b> Columns Perspective</a></li>
<li class="chapter" data-level="5.6.3" data-path="ols.html"><a href="ols.html#parameters-perspective"><i class="fa fa-check"></i><b>5.6.3</b> Parameters Perspective</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="gradient.html"><a href="gradient.html"><i class="fa fa-check"></i><b>6</b> Gradient Descent</a><ul>
<li class="chapter" data-level="6.1" data-path="gradient.html"><a href="gradient.html#error-surface"><i class="fa fa-check"></i><b>6.1</b> Error Surface</a></li>
<li class="chapter" data-level="6.2" data-path="gradient.html"><a href="gradient.html#idea-of-gradient-descent"><i class="fa fa-check"></i><b>6.2</b> Idea of Gradient Descent</a></li>
<li class="chapter" data-level="6.3" data-path="gradient.html"><a href="gradient.html#moving-down-an-error-surface"><i class="fa fa-check"></i><b>6.3</b> Moving Down an Error Surface</a><ul>
<li class="chapter" data-level="6.3.1" data-path="gradient.html"><a href="gradient.html#the-direction-of-mathbfv"><i class="fa fa-check"></i><b>6.3.1</b> The direction of <span class="math inline">\(\mathbf{v}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="gradient.html"><a href="gradient.html#gradient-descent-and-our-model"><i class="fa fa-check"></i><b>6.4</b> Gradient Descent and our Model</a></li>
</ul></li>
<li class="part"><span><b>V Learning Concepts</b></span></li>
<li class="chapter" data-level="7" data-path="learning.html"><a href="learning.html"><i class="fa fa-check"></i><b>7</b> Theoretical Framework</a><ul>
<li class="chapter" data-level="7.1" data-path="learning.html"><a href="learning.html#mental-map"><i class="fa fa-check"></i><b>7.1</b> Mental Map</a></li>
<li class="chapter" data-level="7.2" data-path="learning.html"><a href="learning.html#kinds-of-predictions"><i class="fa fa-check"></i><b>7.2</b> Kinds of Predictions</a><ul>
<li class="chapter" data-level="7.2.1" data-path="learning.html"><a href="learning.html#two-types-of-data"><i class="fa fa-check"></i><b>7.2.1</b> Two Types of Data</a></li>
<li class="chapter" data-level="7.2.2" data-path="learning.html"><a href="learning.html#two-types-of-predictions"><i class="fa fa-check"></i><b>7.2.2</b> Two Types of Predictions</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="learning.html"><a href="learning.html#two-types-of-errors"><i class="fa fa-check"></i><b>7.3</b> Two Types of Errors</a><ul>
<li class="chapter" data-level="7.3.1" data-path="learning.html"><a href="learning.html#individual-errors"><i class="fa fa-check"></i><b>7.3.1</b> Individual Errors</a></li>
<li class="chapter" data-level="7.3.2" data-path="learning.html"><a href="learning.html#overall-errors"><i class="fa fa-check"></i><b>7.3.2</b> Overall Errors</a></li>
<li class="chapter" data-level="7.3.3" data-path="learning.html"><a href="learning.html#auxiliary-technicality"><i class="fa fa-check"></i><b>7.3.3</b> Auxiliary Technicality</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="learning.html"><a href="learning.html#noisy-targets"><i class="fa fa-check"></i><b>7.4</b> Noisy Targets</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="mse.html"><a href="mse.html"><i class="fa fa-check"></i><b>8</b> MSE of Estimator</a><ul>
<li class="chapter" data-level="8.1" data-path="mse.html"><a href="mse.html#mse-of-an-estimator"><i class="fa fa-check"></i><b>8.1</b> MSE of an Estimator</a><ul>
<li class="chapter" data-level="8.1.1" data-path="mse.html"><a href="mse.html#prototypical-cases-of-bias-and-variance"><i class="fa fa-check"></i><b>8.1.1</b> Prototypical Cases of Bias and Variance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="biasvar.html"><a href="biasvar.html"><i class="fa fa-check"></i><b>9</b> Bias-Variance Tradeoff</a><ul>
<li class="chapter" data-level="9.1" data-path="biasvar.html"><a href="biasvar.html#introduction"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="biasvar.html"><a href="biasvar.html#motivation-example"><i class="fa fa-check"></i><b>9.2</b> Motivation Example</a><ul>
<li class="chapter" data-level="9.2.1" data-path="biasvar.html"><a href="biasvar.html#two-hypotheses"><i class="fa fa-check"></i><b>9.2.1</b> Two Hypotheses</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="biasvar.html"><a href="biasvar.html#learning-from-two-points"><i class="fa fa-check"></i><b>9.3</b> Learning from two points</a></li>
<li class="chapter" data-level="9.4" data-path="biasvar.html"><a href="biasvar.html#bias-variance-derivation"><i class="fa fa-check"></i><b>9.4</b> Bias-Variance Derivation</a><ul>
<li class="chapter" data-level="9.4.1" data-path="biasvar.html"><a href="biasvar.html#out-of-sample-predictions"><i class="fa fa-check"></i><b>9.4.1</b> Out-of-Sample Predictions</a></li>
<li class="chapter" data-level="9.4.2" data-path="biasvar.html"><a href="biasvar.html#noisy-target"><i class="fa fa-check"></i><b>9.4.2</b> Noisy Target</a></li>
<li class="chapter" data-level="9.4.3" data-path="biasvar.html"><a href="biasvar.html#types-of-theoretical-mses"><i class="fa fa-check"></i><b>9.4.3</b> Types of Theoretical MSEs</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="biasvar.html"><a href="biasvar.html#the-tradeoff"><i class="fa fa-check"></i><b>9.5</b> The Tradeoff</a><ul>
<li class="chapter" data-level="9.5.1" data-path="biasvar.html"><a href="biasvar.html#bias-variance-tradeoff-picture"><i class="fa fa-check"></i><b>9.5.1</b> Bias-Variance Tradeoff Picture</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="phases.html"><a href="phases.html"><i class="fa fa-check"></i><b>10</b> Learning Phases</a><ul>
<li class="chapter" data-level="10.1" data-path="phases.html"><a href="phases.html#introduction-1"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="phases.html"><a href="phases.html#model-assessment"><i class="fa fa-check"></i><b>10.2</b> Model Assessment</a><ul>
<li class="chapter" data-level="10.2.1" data-path="phases.html"><a href="phases.html#holdout-test-set"><i class="fa fa-check"></i><b>10.2.1</b> Holdout Test Set</a></li>
<li class="chapter" data-level="10.2.2" data-path="phases.html"><a href="phases.html#why-does-a-test-set-work"><i class="fa fa-check"></i><b>10.2.2</b> Why does a test set work?</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="phases.html"><a href="phases.html#model-selection"><i class="fa fa-check"></i><b>10.3</b> Model Selection</a><ul>
<li class="chapter" data-level="10.3.1" data-path="phases.html"><a href="phases.html#three-way-holdout-method"><i class="fa fa-check"></i><b>10.3.1</b> Three-way Holdout Method</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="phases.html"><a href="phases.html#model-training"><i class="fa fa-check"></i><b>10.4</b> Model Training</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="resampling.html"><a href="resampling.html"><i class="fa fa-check"></i><b>11</b> Resample Approaches</a><ul>
<li class="chapter" data-level="11.1" data-path="resampling.html"><a href="resampling.html#general-sampling-blueprint"><i class="fa fa-check"></i><b>11.1</b> General Sampling Blueprint</a></li>
<li class="chapter" data-level="11.2" data-path="resampling.html"><a href="resampling.html#monte-carlo-cross-validation"><i class="fa fa-check"></i><b>11.2</b> Monte Carlo Cross Validation</a></li>
<li class="chapter" data-level="11.3" data-path="resampling.html"><a href="resampling.html#bootstrap-method"><i class="fa fa-check"></i><b>11.3</b> Bootstrap Method</a></li>
<li class="chapter" data-level="11.4" data-path="resampling.html"><a href="resampling.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>11.4</b> <span class="math inline">\(K\)</span>-Fold Cross-Validation</a><ul>
<li class="chapter" data-level="11.4.1" data-path="resampling.html"><a href="resampling.html#leave-one-out-cross-validation-loocv"><i class="fa fa-check"></i><b>11.4.1</b> Leave-One-Out Cross Validation (LOOCV)</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>VI Regularization</b></span></li>
<li class="chapter" data-level="12" data-path="regular.html"><a href="regular.html"><i class="fa fa-check"></i><b>12</b> Regularization Techniques</a><ul>
<li class="chapter" data-level="12.1" data-path="regular.html"><a href="regular.html#multicollinearity-issues"><i class="fa fa-check"></i><b>12.1</b> Multicollinearity Issues</a><ul>
<li class="chapter" data-level="12.1.1" data-path="regular.html"><a href="regular.html#toy-example"><i class="fa fa-check"></i><b>12.1.1</b> Toy Example</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="regular.html"><a href="regular.html#irregular-coefficients"><i class="fa fa-check"></i><b>12.2</b> Irregular Coefficients</a></li>
<li class="chapter" data-level="12.3" data-path="regular.html"><a href="regular.html#connection-to-regularization"><i class="fa fa-check"></i><b>12.3</b> Connection to Regularization</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="pcr.html"><a href="pcr.html"><i class="fa fa-check"></i><b>13</b> Principal Components Regression</a><ul>
<li class="chapter" data-level="13.1" data-path="pcr.html"><a href="pcr.html#motivation-example-1"><i class="fa fa-check"></i><b>13.1</b> Motivation Example</a></li>
<li class="chapter" data-level="13.2" data-path="pcr.html"><a href="pcr.html#the-pcr-model"><i class="fa fa-check"></i><b>13.2</b> The PCR Model</a></li>
<li class="chapter" data-level="13.3" data-path="pcr.html"><a href="pcr.html#how-does-pcr-work"><i class="fa fa-check"></i><b>13.3</b> How does PCR work?</a><ul>
<li class="chapter" data-level="13.3.1" data-path="pcr.html"><a href="pcr.html#transition-formula"><i class="fa fa-check"></i><b>13.3.1</b> Transition Formula</a></li>
<li class="chapter" data-level="13.3.2" data-path="pcr.html"><a href="pcr.html#size-of-coefficients"><i class="fa fa-check"></i><b>13.3.2</b> Size of Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="pcr.html"><a href="pcr.html#selecting-number-of-pcs"><i class="fa fa-check"></i><b>13.4</b> Selecting Number of PCs</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="pls.html"><a href="pls.html"><i class="fa fa-check"></i><b>14</b> Partial Least Squares Regression</a><ul>
<li class="chapter" data-level="14.1" data-path="pls.html"><a href="pls.html#motivation-example-2"><i class="fa fa-check"></i><b>14.1</b> Motivation Example</a></li>
<li class="chapter" data-level="14.2" data-path="pls.html"><a href="pls.html#the-plsr-model"><i class="fa fa-check"></i><b>14.2</b> The PLSR Model</a></li>
<li class="chapter" data-level="14.3" data-path="pls.html"><a href="pls.html#how-does-plsr-work"><i class="fa fa-check"></i><b>14.3</b> How does PLSR work?</a></li>
<li class="chapter" data-level="14.4" data-path="pls.html"><a href="pls.html#plsr-algorithm"><i class="fa fa-check"></i><b>14.4</b> PLSR Algorithm</a><ul>
<li class="chapter" data-level="14.4.1" data-path="pls.html"><a href="pls.html#pls-solution-with-original-variables"><i class="fa fa-check"></i><b>14.4.1</b> PLS Solution with original variables</a></li>
<li class="chapter" data-level="14.4.2" data-path="pls.html"><a href="pls.html#size-of-coefficients-1"><i class="fa fa-check"></i><b>14.4.2</b> Size of Coefficients</a></li>
<li class="chapter" data-level="14.4.3" data-path="pls.html"><a href="pls.html#some-properties"><i class="fa fa-check"></i><b>14.4.3</b> Some Properties</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="pls.html"><a href="pls.html#selecting-number-of-pls-components"><i class="fa fa-check"></i><b>14.5</b> Selecting Number of PLS Components</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="ridge.html"><a href="ridge.html"><i class="fa fa-check"></i><b>15</b> Ridge Regression</a><ul>
<li class="chapter" data-level="15.1" data-path="ridge.html"><a href="ridge.html#a-new-minimization-problem"><i class="fa fa-check"></i><b>15.1</b> A New Minimization Problem</a><ul>
<li class="chapter" data-level="15.1.1" data-path="ridge.html"><a href="ridge.html#constraining-regression-coefficients"><i class="fa fa-check"></i><b>15.1.1</b> Constraining Regression Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="ridge.html"><a href="ridge.html#a-new-minimization-solution"><i class="fa fa-check"></i><b>15.2</b> A New Minimization Solution</a></li>
<li class="chapter" data-level="15.3" data-path="ridge.html"><a href="ridge.html#what-does-rr-accomplish"><i class="fa fa-check"></i><b>15.3</b> What does RR accomplish?</a><ul>
<li class="chapter" data-level="15.3.1" data-path="ridge.html"><a href="ridge.html#how-to-find-lambda"><i class="fa fa-check"></i><b>15.3.1</b> How to find <span class="math inline">\(\lambda\)</span>?</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>VII Extending Linear Regression</b></span></li>
<li class="chapter" data-level="16" data-path="linear-extensions.html"><a href="linear-extensions.html"><i class="fa fa-check"></i><b>16</b> Beyond Linear Regression</a><ul>
<li class="chapter" data-level="16.1" data-path="linear-extensions.html"><a href="linear-extensions.html#introduction-2"><i class="fa fa-check"></i><b>16.1</b> Introduction</a></li>
<li class="chapter" data-level="16.2" data-path="linear-extensions.html"><a href="linear-extensions.html#expanding-the-regression-horizon"><i class="fa fa-check"></i><b>16.2</b> Expanding the Regression Horizon</a><ul>
<li class="chapter" data-level="16.2.1" data-path="linear-extensions.html"><a href="linear-extensions.html#linearity"><i class="fa fa-check"></i><b>16.2.1</b> Linearity</a></li>
<li class="chapter" data-level="16.2.2" data-path="linear-extensions.html"><a href="linear-extensions.html#parametric-and-nonparametric"><i class="fa fa-check"></i><b>16.2.2</b> Parametric and Nonparametric</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="linear-extensions.html"><a href="linear-extensions.html#transforming-features"><i class="fa fa-check"></i><b>16.3</b> Transforming Features</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="nonparametric.html"><a href="nonparametric.html"><i class="fa fa-check"></i><b>17</b> Nonparametric Regression</a><ul>
<li class="chapter" data-level="17.1" data-path="nonparametric.html"><a href="nonparametric.html#conditional-averages"><i class="fa fa-check"></i><b>17.1</b> Conditional Averages</a></li>
<li class="chapter" data-level="17.2" data-path="nonparametric.html"><a href="nonparametric.html#looking-at-the-neighbors"><i class="fa fa-check"></i><b>17.2</b> Looking at the Neighbors</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="knn.html"><a href="knn.html"><i class="fa fa-check"></i><b>18</b> Nearest Neighbor Estimates</a><ul>
<li class="chapter" data-level="18.1" data-path="knn.html"><a href="knn.html#introduction-3"><i class="fa fa-check"></i><b>18.1</b> Introduction</a></li>
<li class="chapter" data-level="18.2" data-path="knn.html"><a href="knn.html#k-nearest-neighbors-knn"><i class="fa fa-check"></i><b>18.2</b> <span class="math inline">\(k\)</span> Nearest Neighbors (KNN)</a><ul>
<li class="chapter" data-level="18.2.1" data-path="knn.html"><a href="knn.html#distance-measures"><i class="fa fa-check"></i><b>18.2.1</b> Distance Measures</a></li>
<li class="chapter" data-level="18.2.2" data-path="knn.html"><a href="knn.html#knn-estimator"><i class="fa fa-check"></i><b>18.2.2</b> KNN Estimator</a></li>
<li class="chapter" data-level="18.2.3" data-path="knn.html"><a href="knn.html#how-to-find-k"><i class="fa fa-check"></i><b>18.2.3</b> How to find <span class="math inline">\(k\)</span>?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html"><i class="fa fa-check"></i><b>19</b> Kernel Smoothers</a><ul>
<li class="chapter" data-level="19.1" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#introduction-4"><i class="fa fa-check"></i><b>19.1</b> Introduction</a></li>
<li class="chapter" data-level="19.2" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#kernel-smoothers-1"><i class="fa fa-check"></i><b>19.2</b> Kernel Smoothers</a><ul>
<li class="chapter" data-level="19.2.1" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#kernel-functions"><i class="fa fa-check"></i><b>19.2.1</b> Kernel Functions</a></li>
<li class="chapter" data-level="19.2.2" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#weights-from-kernels"><i class="fa fa-check"></i><b>19.2.2</b> Weights from Kernels</a></li>
<li class="chapter" data-level="19.2.3" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#kernel-estimator"><i class="fa fa-check"></i><b>19.2.3</b> Kernel Estimator</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#local-polynomial-estimators"><i class="fa fa-check"></i><b>19.3</b> Local Polynomial Estimators</a></li>
</ul></li>
<li class="part"><span><b>VIII Classification</b></span></li>
<li class="chapter" data-level="20" data-path="classif.html"><a href="classif.html"><i class="fa fa-check"></i><b>20</b> Classification</a><ul>
<li class="chapter" data-level="20.1" data-path="classif.html"><a href="classif.html#introduction-5"><i class="fa fa-check"></i><b>20.1</b> Introduction</a><ul>
<li class="chapter" data-level="20.1.1" data-path="classif.html"><a href="classif.html#credit-score-example"><i class="fa fa-check"></i><b>20.1.1</b> Credit Score Example</a></li>
<li class="chapter" data-level="20.1.2" data-path="classif.html"><a href="classif.html#toy-example-1"><i class="fa fa-check"></i><b>20.1.2</b> Toy Example</a></li>
<li class="chapter" data-level="20.1.3" data-path="classif.html"><a href="classif.html#two-class-problem"><i class="fa fa-check"></i><b>20.1.3</b> Two-class Problem</a></li>
<li class="chapter" data-level="20.1.4" data-path="classif.html"><a href="classif.html#bayes-rule-reminder"><i class="fa fa-check"></i><b>20.1.4</b> Bayes’ Rule Reminder</a></li>
</ul></li>
<li class="chapter" data-level="20.2" data-path="classif.html"><a href="classif.html#bayes-classifier"><i class="fa fa-check"></i><b>20.2</b> Bayes Classifier</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="logistic.html"><a href="logistic.html"><i class="fa fa-check"></i><b>21</b> Logistic Regression</a><ul>
<li class="chapter" data-level="21.1" data-path="logistic.html"><a href="logistic.html#motivation-1"><i class="fa fa-check"></i><b>21.1</b> Motivation</a><ul>
<li class="chapter" data-level="21.1.1" data-path="logistic.html"><a href="logistic.html#first-approach-fitting-a-line"><i class="fa fa-check"></i><b>21.1.1</b> First Approach: Fitting a Line</a></li>
<li class="chapter" data-level="21.1.2" data-path="logistic.html"><a href="logistic.html#second-approach-harsh-thresholding"><i class="fa fa-check"></i><b>21.1.2</b> Second Approach: Harsh Thresholding</a></li>
<li class="chapter" data-level="21.1.3" data-path="logistic.html"><a href="logistic.html#third-approach-conditional-means"><i class="fa fa-check"></i><b>21.1.3</b> Third Approach: Conditional Means</a></li>
</ul></li>
<li class="chapter" data-level="21.2" data-path="logistic.html"><a href="logistic.html#logistic-regression-model"><i class="fa fa-check"></i><b>21.2</b> Logistic Regression Model</a><ul>
<li class="chapter" data-level="21.2.1" data-path="logistic.html"><a href="logistic.html#the-criterion-being-optimized"><i class="fa fa-check"></i><b>21.2.1</b> The Criterion Being Optimized</a></li>
<li class="chapter" data-level="21.2.2" data-path="logistic.html"><a href="logistic.html#another-way-to-solve-logistic-regression"><i class="fa fa-check"></i><b>21.2.2</b> Another Way to Solve Logistic Regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="22" data-path="discrim.html"><a href="discrim.html"><i class="fa fa-check"></i><b>22</b> Preamble for Discriminant Analysis</a><ul>
<li class="chapter" data-level="22.1" data-path="discrim.html"><a href="discrim.html#motivation-2"><i class="fa fa-check"></i><b>22.1</b> Motivation</a><ul>
<li class="chapter" data-level="22.1.1" data-path="discrim.html"><a href="discrim.html#distinguishing-species"><i class="fa fa-check"></i><b>22.1.1</b> Distinguishing Species</a></li>
<li class="chapter" data-level="22.1.2" data-path="discrim.html"><a href="discrim.html#sum-of-squares-decomposition"><i class="fa fa-check"></i><b>22.1.2</b> Sum of Squares Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="22.2" data-path="discrim.html"><a href="discrim.html#derived-ratios-from-sum-of-squares"><i class="fa fa-check"></i><b>22.2</b> Derived Ratios from Sum-of-Squares</a><ul>
<li class="chapter" data-level="22.2.1" data-path="discrim.html"><a href="discrim.html#correlation-ratio"><i class="fa fa-check"></i><b>22.2.1</b> Correlation Ratio</a></li>
<li class="chapter" data-level="22.2.2" data-path="discrim.html"><a href="discrim.html#f-ratio"><i class="fa fa-check"></i><b>22.2.2</b> F-Ratio</a></li>
</ul></li>
<li class="chapter" data-level="22.3" data-path="discrim.html"><a href="discrim.html#geometric-perspective"><i class="fa fa-check"></i><b>22.3</b> Geometric Perspective</a><ul>
<li class="chapter" data-level="22.3.1" data-path="discrim.html"><a href="discrim.html#clouds-from-class-structure"><i class="fa fa-check"></i><b>22.3.1</b> Clouds from Class Structure</a></li>
<li class="chapter" data-level="22.3.2" data-path="discrim.html"><a href="discrim.html#dispersion-decomposition"><i class="fa fa-check"></i><b>22.3.2</b> Dispersion Decomposition</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="23" data-path="cda.html"><a href="cda.html"><i class="fa fa-check"></i><b>23</b> Canonical Discriminant Analysis</a><ul>
<li class="chapter" data-level="23.1" data-path="cda.html"><a href="cda.html#cda-semi-supervised-aspect"><i class="fa fa-check"></i><b>23.1</b> CDA: Semi-Supervised Aspect</a></li>
<li class="chapter" data-level="23.2" data-path="cda.html"><a href="cda.html#looking-for-a-discriminant-axis"><i class="fa fa-check"></i><b>23.2</b> Looking for a discriminant axis</a></li>
<li class="chapter" data-level="23.3" data-path="cda.html"><a href="cda.html#looking-for-a-compromise-criterion"><i class="fa fa-check"></i><b>23.3</b> Looking for a Compromise Criterion</a><ul>
<li class="chapter" data-level="23.3.1" data-path="cda.html"><a href="cda.html#correlation-ratio-criterion"><i class="fa fa-check"></i><b>23.3.1</b> Correlation Ratio Criterion</a></li>
<li class="chapter" data-level="23.3.2" data-path="cda.html"><a href="cda.html#f-ratio-criterion"><i class="fa fa-check"></i><b>23.3.2</b> F-ratio Criterion</a></li>
<li class="chapter" data-level="23.3.3" data-path="cda.html"><a href="cda.html#a-special-pca"><i class="fa fa-check"></i><b>23.3.3</b> A Special PCA</a></li>
</ul></li>
<li class="chapter" data-level="23.4" data-path="cda.html"><a href="cda.html#cda-supervised-aspect"><i class="fa fa-check"></i><b>23.4</b> CDA: Supervised Aspect</a><ul>
<li class="chapter" data-level="23.4.1" data-path="cda.html"><a href="cda.html#distance-behind-cda"><i class="fa fa-check"></i><b>23.4.1</b> Distance behind CDA</a></li>
<li class="chapter" data-level="23.4.2" data-path="cda.html"><a href="cda.html#predictive-idea"><i class="fa fa-check"></i><b>23.4.2</b> Predictive Idea</a></li>
<li class="chapter" data-level="23.4.3" data-path="cda.html"><a href="cda.html#cda-classifier"><i class="fa fa-check"></i><b>23.4.3</b> CDA Classifier</a></li>
<li class="chapter" data-level="23.4.4" data-path="cda.html"><a href="cda.html#limitations-of-cda-classifier"><i class="fa fa-check"></i><b>23.4.4</b> Limitations of CDA classifier</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="24" data-path="discanalysis.html"><a href="discanalysis.html"><i class="fa fa-check"></i><b>24</b> Discriminant Analysis</a><ul>
<li class="chapter" data-level="24.1" data-path="discanalysis.html"><a href="discanalysis.html#probabilistic-da"><i class="fa fa-check"></i><b>24.1</b> Probabilistic DA</a><ul>
<li class="chapter" data-level="24.1.1" data-path="discanalysis.html"><a href="discanalysis.html#normal-distributions"><i class="fa fa-check"></i><b>24.1.1</b> Normal Distributions</a></li>
<li class="chapter" data-level="24.1.2" data-path="discanalysis.html"><a href="discanalysis.html#estimating-parameters-of-normal-distributions"><i class="fa fa-check"></i><b>24.1.2</b> Estimating Parameters of Normal Distributions</a></li>
</ul></li>
<li class="chapter" data-level="24.2" data-path="discanalysis.html"><a href="discanalysis.html#discriminant-functions"><i class="fa fa-check"></i><b>24.2</b> Discriminant Functions</a></li>
<li class="chapter" data-level="24.3" data-path="discanalysis.html"><a href="discanalysis.html#quadratic-discriminant-analysis-qda"><i class="fa fa-check"></i><b>24.3</b> Quadratic Discriminant Analysis (QDA)</a></li>
<li class="chapter" data-level="24.4" data-path="discanalysis.html"><a href="discanalysis.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>24.4</b> Linear Discriminant Analysis</a><ul>
<li class="chapter" data-level="24.4.1" data-path="discanalysis.html"><a href="discanalysis.html#canonical-discriminant-analysis"><i class="fa fa-check"></i><b>24.4.1</b> Canonical Discriminant Analysis</a></li>
<li class="chapter" data-level="24.4.2" data-path="discanalysis.html"><a href="discanalysis.html#naive-bayes"><i class="fa fa-check"></i><b>24.4.2</b> Naive Bayes</a></li>
<li class="chapter" data-level="24.4.3" data-path="discanalysis.html"><a href="discanalysis.html#fifth-case"><i class="fa fa-check"></i><b>24.4.3</b> Fifth Case</a></li>
</ul></li>
<li class="chapter" data-level="24.5" data-path="discanalysis.html"><a href="discanalysis.html#comparing-the-cases"><i class="fa fa-check"></i><b>24.5</b> Comparing the Cases</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="classperformance.html"><a href="classperformance.html"><i class="fa fa-check"></i><b>25</b> Performance of Classifiers</a><ul>
<li class="chapter" data-level="25.1" data-path="classperformance.html"><a href="classperformance.html#classification-error-measures"><i class="fa fa-check"></i><b>25.1</b> Classification Error Measures</a><ul>
<li class="chapter" data-level="25.1.1" data-path="classperformance.html"><a href="classperformance.html#errors-for-binary-response"><i class="fa fa-check"></i><b>25.1.1</b> Errors for Binary Response</a></li>
<li class="chapter" data-level="25.1.2" data-path="classperformance.html"><a href="classperformance.html#error-for-categorical-response"><i class="fa fa-check"></i><b>25.1.2</b> Error for Categorical Response</a></li>
</ul></li>
<li class="chapter" data-level="25.2" data-path="classperformance.html"><a href="classperformance.html#confusion-matrices"><i class="fa fa-check"></i><b>25.2</b> Confusion Matrices</a></li>
<li class="chapter" data-level="25.3" data-path="classperformance.html"><a href="classperformance.html#binary-response-example"><i class="fa fa-check"></i><b>25.3</b> Binary Response Example</a><ul>
<li class="chapter" data-level="25.3.1" data-path="classperformance.html"><a href="classperformance.html#application-for-checking-account"><i class="fa fa-check"></i><b>25.3.1</b> Application for Checking Account</a></li>
<li class="chapter" data-level="25.3.2" data-path="classperformance.html"><a href="classperformance.html#application-for-loan"><i class="fa fa-check"></i><b>25.3.2</b> Application for Loan</a></li>
</ul></li>
<li class="chapter" data-level="25.4" data-path="classperformance.html"><a href="classperformance.html#decision-rules-and-errors"><i class="fa fa-check"></i><b>25.4</b> Decision Rules and Errors</a></li>
<li class="chapter" data-level="25.5" data-path="classperformance.html"><a href="classperformance.html#roc-curves"><i class="fa fa-check"></i><b>25.5</b> ROC Curves</a></li>
</ul></li>
<li class="part"><span><b>IX Unsupervised II: Clustering</b></span></li>
<li class="chapter" data-level="26" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>26</b> Clustering</a><ul>
<li class="chapter" data-level="26.1" data-path="clustering.html"><a href="clustering.html#about-clustering"><i class="fa fa-check"></i><b>26.1</b> About Clustering</a><ul>
<li class="chapter" data-level="26.1.1" data-path="clustering.html"><a href="clustering.html#types-of-clustering"><i class="fa fa-check"></i><b>26.1.1</b> Types of Clustering</a></li>
<li class="chapter" data-level="26.1.2" data-path="clustering.html"><a href="clustering.html#hard-clustering"><i class="fa fa-check"></i><b>26.1.2</b> Hard Clustering</a></li>
</ul></li>
<li class="chapter" data-level="26.2" data-path="clustering.html"><a href="clustering.html#dispersion-measures"><i class="fa fa-check"></i><b>26.2</b> Dispersion Measures</a></li>
<li class="chapter" data-level="26.3" data-path="clustering.html"><a href="clustering.html#complexity-in-clustering"><i class="fa fa-check"></i><b>26.3</b> Complexity in Clustering</a></li>
</ul></li>
<li class="chapter" data-level="27" data-path="kmeans.html"><a href="kmeans.html"><i class="fa fa-check"></i><b>27</b> K-Means</a><ul>
<li class="chapter" data-level="27.1" data-path="kmeans.html"><a href="kmeans.html#toy-example-2"><i class="fa fa-check"></i><b>27.1</b> Toy Example</a></li>
<li class="chapter" data-level="27.2" data-path="kmeans.html"><a href="kmeans.html#what-does-k-means-do"><i class="fa fa-check"></i><b>27.2</b> What does K-means do?</a></li>
<li class="chapter" data-level="27.3" data-path="kmeans.html"><a href="kmeans.html#k-means-algorithms"><i class="fa fa-check"></i><b>27.3</b> K-Means Algorithms</a><ul>
<li class="chapter" data-level="27.3.1" data-path="kmeans.html"><a href="kmeans.html#classic-version"><i class="fa fa-check"></i><b>27.3.1</b> Classic Version</a></li>
<li class="chapter" data-level="27.3.2" data-path="kmeans.html"><a href="kmeans.html#moving-centers-algorithm"><i class="fa fa-check"></i><b>27.3.2</b> Moving Centers Algorithm</a></li>
<li class="chapter" data-level="27.3.3" data-path="kmeans.html"><a href="kmeans.html#dynamic-clouds"><i class="fa fa-check"></i><b>27.3.3</b> Dynamic Clouds</a></li>
<li class="chapter" data-level="27.3.4" data-path="kmeans.html"><a href="kmeans.html#choosing-k"><i class="fa fa-check"></i><b>27.3.4</b> Choosing <span class="math inline">\(k\)</span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="28" data-path="hclus.html"><a href="hclus.html"><i class="fa fa-check"></i><b>28</b> Hierarchical Clustering</a><ul>
<li class="chapter" data-level="28.1" data-path="hclus.html"><a href="hclus.html#agglomerative-methods"><i class="fa fa-check"></i><b>28.1</b> Agglomerative Methods</a></li>
<li class="chapter" data-level="28.2" data-path="hclus.html"><a href="hclus.html#example-single-linkage"><i class="fa fa-check"></i><b>28.2</b> Example: Single Linkage</a><ul>
<li class="chapter" data-level="28.2.1" data-path="hclus.html"><a href="hclus.html#dendrogram"><i class="fa fa-check"></i><b>28.2.1</b> Dendrogram</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>X Tree-based Methods</b></span></li>
<li class="chapter" data-level="29" data-path="trees.html"><a href="trees.html"><i class="fa fa-check"></i><b>29</b> Intro to Decision Trees</a><ul>
<li class="chapter" data-level="29.1" data-path="trees.html"><a href="trees.html#introduction-6"><i class="fa fa-check"></i><b>29.1</b> Introduction</a></li>
<li class="chapter" data-level="29.2" data-path="trees.html"><a href="trees.html#binary-trees"><i class="fa fa-check"></i><b>29.2</b> Binary Trees</a><ul>
<li class="chapter" data-level="29.2.1" data-path="trees.html"><a href="trees.html#the-process-of-building-a-tree"><i class="fa fa-check"></i><b>29.2.1</b> The Process of Building a Tree</a></li>
<li class="chapter" data-level="29.2.2" data-path="trees.html"><a href="trees.html#binary-partitions"><i class="fa fa-check"></i><b>29.2.2</b> Binary Partitions</a></li>
</ul></li>
<li class="chapter" data-level="29.3" data-path="trees.html"><a href="trees.html#measures-of-heterogeneity"><i class="fa fa-check"></i><b>29.3</b> Measures of Heterogeneity</a><ul>
<li class="chapter" data-level="29.3.1" data-path="trees.html"><a href="trees.html#entropy"><i class="fa fa-check"></i><b>29.3.1</b> Entropy</a></li>
<li class="chapter" data-level="29.3.2" data-path="trees.html"><a href="trees.html#the-math-behind-entropy"><i class="fa fa-check"></i><b>29.3.2</b> The Math Behind Entropy</a></li>
<li class="chapter" data-level="29.3.3" data-path="trees.html"><a href="trees.html#gini-impurity"><i class="fa fa-check"></i><b>29.3.3</b> Gini Impurity</a></li>
<li class="chapter" data-level="29.3.4" data-path="trees.html"><a href="trees.html#toy-example-3"><i class="fa fa-check"></i><b>29.3.4</b> Toy Example</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Made with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">All Models Are Wrong</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ols" class="section level1">
<h1><span class="header-section-number">5</span> Linear Regression</h1>
<p>Before entering supervised learning territory, we want to discuss the general
framework of linear regression. We will introduce this topic from a pure
model-fitting point of view. In other words, we will postpone the learning
aspect (the prediction of new data) after the
<a href="learning.html#learning">chapter of theory of learning</a>.</p>
<p>The reason to cover linear regression in this way is for us to have something
to work with when we start talking about the theory of supervised learning.</p>
<div id="motivation" class="section level2">
<h2><span class="header-section-number">5.1</span> Motivation</h2>
<p>Consider, again, the NBA dataset example from previous chapters. Suppose we want
to use this data to predict the salary of NBA players, in terms of certain
variables like player’s team, player’s height, player’s weight,
player’s position, player’s years of professional experience,
player’s number of 2pts, player’s number of 3 points, number of blocks, etc.
Of course, we need information on the salaries of some current NBA player’s:</p>
<table>
<thead>
<tr class="header">
<th align="center">Player</th>
<th align="center">Height</th>
<th align="center">Weight</th>
<th align="center">Yrs Expr</th>
<th align="center">2 Points</th>
<th align="center">3 Points</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="center"><span class="math inline">\(\bigcirc\)</span></td>
<td align="center"><span class="math inline">\(\bigcirc\)</span></td>
<td align="center"><span class="math inline">\(\bigcirc\)</span></td>
<td align="center"><span class="math inline">\(\bigcirc\)</span></td>
<td align="center"><span class="math inline">\(\bigcirc\)</span></td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="center"><span class="math inline">\(\bigcirc\)</span></td>
<td align="center"><span class="math inline">\(\bigcirc\)</span></td>
<td align="center"><span class="math inline">\(\bigcirc\)</span></td>
<td align="center"><span class="math inline">\(\bigcirc\)</span></td>
<td align="center"><span class="math inline">\(\bigcirc\)</span></td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td align="center"><span class="math inline">\(\bigcirc\)</span></td>
<td align="center"><span class="math inline">\(\bigcirc\)</span></td>
<td align="center"><span class="math inline">\(\bigcirc\)</span></td>
<td align="center"><span class="math inline">\(\bigcirc\)</span></td>
<td align="center"><span class="math inline">\(\bigcirc\)</span></td>
</tr>
<tr class="even">
<td align="center">…</td>
<td align="center">…</td>
<td align="center">…</td>
<td align="center">…</td>
<td align="center">…</td>
<td align="center">…</td>
</tr>
</tbody>
</table>
<p>As usual, we use the symbol <span class="math inline">\(\mathbf{x_i}\)</span> to denote the vector of measurements
of player <span class="math inline">\(i\)</span>’s statistics (e.g. height, weight, etc.); in turn, the salary
of the <span class="math inline">\(i\)</span>-th player is represented with <span class="math inline">\(y_i\)</span>.</p>
<p>Ideally, we assume the existance of some function <span class="math inline">\(f : \mathcal{X} \to \mathcal{y}\)</span>
(i.e. a function that takes values from <span class="math inline">\(\mathcal{X}\)</span> space and maps them to a
single value <span class="math inline">\(y\)</span>). We will refer to this function the ideal “formula” for salary.
Here we are using the word <em>formula</em> in a very lose sense, and not necessarily
using the word “formula” in the mathematical sense. We now seek a hypothesized model (which we call <span class="math inline">\(\widehat{f} : \mathcal{X} \to y\)</span>), which we select from
some set of candidate functions <span class="math inline">\(h_1, h_2, \dots, h_m\)</span>.
Our task is to obtain <span class="math inline">\(\widehat{f}\)</span> in a way that we can claim that it is a
good approximation of the (unknown) function <span class="math inline">\(f\)</span>.</p>
</div>
<div id="the-ideaintuition-of-regression" class="section level2">
<h2><span class="header-section-number">5.2</span> The Idea/Intuition of Regression</h2>
<p>Let’s go back to our example with NBA players. Recall that <span class="math inline">\(y_i\)</span> denotes the
salary for the <span class="math inline">\(i\)</span>-th player. For simplicity’s sake let’s not worry about
inflation. Say we now have a new prospective player from Europe; and we are
tasked with predicting their salary denoted by <span class="math inline">\(y_0\)</span>. Let’s review a couple of
scenarios to get a high-level intuition for this task.</p>
<p><strong>Scenario 1</strong>. Suppose we have <strong>no</strong> information
on this new player. How would we compute <span class="math inline">\(y_{0}\)</span> (i.e. the salary of this new
player)? One possibility is to guesstimate <span class="math inline">\(y_0\)</span> using the historical average
salary <span class="math inline">\(\bar{y}\)</span> of NBA players. In other words, we would simply calculate:
<span class="math inline">\(\hat{y}_0 = \bar{y}\)</span>.
In this case we are using <span class="math inline">\(\bar{y}\)</span> as the <em>typical</em> score (e.g. a measure of
center) as a plausible guesstimate for <span class="math inline">\(y_0\)</span>. We could also look at the median
of the existing salaries, if we are concerned about outliers or some skewed
distribution of salaries.</p>
<p><strong>Scenario 2</strong>. Now, suppose we know that this new player will sign on to the
LA Lakers. Compared to scenario 1, we now have a new bit of information since
we know which team will hire this player. Therefore, we can use this fact to
have a more educated guess for <span class="math inline">\(y_0\)</span>. How? Instead of using the salaries of all
playes, we can focus on the salaries of Laker’s players. We could then use
<span class="math inline">\(\hat{y}_0 = \text{avg}(\text{Laker&#39;s Salaries})\)</span>:
that is, the average salary of all Laker’s players. It is reasonable that
<span class="math inline">\(\hat{y}_0\)</span> is “closer” to the average salary of Laker’s than to the overall
average salary of all NBA players.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-39"></span>
<img src="images/ols/ols-avg-salary-team.svg" alt="Average salary by team" width="70%" />
<p class="caption">
Figure 5.1: Average salary by team
</p>
</div>
<p><strong>Scenario 3</strong>. Similarly, if we know this new player’s years of experience
(e.g. 6 years), we would look at the average of salaries corresponding to
players with the same level of experience.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-40"></span>
<img src="images/ols/ols-avg-salary-experience.svg" alt="Average salary by years of experience" width="70%" />
<p class="caption">
Figure 5.2: Average salary by years of experience
</p>
</div>
<p>What do the three previous scenarios correspond to?
In all of these examples, the prediction is basically a conditional mean:
<span class="math inline">\(\hat{y}_0 = \text{ave}(y_i|x_i = x_0)\)</span>.</p>
<p>Of course, the previous strategy only makes sense when we have data points <span class="math inline">\(x_i\)</span>
that are equal to <span class="math inline">\(x_0\)</span>. But what if none of the available <span class="math inline">\(x_i\)</span> values are
equal to <span class="math inline">\(x_0\)</span>? We’ll talk about this later.</p>
<p>The previous hypothetical scenarios illustrate the core idea of regression:
we predict using quantities of the form <span class="math inline">\(\text{ave}(y_i|x_i = x_0)\)</span> which can
be formalized—under some assumptions—into the notion of conditional
expectations of the form:</p>
<p><span class="math display">\[
\mathbb{E}(y_i \mid x_{i1}^{*} , x_{i2}^{*}, \dots, x_{ip}^{*})
\]</span></p>
<p>where <span class="math inline">\(x_{ij}^{*}\)</span> represents the <span class="math inline">\(i\)</span>-th measurement of the <span class="math inline">\(j\)</span>-th variable.
The above equation is what we call the <strong>regression function</strong>; note that the
regression function is nothing more than a conditional expectation!</p>
</div>
<div id="the-linear-regression-model" class="section level2">
<h2><span class="header-section-number">5.3</span> The Linear Regression Model</h2>
<p>In a regression model we use one or more features <span class="math inline">\(X\)</span> to say something about the
reponse <span class="math inline">\(Y\)</span>. In turn, a <strong>linear regression</strong> model tells us to combine our
features in a <strong>linear</strong> way in order to approximate the response:</p>
<p><span class="math display" id="eq:301-1">\[
\hat{Y} = b_0 + b_1 X
\tag{5.1}
\]</span></p>
<p>In pointwise format, that is for a given individual <span class="math inline">\(i\)</span>, we have:</p>
<p><span class="math display" id="eq:301-2">\[
\hat{y}_i = b_0 + b_1 x_i
\tag{5.2}
\]</span></p>
<p>In vector notation:</p>
<p><span class="math display" id="eq:301-3">\[
\mathbf{\hat{y}} = b_0 + b_1 \mathbf{x}
\tag{5.3}
\]</span></p>
<p>To simplify notation, sometimes we prefer to add an auxiliary constant feature
in the form of a vector of 1’s with <span class="math inline">\(n\)</span> elements.</p>
<p><span class="math display">\[
\mathbf{X} = 
\
\begin{bmatrix} 
1 &amp; x_{1} \\
1 &amp; x_{2} \\
\vdots &amp; \vdots \\
1 &amp; x_{n} \\
\end{bmatrix},
\qquad
\mathbf{\hat{y}} = 
\begin{bmatrix} 
\hat{y}_{1} \\
\hat{y}_{2} \\
\vdots \\
\hat{y}_{n} \\
\end{bmatrix},
\qquad
\mathbf{b} = 
\begin{bmatrix} 
b_{0} \\
b_{1} \\
\end{bmatrix}
\]</span></p>
<p>In the multidimensional case when we have <span class="math inline">\(p&gt;1\)</span> predictors:</p>
<p><span class="math display">\[
\mathbf{X} = 
\
\begin{bmatrix} 
1 &amp; x_{11} &amp; \dots &amp; x_{1p} \\
1 &amp; x_{21} &amp; \dots &amp; x_{2p} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1 &amp; x_{n1} &amp; \dots &amp; x_{np} \\
\end{bmatrix},
\qquad
\mathbf{\hat{y}} = 
\begin{bmatrix} 
\hat{y}_{1} \\
\hat{y}_{2} \\
\vdots \\
\hat{y}_{n} \\
\end{bmatrix},
\qquad
\mathbf{b} = 
\begin{bmatrix} 
b_{0} \\
b_{1} \\
\vdots \\
b_{p} \\
\end{bmatrix}
\]</span></p>
<p>With the matrix of features, the response, and the coefficients we have:</p>
<p><span class="math display">\[
\mathbf{\hat{y}} = \mathbf{Xb}
\]</span></p>
<p>In path diagram form, the linear model looks like this:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-41"></span>
<img src="images/ols/ols-path-diagram-betas0.svg" alt="Linear combination with constant term" width="40%" />
<p class="caption">
Figure 5.3: Linear combination with constant term
</p>
</div>
<p>If we assume that the predictors and the response are mean-centered, then we
don’t have to worry about the constant term:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-42"></span>
<img src="images/ols/ols-path-diagram-betas1.svg" alt="Linear combination without constant term" width="40%" />
<p class="caption">
Figure 5.4: Linear combination without constant term
</p>
</div>
<p>Obviously the question becomes: how do we obtain the vector of coefficients <span class="math inline">\(\mathbf{b}\)</span>?</p>
</div>
<div id="the-error-measure" class="section level2">
<h2><span class="header-section-number">5.4</span> The Error Measure</h2>
<p>We would like to get <span class="math inline">\(\hat{y}_i\)</span> to be “as close as” possible to <span class="math inline">\(y_i\)</span>.
This requires to come up with some type of measure of <em>closeness</em>. Among the
various functions that we can use to measure how close <span class="math inline">\(\hat{y}_i\)</span> and <span class="math inline">\(y_i\)</span>
are, the most common option is to use the squared distance between such values:</p>
<p><span class="math display" id="eq:301-4">\[
d^2(y_i, \hat{y}_i) = (y_i - \hat{y}_i)^2 = (\hat{y}_i - y_i)^2 
\tag{5.4}
\]</span></p>
<p>Replacing <span class="math inline">\(\hat{y}_i\)</span> with <span class="math inline">\(\mathbf{b^\mathsf{T}\vec{x}_i}\)</span> we have:</p>
<p><span class="math display" id="eq:301-5">\[
d^2(y_i, \hat{y}_i) = (\mathbf{b^\mathsf{T}\vec{x}_i} - y_i)^2
\tag{5.5}
\]</span></p>
<p>Notice that <span class="math inline">\(d^2(y_i, \hat{y}_i)\)</span> is a pointwise error measure. But we need to
define a global measure of error. This is typically done by adding all the
pointwise error measures <span class="math inline">\(e_{i}^{2} = (\hat{y}_i - y_i)^2\)</span>.</p>
<p>There are two flavors of overall error measures based on squared pointwise
differences: (1) the sum of squared errors or <span class="math inline">\(\text{SSE}\)</span>, and (2) the mean
squared error or <span class="math inline">\(\text{MSE}\)</span>.</p>
<p>The sum of squared errors, <span class="math inline">\(\text{SSE}\)</span>, is defined as:</p>
<p><span class="math display" id="eq:301-6">\[
\text{SSE} = \sum_{i=1}^{n} \text{err}_i 
\tag{5.6}
\]</span></p>
<p>The mean squared error, <span class="math inline">\(\text{MSE}\)</span>, is defined as:</p>
<p><span class="math display" id="eq:301-7">\[
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} \text{err}_i 
\tag{5.7}
\]</span></p>
<p>As you can tell, <span class="math inline">\(\text{SSE} = n \text{MSE}\)</span>, and viceversa, <span class="math inline">\(\text{MSE} = \text{SSE} / n\)</span></p>
<p>Throughout this book, unless mentioned otherwise, when dealing with regression
problems, we will consider the <span class="math inline">\(\text{MSE}\)</span> as the default overall error
function to be minimized (you could also take <span class="math inline">\(\text{SSE}\)</span> instead). Doing some
algebra, it’s easy to see that:</p>
<p><span class="math display" id="eq:301-8">\[\begin{align}
\text{MSE} &amp;= \frac{1}{n} \sum_{i=1}^{n} e_{i}^{2} \\
&amp;= \frac{1}{n} \sum_{i=1}^{n} (\hat{y}_i - y_i)^2 \\
&amp;= \frac{1}{n} \sum_{i=1}^{n} (\mathbf{b^\mathsf{T}\vec{x}_i} - y_i)^2 \\
&amp;= \frac{1}{n} (\mathbf{Xb} - \mathbf{y})^\mathsf{T} (\mathbf{Xb} - \mathbf{y}) \\
&amp;= \frac{1}{n} \| \mathbf{Xb} - \mathbf{y} \|^2 \\
&amp;= \frac{1}{n} \| \mathbf{\hat{y}} - \mathbf{y} \|^2
\tag{5.8}
\end{align}\]</span></p>
<p>As you can tell, the Mean Squared Error <span class="math inline">\(\text{MSE}\)</span> is proportional to the
squared norm of the difference vector <span class="math inline">\(\mathbf{e} = \mathbf{\hat{y}} - \mathbf{y}\)</span>.</p>
</div>
<div id="the-least-squares-algorithm" class="section level2">
<h2><span class="header-section-number">5.5</span> The Least Squares Algorithm</h2>
<p>We want to minimize the mean of squared errors. So we compute the derivative
of <span class="math inline">\(\text{MSE}\)</span> with respect to <span class="math inline">\(\mathbf{b}\)</span>. In other words, we compute the
gradient of <span class="math inline">\(\text{MSE}\)</span>, denoted <span class="math inline">\(\nabla \text{MSE}(\mathbf{b})\)</span>, which is the
vector of partial derivatives of <span class="math inline">\(\text{MSE}\)</span> with respecto to each parameter
<span class="math inline">\(b_0, b_1, \dots, b_p\)</span>:</p>
<p><span class="math display" id="eq:301-9">\[
\nabla \text{MSE}(\mathbf{b}) = \frac{\partial }{\partial \mathbf{b}}\text{MSE}(\mathbf{b}) = \frac{\partial }{\partial \mathbf{b}} \left (\frac{1}{n} \mathbf{b^\mathsf{T}X^\mathsf{T}Xb} - \frac{2}{n} \mathbf{b^\mathsf{T}X^\mathsf{T}y} + \frac{1}{n} \mathbf{y^\mathsf{T}y} \right)
\tag{5.9}
\]</span></p>
<p>which becomes:</p>
<p><span class="math display" id="eq:301-10">\[
\nabla \text{MSE}(\mathbf{b}) = \frac{2}{n} \mathbf{X^\mathsf{T}Xb} - \frac{2}{n} \mathbf{X^\mathsf{T}y}
\tag{5.10}
\]</span></p>
<p>Equating to zero we get that:</p>
<p><span class="math display" id="eq:301-11">\[
\mathbf{X^\mathsf{T}Xb} = \mathbf{X^\mathsf{T}y} \quad (\text{normal equations})
\tag{5.11}
\]</span></p>
<p>These are the so-called <em>Normal</em> equations. It is a system of <span class="math inline">\(n\)</span> equations with
<span class="math inline">\(p+1\)</span> unknowns.</p>
<p>If the cross-product matrix <span class="math inline">\(\mathbf{X^\mathsf{T}X}\)</span> is invertible, which is not
a minor assumption, then the vector of regression coefficients <span class="math inline">\(\mathbf{b}\)</span> that
we are looking for is given by:</p>
<p><span class="math display" id="eq:301-12">\[
\mathbf{b} = (\mathbf{X^\mathsf{T}X})^{-1} \mathbf{X^\mathsf{T}y}
\tag{5.12}
\]</span></p>
<p>Having obtained <span class="math inline">\(\mathbf{b}\)</span>, we can easily compute the response vector:</p>
<p><span class="math display" id="eq:301-13">\[\begin{align*}
\mathbf{\hat{y}} &amp;= \mathbf{Xb} \\
&amp;= \mathbf{X} (\mathbf{X^\mathsf{T}X})^{-1} \mathbf{X^\mathsf{T} y}
\tag{5.13}
\end{align*}\]</span></p>
<p>If we denote <span class="math inline">\(\mathbf{H} = \mathbf{X} (\mathbf{X^\mathsf{T}X})^{-1} \mathbf{X^\mathsf{T}}\)</span>, then the predicted response is:
<span class="math inline">\(\mathbf{\hat{y}} = \mathbf{Hy}\)</span>. This matrix is better known as the
<strong>hat matrix</strong>, because it puts the hat on the response. More importantly,
the matrix <span class="math inline">\(\mathbf{H}\)</span> is an orthogonal projector.</p>
<p>From linear algebra, orthogonal projectors have very interesting properties:</p>
<ul>
<li>they are symmetric</li>
<li>they are idempotent</li>
<li>their eigenvalues are either 0 or 1</li>
</ul>
</div>
<div id="geometries-of-ols" class="section level2">
<h2><span class="header-section-number">5.6</span> Geometries of OLS</h2>
<p>Now that we’ve seen the algebra, it’s time to look at the geometric interpretation
of all the action that is going on within linear regression via OLS.
We will discuss three geometric perspectives:</p>
<ol style="list-style-type: decimal">
<li><p>OLS from the individuals point of view (i.e. rows of the data matrix).</p></li>
<li><p>OLS from the variables point of view (i.e. columns of the data matrix).</p></li>
<li><p>OLS from the parameters point of view, and the error surface.</p></li>
</ol>
<div id="rows-perspective" class="section level3">
<h3><span class="header-section-number">5.6.1</span> Rows Perspective</h3>
<p>This is probably the most popular perspective covered in most textbooks.</p>
<p>Consider a <span class="math inline">\((p + 1)\)</span>-dimensional space. For illustration purposes let’s assume that
our data has just <span class="math inline">\(p=1\)</span> predictor. In other words, we have the response <span class="math inline">\(Y\)</span> and
one predictor <span class="math inline">\(X\)</span>. We can depict individuals as points in this space:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-43"></span>
<img src="images/ols/ols-geom-obs0.svg" alt="Scatterplot of individuals" width="60%" />
<p class="caption">
Figure 5.5: Scatterplot of individuals
</p>
</div>
<p>In linear regression, we want to predict <span class="math inline">\(y_i\)</span> by linearly mixing the inputs
<span class="math inline">\(\hat{y}_{i} = b_0 + b_1 x_i\)</span>. In two dimensions, the fitted model corresponds
to a line. In three dimensions it would correspond to a plane. And in higher
dimensions this would corresponds to a hyperplane.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-44"></span>
<img src="images/ols/ols-geom-obs1.svg" alt="Scatterplot with regression line" width="65%" />
<p class="caption">
Figure 5.6: Scatterplot with regression line
</p>
</div>
<p>With a fitted line, we obtain predicted values <span class="math inline">\(\hat{y}_i\)</span>. Some predicted values
may be equal to the observed values. Other predicted values will be greater than
the observed values. And some predicted values will be smaller than
the observed values.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-45"></span>
<img src="images/ols/ols-geom-obs2.svg" alt="Observed values and predicted values" width="65%" />
<p class="caption">
Figure 5.7: Observed values and predicted values
</p>
</div>
<p>As you can imagine, given a set of data points, you can fit an infinite number
of lines (in general). So which line are we looking for? We want to obtain the
line that minimizes the square of the errors <span class="math inline">\(e_i = \hat{y}_{i} - y_{i}\)</span>.
In the figure below, these errors are represented by the vertical difference
between the observed values <span class="math inline">\(y_i\)</span> and the predicted values <span class="math inline">\(\hat{y}_i\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-46"></span>
<img src="images/ols/ols-geom-obs3.svg" alt="OLS focuses on minimizing the squared errors" width="65%" />
<p class="caption">
Figure 5.8: OLS focuses on minimizing the squared errors
</p>
</div>
<p>Combining all residuals, we want to obtain parameters <span class="math inline">\(b_0, \dots, b_p\)</span>
that minimize the squared norm of the residuals:</p>
<p><span class="math display" id="eq:301-14">\[
\sum_{i=1}^{n} e_{i}^{2} = \sum_{i=1}^{n} (\hat{y}_i - y_i)^2 = \sum_{i=1}^{n} (b_0 + b_1 x_i - y_i)^2
\tag{5.14}
\]</span></p>
<p>In vector-matrix form we have:</p>
<p><span class="math display" id="eq:301-15">\[\begin{align*}
\| \mathbf{e} \|^2 &amp;= \| \mathbf{\hat{y}} - \mathbf{y} \|^2 \\
&amp;= \| \mathbf{Xb} - \mathbf{y} \|^2 \\
&amp;= (\mathbf{Xb} - \mathbf{y})^\mathsf{T} (\mathbf{Xb} - \mathbf{y})
\tag{5.15}
\end{align*}\]</span></p>
<p>which is equivalent to minimize the mean squared error (MSE).</p>
</div>
<div id="columns-perspective" class="section level3">
<h3><span class="header-section-number">5.6.2</span> Columns Perspective</h3>
<p>This is less common than the rows perspective.</p>
<p>Imagine variables in an <span class="math inline">\(n\)</span>-dimensional space, both the response and the predictors.
In this space, the <span class="math inline">\(X\)</span> variables will span some subspace <span class="math inline">\(\mathbb{S}_{X}\)</span>.
This subspace is not supposed to contain the response—unless <span class="math inline">\(Y\)</span> happens to be
a linear combination of <span class="math inline">\(X_1, \dots, X_p\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-47"></span>
<img src="images/ols/ols-geomvar1.svg" alt="Features and Response view" width="85%" />
<p class="caption">
Figure 5.9: Features and Response view
</p>
</div>
<p>What are we looking for? We’re looking for a linear combination <span class="math inline">\(\mathbf{Xb}\)</span>.
As you can tell, there’s an infinite number of linear combinations that can be
formed with <span class="math inline">\(X_1, \dots, X_p\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-48"></span>
<img src="images/ols/ols-geomvar2.svg" alt="Linear combination of features" width="85%" />
<p class="caption">
Figure 5.10: Linear combination of features
</p>
</div>
<p>The mix of features that we are interested in, <span class="math inline">\(\mathbf{\hat{y}} = \mathbf{Xb}\)</span>,
is the one that gives us the closest approximation to <span class="math inline">\(\mathbf{y}\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-49"></span>
<img src="images/ols/ols-geomvar3.svg" alt="Linear combination to be as close as possible to response" width="85%" />
<p class="caption">
Figure 5.11: Linear combination to be as close as possible to response
</p>
</div>
<p>Now, what do we mean by <em>closest approximation</em>?
How do we determine the closeness between <span class="math inline">\(\mathbf{\hat{y}}\)</span> and <span class="math inline">\(\mathbf{y}\)</span>?
By looking at the difference, which results in a vector
<span class="math inline">\(\mathbf{e} = \mathbf{\hat{y}} - \mathbf{y}\)</span>. And then measuring the size
or <em>norm</em> of this vector. Well, the squared norm to be precise. In other words,
we want to obtain <span class="math inline">\(\mathbf{\hat{y}}\)</span> such that the squared norm <span class="math inline">\(\| \mathbf{e} \|^2\)</span>
is as small as possible.</p>
<p><span class="math display" id="eq:301-16">\[
\text{Minimize} \quad \| \mathbf{e} \|^2 = \| \mathbf{\hat{y}} - \mathbf{y} \|^2
\tag{5.16}
\]</span></p>
</div>
<div id="parameters-perspective" class="section level3">
<h3><span class="header-section-number">5.6.3</span> Parameters Perspective</h3>
<p>We could also visualize the regression problem from the point of view of the
parameters <span class="math inline">\(\mathbf{b}\)</span> and the error surface.
This is the least common perspective discussed in the literature that has
to do with linear regression in general. However, it is not that uncommon
within the Statistical Learning literature.</p>
<p>For illustration purposes, assume that we have only two predictors <span class="math inline">\(X_1\)</span> and
<span class="math inline">\(X_2\)</span>. Recall that the Mean Squared Error (MSE) is:</p>
<p><span class="math display" id="eq:301-17">\[
E(\mathbf{y}, \mathbf{\hat{y}}) = \frac{1}{n} \left( \mathbf{b^\mathsf{T} X^\mathsf{T}} \mathbf{X b} - 2 \mathbf{b^\mathsf{T} X^\mathsf{T} y} + \mathbf{y^\mathsf{T}y} \right)
\tag{5.17}
\]</span></p>
<p>Now, from the point of view of <span class="math inline">\(\mathbf{b} = (b_1, b_2)\)</span>, we can classify the
order of each term:</p>
<p><span class="math display" id="eq:301-18">\[
E(\mathbf{y}, \mathbf{\hat{y}}) = \frac{1}{n} ( \underbrace{\mathbf{b^\mathsf{T} X^\mathsf{T}} \mathbf{X b}}_{\text{Quadratic Form}} - \underbrace{ 2 \mathbf{b^\mathsf{T} X^\mathsf{T} y}}_{\text{Linear}} + \underbrace{ \mathbf{y^\mathsf{T}y}}_{\text{Constant}} )
\tag{5.18}
\]</span></p>
<p>Since <span class="math inline">\(\mathbf{X^\mathsf{T} X}\)</span> is positive semidefinite, we know that
<span class="math inline">\(\mathbf{b^\mathsf{T} X^\mathsf{T} Xb} \geq 0\)</span>. Furthermore, we know that
(from vector calculus) it will be a paraboloid (bowl-shaped surface) in the
<span class="math inline">\((E, b_1, b_2)\)</span> space. The following diagram depicts this situation.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-50"></span>
<img src="images/ols/ols-error-surface0.svg" alt="Error Surface" width="60%" />
<p class="caption">
Figure 5.12: Error Surface
</p>
</div>
<p>Imagine that we get horizontal slices of the error surface. For any of those
slices, we can project them onto the plane spanned by the parameters
<span class="math inline">\(b_1\)</span> and <span class="math inline">\(b_2\)</span>. The resulting projections will be like a topographic map,
with error contours on this plane. In general, those contours will be ellipses.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-51"></span>
<img src="images/ols/ols-error-surface1.svg" alt="Error Surface with slices, and their projections" width="60%" />
<p class="caption">
Figure 5.13: Error Surface with slices, and their projections
</p>
</div>
<p>With quadratic error surfaces like this, they have a minimum value, and we are
guaranteed the existence of
<span class="math inline">\(\mathbf{b}^* = (b_1^{*}, b_2^{*})\)</span> s.t. <span class="math inline">\(\mathbf{b^\mathsf{T} X^\mathsf{T} X b}\)</span>
is minimized. This is a powerful result!
Consider, for example, a <a href="http://mathworld.wolfram.com/ParabolicCylinder.html">parabolic cylinder</a>.
Such a shape has no unique minimum;
rather, it has an infinite number of points (all lying on a line) that minimize
the function. The point being; with positive semi-definite matrices, we
<strong>never</strong> have this latter case.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-52"></span>
<img src="images/ols/ols-error-surface2.svg" alt="Error Surface with contour errors and the minimum" width="60%" />
<p class="caption">
Figure 5.14: Error Surface with contour errors and the minimum
</p>
</div>
<p>The minimum of the error surface occurs at the point <span class="math inline">\((b_{1}^{*}, b_{2}^{*})\)</span>.
This is the precisely the OLS solution.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="pca.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="gradient.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["allmodelsarewrong.pdf", "allmodelsarewrong.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
