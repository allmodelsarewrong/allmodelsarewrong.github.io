<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>26 Intro to Decision Trees | All Models Are Wrong</title>
  <meta name="description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  <meta name="generator" content="bookdown 0.16 and GitBook 2.6.7" />

  <meta property="og:title" content="26 Intro to Decision Trees | All Models Are Wrong" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://allmodelsarewrong.github.io" />
  
  <meta property="og:description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  <meta name="github-repo" content="allmodelsarewrong/allmodelsarewrong.github.io" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="26 Intro to Decision Trees | All Models Are Wrong" />
  
  <meta name="twitter:description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  

<meta name="author" content="by Gaston Sanchez, and Ethan Marzban" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="hclus.html"/>

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><b>All Models Are Wrong (CSL)</b><br><small>by G. Sanchez & E. Marzban</small></a></li>

<li class="divider"></li>
<li class="part"><span><b>I Welcome</b></span></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="about.html"><a href="about.html"><i class="fa fa-check"></i><b>1</b> About this book</a><ul>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#prerequisites"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="part"><span><b>II Intro</b></span></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#basic-notation"><i class="fa fa-check"></i><b>2.1</b> Basic Notation</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="duality.html"><a href="duality.html"><i class="fa fa-check"></i><b>3</b> Geometric Duality</a><ul>
<li class="chapter" data-level="3.1" data-path="duality.html"><a href="duality.html#rows-space"><i class="fa fa-check"></i><b>3.1</b> Rows Space</a></li>
<li class="chapter" data-level="3.2" data-path="duality.html"><a href="duality.html#columns-space"><i class="fa fa-check"></i><b>3.2</b> Columns Space</a></li>
<li class="chapter" data-level="3.3" data-path="duality.html"><a href="duality.html#cloud-of-individuals"><i class="fa fa-check"></i><b>3.3</b> Cloud of Individuals</a><ul>
<li class="chapter" data-level="3.3.1" data-path="duality.html"><a href="duality.html#average-individual"><i class="fa fa-check"></i><b>3.3.1</b> Average Individual</a></li>
<li class="chapter" data-level="3.3.2" data-path="duality.html"><a href="duality.html#centered-data"><i class="fa fa-check"></i><b>3.3.2</b> Centered Data</a></li>
<li class="chapter" data-level="3.3.3" data-path="duality.html"><a href="duality.html#distance-between-individuals"><i class="fa fa-check"></i><b>3.3.3</b> Distance between individuals</a></li>
<li class="chapter" data-level="3.3.4" data-path="duality.html"><a href="duality.html#distance-to-the-centroid"><i class="fa fa-check"></i><b>3.3.4</b> Distance to the centroid</a></li>
<li class="chapter" data-level="3.3.5" data-path="duality.html"><a href="duality.html#measures-of-dispersion"><i class="fa fa-check"></i><b>3.3.5</b> Measures of Dispersion</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="duality.html"><a href="duality.html#cloud-of-variables"><i class="fa fa-check"></i><b>3.4</b> Cloud of Variables</a><ul>
<li class="chapter" data-level="3.4.1" data-path="duality.html"><a href="duality.html#mean-of-a-variable"><i class="fa fa-check"></i><b>3.4.1</b> Mean of a Variable</a></li>
<li class="chapter" data-level="3.4.2" data-path="duality.html"><a href="duality.html#variance-of-a-variable"><i class="fa fa-check"></i><b>3.4.2</b> Variance of a Variable</a></li>
<li class="chapter" data-level="3.4.3" data-path="duality.html"><a href="duality.html#variance-with-vector-notation"><i class="fa fa-check"></i><b>3.4.3</b> Variance with Vector Notation</a></li>
<li class="chapter" data-level="3.4.4" data-path="duality.html"><a href="duality.html#standard-deviation-as-a-norm"><i class="fa fa-check"></i><b>3.4.4</b> Standard Deviation as a Norm</a></li>
<li class="chapter" data-level="3.4.5" data-path="duality.html"><a href="duality.html#covariance"><i class="fa fa-check"></i><b>3.4.5</b> Covariance</a></li>
<li class="chapter" data-level="3.4.6" data-path="duality.html"><a href="duality.html#correlation"><i class="fa fa-check"></i><b>3.4.6</b> Correlation</a></li>
<li class="chapter" data-level="3.4.7" data-path="duality.html"><a href="duality.html#geometry-of-correlation"><i class="fa fa-check"></i><b>3.4.7</b> Geometry of Correlation</a></li>
<li class="chapter" data-level="3.4.8" data-path="duality.html"><a href="duality.html#orthogonal-projections"><i class="fa fa-check"></i><b>3.4.8</b> Orthogonal Projections</a></li>
<li class="chapter" data-level="3.4.9" data-path="duality.html"><a href="duality.html#the-mean-as-an-orthogonal-projection"><i class="fa fa-check"></i><b>3.4.9</b> The mean as an orthogonal projection</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Unsupervised I: PCA</b></span></li>
<li class="chapter" data-level="4" data-path="pca.html"><a href="pca.html"><i class="fa fa-check"></i><b>4</b> Principal Components Analysis</a><ul>
<li class="chapter" data-level="4.1" data-path="pca.html"><a href="pca.html#low-dimensional-representations"><i class="fa fa-check"></i><b>4.1</b> Low-dimensional Representations</a></li>
<li class="chapter" data-level="4.2" data-path="pca.html"><a href="pca.html#projections"><i class="fa fa-check"></i><b>4.2</b> Projections</a><ul>
<li class="chapter" data-level="4.2.1" data-path="pca.html"><a href="pca.html#vector-and-scalar-projections"><i class="fa fa-check"></i><b>4.2.1</b> Vector and Scalar Projections</a></li>
<li class="chapter" data-level="4.2.2" data-path="pca.html"><a href="pca.html#projected-inertia"><i class="fa fa-check"></i><b>4.2.2</b> Projected Inertia</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="pca.html"><a href="pca.html#maximization-problem"><i class="fa fa-check"></i><b>4.3</b> Maximization Problem</a><ul>
<li class="chapter" data-level="4.3.1" data-path="pca.html"><a href="pca.html#eigenvectors-of-mathbfs"><i class="fa fa-check"></i><b>4.3.1</b> Eigenvectors of <span class="math inline">\(\mathbf{S}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="pca.html"><a href="pca.html#another-perspective-of-pca"><i class="fa fa-check"></i><b>4.4</b> Another Perspective of PCA</a><ul>
<li class="chapter" data-level="4.4.1" data-path="pca.html"><a href="pca.html#finding-principal-components"><i class="fa fa-check"></i><b>4.4.1</b> Finding Principal Components</a></li>
<li class="chapter" data-level="4.4.2" data-path="pca.html"><a href="pca.html#finding-the-first-pc"><i class="fa fa-check"></i><b>4.4.2</b> Finding the first PC</a></li>
<li class="chapter" data-level="4.4.3" data-path="pca.html"><a href="pca.html#finding-the-second-pc"><i class="fa fa-check"></i><b>4.4.3</b> Finding the second PC</a></li>
<li class="chapter" data-level="4.4.4" data-path="pca.html"><a href="pca.html#finding-all-pcs"><i class="fa fa-check"></i><b>4.4.4</b> Finding all PCs</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="pca.html"><a href="pca.html#data-decomposition-model"><i class="fa fa-check"></i><b>4.5</b> Data Decomposition Model</a><ul>
<li class="chapter" data-level="4.5.1" data-path="pca.html"><a href="pca.html#alternative-approaches"><i class="fa fa-check"></i><b>4.5.1</b> Alternative Approaches</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Linear Regression</b></span></li>
<li class="chapter" data-level="5" data-path="ols.html"><a href="ols.html"><i class="fa fa-check"></i><b>5</b> Linear Regression</a><ul>
<li class="chapter" data-level="5.1" data-path="ols.html"><a href="ols.html#motivation"><i class="fa fa-check"></i><b>5.1</b> Motivation</a></li>
<li class="chapter" data-level="5.2" data-path="ols.html"><a href="ols.html#the-ideaintuition-of-regression"><i class="fa fa-check"></i><b>5.2</b> The Idea/Intuition of Regression</a></li>
<li class="chapter" data-level="5.3" data-path="ols.html"><a href="ols.html#the-linear-regression-model"><i class="fa fa-check"></i><b>5.3</b> The Linear Regression Model</a></li>
<li class="chapter" data-level="5.4" data-path="ols.html"><a href="ols.html#the-error-measure"><i class="fa fa-check"></i><b>5.4</b> The Error Measure</a></li>
<li class="chapter" data-level="5.5" data-path="ols.html"><a href="ols.html#the-least-squares-algorithm"><i class="fa fa-check"></i><b>5.5</b> The Least Squares Algorithm</a></li>
<li class="chapter" data-level="5.6" data-path="ols.html"><a href="ols.html#geometries-of-ols"><i class="fa fa-check"></i><b>5.6</b> Geometries of OLS</a><ul>
<li class="chapter" data-level="5.6.1" data-path="ols.html"><a href="ols.html#rows-perspective"><i class="fa fa-check"></i><b>5.6.1</b> Rows Perspective</a></li>
<li class="chapter" data-level="5.6.2" data-path="ols.html"><a href="ols.html#columns-perspective"><i class="fa fa-check"></i><b>5.6.2</b> Columns Perspective</a></li>
<li class="chapter" data-level="5.6.3" data-path="ols.html"><a href="ols.html#parameters-perspective"><i class="fa fa-check"></i><b>5.6.3</b> Parameters Perspective</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="gradient.html"><a href="gradient.html"><i class="fa fa-check"></i><b>6</b> Gradient Descent</a><ul>
<li class="chapter" data-level="6.1" data-path="gradient.html"><a href="gradient.html#error-surface"><i class="fa fa-check"></i><b>6.1</b> Error Surface</a></li>
<li class="chapter" data-level="6.2" data-path="gradient.html"><a href="gradient.html#idea-of-gradient-descent"><i class="fa fa-check"></i><b>6.2</b> Idea of Gradient Descent</a></li>
<li class="chapter" data-level="6.3" data-path="gradient.html"><a href="gradient.html#moving-down-an-error-surface"><i class="fa fa-check"></i><b>6.3</b> Moving Down an Error Surface</a><ul>
<li class="chapter" data-level="6.3.1" data-path="gradient.html"><a href="gradient.html#the-direction-of-mathbfv"><i class="fa fa-check"></i><b>6.3.1</b> The direction of <span class="math inline">\(\mathbf{v}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="gradient.html"><a href="gradient.html#gradient-descent-and-our-model"><i class="fa fa-check"></i><b>6.4</b> Gradient Descent and our Model</a></li>
</ul></li>
<li class="part"><span><b>V Learning Concepts</b></span></li>
<li class="chapter" data-level="7" data-path="learning.html"><a href="learning.html"><i class="fa fa-check"></i><b>7</b> Theoretical Framework</a><ul>
<li class="chapter" data-level="7.1" data-path="learning.html"><a href="learning.html#mental-map"><i class="fa fa-check"></i><b>7.1</b> Mental Map</a></li>
<li class="chapter" data-level="7.2" data-path="learning.html"><a href="learning.html#kinds-of-predictions"><i class="fa fa-check"></i><b>7.2</b> Kinds of Predictions</a><ul>
<li class="chapter" data-level="7.2.1" data-path="learning.html"><a href="learning.html#two-types-of-data"><i class="fa fa-check"></i><b>7.2.1</b> Two Types of Data</a></li>
<li class="chapter" data-level="7.2.2" data-path="learning.html"><a href="learning.html#two-types-of-predictions"><i class="fa fa-check"></i><b>7.2.2</b> Two Types of Predictions</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="learning.html"><a href="learning.html#two-types-of-errors"><i class="fa fa-check"></i><b>7.3</b> Two Types of Errors</a><ul>
<li class="chapter" data-level="7.3.1" data-path="learning.html"><a href="learning.html#individual-errors"><i class="fa fa-check"></i><b>7.3.1</b> Individual Errors</a></li>
<li class="chapter" data-level="7.3.2" data-path="learning.html"><a href="learning.html#overall-errors"><i class="fa fa-check"></i><b>7.3.2</b> Overall Errors</a></li>
<li class="chapter" data-level="7.3.3" data-path="learning.html"><a href="learning.html#auxiliary-technicality"><i class="fa fa-check"></i><b>7.3.3</b> Auxiliary Technicality</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="learning.html"><a href="learning.html#noisy-targets"><i class="fa fa-check"></i><b>7.4</b> Noisy Targets</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="mse.html"><a href="mse.html"><i class="fa fa-check"></i><b>8</b> MSE of Estimator</a><ul>
<li class="chapter" data-level="8.1" data-path="mse.html"><a href="mse.html#mse-of-an-estimator"><i class="fa fa-check"></i><b>8.1</b> MSE of an Estimator</a><ul>
<li class="chapter" data-level="8.1.1" data-path="mse.html"><a href="mse.html#prototypical-cases-of-bias-and-variance"><i class="fa fa-check"></i><b>8.1.1</b> Prototypical Cases of Bias and Variance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="biasvar.html"><a href="biasvar.html"><i class="fa fa-check"></i><b>9</b> Bias-Variance Tradeoff</a><ul>
<li class="chapter" data-level="9.1" data-path="biasvar.html"><a href="biasvar.html#introduction"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="biasvar.html"><a href="biasvar.html#motivation-example"><i class="fa fa-check"></i><b>9.2</b> Motivation Example</a><ul>
<li class="chapter" data-level="9.2.1" data-path="biasvar.html"><a href="biasvar.html#two-hypotheses"><i class="fa fa-check"></i><b>9.2.1</b> Two Hypotheses</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="biasvar.html"><a href="biasvar.html#learning-from-two-points"><i class="fa fa-check"></i><b>9.3</b> Learning from two points</a></li>
<li class="chapter" data-level="9.4" data-path="biasvar.html"><a href="biasvar.html#bias-variance-derivation"><i class="fa fa-check"></i><b>9.4</b> Bias-Variance Derivation</a><ul>
<li class="chapter" data-level="9.4.1" data-path="biasvar.html"><a href="biasvar.html#out-of-sample-predictions"><i class="fa fa-check"></i><b>9.4.1</b> Out-of-Sample Predictions</a></li>
<li class="chapter" data-level="9.4.2" data-path="biasvar.html"><a href="biasvar.html#noisy-target"><i class="fa fa-check"></i><b>9.4.2</b> Noisy Target</a></li>
<li class="chapter" data-level="9.4.3" data-path="biasvar.html"><a href="biasvar.html#types-of-theoretical-mses"><i class="fa fa-check"></i><b>9.4.3</b> Types of Theoretical MSEs</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="biasvar.html"><a href="biasvar.html#the-tradeoff"><i class="fa fa-check"></i><b>9.5</b> The Tradeoff</a><ul>
<li class="chapter" data-level="9.5.1" data-path="biasvar.html"><a href="biasvar.html#bias-variance-tradeoff-picture"><i class="fa fa-check"></i><b>9.5.1</b> Bias-Variance Tradeoff Picture</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="phases.html"><a href="phases.html"><i class="fa fa-check"></i><b>10</b> Learning Phases</a><ul>
<li class="chapter" data-level="10.1" data-path="phases.html"><a href="phases.html#introduction-1"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="phases.html"><a href="phases.html#model-assessment"><i class="fa fa-check"></i><b>10.2</b> Model Assessment</a><ul>
<li class="chapter" data-level="10.2.1" data-path="phases.html"><a href="phases.html#holdout-test-set"><i class="fa fa-check"></i><b>10.2.1</b> Holdout Test Set</a></li>
<li class="chapter" data-level="10.2.2" data-path="phases.html"><a href="phases.html#why-does-a-test-set-work"><i class="fa fa-check"></i><b>10.2.2</b> Why does a test set work?</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="phases.html"><a href="phases.html#model-selection"><i class="fa fa-check"></i><b>10.3</b> Model Selection</a><ul>
<li class="chapter" data-level="10.3.1" data-path="phases.html"><a href="phases.html#three-way-holdout-method"><i class="fa fa-check"></i><b>10.3.1</b> Three-way Holdout Method</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="phases.html"><a href="phases.html#model-training"><i class="fa fa-check"></i><b>10.4</b> Model Training</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="resampling.html"><a href="resampling.html"><i class="fa fa-check"></i><b>11</b> Resample Approaches</a><ul>
<li class="chapter" data-level="11.1" data-path="resampling.html"><a href="resampling.html#general-sampling-blueprint"><i class="fa fa-check"></i><b>11.1</b> General Sampling Blueprint</a></li>
<li class="chapter" data-level="11.2" data-path="resampling.html"><a href="resampling.html#monte-carlo-cross-validation"><i class="fa fa-check"></i><b>11.2</b> Monte Carlo Cross Validation</a></li>
<li class="chapter" data-level="11.3" data-path="resampling.html"><a href="resampling.html#bootstrap-method"><i class="fa fa-check"></i><b>11.3</b> Bootstrap Method</a></li>
<li class="chapter" data-level="11.4" data-path="resampling.html"><a href="resampling.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>11.4</b> <span class="math inline">\(K\)</span>-Fold Cross-Validation</a><ul>
<li class="chapter" data-level="11.4.1" data-path="resampling.html"><a href="resampling.html#leave-one-out-cross-validation-loocv"><i class="fa fa-check"></i><b>11.4.1</b> Leave-One-Out Cross Validation (LOOCV)</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>VI Regularization</b></span></li>
<li class="chapter" data-level="12" data-path="regular.html"><a href="regular.html"><i class="fa fa-check"></i><b>12</b> Regularization Techniques</a><ul>
<li class="chapter" data-level="12.1" data-path="regular.html"><a href="regular.html#multicollinearity-issues"><i class="fa fa-check"></i><b>12.1</b> Multicollinearity Issues</a><ul>
<li class="chapter" data-level="12.1.1" data-path="regular.html"><a href="regular.html#toy-example"><i class="fa fa-check"></i><b>12.1.1</b> Toy Example</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="regular.html"><a href="regular.html#irregular-coefficients"><i class="fa fa-check"></i><b>12.2</b> Irregular Coefficients</a></li>
<li class="chapter" data-level="12.3" data-path="regular.html"><a href="regular.html#connection-to-regularization"><i class="fa fa-check"></i><b>12.3</b> Connection to Regularization</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="pcr.html"><a href="pcr.html"><i class="fa fa-check"></i><b>13</b> Principal Components Regression</a><ul>
<li class="chapter" data-level="13.1" data-path="pcr.html"><a href="pcr.html#motivation-example-1"><i class="fa fa-check"></i><b>13.1</b> Motivation Example</a></li>
<li class="chapter" data-level="13.2" data-path="pcr.html"><a href="pcr.html#the-pcr-model"><i class="fa fa-check"></i><b>13.2</b> The PCR Model</a></li>
<li class="chapter" data-level="13.3" data-path="pcr.html"><a href="pcr.html#how-does-pcr-work"><i class="fa fa-check"></i><b>13.3</b> How does PCR work?</a><ul>
<li class="chapter" data-level="13.3.1" data-path="pcr.html"><a href="pcr.html#transition-formula"><i class="fa fa-check"></i><b>13.3.1</b> Transition Formula</a></li>
<li class="chapter" data-level="13.3.2" data-path="pcr.html"><a href="pcr.html#size-of-coefficients"><i class="fa fa-check"></i><b>13.3.2</b> Size of Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="pcr.html"><a href="pcr.html#selecting-number-of-pcs"><i class="fa fa-check"></i><b>13.4</b> Selecting Number of PCs</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="pls.html"><a href="pls.html"><i class="fa fa-check"></i><b>14</b> Partial Least Squares Regression</a><ul>
<li class="chapter" data-level="14.1" data-path="pls.html"><a href="pls.html#motivation-example-2"><i class="fa fa-check"></i><b>14.1</b> Motivation Example</a></li>
<li class="chapter" data-level="14.2" data-path="pls.html"><a href="pls.html#the-plsr-model"><i class="fa fa-check"></i><b>14.2</b> The PLSR Model</a></li>
<li class="chapter" data-level="14.3" data-path="pls.html"><a href="pls.html#how-does-plsr-work"><i class="fa fa-check"></i><b>14.3</b> How does PLSR work?</a></li>
<li class="chapter" data-level="14.4" data-path="pls.html"><a href="pls.html#plsr-algorithm"><i class="fa fa-check"></i><b>14.4</b> PLSR Algorithm</a><ul>
<li class="chapter" data-level="14.4.1" data-path="pls.html"><a href="pls.html#pls-solution-with-original-variables"><i class="fa fa-check"></i><b>14.4.1</b> PLS Solution with original variables</a></li>
<li class="chapter" data-level="14.4.2" data-path="pls.html"><a href="pls.html#size-of-coefficients-1"><i class="fa fa-check"></i><b>14.4.2</b> Size of Coefficients</a></li>
<li class="chapter" data-level="14.4.3" data-path="pls.html"><a href="pls.html#some-properties"><i class="fa fa-check"></i><b>14.4.3</b> Some Properties</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="pls.html"><a href="pls.html#selecting-number-of-pls-components"><i class="fa fa-check"></i><b>14.5</b> Selecting Number of PLS Components</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="ridge.html"><a href="ridge.html"><i class="fa fa-check"></i><b>15</b> Ridge Regression</a><ul>
<li class="chapter" data-level="15.1" data-path="ridge.html"><a href="ridge.html#a-new-minimization-problem"><i class="fa fa-check"></i><b>15.1</b> A New Minimization Problem</a><ul>
<li class="chapter" data-level="15.1.1" data-path="ridge.html"><a href="ridge.html#constraining-regression-coefficients"><i class="fa fa-check"></i><b>15.1.1</b> Constraining Regression Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="ridge.html"><a href="ridge.html#a-new-minimization-solution"><i class="fa fa-check"></i><b>15.2</b> A New Minimization Solution</a></li>
<li class="chapter" data-level="15.3" data-path="ridge.html"><a href="ridge.html#what-does-rr-accomplish"><i class="fa fa-check"></i><b>15.3</b> What does RR accomplish?</a><ul>
<li class="chapter" data-level="15.3.1" data-path="ridge.html"><a href="ridge.html#how-to-find-lambda"><i class="fa fa-check"></i><b>15.3.1</b> How to find <span class="math inline">\(\lambda\)</span>?</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>VII Extending Linear Regression</b></span></li>
<li class="chapter" data-level="16" data-path="linear-extensions.html"><a href="linear-extensions.html"><i class="fa fa-check"></i><b>16</b> Beyond Linear Regression</a><ul>
<li class="chapter" data-level="16.1" data-path="linear-extensions.html"><a href="linear-extensions.html#introduction-2"><i class="fa fa-check"></i><b>16.1</b> Introduction</a><ul>
<li class="chapter" data-level="16.1.1" data-path="linear-extensions.html"><a href="linear-extensions.html#expanding-the-regression-horizon"><i class="fa fa-check"></i><b>16.1.1</b> Expanding the Regression Horizon</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="linear-extensions.html"><a href="linear-extensions.html#transforming-features"><i class="fa fa-check"></i><b>16.2</b> Transforming Features</a></li>
</ul></li>
<li class="part"><span><b>VIII Classification</b></span></li>
<li class="chapter" data-level="17" data-path="classif.html"><a href="classif.html"><i class="fa fa-check"></i><b>17</b> Classification Methods</a></li>
<li class="chapter" data-level="18" data-path="logistic.html"><a href="logistic.html"><i class="fa fa-check"></i><b>18</b> Logistic Regression</a><ul>
<li class="chapter" data-level="18.1" data-path="logistic.html"><a href="logistic.html#motivation-1"><i class="fa fa-check"></i><b>18.1</b> Motivation</a><ul>
<li class="chapter" data-level="18.1.1" data-path="logistic.html"><a href="logistic.html#first-approach-fitting-a-line"><i class="fa fa-check"></i><b>18.1.1</b> First Approach: Fitting a Line</a></li>
<li class="chapter" data-level="18.1.2" data-path="logistic.html"><a href="logistic.html#secodn-approach-harsh-thresholding"><i class="fa fa-check"></i><b>18.1.2</b> Secodn Approach: Harsh Thresholding</a></li>
<li class="chapter" data-level="18.1.3" data-path="logistic.html"><a href="logistic.html#third-approach-conditional-means"><i class="fa fa-check"></i><b>18.1.3</b> Third Approach: Conditional Means</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="logistic.html"><a href="logistic.html#logistic-regression-model"><i class="fa fa-check"></i><b>18.2</b> Logistic Regression Model</a><ul>
<li class="chapter" data-level="18.2.1" data-path="logistic.html"><a href="logistic.html#the-criterion-being-optimized"><i class="fa fa-check"></i><b>18.2.1</b> The Criterion Being Optimized</a></li>
<li class="chapter" data-level="18.2.2" data-path="logistic.html"><a href="logistic.html#another-way-to-solve-logistic-regression"><i class="fa fa-check"></i><b>18.2.2</b> Another Way to Solve Logistic Regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="discrim.html"><a href="discrim.html"><i class="fa fa-check"></i><b>19</b> Preamble for Discriminant Analysis</a><ul>
<li class="chapter" data-level="19.1" data-path="discrim.html"><a href="discrim.html#motivation-2"><i class="fa fa-check"></i><b>19.1</b> Motivation</a><ul>
<li class="chapter" data-level="19.1.1" data-path="discrim.html"><a href="discrim.html#distinguishing-species"><i class="fa fa-check"></i><b>19.1.1</b> Distinguishing Species</a></li>
</ul></li>
<li class="chapter" data-level="19.2" data-path="discrim.html"><a href="discrim.html#derived-ratios-from-sum-of-squares"><i class="fa fa-check"></i><b>19.2</b> Derived Ratios from Sum-of-Squares</a><ul>
<li class="chapter" data-level="19.2.1" data-path="discrim.html"><a href="discrim.html#correlation-ratio"><i class="fa fa-check"></i><b>19.2.1</b> Correlation Ratio</a></li>
<li class="chapter" data-level="19.2.2" data-path="discrim.html"><a href="discrim.html#f-ratio"><i class="fa fa-check"></i><b>19.2.2</b> F-Ratio</a></li>
<li class="chapter" data-level="19.2.3" data-path="discrim.html"><a href="discrim.html#example-with-iris-data"><i class="fa fa-check"></i><b>19.2.3</b> Example with Iris data</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="discrim.html"><a href="discrim.html#geometric-perspective"><i class="fa fa-check"></i><b>19.3</b> Geometric Perspective</a><ul>
<li class="chapter" data-level="19.3.1" data-path="discrim.html"><a href="discrim.html#clouds-from-class-structure"><i class="fa fa-check"></i><b>19.3.1</b> Clouds from Class Structure</a></li>
<li class="chapter" data-level="19.3.2" data-path="discrim.html"><a href="discrim.html#dispersion-decomposition"><i class="fa fa-check"></i><b>19.3.2</b> Dispersion Decomposition</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="20" data-path="cda.html"><a href="cda.html"><i class="fa fa-check"></i><b>20</b> Canonical Discriminant Analysis</a><ul>
<li class="chapter" data-level="20.0.1" data-path="cda.html"><a href="cda.html#looking-for-a-discriminant-axis"><i class="fa fa-check"></i><b>20.0.1</b> Looking for a discriminant axis</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="discanalysis.html"><a href="discanalysis.html"><i class="fa fa-check"></i><b>21</b> Discriminant Analysis</a><ul>
<li class="chapter" data-level="21.1" data-path="discanalysis.html"><a href="discanalysis.html#probabilistic-da"><i class="fa fa-check"></i><b>21.1</b> Probabilistic DA</a></li>
<li class="chapter" data-level="21.2" data-path="discanalysis.html"><a href="discanalysis.html#estimations"><i class="fa fa-check"></i><b>21.2</b> Estimations</a><ul>
<li class="chapter" data-level="21.2.1" data-path="discanalysis.html"><a href="discanalysis.html#univariate-data-boldsymbolp-1"><i class="fa fa-check"></i><b>21.2.1</b> Univariate Data (<span class="math inline">\(\boldsymbol{p = 1}\)</span>)</a></li>
<li class="chapter" data-level="21.2.2" data-path="discanalysis.html"><a href="discanalysis.html#multivariate-data-boldsymbolp-1"><i class="fa fa-check"></i><b>21.2.2</b> Multivariate Data (<span class="math inline">\(\boldsymbol{p &gt; 1}\)</span>)</a></li>
<li class="chapter" data-level="21.2.3" data-path="discanalysis.html"><a href="discanalysis.html#multivariate-estimations"><i class="fa fa-check"></i><b>21.2.3</b> Multivariate Estimations</a></li>
</ul></li>
<li class="chapter" data-level="21.3" data-path="discanalysis.html"><a href="discanalysis.html#discriminant-functions"><i class="fa fa-check"></i><b>21.3</b> Discriminant Functions</a></li>
<li class="chapter" data-level="21.4" data-path="discanalysis.html"><a href="discanalysis.html#quadratic-discriminant-analysis-qda"><i class="fa fa-check"></i><b>21.4</b> Quadratic Discriminant Analysis (QDA)</a></li>
<li class="chapter" data-level="21.5" data-path="discanalysis.html"><a href="discanalysis.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>21.5</b> Linear Discriminant Analysis</a></li>
<li class="chapter" data-level="21.6" data-path="discanalysis.html"><a href="discanalysis.html#canonical-discriminant-analysis"><i class="fa fa-check"></i><b>21.6</b> Canonical Discriminant Analysis</a></li>
<li class="chapter" data-level="21.7" data-path="discanalysis.html"><a href="discanalysis.html#naive-bayes"><i class="fa fa-check"></i><b>21.7</b> Naive Bayes</a></li>
<li class="chapter" data-level="21.8" data-path="discanalysis.html"><a href="discanalysis.html#fifth-case"><i class="fa fa-check"></i><b>21.8</b> Fifth Case</a></li>
<li class="chapter" data-level="21.9" data-path="discanalysis.html"><a href="discanalysis.html#comparing-the-cases"><i class="fa fa-check"></i><b>21.9</b> Comparing the Cases</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="classperformance.html"><a href="classperformance.html"><i class="fa fa-check"></i><b>22</b> Performance of Classifiers</a><ul>
<li class="chapter" data-level="22.1" data-path="classperformance.html"><a href="classperformance.html#error-for-binary-response"><i class="fa fa-check"></i><b>22.1</b> Error for Binary Response</a></li>
<li class="chapter" data-level="22.2" data-path="classperformance.html"><a href="classperformance.html#categorical-response"><i class="fa fa-check"></i><b>22.2</b> Categorical Response</a></li>
<li class="chapter" data-level="22.3" data-path="classperformance.html"><a href="classperformance.html#confusion-matrices"><i class="fa fa-check"></i><b>22.3</b> Confusion Matrices</a></li>
<li class="chapter" data-level="22.4" data-path="classperformance.html"><a href="classperformance.html#binary-response-example"><i class="fa fa-check"></i><b>22.4</b> Binary Response: Example</a><ul>
<li class="chapter" data-level="22.4.1" data-path="classperformance.html"><a href="classperformance.html#types-of-errors-application-for-savings-account"><i class="fa fa-check"></i><b>22.4.1</b> Types of Errors: Application for Savings Account</a></li>
<li class="chapter" data-level="22.4.2" data-path="classperformance.html"><a href="classperformance.html#another-example-loans"><i class="fa fa-check"></i><b>22.4.2</b> Another Example: Loans</a></li>
<li class="chapter" data-level="22.4.3" data-path="classperformance.html"><a href="classperformance.html#the-ideal-situation"><i class="fa fa-check"></i><b>22.4.3</b> The Ideal Situation</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IX Unsupervised II: Clustering</b></span></li>
<li class="chapter" data-level="23" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>23</b> Clustering</a><ul>
<li class="chapter" data-level="23.1" data-path="clustering.html"><a href="clustering.html#about-clustering"><i class="fa fa-check"></i><b>23.1</b> About Clustering</a><ul>
<li class="chapter" data-level="23.1.1" data-path="clustering.html"><a href="clustering.html#types-of-clustering"><i class="fa fa-check"></i><b>23.1.1</b> Types of Clustering</a></li>
<li class="chapter" data-level="23.1.2" data-path="clustering.html"><a href="clustering.html#hard-clustering"><i class="fa fa-check"></i><b>23.1.2</b> Hard Clustering</a></li>
</ul></li>
<li class="chapter" data-level="23.2" data-path="clustering.html"><a href="clustering.html#dispersion-measures"><i class="fa fa-check"></i><b>23.2</b> Dispersion Measures</a></li>
<li class="chapter" data-level="23.3" data-path="clustering.html"><a href="clustering.html#complexity-in-clustering"><i class="fa fa-check"></i><b>23.3</b> Complexity in Clustering</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="kmeans.html"><a href="kmeans.html"><i class="fa fa-check"></i><b>24</b> K-Means</a><ul>
<li class="chapter" data-level="24.1" data-path="kmeans.html"><a href="kmeans.html#toy-example-1"><i class="fa fa-check"></i><b>24.1</b> Toy Example</a></li>
<li class="chapter" data-level="24.2" data-path="kmeans.html"><a href="kmeans.html#what-does-k-means-do"><i class="fa fa-check"></i><b>24.2</b> What does K-means do?</a></li>
<li class="chapter" data-level="24.3" data-path="kmeans.html"><a href="kmeans.html#k-means-algorithms"><i class="fa fa-check"></i><b>24.3</b> K-Means Algorithms</a><ul>
<li class="chapter" data-level="24.3.1" data-path="kmeans.html"><a href="kmeans.html#classic-version"><i class="fa fa-check"></i><b>24.3.1</b> Classic Version</a></li>
<li class="chapter" data-level="24.3.2" data-path="kmeans.html"><a href="kmeans.html#moving-centers-algorithm"><i class="fa fa-check"></i><b>24.3.2</b> Moving Centers Algorithm</a></li>
<li class="chapter" data-level="24.3.3" data-path="kmeans.html"><a href="kmeans.html#dynamic-clouds"><i class="fa fa-check"></i><b>24.3.3</b> Dynamic Clouds</a></li>
<li class="chapter" data-level="24.3.4" data-path="kmeans.html"><a href="kmeans.html#choosing-k"><i class="fa fa-check"></i><b>24.3.4</b> Choosing <span class="math inline">\(k\)</span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="25" data-path="hclus.html"><a href="hclus.html"><i class="fa fa-check"></i><b>25</b> Hierarchical Clustering</a><ul>
<li class="chapter" data-level="25.1" data-path="hclus.html"><a href="hclus.html#agglomerative-methods"><i class="fa fa-check"></i><b>25.1</b> Agglomerative Methods</a></li>
<li class="chapter" data-level="25.2" data-path="hclus.html"><a href="hclus.html#example-single-linkage"><i class="fa fa-check"></i><b>25.2</b> Example: Single Linkage</a><ul>
<li class="chapter" data-level="25.2.1" data-path="hclus.html"><a href="hclus.html#dendrogram"><i class="fa fa-check"></i><b>25.2.1</b> Dendrogram</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>X Tree-based Methods</b></span></li>
<li class="chapter" data-level="26" data-path="trees.html"><a href="trees.html"><i class="fa fa-check"></i><b>26</b> Intro to Decision Trees</a><ul>
<li class="chapter" data-level="26.1" data-path="trees.html"><a href="trees.html#introduction-3"><i class="fa fa-check"></i><b>26.1</b> Introduction</a></li>
<li class="chapter" data-level="26.2" data-path="trees.html"><a href="trees.html#binary-trees"><i class="fa fa-check"></i><b>26.2</b> Binary Trees</a><ul>
<li class="chapter" data-level="26.2.1" data-path="trees.html"><a href="trees.html#the-process-of-building-a-tree"><i class="fa fa-check"></i><b>26.2.1</b> The Process of Building a Tree</a></li>
<li class="chapter" data-level="26.2.2" data-path="trees.html"><a href="trees.html#binary-partitions"><i class="fa fa-check"></i><b>26.2.2</b> Binary Partitions</a></li>
</ul></li>
<li class="chapter" data-level="26.3" data-path="trees.html"><a href="trees.html#measures-of-heterogeneity"><i class="fa fa-check"></i><b>26.3</b> Measures of Heterogeneity</a><ul>
<li class="chapter" data-level="26.3.1" data-path="trees.html"><a href="trees.html#entropy"><i class="fa fa-check"></i><b>26.3.1</b> Entropy</a></li>
<li class="chapter" data-level="26.3.2" data-path="trees.html"><a href="trees.html#the-math-behind-entropy"><i class="fa fa-check"></i><b>26.3.2</b> The Math Behind Entropy</a></li>
<li class="chapter" data-level="26.3.3" data-path="trees.html"><a href="trees.html#gini-impurity"><i class="fa fa-check"></i><b>26.3.3</b> Gini Impurity</a></li>
<li class="chapter" data-level="26.3.4" data-path="trees.html"><a href="trees.html#toy-example-2"><i class="fa fa-check"></i><b>26.3.4</b> Toy Example</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Made with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">All Models Are Wrong</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="trees" class="section level1">
<h1><span class="header-section-number">26</span> Intro to Decision Trees</h1>
<p>In this chapter we introduce decision trees, which are one the most popular and
intuitive supervised learning tools.</p>
<div id="introduction-3" class="section level2">
<h2><span class="header-section-number">26.1</span> Introduction</h2>
<p>Trees are used in classification and regression tasks by detecting a set of
rules allowing the analyst to assign the individuals in data into a class
or assign it a predicted value.</p>
<p>The reason they are called trees is because the set of rules can organized
into a hierarchical <em>tree</em>. Moreover, this structure can be visually
displayed as an inverted tree diagram (although the tree is generally displayed
upside down) like in the image below.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-258"></span>
<img src="images/trees/tree-diagram1.svg" alt="Diagram of a tree structure" width="85%" />
<p class="caption">
Figure 26.1: Diagram of a tree structure
</p>
</div>
<p>Decision trees have three types of nodes: root, internal, and leaf nodes.</p>
<ul>
<li><p>There is exactly one root node that contains the entire set of elements in the sample; the root node has no incoming branches, and it may have zero, two, or more outgoing branches.</p></li>
<li><p>Internal nodes, each of which is hoped to contain as many elements as possible belonging to one class; each internal node has one incoming branch and two (or more) outgoing branches.</p></li>
<li><p>Leaf or terminal nodes which, unlike internal nodes, have no outgoing branches.</p></li>
</ul>
<p>The root node can be regarded as the starting node. The branches are the lines
connecting the nodes. Every node that is preceded by a higher level node is
called a “child” node. A node that gives origin to two (or more) child nodes is
called a parent node. Terminal nodes are nodes without children. The most
common way of visually representing a tree is by means of the classical
node-link diagram that connects nodes together with line segments.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-259"></span>
<img src="images/trees/tree-diagram2.svg" alt="Common elements of a tree diagram" width="85%" />
<p class="caption">
Figure 26.2: Common elements of a tree diagram
</p>
</div>
</div>
<div id="binary-trees" class="section level2">
<h2><span class="header-section-number">26.2</span> Binary Trees</h2>
<p>One special kind of decision trees are binary trees. They are called <em>binary</em>
because the nodes are only partitioned in two (see diagram below).</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-260"></span>
<img src="images/trees/tree-binary-diagram1.svg" alt="Example of a binary tree diagram" width="85%" />
<p class="caption">
Figure 26.3: Example of a binary tree diagram
</p>
</div>
<p>The usual process to build an ordinary binary decision tree is as follows.
First, a search among all the predictor variables is done in order to find the
single variable which best splits the data in two groups. In other words, one
must find the segmentation variable that splits the data in two subsets so that
they are as different as possible with respect to the response variable. This
process is applied separately to each subgroup, and so on recursively until the
subgroups either reach a minimum size or until no improvement can be made.
In each dichotomy the two parts are the most contrasting according to the
response variable.</p>
<div id="the-process-of-building-a-tree" class="section level3">
<h3><span class="header-section-number">26.2.1</span> The Process of Building a Tree</h3>
<p>Let us begin with the general idea behind trees. The main idea is to build a
tree structure by successively (i.e. recursively) dividing the set of objects
with the help of features <span class="math inline">\(X_1, \dots, X_p\)</span>, in order to obtain the terminal
nodes (i.e. final segments) that are as homogeneous as possible (with respect
to the response).</p>
<p>The tree building process requires three main components</p>
<ul>
<li>Establish the set of admissible divisions for each node: compute all possible
binary splits</li>
<li>Define a criterion to find the “best” node split: some optimization criterion
to be optimized</li>
<li>some measure of heterogeneity (within a given node)</li>
<li>Define a rule to declare a given node as internal node or leaf node</li>
</ul>
<p>Establishing the set of admissible splits for each node is related with the
nature of the segmentation variables in the data. The number and type of
possible binary splits will vary depending on the scale (e.g., binary, nominal,
ordinal, and continuous) of the predictor variables. Further discussion is
given in the next section.</p>
<p>The second aspect to build a segmentation tree is the definition of a node
splitting rule. This rule is necessary to find at each internal node a test for
splitting the data into subgroups. Since finding a split involves finding the
predictor variable that is the most discriminating, the splitting test helps to
rank variables according to their discriminating power.</p>
<p>The last component of the tree building process has to do with the definition
of a node termination rule. In essence, there are two different ways to deal
with this issue: 1) pre-pruning and 2) post-pruning. Pre-pruning implies that
the decision of when to stop the growth of a tree is made prospectively.
Post-pruning refers to reducing the size of a fully expanded tree by pruning
some of its branches retrospectively.</p>
</div>
<div id="binary-partitions" class="section level3">
<h3><span class="header-section-number">26.2.2</span> Binary Partitions</h3>
<p>In binary segmentation trees, the set of admissible splits for each node
depends on the nature of the segmentation variables in the data. Predictor
variables can be of different type. In practice, most variables are of
categorical nature although sometimes continuous variables may be present.
We can divide them in four classes:</p>
<ol style="list-style-type: decimal">
<li>Binary variables</li>
<li>Ordinal variables</li>
<li>Nominal variables</li>
<li>Continuous variables</li>
</ol>
<p>The number of possible binary splits for each type of segmentation variable is
described below.</p>
<div id="binary-variables" class="section level4 unnumbered">
<h4>Binary variables</h4>
<p>Because binary variables have two values, they only generate one binary
partition. For example, the variable <em>Gender</em> with two values, female and male,
can only de divided in one possible way.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-261"></span>
<img src="images/trees/splits_binary.svg" alt="Example of a binary partition for a binary variable" width="80%" />
<p class="caption">
Figure 26.4: Example of a binary partition for a binary variable
</p>
</div>
</div>
<div id="nominal-variables" class="section level4 unnumbered">
<h4>Nominal Variables</h4>
<p>A nominal variable may have many categories and the number of binary splits
depends on the number of distinct categories for the corresponding variable.
The total number of possible binary partitions for a nominal variable with <span class="math inline">\(q\)</span>
categories is <span class="math inline">\(2^{q-1} - 1\)</span>. For example, suppose a variable <em>Color</em> with three
categories: Green, White and Red. The number of binary splits is
<span class="math inline">\(2^{3-1} - 1 = 3\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-262"></span>
<img src="images/trees/splits_nominal.svg" alt="Example of binary splits for a nominal variable" width="80%" />
<p class="caption">
Figure 26.5: Example of binary splits for a nominal variable
</p>
</div>
</div>
<div id="oridnal-variables" class="section level4 unnumbered">
<h4>Oridnal Variables</h4>
<p>Binary splits for ordinal variables must respect the order property of the
categories, that is, the grouping of categories must preserve the order among
the values. The total number of binary partitions for an ordinal variable with
<span class="math inline">\(q\)</span> categories is <span class="math inline">\(q-1\)</span>. For example, the <em>Apparel size</em> may have four
categories: small (S), medium (M), large (L) and extra-large (XL). In this
case, the number of binary splits will be <span class="math inline">\(4-1 = 3\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-263"></span>
<img src="images/trees/splits_ordinal.svg" alt="Example of binary splits for an ordinal variable" width="80%" />
<p class="caption">
Figure 26.6: Example of binary splits for an ordinal variable
</p>
</div>
</div>
<div id="continuous-variables" class="section level4 unnumbered">
<h4>Continuous variables</h4>
<p>The treatment for continuous variables depends on the number of different
values the variable takes. Suppose a continuous variable with <span class="math inline">\(q\)</span> different
values. If we consider that <span class="math inline">\(q\)</span> is “adequate” in some sense, we can treat the
continuous variable like an ordinal variable. If we consider <span class="math inline">\(q\)</span> to be large we
may group its values and reduced their number in <span class="math inline">\(q^*\)</span> (<span class="math inline">\(q^*&lt;q\)</span>). In both cases,
the variable is treated as an ordinal variable and the number of binary splits
will be <span class="math inline">\(q-1\)</span> or <span class="math inline">\(q^*-1\)</span>. For example, imagine the continuous variable <em>Age</em>
(measured in years) with five values 5, 7, 8, 9 and 10. The number of binary
splits is <span class="math inline">\(5 - 1 = 4\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-264"></span>
<img src="images/trees/splits_quantitative.svg" alt="Example of binary splits for a continuous variable" width="80%" />
<p class="caption">
Figure 26.7: Example of binary splits for a continuous variable
</p>
</div>
</div>
</div>
</div>
<div id="measures-of-heterogeneity" class="section level2">
<h2><span class="header-section-number">26.3</span> Measures of Heterogeneity</h2>
<p>For illustrative purposes, let us focus on a single node with <span class="math inline">\(n = 14\)</span> points
and <span class="math inline">\(2\)</span> classes (with <span class="math inline">\(7\)</span> objects of each class). Suppose this node splits in
the following manner:</p>
<p><span class="math display">\[
\textbf{[Tree; left node with 5 circles 2 triangles, right with opposite]}
\]</span></p>
<p>We have an intuitive feeling that the parent node has more heterogeneity than
its child nodes. In fact, this is a general result: a split cannot result in
child nodes with more heterogeneity than the parent node. Stated differently,
you can never do worse than the parent node.</p>
<p>Most measures of heterogeneity (and therefore homogeneity) relate to
proportions. Let’s spitball some ideas as to how to do this:</p>
<ul>
<li><p><span class="math inline">\(\mathrm{prop}(\bigcirc) - \mathrm{prop}(\Delta)\)</span>; you would need to assign
some sort of interpretation to the value <span class="math inline">\(0\)</span> in this case.</p></li>
<li><p>Entropy (infromation, as viewed from a computer science standpoint). Could
also refer to this as a measure of “disorder” within a node.</p></li>
<li><p>Impurity (how mixed, diverse)</p></li>
</ul>
<div id="entropy" class="section level3">
<h3><span class="header-section-number">26.3.1</span> Entropy</h3>
<p>In the parent node, there is an equal probability of being a <span class="math inline">\(\bigcirc\)</span> as
there is to be a <span class="math inline">\(\Delta\)</span>. However, in the left node, there is a higher chance
of being a <span class="math inline">\(\bigcirc\)</span> than <span class="math inline">\(\Delta\)</span> (the reverse is true of the right node).</p>
<p>Let’s start with an (abstract) toy example: say we have a set of 12 objects,
all of the same class (say, <span class="math inline">\(\bigcirc\)</span>).</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-265"></span>
<img src="images/trees/entropy_levels1.svg" alt="Set of 12 objects of same type" width="35%" />
<p class="caption">
Figure 26.8: Set of 12 objects of same type
</p>
</div>
<p>If we randomly select an object from this box, we have zero uncertainty about
its class: in other words, there is <strong>zero entropy</strong> associated with this
situation.</p>
<p>Now, suppose three of those <span class="math inline">\(\bigcirc\)</span>’s were replaced with <span class="math inline">\(\square\)</span>’s.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-266"></span>
<img src="images/trees/entropy_levels2.svg" alt="Set with two different types of objects" width="35%" />
<p class="caption">
Figure 26.9: Set with two different types of objects
</p>
</div>
<p>If we were to draw an object from this box, there would be <em>some</em> amount of uncertainty as to its class; that is, there is <strong>some entropy</strong>.</p>
<p>Finally, in the most extreme case, say we have 12 objects of different classes:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-267"></span>
<img src="images/trees/entropy_levels3.svg" alt="Set with 12 different types of objects" width="35%" />
<p class="caption">
Figure 26.10: Set with 12 different types of objects
</p>
</div>
<p>We are quite uncertain as to the classes of objects drawn from this box;
that is, we have <strong>maximum entropy</strong>.</p>
<p>In Short: the intuition behind entropy relates to how “mixed” a box is, with
respect to the classes contained inside. If a box is “pure” (i.e. has only one
class), it has zero entropy where as if a box is “highly impure” (i.e. has many
different classes), it has some nonzero amount of entropy.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-268"></span>
<img src="images/trees/entropy_levels4.svg" alt="Set with 12 different types of objects" width="90%" />
<p class="caption">
Figure 26.11: Set with 12 different types of objects
</p>
</div>
<p>Now, let’s see how we can link our idea of entropy with the idea of
<em>probability</em>. From the examples above, we can see the following relationship:</p>
<p><span class="math display">\[
\begin{array}{lcl} \text{large probability} &amp; \Rightarrow &amp; \text{small entropy value} \\ 
\text{small probability} &amp; \Rightarrow &amp; \text{large entropy value} \\ 
\end{array}
\]</span></p>
<p>Hence, it makes sense to try and model entropy as a function of <span class="math inline">\(P\)</span>, the probability of selecting a certain class. From our discussion above, we see
that our entropy function should be small when <span class="math inline">\(P\)</span> is around 1, and large when
<span class="math inline">\(P\)</span> is around <span class="math inline">\(0\)</span>.</p>
<p>There are quite a few functions that display this behavior. The one we will use
is the logarithm, of any base <span class="math inline">\(b\)</span>. Note that</p>
<p><span class="math display">\[
\log_b(P) : [0, 1] \to (-\infty, 0)
\]</span></p>
<p>That is, since our probability values are restricted to lie in the unit interval,
the entropy will lie somewhere on the negative real axis.</p>
</div>
<div id="the-math-behind-entropy" class="section level3">
<h3><span class="header-section-number">26.3.2</span> The Math Behind Entropy</h3>
<p>Letting <span class="math inline">\(H(\text{node})\)</span> denote the entropy of a particular node, we use the
following definition:</p>
<p><span class="math display">\[
H(\text{node}) = - \sum_{k=1}^{h} p_k \log_2(p_k)
\]</span></p>
<p>where <span class="math inline">\(p_k\)</span> denotes the probability of randomly selecting an object that comes
from class <span class="math inline">\(k\)</span>. Note that convention dictates that we use a log of base 2. The
reason rationale behind picking <span class="math inline">\(2\)</span> comes from information theory, as base-2
logs can be interpreted in terms of bits.</p>
<p>Let’s see if this definition of entropy is consistent with our intuition. For
example, let us consider a <strong>pure</strong> node; that is, a node in which all
objects share the same class. In this case <span class="math inline">\(p_k = 1\)</span>, and we have only one
class so</p>
<p><span class="math display">\[
H(\text{node}) = -(1) \cdot \log_2(1) = 0 \cdot 0 = 0
\]</span></p>
<p>Now, for our second example above, we have <span class="math inline">\(h = 2\)</span> classes with <span class="math inline">\(p_1 = 9/12\)</span>
and <span class="math inline">\(p_2 = 3/12\)</span>, so that</p>
<p><span class="math display">\[
H(\text{node}) = - \frac{9}{12} \log_2\left( \frac{9}{12} \right)  \frac{3}{12} \log_2\left( \frac{3}{12} \right) \approx 0.8112
\]</span></p>
<p>In a node where there is a 50-50 split, we have</p>
<p><span class="math display">\[
H(\text{node}) = - \frac{1}{2} \log_2\left( \frac{1}{2} \right) - \frac{1}{2} \log_2\left( \frac{1}{2} \right) = 1
\]</span></p>
<p>The following figure depicts several examples of sets and their entropy values.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-269"></span>
<img src="images/trees/entropy_levels5.svg" alt="Different sets with their entropy values" width="90%" />
<p class="caption">
Figure 26.12: Different sets with their entropy values
</p>
</div>
</div>
<div id="gini-impurity" class="section level3">
<h3><span class="header-section-number">26.3.3</span> Gini Impurity</h3>
<p>Let us return to our box example from the previous section. Let’s suppose I
select a ball, and note what class it belongs to. I then ask you (without
giving you any information about the box, other than the fact that there are
<span class="math inline">\(\bigcirc\)</span>’s and <span class="math inline">\(\square\)</span>’s), and ask you to guess what my selected object
belongs to. There are 4 possible outcomes to this situation:</p>
<ul>
<li>I select <span class="math inline">\(\bigcirc\)</span>, you guess <span class="math inline">\(\bigcirc\)</span>.</li>
<li>I select <span class="math inline">\(\bigcirc\)</span>, you guess <span class="math inline">\(\square\)</span>.</li>
<li>I select <span class="math inline">\(\square\)</span>, you guess <span class="math inline">\(\bigcirc\)</span>.</li>
<li>I select <span class="math inline">\(\square\)</span>, you guess <span class="math inline">\(\square\)</span>.</li>
</ul>
<p>The Gini index pays attention to the <strong>misclassifications</strong>; that is, the
situations in which your guess was wrong. For instance, in our example above,
there are a total of 2 possible misclassifications.</p>
<p>Now, suppose we know that we are in the second box (where there are 9
<span class="math inline">\(\bigcirc\)</span>’s and 3 <span class="math inline">\(\square\)</span>’s). The Gini Index would be computed as follows:</p>
<p><span class="math display">\[\begin{align*}
\text{Gini Index} &amp; = \underbrace{ Prob(\bigcirc) }_{\text{me}} \underbrace{ Prob(\square) }_{\text{you}} + \underbrace{ Prob(\square) }_{\text{me}} \underbrace{ Prob(\bigcirc) }_{\text{you}} \\
&amp; = \left( \frac{9}{12} \right) \left( \frac{3}{12} \right)  +  \left( \frac{3}{12} \right) \left( \frac{9}{12} \right)  = \frac{1}{8} = 0.375
\end{align*}\]</span></p>
<p>The general definition for the Gini Index is as follows:</p>
<p><span class="math display">\[
\sum_{k=1}^{h} Pr(k) \left[ 1- Pr(k) \right] = 1 - \sum_{k=1}^{h} Pr^2(k) = \sum_{k \neq \ell} Pr(k) Pr(\ell)
\]</span></p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-270"></span>
<img src="images/trees/gini_impurities.svg" alt="Different sets with their gini impurities" width="90%" />
<p class="caption">
Figure 26.13: Different sets with their gini impurities
</p>
</div>
</div>
<div id="toy-example-2" class="section level3">
<h3><span class="header-section-number">26.3.4</span> Toy Example</h3>
<p>Let us return to our credit data example.</p>
<table>
<thead>
<tr class="header">
<th></th>
<th align="left">gender</th>
<th align="left">job</th>
<th align="left">region</th>
<th align="left">status</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>a</td>
<td align="left">male</td>
<td align="left">yes</td>
<td align="left">A</td>
<td align="left">good</td>
</tr>
<tr class="even">
<td>b</td>
<td align="left">female</td>
<td align="left">yes</td>
<td align="left">B</td>
<td align="left">good</td>
</tr>
<tr class="odd">
<td>c</td>
<td align="left">male</td>
<td align="left">no</td>
<td align="left">C</td>
<td align="left">good</td>
</tr>
<tr class="even">
<td>d</td>
<td align="left">female</td>
<td align="left">yes</td>
<td align="left">B</td>
<td align="left">good</td>
</tr>
<tr class="odd">
<td>e</td>
<td align="left">male</td>
<td align="left">no</td>
<td align="left">A</td>
<td align="left">bad</td>
</tr>
<tr class="even">
<td>f</td>
<td align="left">female</td>
<td align="left">no</td>
<td align="left">B</td>
<td align="left">bad</td>
</tr>
<tr class="odd">
<td>g</td>
<td align="left">male</td>
<td align="left">yes</td>
<td align="left">C</td>
<td align="left">bad</td>
</tr>
<tr class="even">
<td>h</td>
<td align="left">female</td>
<td align="left">no</td>
<td align="left">C</td>
<td align="left">bad</td>
</tr>
</tbody>
</table>
<p>Let us encode <span class="math inline">\(\text{good}\)</span> as a circle <span class="math inline">\(\bigcirc\)</span>, and let us encode
<span class="math inline">\(\text{bad}\)</span> as a square <span class="math inline">\(\square\)</span>. Our root node then contains 8 objects,
with a 50-50 split in credit scores:</p>
<p><strong>INSERT TREE IMAGE</strong></p>
<p>The entropy of the root node would therefore be</p>
<p><span class="math display">\[
H(t) = - \underbrace{ \frac{1}{2} \log_2\left( \frac{1}{2} \right) }_{\text{good}} - \underbrace{ \frac{1}{2} \log_2\left( \frac{1}{2} \right)  }_{\text{bad}} = 1
\]</span></p>
<div id="partition-by-gender" class="section level4 unnumbered">
<h4>Partition by Gender</h4>
<p>Now, say we choose to partition by gender, Our parent node would then split as
follows:</p>
<p><span class="math display">\[
\textbf{[Node Splits]}
\]</span></p>
<p>Each of these child nodes have the same entropy:</p>
<p><span class="math display">\[
H(\text{left}) =  -\frac{1}{2} \log_2\left( \frac{1}{2} \right)  -\frac{1}{2} \log_2\left( \frac{1}{2} \right)  = 1 = H(\text{right})
\]</span></p>
<p>Now, we want to compare the combined entropies of these two child nodes and the
entropy of the root node. That is, we compare</p>
<p><span class="math display">\[
H(t) \ \text{|vs|} \ \left( \frac{4}{8} \right) H(\text{left}) + \left( \frac{4}{8} \right) H(\text{right})
\]</span></p>
<p>Hence, let us define the <strong>information gain</strong> by the quantity</p>
<p><span class="math display">\[\begin{align*}
\Delta I &amp; = H(t) - \left[ \left( \frac{4}{8} \right) H(\text{left}) + \left( \frac{4}{8} \right) H(\text{right})  \right] \\
&amp; = 1 - \left[ \frac{4}{8} + \frac{4}{8} \right] = 1 - 1 = 0 
\end{align*}\]</span></p>
<p>In this way, we see that gender is not a good predictor to base our partitions
on!</p>
<p><em>As an Exercise to the Reader:</em> Try computing information gain when we use
<span class="math inline">\(\text{job}\)</span> as the splitting variable. The procedure will be very similar to
the computations above.</p>

</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="hclus.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["allmodelsarewrong.pdf", "allmodelsarewrong.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
