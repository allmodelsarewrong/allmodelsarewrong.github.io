<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>8 MSE of Estimator | All Models Are Wrong</title>
  <meta name="description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  <meta name="generator" content="bookdown 0.14 and GitBook 2.6.7" />

  <meta property="og:title" content="8 MSE of Estimator | All Models Are Wrong" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://allmodelsarewrong.github.io" />
  
  <meta property="og:description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  <meta name="github-repo" content="allmodelsarewrong/allmodelsarewrong.github.io" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="8 MSE of Estimator | All Models Are Wrong" />
  
  <meta name="twitter:description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  

<meta name="author" content="by Gaston Sanchez, and Ethan Marzban" />


<meta name="date" content="2019-11-04" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="learning.html"/>

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><b>All Models Are Wrong (CSL)</b><br>by G. Sanchez & E. Marzban</a></li>

<li class="divider"></li>
<li class="part"><span><b>I Welcome</b></span></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="about.html"><a href="about.html"><i class="fa fa-check"></i><b>1</b> About this book</a><ul>
<li class="chapter" data-level="1.1" data-path="about.html"><a href="about.html#prerequisites"><i class="fa fa-check"></i><b>1.1</b> Prerequisites</a></li>
<li class="chapter" data-level="1.2" data-path="about.html"><a href="about.html#acknowledgements"><i class="fa fa-check"></i><b>1.2</b> Acknowledgements</a></li>
</ul></li>
<li class="part"><span><b>II Intro</b></span></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#basic-notation"><i class="fa fa-check"></i><b>2.1</b> Basic Notation</a></li>
<li class="chapter" data-level="2.2" data-path="intro.html"><a href="intro.html#about-statistical-learning"><i class="fa fa-check"></i><b>2.2</b> About Statistical Learning</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="duality.html"><a href="duality.html"><i class="fa fa-check"></i><b>3</b> Geometric Duality</a><ul>
<li class="chapter" data-level="3.1" data-path="duality.html"><a href="duality.html#rows-space"><i class="fa fa-check"></i><b>3.1</b> Rows Space</a></li>
<li class="chapter" data-level="3.2" data-path="duality.html"><a href="duality.html#columns-space"><i class="fa fa-check"></i><b>3.2</b> Columns Space</a></li>
<li class="chapter" data-level="3.3" data-path="duality.html"><a href="duality.html#cloud-of-individuals"><i class="fa fa-check"></i><b>3.3</b> Cloud of Individuals</a><ul>
<li class="chapter" data-level="3.3.1" data-path="duality.html"><a href="duality.html#average-individual"><i class="fa fa-check"></i><b>3.3.1</b> Average Individual</a></li>
<li class="chapter" data-level="3.3.2" data-path="duality.html"><a href="duality.html#centered-data"><i class="fa fa-check"></i><b>3.3.2</b> Centered Data</a></li>
<li class="chapter" data-level="3.3.3" data-path="duality.html"><a href="duality.html#distance-between-individuals"><i class="fa fa-check"></i><b>3.3.3</b> Distance between individuals</a></li>
<li class="chapter" data-level="3.3.4" data-path="duality.html"><a href="duality.html#distance-to-the-centroid"><i class="fa fa-check"></i><b>3.3.4</b> Distance to the centroid</a></li>
<li class="chapter" data-level="3.3.5" data-path="duality.html"><a href="duality.html#measures-of-dispersion"><i class="fa fa-check"></i><b>3.3.5</b> Measures of Dispersion</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="duality.html"><a href="duality.html#cloud-of-variables"><i class="fa fa-check"></i><b>3.4</b> Cloud of Variables</a><ul>
<li class="chapter" data-level="3.4.1" data-path="duality.html"><a href="duality.html#mean-of-a-variable"><i class="fa fa-check"></i><b>3.4.1</b> Mean of a Variable</a></li>
<li class="chapter" data-level="3.4.2" data-path="duality.html"><a href="duality.html#variance-of-a-variable"><i class="fa fa-check"></i><b>3.4.2</b> Variance of a Variable</a></li>
<li class="chapter" data-level="3.4.3" data-path="duality.html"><a href="duality.html#variance-with-vector-notation"><i class="fa fa-check"></i><b>3.4.3</b> Variance with Vector Notation</a></li>
<li class="chapter" data-level="3.4.4" data-path="duality.html"><a href="duality.html#standard-deviation-as-a-norm"><i class="fa fa-check"></i><b>3.4.4</b> Standard Deviation as a Norm</a></li>
<li class="chapter" data-level="3.4.5" data-path="duality.html"><a href="duality.html#covariance"><i class="fa fa-check"></i><b>3.4.5</b> Covariance</a></li>
<li class="chapter" data-level="3.4.6" data-path="duality.html"><a href="duality.html#correlation"><i class="fa fa-check"></i><b>3.4.6</b> Correlation</a></li>
<li class="chapter" data-level="3.4.7" data-path="duality.html"><a href="duality.html#geometry-of-correlation"><i class="fa fa-check"></i><b>3.4.7</b> Geometry of Correlation</a></li>
<li class="chapter" data-level="3.4.8" data-path="duality.html"><a href="duality.html#orthogonal-projections"><i class="fa fa-check"></i><b>3.4.8</b> Orthogonal Projections</a></li>
<li class="chapter" data-level="3.4.9" data-path="duality.html"><a href="duality.html#the-mean-as-an-orthogonal-projection"><i class="fa fa-check"></i><b>3.4.9</b> The mean as an orthogonal projection</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Unsupervised I: PCA</b></span></li>
<li class="chapter" data-level="4" data-path="pca.html"><a href="pca.html"><i class="fa fa-check"></i><b>4</b> Principal Components Analysis</a><ul>
<li class="chapter" data-level="4.1" data-path="pca.html"><a href="pca.html#low-dimensional-representations"><i class="fa fa-check"></i><b>4.1</b> Low-dimensional Representations</a></li>
<li class="chapter" data-level="4.2" data-path="pca.html"><a href="pca.html#pca-idea"><i class="fa fa-check"></i><b>4.2</b> PCA Idea</a><ul>
<li class="chapter" data-level="4.2.1" data-path="pca.html"><a href="pca.html#finding-principal-components"><i class="fa fa-check"></i><b>4.2.1</b> Finding Principal Components</a></li>
<li class="chapter" data-level="4.2.2" data-path="pca.html"><a href="pca.html#finding-the-first-pc"><i class="fa fa-check"></i><b>4.2.2</b> Finding the first PC</a></li>
<li class="chapter" data-level="4.2.3" data-path="pca.html"><a href="pca.html#finding-the-second-pc"><i class="fa fa-check"></i><b>4.2.3</b> Finding the second PC</a></li>
<li class="chapter" data-level="4.2.4" data-path="pca.html"><a href="pca.html#finding-all-pcs"><i class="fa fa-check"></i><b>4.2.4</b> Finding all PCs</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="pca.html"><a href="pca.html#pca-model"><i class="fa fa-check"></i><b>4.3</b> PCA Model</a></li>
<li class="chapter" data-level="4.4" data-path="pca.html"><a href="pca.html#another-perspective"><i class="fa fa-check"></i><b>4.4</b> Another Perspective</a></li>
</ul></li>
<li class="part"><span><b>IV Linear Regression</b></span></li>
<li class="chapter" data-level="5" data-path="ols.html"><a href="ols.html"><i class="fa fa-check"></i><b>5</b> Linear Regression</a><ul>
<li class="chapter" data-level="5.1" data-path="ols.html"><a href="ols.html#motivation"><i class="fa fa-check"></i><b>5.1</b> Motivation</a></li>
<li class="chapter" data-level="5.2" data-path="ols.html"><a href="ols.html#the-model"><i class="fa fa-check"></i><b>5.2</b> The Model</a></li>
<li class="chapter" data-level="5.3" data-path="ols.html"><a href="ols.html#the-error-measure"><i class="fa fa-check"></i><b>5.3</b> The Error Measure</a></li>
<li class="chapter" data-level="5.4" data-path="ols.html"><a href="ols.html#the-algorithm"><i class="fa fa-check"></i><b>5.4</b> The Algorithm</a></li>
<li class="chapter" data-level="5.5" data-path="ols.html"><a href="ols.html#geometries-of-ols"><i class="fa fa-check"></i><b>5.5</b> Geometries of OLS</a><ul>
<li class="chapter" data-level="5.5.1" data-path="ols.html"><a href="ols.html#rows-perspective"><i class="fa fa-check"></i><b>5.5.1</b> Rows Perspective</a></li>
<li class="chapter" data-level="5.5.2" data-path="ols.html"><a href="ols.html#columns-perspective"><i class="fa fa-check"></i><b>5.5.2</b> Columns Perspective</a></li>
<li class="chapter" data-level="5.5.3" data-path="ols.html"><a href="ols.html#parameters-perspective"><i class="fa fa-check"></i><b>5.5.3</b> Parameters Perspective</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="gradient.html"><a href="gradient.html"><i class="fa fa-check"></i><b>6</b> Gradient Descent</a><ul>
<li class="chapter" data-level="6.1" data-path="gradient.html"><a href="gradient.html#error-surface"><i class="fa fa-check"></i><b>6.1</b> Error Surface</a></li>
<li class="chapter" data-level="6.2" data-path="gradient.html"><a href="gradient.html#idea-of-gradient-descent"><i class="fa fa-check"></i><b>6.2</b> Idea of Gradient Descent</a><ul>
<li class="chapter" data-level="6.2.1" data-path="gradient.html"><a href="gradient.html#the-direction-of-mathbfv"><i class="fa fa-check"></i><b>6.2.1</b> The direction of <span class="math inline">\(\mathbf{v}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="gradient.html"><a href="gradient.html#gradient-descent-and-our-model"><i class="fa fa-check"></i><b>6.3</b> Gradient Descent and our Model</a></li>
</ul></li>
<li class="part"><span><b>V Learning Concepts</b></span></li>
<li class="chapter" data-level="7" data-path="learning.html"><a href="learning.html"><i class="fa fa-check"></i><b>7</b> Theoretical Framework</a><ul>
<li class="chapter" data-level="7.1" data-path="learning.html"><a href="learning.html#mental-map"><i class="fa fa-check"></i><b>7.1</b> Mental Map</a></li>
<li class="chapter" data-level="7.2" data-path="learning.html"><a href="learning.html#two-types-of-predictions"><i class="fa fa-check"></i><b>7.2</b> Two Types of Predictions</a></li>
<li class="chapter" data-level="7.3" data-path="learning.html"><a href="learning.html#two-types-of-data"><i class="fa fa-check"></i><b>7.3</b> Two Types of Data</a></li>
<li class="chapter" data-level="7.4" data-path="learning.html"><a href="learning.html#types-of-errors"><i class="fa fa-check"></i><b>7.4</b> Types of Errors</a><ul>
<li class="chapter" data-level="7.4.1" data-path="learning.html"><a href="learning.html#overall-errors"><i class="fa fa-check"></i><b>7.4.1</b> Overall Errors</a></li>
<li class="chapter" data-level="7.4.2" data-path="learning.html"><a href="learning.html#individual-errors"><i class="fa fa-check"></i><b>7.4.2</b> Individual Errors</a></li>
<li class="chapter" data-level="7.4.3" data-path="learning.html"><a href="learning.html#auxiliary-technicality"><i class="fa fa-check"></i><b>7.4.3</b> Auxiliary Technicality</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="learning.html"><a href="learning.html#noisy-targets"><i class="fa fa-check"></i><b>7.5</b> Noisy Targets</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="mse.html"><a href="mse.html"><i class="fa fa-check"></i><b>8</b> MSE of Estimator</a><ul>
<li class="chapter" data-level="8.1" data-path="mse.html"><a href="mse.html#mse-of-an-estimator"><i class="fa fa-check"></i><b>8.1</b> MSE of an Estimator</a><ul>
<li class="chapter" data-level="8.1.1" data-path="mse.html"><a href="mse.html#prototyipcal-cases-of-bias-and-variance"><i class="fa fa-check"></i><b>8.1.1</b> Prototyipcal Cases of Bias and Variance</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Made with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">All Models Are Wrong</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="mse" class="section level1">
<h1><span class="header-section-number">8</span> MSE of Estimator</h1>
<p>In this chapter we provide a preliminary review of the Mean Squared Error
(MSE) of an estimator. This will allow us to have a more gentle introduction
to the next chapter about the famous Bias-Variance tradeoff.</p>
<div id="mse-of-an-estimator" class="section level2">
<h2><span class="header-section-number">8.1</span> MSE of an Estimator</h2>
<p>In order to discuss the bias-variance decomposition of a regression function and
its expected MSE, we would like to first review the concept of the mean squared
error of an estimator. Recall that <em>estimation</em> consists of
providing an approximate value to the parameter of a population, using a (
random) sample of observations drawn from such population.</p>
<p>Say we have a population of <span class="math inline">\(n\)</span> objects and we are interested in describing them
with some numeric characteristic <span class="math inline">\(\theta\)</span>. For example, our population may be
formed by all students in some college, and we want to know their average
height. We call this (theoretical) average the <em>parameter</em>.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-51"></span>
<img src="images/theory/population-parameter.png" alt="Population described by some parameter of interest." width="55%" />
<p class="caption">
Figure 8.1: Population described by some parameter of interest.
</p>
</div>
<p>To estimate the
value of the parameter, we may draw a random sample of <span class="math inline">\(m &lt; n\)</span> students from the
population and compute a statistic <span class="math inline">\(\hat{\theta}\)</span>. Ideally, we would use some
statistic <span class="math inline">\(\hat{\theta}\)</span> that approximates well the parameter <span class="math inline">\(\theta\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-52"></span>
<img src="images/theory/random-sample.png" alt="Random sample from a population" width="70%" />
<p class="caption">
Figure 8.2: Random sample from a population
</p>
</div>
<p>In practice, this is the typical process that you would carry out:</p>
<ol style="list-style-type: decimal">
<li>Get a random sample from a population.</li>
<li>Use the limited amount of data in the sample to estimate <span class="math inline">\(\theta\)</span> using some
formula to compute <span class="math inline">\(\hat{\theta}\)</span>.</li>
<li>Make a statement about how reliable of an estimator <span class="math inline">\(\hat{\theta}\)</span> is.</li>
</ol>
<p>Now, for illustration purposes, let’s do the following mental experiment.
Pretend that you can draw multiple random samples, all of the same size <span class="math inline">\(m\)</span>,
from the population.
In fact, you should pretend that you can get an infinite number of samples.
And suppose that for each sample you will compute a statistic <span class="math inline">\(\hat{\theta}\)</span>.
A first random sample of size <span class="math inline">\(m\)</span> would result in <span class="math inline">\(\hat{\theta}_1\)</span>.
A second random sample of size <span class="math inline">\(m\)</span> would result in <span class="math inline">\(\hat{\theta}_2\)</span>. And so on.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-53"></span>
<img src="images/theory/sampling-estimators.png" alt="Various random samples of equal size and their statistics" width="70%" />
<p class="caption">
Figure 8.3: Various random samples of equal size and their statistics
</p>
</div>
<p>A couple of important things to notice:</p>
<ul>
<li>An estimator is a random variable"
<ul>
<li>A first sample will result in <span class="math inline">\(\hat{\theta}_1\)</span></li>
<li>A second sample will result in <span class="math inline">\(\hat{\theta}_2\)</span></li>
<li>A third sample will result in <span class="math inline">\(\hat{\theta}_3\)</span></li>
<li>and so on …</li>
</ul></li>
<li>Some samples will yield a <span class="math inline">\(\hat{\theta}_k\)</span> that overestimates <span class="math inline">\(\theta\)</span></li>
<li>Other samples will yield a <span class="math inline">\(\hat{\theta}_k\)</span> that underestimates <span class="math inline">\(\theta\)</span></li>
<li>Some samples will yield a <span class="math inline">\(\hat{\theta}_k\)</span> matching <span class="math inline">\(\theta\)</span></li>
</ul>
<p>In theory, we could get a very large number of samples, and visualize the
distribution of <span class="math inline">\(\hat{\theta}\)</span>, like in the figure below:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-54"></span>
<img src="images/theory/theta-hat-distribution1.png" alt="Distribution of an estimator" width="60%" />
<p class="caption">
Figure 8.4: Distribution of an estimator
</p>
</div>
<p>As you would expect, some estimators will be close to the parameter <span class="math inline">\(\theta\)</span>,
while others not so much.</p>
<p>Under general assumptions, we can also
assume that the estimator has expected value <span class="math inline">\(\mathbb{E}(\hat{\theta})\)</span>, with
finite variance <span class="math inline">\(var(\hat{\theta})\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-55"></span>
<img src="images/theory/theta-hat-distribution2.png" alt="Distribution of an estimator" width="60%" />
<p class="caption">
Figure 8.5: Distribution of an estimator
</p>
</div>
<p>An interesting question to consider is:</p>
<p>In general, how much different—or similar—is <span class="math inline">\(\hat{\theta}\)</span> from <span class="math inline">\(\theta\)</span>?</p>
<p>To be more concrete: on average, how close
we expect the estimator to be from the parameter? To answer this question
we can look for a measure to assess the typical distance of estimators from
the parameter.</p>
<p>This involves looking at the difference: <span class="math inline">\(\hat{\theta} - \theta\)</span>,
which is commonly referred to as the <strong>estimation error</strong>:</p>
<p><span class="math display">\[
\text{estimation error} = \hat{\theta} - \theta
\]</span></p>
<p>We would like to measure the “size” of such difference. Notice that the estimation error is also a random variable:</p>
<ul>
<li>A first sample will result in an error <span class="math inline">\(\hat{\theta}_1 - \theta\)</span></li>
<li>A second sample will result in an error <span class="math inline">\(\hat{\theta}_2 - \theta\)</span></li>
<li>A third sample will result in an error <span class="math inline">\(\hat{\theta}_3 - \theta\)</span></li>
<li>and so on …</li>
</ul>
<p>So how do we measure the “size” of the estimation errors? The typical way to
quantify the amount of estimation error is by calculating the squared errors,
and then averaging over all the possible values of the estimators.
This is known as the <strong>Mean Squared Error</strong> (MSE) of <span class="math inline">\(\hat{\theta}\)</span>:</p>
<p><span class="math display">\[
\text{MSE}(\hat{\theta}) = \mathbb{E} [(\hat{\theta} - \theta)^2]
\]</span></p>
<p>MSE is the squared distance from our estimator <span class="math inline">\(\hat{\theta}\)</span> to the true value
<span class="math inline">\(\theta\)</span>, averaged over all possible samples.</p>
<p>It is convenient to regard the estimation error, <span class="math inline">\(\hat{\theta} - \theta\)</span>, with
respect to <span class="math inline">\(\mathbb{E}(\hat{\theta})\)</span>. In other words, the distance between <span class="math inline">\(\hat{\theta}\)</span>
and <span class="math inline">\(\theta\)</span> can be expressed with respect
to the expected value <span class="math inline">\(\mathbb{E}(\hat{\theta})\)</span>:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-56"></span>
<img src="images/theory/theta-hat-distribution3.png" alt="Estimator, its mean, and the parameter" width="95%" />
<p class="caption">
Figure 8.6: Estimator, its mean, and the parameter
</p>
</div>
<p>Let’s rewrite <span class="math inline">\((\hat{\theta} - \theta)^2\)</span> as <span class="math inline">\(( \hat{\theta} - \mathbb{E}(\hat{\theta}) + \mathbb{E}(\hat{\theta}) - \theta)^2\)</span>, and let <span class="math inline">\(\mathbb{E}(\hat{\theta}) = \mu_{\hat{\theta}}\)</span>.
Then:</p>
<p><span class="math display">\[\begin{align*}
(\hat{\theta} - \theta)^2 &amp;= \left ( \hat{\theta} - \mathbb{E}(\hat{\theta}) + \mathbb{E}(\hat{\theta}) - \theta \right )^2 \\
&amp;= ( \hat{\theta} - \mu_{\hat{\theta}} + \mu_{\hat{\theta}} - \theta )^2 \\
&amp;= (\underbrace{\hat{\theta} - \mu_{\hat{\theta}}}_{a} + \underbrace{\mu_{\hat{\theta}} - \theta}_{b})^2 \\
&amp;= a^2 + b^2 + 2ab \\
\Longrightarrow \mathbb{E} \left [ (\hat{\theta} - \theta)^2 \right ] &amp;= \mathbb{E}[a^2 + b^2 + 2ab]
\end{align*}\]</span></p>
<p>We have that <span class="math inline">\(\text{MSE}(\hat{\theta}) = \mathbb{E} [(\hat{\theta} - \theta)^2]\)</span> can be decomposed as:</p>
<p><span class="math display">\[\begin{align*}
\mathbb{E} \left [ (\hat{\theta} - \theta)^2 \right ] &amp;= \mathbb{E}[a^2 + b^2 + 2ab] \\
&amp;= \mathbb{E}(a^2) + \mathbb{E}(b^2) + 2\mathbb{E}(ab) \\
&amp;= \mathbb{E} [ (\hat{\theta} - \mu_{\hat{\theta}})^2 ] +
\mathbb{E} [ (\mu_{\hat{\theta}} - \theta)^2 ] + 2\mathbb{E}(ab)
\end{align*}\]</span></p>
<p>Notice that <span class="math inline">\(\mathbb{E}(ab)\)</span>:</p>
<p><span class="math display">\[
\mathbb{E}(ab) = \mathbb{E}[ (\hat{\theta} - \mu_{\hat{\theta}}) (\mu_{\hat{\theta}} - \theta) ] = 0
\]</span></p>
<p>Consequently</p>
<p><span class="math display">\[\begin{align*}
\text{MSE}(\hat{\theta}) &amp;= \mathbb{E} \left [ (\hat{\theta} - \theta)^2 \right ] \\
&amp; \\
&amp;= \mathbb{E} [ (\hat{\theta} - \mu_{\hat{\theta}})^2 ] +
\mathbb{E} [ (\mu_{\hat{\theta}} - \theta)^2 ] \\
&amp; \\
&amp;= \mathbb{E} [(\hat{\theta} - \mu_{\hat{\theta}})^2] + 
\mathbb{E} [ (\mu_{\hat{\theta}} - \theta) ]^2 \\
&amp; \\
&amp;= \underbrace{\mathbb{E} [(\hat{\theta} - \mu_{\hat{\theta}})^2]}_{\text{Variance}} + 
(\underbrace{\mu_{\hat{\theta}} - \theta}_{\text{Bias}})^2 \\
&amp; \\
&amp;= \text{Var}(\hat{\theta}) + \text{Bias}^{2} (\hat{\theta})
\end{align*}\]</span></p>
<p>The MSE of an estimator can be decomposed in terms of Bias and Variance.</p>
<ul>
<li><p><strong>Bias</strong>, <span class="math inline">\(\mu_{\hat{\theta}} - \theta\)</span>, is the tendency of <span class="math inline">\(\hat{\theta}\)</span>
to overestimate or underestimate <span class="math inline">\(\theta\)</span> over all possible samples.</p></li>
<li><p><strong>Variance</strong>, <span class="math inline">\(\text{Var}(\hat{\theta})\)</span>, simply measures the average
variability of the estimators around their mean <span class="math inline">\(\mathbb{E}(\hat{\theta})\)</span>.</p></li>
</ul>
<div id="prototyipcal-cases-of-bias-and-variance" class="section level3">
<h3><span class="header-section-number">8.1.1</span> Prototyipcal Cases of Bias and Variance</h3>
<p>Depending on the type of estimator <span class="math inline">\(\hat{\theta}\)</span>, and the sample size <span class="math inline">\(m\)</span>,
we can get statistics having different behaviors. The following diagram illustrates
four classic scenarios contrasting low and high values for both the bias and
the variance.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-57"></span>
<img src="images/theory/bias-variance-targets.png" alt="Prototypical scenarios for Bias-Variance" width="70%" />
<p class="caption">
Figure 8.7: Prototypical scenarios for Bias-Variance
</p>
</div>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="learning.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["allmodelsarewrong.pdf", "allmodelsarewrong.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
