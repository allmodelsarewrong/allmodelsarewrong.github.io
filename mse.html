<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>8 MSE of Estimator | All Models Are Wrong</title>
  <meta name="description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  <meta name="generator" content="bookdown 0.16 and GitBook 2.6.7" />

  <meta property="og:title" content="8 MSE of Estimator | All Models Are Wrong" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://allmodelsarewrong.github.io" />
  
  <meta property="og:description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  <meta name="github-repo" content="allmodelsarewrong/allmodelsarewrong.github.io" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="8 MSE of Estimator | All Models Are Wrong" />
  
  <meta name="twitter:description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  

<meta name="author" content="by Gaston Sanchez, and Ethan Marzban" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="learning.html"/>
<link rel="next" href="biasvar.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><b>All Models Are Wrong (CSL)</b><br><small>by G. Sanchez & E. Marzban</small></a></li>

<li class="divider"></li>
<li class="part"><span><b>I Welcome</b></span></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="about.html"><a href="about.html"><i class="fa fa-check"></i><b>1</b> About this book</a><ul>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#prerequisites"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="part"><span><b>II Intro</b></span></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#basic-notation"><i class="fa fa-check"></i><b>2.1</b> Basic Notation</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="duality.html"><a href="duality.html"><i class="fa fa-check"></i><b>3</b> Geometric Duality</a><ul>
<li class="chapter" data-level="3.1" data-path="duality.html"><a href="duality.html#rows-space"><i class="fa fa-check"></i><b>3.1</b> Rows Space</a></li>
<li class="chapter" data-level="3.2" data-path="duality.html"><a href="duality.html#columns-space"><i class="fa fa-check"></i><b>3.2</b> Columns Space</a></li>
<li class="chapter" data-level="3.3" data-path="duality.html"><a href="duality.html#cloud-of-individuals"><i class="fa fa-check"></i><b>3.3</b> Cloud of Individuals</a><ul>
<li class="chapter" data-level="3.3.1" data-path="duality.html"><a href="duality.html#average-individual"><i class="fa fa-check"></i><b>3.3.1</b> Average Individual</a></li>
<li class="chapter" data-level="3.3.2" data-path="duality.html"><a href="duality.html#centered-data"><i class="fa fa-check"></i><b>3.3.2</b> Centered Data</a></li>
<li class="chapter" data-level="3.3.3" data-path="duality.html"><a href="duality.html#distance-between-individuals"><i class="fa fa-check"></i><b>3.3.3</b> Distance between individuals</a></li>
<li class="chapter" data-level="3.3.4" data-path="duality.html"><a href="duality.html#distance-to-the-centroid"><i class="fa fa-check"></i><b>3.3.4</b> Distance to the centroid</a></li>
<li class="chapter" data-level="3.3.5" data-path="duality.html"><a href="duality.html#measures-of-dispersion"><i class="fa fa-check"></i><b>3.3.5</b> Measures of Dispersion</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="duality.html"><a href="duality.html#cloud-of-variables"><i class="fa fa-check"></i><b>3.4</b> Cloud of Variables</a><ul>
<li class="chapter" data-level="3.4.1" data-path="duality.html"><a href="duality.html#mean-of-a-variable"><i class="fa fa-check"></i><b>3.4.1</b> Mean of a Variable</a></li>
<li class="chapter" data-level="3.4.2" data-path="duality.html"><a href="duality.html#variance-of-a-variable"><i class="fa fa-check"></i><b>3.4.2</b> Variance of a Variable</a></li>
<li class="chapter" data-level="3.4.3" data-path="duality.html"><a href="duality.html#variance-with-vector-notation"><i class="fa fa-check"></i><b>3.4.3</b> Variance with Vector Notation</a></li>
<li class="chapter" data-level="3.4.4" data-path="duality.html"><a href="duality.html#standard-deviation-as-a-norm"><i class="fa fa-check"></i><b>3.4.4</b> Standard Deviation as a Norm</a></li>
<li class="chapter" data-level="3.4.5" data-path="duality.html"><a href="duality.html#covariance"><i class="fa fa-check"></i><b>3.4.5</b> Covariance</a></li>
<li class="chapter" data-level="3.4.6" data-path="duality.html"><a href="duality.html#correlation"><i class="fa fa-check"></i><b>3.4.6</b> Correlation</a></li>
<li class="chapter" data-level="3.4.7" data-path="duality.html"><a href="duality.html#geometry-of-correlation"><i class="fa fa-check"></i><b>3.4.7</b> Geometry of Correlation</a></li>
<li class="chapter" data-level="3.4.8" data-path="duality.html"><a href="duality.html#orthogonal-projections"><i class="fa fa-check"></i><b>3.4.8</b> Orthogonal Projections</a></li>
<li class="chapter" data-level="3.4.9" data-path="duality.html"><a href="duality.html#the-mean-as-an-orthogonal-projection"><i class="fa fa-check"></i><b>3.4.9</b> The mean as an orthogonal projection</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Unsupervised I: PCA</b></span></li>
<li class="chapter" data-level="4" data-path="pca.html"><a href="pca.html"><i class="fa fa-check"></i><b>4</b> Principal Components Analysis</a><ul>
<li class="chapter" data-level="4.1" data-path="pca.html"><a href="pca.html#low-dimensional-representations"><i class="fa fa-check"></i><b>4.1</b> Low-dimensional Representations</a></li>
<li class="chapter" data-level="4.2" data-path="pca.html"><a href="pca.html#projections"><i class="fa fa-check"></i><b>4.2</b> Projections</a><ul>
<li class="chapter" data-level="4.2.1" data-path="pca.html"><a href="pca.html#vector-and-scalar-projections"><i class="fa fa-check"></i><b>4.2.1</b> Vector and Scalar Projections</a></li>
<li class="chapter" data-level="4.2.2" data-path="pca.html"><a href="pca.html#projected-inertia"><i class="fa fa-check"></i><b>4.2.2</b> Projected Inertia</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="pca.html"><a href="pca.html#maximization-problem"><i class="fa fa-check"></i><b>4.3</b> Maximization Problem</a><ul>
<li class="chapter" data-level="4.3.1" data-path="pca.html"><a href="pca.html#eigenvectors-of-mathbfs"><i class="fa fa-check"></i><b>4.3.1</b> Eigenvectors of <span class="math inline">\(\mathbf{S}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="pca.html"><a href="pca.html#another-perspective-of-pca"><i class="fa fa-check"></i><b>4.4</b> Another Perspective of PCA</a><ul>
<li class="chapter" data-level="4.4.1" data-path="pca.html"><a href="pca.html#finding-principal-components"><i class="fa fa-check"></i><b>4.4.1</b> Finding Principal Components</a></li>
<li class="chapter" data-level="4.4.2" data-path="pca.html"><a href="pca.html#finding-the-first-pc"><i class="fa fa-check"></i><b>4.4.2</b> Finding the first PC</a></li>
<li class="chapter" data-level="4.4.3" data-path="pca.html"><a href="pca.html#finding-the-second-pc"><i class="fa fa-check"></i><b>4.4.3</b> Finding the second PC</a></li>
<li class="chapter" data-level="4.4.4" data-path="pca.html"><a href="pca.html#finding-all-pcs"><i class="fa fa-check"></i><b>4.4.4</b> Finding all PCs</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="pca.html"><a href="pca.html#data-decomposition-model"><i class="fa fa-check"></i><b>4.5</b> Data Decomposition Model</a><ul>
<li class="chapter" data-level="4.5.1" data-path="pca.html"><a href="pca.html#alternative-approaches"><i class="fa fa-check"></i><b>4.5.1</b> Alternative Approaches</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Linear Regression</b></span></li>
<li class="chapter" data-level="5" data-path="ols.html"><a href="ols.html"><i class="fa fa-check"></i><b>5</b> Linear Regression</a><ul>
<li class="chapter" data-level="5.1" data-path="ols.html"><a href="ols.html#motivation"><i class="fa fa-check"></i><b>5.1</b> Motivation</a></li>
<li class="chapter" data-level="5.2" data-path="ols.html"><a href="ols.html#the-ideaintuition-of-regression"><i class="fa fa-check"></i><b>5.2</b> The Idea/Intuition of Regression</a></li>
<li class="chapter" data-level="5.3" data-path="ols.html"><a href="ols.html#the-linear-regression-model"><i class="fa fa-check"></i><b>5.3</b> The Linear Regression Model</a></li>
<li class="chapter" data-level="5.4" data-path="ols.html"><a href="ols.html#the-error-measure"><i class="fa fa-check"></i><b>5.4</b> The Error Measure</a></li>
<li class="chapter" data-level="5.5" data-path="ols.html"><a href="ols.html#the-least-squares-algorithm"><i class="fa fa-check"></i><b>5.5</b> The Least Squares Algorithm</a></li>
<li class="chapter" data-level="5.6" data-path="ols.html"><a href="ols.html#geometries-of-ols"><i class="fa fa-check"></i><b>5.6</b> Geometries of OLS</a><ul>
<li class="chapter" data-level="5.6.1" data-path="ols.html"><a href="ols.html#rows-perspective"><i class="fa fa-check"></i><b>5.6.1</b> Rows Perspective</a></li>
<li class="chapter" data-level="5.6.2" data-path="ols.html"><a href="ols.html#columns-perspective"><i class="fa fa-check"></i><b>5.6.2</b> Columns Perspective</a></li>
<li class="chapter" data-level="5.6.3" data-path="ols.html"><a href="ols.html#parameters-perspective"><i class="fa fa-check"></i><b>5.6.3</b> Parameters Perspective</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="gradient.html"><a href="gradient.html"><i class="fa fa-check"></i><b>6</b> Gradient Descent</a><ul>
<li class="chapter" data-level="6.1" data-path="gradient.html"><a href="gradient.html#error-surface"><i class="fa fa-check"></i><b>6.1</b> Error Surface</a></li>
<li class="chapter" data-level="6.2" data-path="gradient.html"><a href="gradient.html#idea-of-gradient-descent"><i class="fa fa-check"></i><b>6.2</b> Idea of Gradient Descent</a></li>
<li class="chapter" data-level="6.3" data-path="gradient.html"><a href="gradient.html#moving-down-an-error-surface"><i class="fa fa-check"></i><b>6.3</b> Moving Down an Error Surface</a><ul>
<li class="chapter" data-level="6.3.1" data-path="gradient.html"><a href="gradient.html#the-direction-of-mathbfv"><i class="fa fa-check"></i><b>6.3.1</b> The direction of <span class="math inline">\(\mathbf{v}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="gradient.html"><a href="gradient.html#gradient-descent-and-our-model"><i class="fa fa-check"></i><b>6.4</b> Gradient Descent and our Model</a></li>
</ul></li>
<li class="part"><span><b>V Learning Concepts</b></span></li>
<li class="chapter" data-level="7" data-path="learning.html"><a href="learning.html"><i class="fa fa-check"></i><b>7</b> Theoretical Framework</a><ul>
<li class="chapter" data-level="7.1" data-path="learning.html"><a href="learning.html#mental-map"><i class="fa fa-check"></i><b>7.1</b> Mental Map</a></li>
<li class="chapter" data-level="7.2" data-path="learning.html"><a href="learning.html#kinds-of-predictions"><i class="fa fa-check"></i><b>7.2</b> Kinds of Predictions</a><ul>
<li class="chapter" data-level="7.2.1" data-path="learning.html"><a href="learning.html#two-types-of-data"><i class="fa fa-check"></i><b>7.2.1</b> Two Types of Data</a></li>
<li class="chapter" data-level="7.2.2" data-path="learning.html"><a href="learning.html#two-types-of-predictions"><i class="fa fa-check"></i><b>7.2.2</b> Two Types of Predictions</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="learning.html"><a href="learning.html#two-types-of-errors"><i class="fa fa-check"></i><b>7.3</b> Two Types of Errors</a><ul>
<li class="chapter" data-level="7.3.1" data-path="learning.html"><a href="learning.html#individual-errors"><i class="fa fa-check"></i><b>7.3.1</b> Individual Errors</a></li>
<li class="chapter" data-level="7.3.2" data-path="learning.html"><a href="learning.html#overall-errors"><i class="fa fa-check"></i><b>7.3.2</b> Overall Errors</a></li>
<li class="chapter" data-level="7.3.3" data-path="learning.html"><a href="learning.html#auxiliary-technicality"><i class="fa fa-check"></i><b>7.3.3</b> Auxiliary Technicality</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="learning.html"><a href="learning.html#noisy-targets"><i class="fa fa-check"></i><b>7.4</b> Noisy Targets</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="mse.html"><a href="mse.html"><i class="fa fa-check"></i><b>8</b> MSE of Estimator</a><ul>
<li class="chapter" data-level="8.1" data-path="mse.html"><a href="mse.html#mse-of-an-estimator"><i class="fa fa-check"></i><b>8.1</b> MSE of an Estimator</a><ul>
<li class="chapter" data-level="8.1.1" data-path="mse.html"><a href="mse.html#prototypical-cases-of-bias-and-variance"><i class="fa fa-check"></i><b>8.1.1</b> Prototypical Cases of Bias and Variance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="biasvar.html"><a href="biasvar.html"><i class="fa fa-check"></i><b>9</b> Bias-Variance Tradeoff</a><ul>
<li class="chapter" data-level="9.1" data-path="biasvar.html"><a href="biasvar.html#bias-variance-tradeoff"><i class="fa fa-check"></i><b>9.1</b> Bias-Variance Tradeoff</a></li>
<li class="chapter" data-level="9.2" data-path="biasvar.html"><a href="biasvar.html#motivation-example"><i class="fa fa-check"></i><b>9.2</b> Motivation Example</a><ul>
<li class="chapter" data-level="9.2.1" data-path="biasvar.html"><a href="biasvar.html#two-hypotheses"><i class="fa fa-check"></i><b>9.2.1</b> Two Hypotheses</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="biasvar.html"><a href="biasvar.html#learning-from-two-points"><i class="fa fa-check"></i><b>9.3</b> Learning from two points</a></li>
<li class="chapter" data-level="9.4" data-path="biasvar.html"><a href="biasvar.html#bias-variance-derivation"><i class="fa fa-check"></i><b>9.4</b> Bias-Variance Derivation</a><ul>
<li class="chapter" data-level="9.4.1" data-path="biasvar.html"><a href="biasvar.html#noisy-target"><i class="fa fa-check"></i><b>9.4.1</b> Noisy Target</a></li>
<li class="chapter" data-level="9.4.2" data-path="biasvar.html"><a href="biasvar.html#types-of-theoretical-mses"><i class="fa fa-check"></i><b>9.4.2</b> Types of Theoretical MSEs</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="biasvar.html"><a href="biasvar.html#the-tradeoff"><i class="fa fa-check"></i><b>9.5</b> The Tradeoff</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="validation.html"><a href="validation.html"><i class="fa fa-check"></i><b>10</b> Validation</a><ul>
<li class="chapter" data-level="10.1" data-path="validation.html"><a href="validation.html#model-assessment"><i class="fa fa-check"></i><b>10.1</b> Model Assessment</a></li>
<li class="chapter" data-level="10.2" data-path="validation.html"><a href="validation.html#holdout-method"><i class="fa fa-check"></i><b>10.2</b> Holdout Method</a><ul>
<li class="chapter" data-level="10.2.1" data-path="validation.html"><a href="validation.html#rationale-behind-holdout-method"><i class="fa fa-check"></i><b>10.2.1</b> Rationale Behind Holdout Method</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="validation.html"><a href="validation.html#repeated-holdout-method"><i class="fa fa-check"></i><b>10.3</b> Repeated Holdout Method</a></li>
<li class="chapter" data-level="10.4" data-path="validation.html"><a href="validation.html#bootstrap-method"><i class="fa fa-check"></i><b>10.4</b> Bootstrap Method</a></li>
<li class="chapter" data-level="10.5" data-path="validation.html"><a href="validation.html#model-selection"><i class="fa fa-check"></i><b>10.5</b> Model Selection</a><ul>
<li class="chapter" data-level="10.5.1" data-path="validation.html"><a href="validation.html#three-way-holdout-method"><i class="fa fa-check"></i><b>10.5.1</b> Three-way Holdout Method</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="validation.html"><a href="validation.html#cross-validation"><i class="fa fa-check"></i><b>10.6</b> Cross-Validation</a><ul>
<li class="chapter" data-level="10.6.1" data-path="validation.html"><a href="validation.html#leave-one-out-cross-validation-loocv"><i class="fa fa-check"></i><b>10.6.1</b> Leave-One-Out Cross Validation (LOOCV)</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>VI Regularization</b></span></li>
<li class="chapter" data-level="11" data-path="regular.html"><a href="regular.html"><i class="fa fa-check"></i><b>11</b> Regularization Techniques</a><ul>
<li class="chapter" data-level="11.1" data-path="regular.html"><a href="regular.html#multicollinearity-issues"><i class="fa fa-check"></i><b>11.1</b> Multicollinearity Issues</a><ul>
<li class="chapter" data-level="11.1.1" data-path="regular.html"><a href="regular.html#toy-example"><i class="fa fa-check"></i><b>11.1.1</b> Toy Example</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="regular.html"><a href="regular.html#irregular-coefficients"><i class="fa fa-check"></i><b>11.2</b> Irregular Coefficients</a></li>
<li class="chapter" data-level="11.3" data-path="regular.html"><a href="regular.html#connection-to-regularization"><i class="fa fa-check"></i><b>11.3</b> Connection to Regularization</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="pcr.html"><a href="pcr.html"><i class="fa fa-check"></i><b>12</b> Principal Components Regression</a><ul>
<li class="chapter" data-level="12.1" data-path="pcr.html"><a href="pcr.html#motivation-example-1"><i class="fa fa-check"></i><b>12.1</b> Motivation Example</a></li>
<li class="chapter" data-level="12.2" data-path="pcr.html"><a href="pcr.html#the-pcr-model"><i class="fa fa-check"></i><b>12.2</b> The PCR Model</a></li>
<li class="chapter" data-level="12.3" data-path="pcr.html"><a href="pcr.html#how-does-pcr-work"><i class="fa fa-check"></i><b>12.3</b> How does PCR work?</a><ul>
<li class="chapter" data-level="12.3.1" data-path="pcr.html"><a href="pcr.html#transition-formula"><i class="fa fa-check"></i><b>12.3.1</b> Transition Formula</a></li>
<li class="chapter" data-level="12.3.2" data-path="pcr.html"><a href="pcr.html#size-of-coefficients"><i class="fa fa-check"></i><b>12.3.2</b> Size of Coefficients</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="pls.html"><a href="pls.html"><i class="fa fa-check"></i><b>13</b> Partial Least Squares Regression</a><ul>
<li class="chapter" data-level="13.1" data-path="pls.html"><a href="pls.html#motivation-example-2"><i class="fa fa-check"></i><b>13.1</b> Motivation Example</a></li>
<li class="chapter" data-level="13.2" data-path="pls.html"><a href="pls.html#the-plsr-model"><i class="fa fa-check"></i><b>13.2</b> The PLSR Model</a></li>
<li class="chapter" data-level="13.3" data-path="pls.html"><a href="pls.html#how-does-plsr-work"><i class="fa fa-check"></i><b>13.3</b> How does PLSR work?</a></li>
<li class="chapter" data-level="13.4" data-path="pls.html"><a href="pls.html#plsr-algorithm"><i class="fa fa-check"></i><b>13.4</b> PLSR Algorithm</a><ul>
<li class="chapter" data-level="13.4.1" data-path="pls.html"><a href="pls.html#pls-solution-with-original-variables"><i class="fa fa-check"></i><b>13.4.1</b> PLS Solution with original variables</a></li>
<li class="chapter" data-level="13.4.2" data-path="pls.html"><a href="pls.html#size-of-coefficients-1"><i class="fa fa-check"></i><b>13.4.2</b> Size of Coefficients</a></li>
<li class="chapter" data-level="13.4.3" data-path="pls.html"><a href="pls.html#some-properties"><i class="fa fa-check"></i><b>13.4.3</b> Some Properties</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="ridge.html"><a href="ridge.html"><i class="fa fa-check"></i><b>14</b> Ridge Regression</a></li>
<li class="part"><span><b>VII Classification</b></span></li>
<li class="chapter" data-level="15" data-path="classif.html"><a href="classif.html"><i class="fa fa-check"></i><b>15</b> Classification Methods</a></li>
<li class="chapter" data-level="16" data-path="logistic.html"><a href="logistic.html"><i class="fa fa-check"></i><b>16</b> Logistic Regression</a><ul>
<li class="chapter" data-level="16.1" data-path="logistic.html"><a href="logistic.html#motivation-1"><i class="fa fa-check"></i><b>16.1</b> Motivation</a><ul>
<li class="chapter" data-level="16.1.1" data-path="logistic.html"><a href="logistic.html#first-approach-fitting-a-line"><i class="fa fa-check"></i><b>16.1.1</b> First Approach: Fitting a Line</a></li>
<li class="chapter" data-level="16.1.2" data-path="logistic.html"><a href="logistic.html#secodn-approach-harsh-thresholding"><i class="fa fa-check"></i><b>16.1.2</b> Secodn Approach: Harsh Thresholding</a></li>
<li class="chapter" data-level="16.1.3" data-path="logistic.html"><a href="logistic.html#third-approach-conditional-means"><i class="fa fa-check"></i><b>16.1.3</b> Third Approach: Conditional Means</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="logistic.html"><a href="logistic.html#logistic-regression-model"><i class="fa fa-check"></i><b>16.2</b> Logistic Regression Model</a><ul>
<li class="chapter" data-level="16.2.1" data-path="logistic.html"><a href="logistic.html#the-criterion-being-optimized"><i class="fa fa-check"></i><b>16.2.1</b> The Criterion Being Optimized</a></li>
<li class="chapter" data-level="16.2.2" data-path="logistic.html"><a href="logistic.html#another-way-to-solve-logistic-regression"><i class="fa fa-check"></i><b>16.2.2</b> Another Way to Solve Logistic Regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="17" data-path="discrim.html"><a href="discrim.html"><i class="fa fa-check"></i><b>17</b> Preamble for Discriminant Analysis</a><ul>
<li class="chapter" data-level="17.1" data-path="discrim.html"><a href="discrim.html#motivation-2"><i class="fa fa-check"></i><b>17.1</b> Motivation</a><ul>
<li class="chapter" data-level="17.1.1" data-path="discrim.html"><a href="discrim.html#distinguishing-species"><i class="fa fa-check"></i><b>17.1.1</b> Distinguishing Species</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="discrim.html"><a href="discrim.html#derived-ratios-from-sum-of-squares"><i class="fa fa-check"></i><b>17.2</b> Derived Ratios from Sum-of-Squares</a><ul>
<li class="chapter" data-level="17.2.1" data-path="discrim.html"><a href="discrim.html#correlation-ratio"><i class="fa fa-check"></i><b>17.2.1</b> Correlation Ratio</a></li>
<li class="chapter" data-level="17.2.2" data-path="discrim.html"><a href="discrim.html#f-ratio"><i class="fa fa-check"></i><b>17.2.2</b> F-Ratio</a></li>
<li class="chapter" data-level="17.2.3" data-path="discrim.html"><a href="discrim.html#example-with-iris-data"><i class="fa fa-check"></i><b>17.2.3</b> Example with Iris data</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="discrim.html"><a href="discrim.html#geometric-perspective"><i class="fa fa-check"></i><b>17.3</b> Geometric Perspective</a><ul>
<li class="chapter" data-level="17.3.1" data-path="discrim.html"><a href="discrim.html#clouds-from-class-structure"><i class="fa fa-check"></i><b>17.3.1</b> Clouds from Class Structure</a></li>
<li class="chapter" data-level="17.3.2" data-path="discrim.html"><a href="discrim.html#dispersion-decomposition"><i class="fa fa-check"></i><b>17.3.2</b> Dispersion Decomposition</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="cda.html"><a href="cda.html"><i class="fa fa-check"></i><b>18</b> Canonical Discriminant Analysis</a><ul>
<li class="chapter" data-level="18.0.1" data-path="cda.html"><a href="cda.html#looking-for-a-discriminant-axis"><i class="fa fa-check"></i><b>18.0.1</b> Looking for a discriminant axis</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="discanalysis.html"><a href="discanalysis.html"><i class="fa fa-check"></i><b>19</b> Discriminant Analysis</a><ul>
<li class="chapter" data-level="19.1" data-path="discanalysis.html"><a href="discanalysis.html#probabilistic-da"><i class="fa fa-check"></i><b>19.1</b> Probabilistic DA</a></li>
<li class="chapter" data-level="19.2" data-path="discanalysis.html"><a href="discanalysis.html#estimations"><i class="fa fa-check"></i><b>19.2</b> Estimations</a><ul>
<li class="chapter" data-level="19.2.1" data-path="discanalysis.html"><a href="discanalysis.html#univariate-data-boldsymbolp-1"><i class="fa fa-check"></i><b>19.2.1</b> Univariate Data (<span class="math inline">\(\boldsymbol{p = 1}\)</span>)</a></li>
<li class="chapter" data-level="19.2.2" data-path="discanalysis.html"><a href="discanalysis.html#multivariate-data-boldsymbolp-1"><i class="fa fa-check"></i><b>19.2.2</b> Multivariate Data (<span class="math inline">\(\boldsymbol{p &gt; 1}\)</span>)</a></li>
<li class="chapter" data-level="19.2.3" data-path="discanalysis.html"><a href="discanalysis.html#multivariate-estimations"><i class="fa fa-check"></i><b>19.2.3</b> Multivariate Estimations</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="discanalysis.html"><a href="discanalysis.html#discriminant-functions"><i class="fa fa-check"></i><b>19.3</b> Discriminant Functions</a></li>
<li class="chapter" data-level="19.4" data-path="discanalysis.html"><a href="discanalysis.html#quadratic-discriminant-analysis-qda"><i class="fa fa-check"></i><b>19.4</b> Quadratic Discriminant Analysis (QDA)</a></li>
<li class="chapter" data-level="19.5" data-path="discanalysis.html"><a href="discanalysis.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>19.5</b> Linear Discriminant Analysis</a></li>
<li class="chapter" data-level="19.6" data-path="discanalysis.html"><a href="discanalysis.html#canonical-discriminant-analysis"><i class="fa fa-check"></i><b>19.6</b> Canonical Discriminant Analysis</a></li>
<li class="chapter" data-level="19.7" data-path="discanalysis.html"><a href="discanalysis.html#naive-bayes"><i class="fa fa-check"></i><b>19.7</b> Naive Bayes</a></li>
<li class="chapter" data-level="19.8" data-path="discanalysis.html"><a href="discanalysis.html#fifth-case"><i class="fa fa-check"></i><b>19.8</b> Fifth Case</a></li>
<li class="chapter" data-level="19.9" data-path="discanalysis.html"><a href="discanalysis.html#comparing-the-cases"><i class="fa fa-check"></i><b>19.9</b> Comparing the Cases</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="classperformance.html"><a href="classperformance.html"><i class="fa fa-check"></i><b>20</b> Performance of Classifiers</a><ul>
<li class="chapter" data-level="20.1" data-path="classperformance.html"><a href="classperformance.html#error-for-binary-response"><i class="fa fa-check"></i><b>20.1</b> Error for Binary Response</a></li>
<li class="chapter" data-level="20.2" data-path="classperformance.html"><a href="classperformance.html#categorical-response"><i class="fa fa-check"></i><b>20.2</b> Categorical Response</a></li>
<li class="chapter" data-level="20.3" data-path="classperformance.html"><a href="classperformance.html#confusion-matrices"><i class="fa fa-check"></i><b>20.3</b> Confusion Matrices</a></li>
<li class="chapter" data-level="20.4" data-path="classperformance.html"><a href="classperformance.html#binary-response-example"><i class="fa fa-check"></i><b>20.4</b> Binary Response: Example</a><ul>
<li class="chapter" data-level="20.4.1" data-path="classperformance.html"><a href="classperformance.html#types-of-errors-application-for-savings-account"><i class="fa fa-check"></i><b>20.4.1</b> Types of Errors: Application for Savings Account</a></li>
<li class="chapter" data-level="20.4.2" data-path="classperformance.html"><a href="classperformance.html#another-example-loans"><i class="fa fa-check"></i><b>20.4.2</b> Another Example: Loans</a></li>
<li class="chapter" data-level="20.4.3" data-path="classperformance.html"><a href="classperformance.html#the-ideal-situation"><i class="fa fa-check"></i><b>20.4.3</b> The Ideal Situation</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>VIII Clustering</b></span></li>
<li class="chapter" data-level="21" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>21</b> Clustering</a><ul>
<li class="chapter" data-level="21.1" data-path="clustering.html"><a href="clustering.html#about-clustering"><i class="fa fa-check"></i><b>21.1</b> About Clustering</a><ul>
<li class="chapter" data-level="21.1.1" data-path="clustering.html"><a href="clustering.html#types-of-clustering"><i class="fa fa-check"></i><b>21.1.1</b> Types of Clustering</a></li>
<li class="chapter" data-level="21.1.2" data-path="clustering.html"><a href="clustering.html#hard-clustering"><i class="fa fa-check"></i><b>21.1.2</b> Hard Clustering</a></li>
</ul></li>
<li class="chapter" data-level="21.2" data-path="clustering.html"><a href="clustering.html#dispersion-measures"><i class="fa fa-check"></i><b>21.2</b> Dispersion Measures</a></li>
<li class="chapter" data-level="21.3" data-path="clustering.html"><a href="clustering.html#complexity-in-clustering"><i class="fa fa-check"></i><b>21.3</b> Complexity in Clustering</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="kmeans.html"><a href="kmeans.html"><i class="fa fa-check"></i><b>22</b> K-Means</a><ul>
<li class="chapter" data-level="22.1" data-path="kmeans.html"><a href="kmeans.html#toy-example-1"><i class="fa fa-check"></i><b>22.1</b> Toy Example</a></li>
<li class="chapter" data-level="22.2" data-path="kmeans.html"><a href="kmeans.html#what-does-k-means-do"><i class="fa fa-check"></i><b>22.2</b> What does K-means do?</a></li>
<li class="chapter" data-level="22.3" data-path="kmeans.html"><a href="kmeans.html#k-means-algorithms"><i class="fa fa-check"></i><b>22.3</b> K-Means Algorithms</a><ul>
<li class="chapter" data-level="22.3.1" data-path="kmeans.html"><a href="kmeans.html#classic-version"><i class="fa fa-check"></i><b>22.3.1</b> Classic Version</a></li>
<li class="chapter" data-level="22.3.2" data-path="kmeans.html"><a href="kmeans.html#moving-centers-algorithm"><i class="fa fa-check"></i><b>22.3.2</b> Moving Centers Algorithm</a></li>
<li class="chapter" data-level="22.3.3" data-path="kmeans.html"><a href="kmeans.html#dynamic-clouds"><i class="fa fa-check"></i><b>22.3.3</b> Dynamic Clouds</a></li>
<li class="chapter" data-level="22.3.4" data-path="kmeans.html"><a href="kmeans.html#choosing-k"><i class="fa fa-check"></i><b>22.3.4</b> Choosing <span class="math inline">\(k\)</span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="23" data-path="hclus.html"><a href="hclus.html"><i class="fa fa-check"></i><b>23</b> Hierarchical Clustering</a><ul>
<li class="chapter" data-level="23.1" data-path="hclus.html"><a href="hclus.html#agglomerative-methods"><i class="fa fa-check"></i><b>23.1</b> Agglomerative Methods</a></li>
<li class="chapter" data-level="23.2" data-path="hclus.html"><a href="hclus.html#example-single-linkage"><i class="fa fa-check"></i><b>23.2</b> Example: Single Linkage</a><ul>
<li class="chapter" data-level="23.2.1" data-path="hclus.html"><a href="hclus.html#dendrogram"><i class="fa fa-check"></i><b>23.2.1</b> Dendrogram</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Made with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">All Models Are Wrong</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="mse" class="section level1">
<h1><span class="header-section-number">8</span> MSE of Estimator</h1>
<p>In this chapter we provide a preliminary review of the Mean Squared Error
(MSE) of an estimator. This will allow us to have a more gentle introduction
to the next chapter about the famous Bias-Variance tradeoff.</p>
<div id="mse-of-an-estimator" class="section level2">
<h2><span class="header-section-number">8.1</span> MSE of an Estimator</h2>
<p>In order to discuss the bias-variance decomposition of a regression function and
its expected MSE, we would like to first review the concept of the mean squared
error of an estimator. Recall that <em>estimation</em> consists of
providing an approximate value to the parameter of a population, using a (
random) sample of observations drawn from such population.</p>
<p>Say we have a population of <span class="math inline">\(n\)</span> objects and we are interested in describing them
with some numeric characteristic <span class="math inline">\(\theta\)</span>. For example, our population may be
formed by all students in some college, and we want to know their average
height. We call this (theoretical) average the <em>parameter</em>.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-66"></span>
<img src="images/theory/population-parameter.svg" alt="Population described by some parameter of interest." width="55%" />
<p class="caption">
Figure 8.1: Population described by some parameter of interest.
</p>
</div>
<p>To estimate the
value of the parameter, we may draw a random sample of <span class="math inline">\(m &lt; n\)</span> students from the
population and compute a statistic <span class="math inline">\(\hat{\theta}\)</span>. Ideally, we would use some
statistic <span class="math inline">\(\hat{\theta}\)</span> that approximates well the parameter <span class="math inline">\(\theta\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-67"></span>
<img src="images/theory/random-sample.svg" alt="Random sample from a population" width="70%" />
<p class="caption">
Figure 8.2: Random sample from a population
</p>
</div>
<p>In practice, this is the typical process that you would carry out:</p>
<ol style="list-style-type: decimal">
<li>Get a random sample from a population.</li>
<li>Use the limited amount of data in the sample to estimate <span class="math inline">\(\theta\)</span> using some
formula to compute <span class="math inline">\(\hat{\theta}\)</span>.</li>
<li>Make a statement about how reliable of an estimator <span class="math inline">\(\hat{\theta}\)</span> is.</li>
</ol>
<p>Now, for illustration purposes, let’s do the following mental experiment.
Pretend that you can draw multiple random samples, all of the same size <span class="math inline">\(m\)</span>,
from the population.
In fact, you should pretend that you can get an infinite number of samples.
And suppose that for each sample you will compute a statistic <span class="math inline">\(\hat{\theta}\)</span>.
A first random sample of size <span class="math inline">\(m\)</span> would result in <span class="math inline">\(\hat{\theta}_1\)</span>.
A second random sample of size <span class="math inline">\(m\)</span> would result in <span class="math inline">\(\hat{\theta}_2\)</span>. And so on.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-68"></span>
<img src="images/theory/sampling-estimators.svg" alt="Various random samples of equal size and their statistics" width="70%" />
<p class="caption">
Figure 8.3: Various random samples of equal size and their statistics
</p>
</div>
<p>A couple of important things to notice:</p>
<ul>
<li>An estimator is a random variable"
<ul>
<li>A first sample will result in <span class="math inline">\(\hat{\theta}_1\)</span></li>
<li>A second sample will result in <span class="math inline">\(\hat{\theta}_2\)</span></li>
<li>A third sample will result in <span class="math inline">\(\hat{\theta}_3\)</span></li>
<li>and so on …</li>
</ul></li>
<li>Some samples will yield a <span class="math inline">\(\hat{\theta}_k\)</span> that overestimates <span class="math inline">\(\theta\)</span></li>
<li>Other samples will yield a <span class="math inline">\(\hat{\theta}_k\)</span> that underestimates <span class="math inline">\(\theta\)</span></li>
<li>Some samples will yield a <span class="math inline">\(\hat{\theta}_k\)</span> matching <span class="math inline">\(\theta\)</span></li>
</ul>
<p>In theory, we could get a very large number of samples, and visualize the
distribution of <span class="math inline">\(\hat{\theta}\)</span>, like in the figure below:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-69"></span>
<img src="images/theory/theta-hat-distribution1.svg" alt="Distribution of an estimator" width="60%" />
<p class="caption">
Figure 8.4: Distribution of an estimator
</p>
</div>
<p>As you would expect, some estimators will be close to the parameter <span class="math inline">\(\theta\)</span>,
while others not so much.</p>
<p>Under general assumptions, we can also
assume that the estimator has expected value <span class="math inline">\(\mathbb{E}(\hat{\theta})\)</span>, with
finite variance <span class="math inline">\(var(\hat{\theta})\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-70"></span>
<img src="images/theory/theta-hat-distribution2.svg" alt="Distribution of an estimator" width="60%" />
<p class="caption">
Figure 8.5: Distribution of an estimator
</p>
</div>
<p>An interesting question to consider is:</p>
<p>In general, how much different—or similar—is <span class="math inline">\(\hat{\theta}\)</span> from <span class="math inline">\(\theta\)</span>?</p>
<p>To be more concrete: on average, how close
we expect the estimator to be from the parameter? To answer this question
we can look for a measure to assess the typical distance of estimators from
the parameter.</p>
<p>This involves looking at the difference: <span class="math inline">\(\hat{\theta} - \theta\)</span>,
which is commonly referred to as the <strong>estimation error</strong>:</p>
<p><span class="math display">\[
\text{estimation error} = \hat{\theta} - \theta
\]</span></p>
<p>We would like to measure the “size” of such difference. Notice that the estimation error is also a random variable:</p>
<ul>
<li>A first sample will result in an error <span class="math inline">\(\hat{\theta}_1 - \theta\)</span></li>
<li>A second sample will result in an error <span class="math inline">\(\hat{\theta}_2 - \theta\)</span></li>
<li>A third sample will result in an error <span class="math inline">\(\hat{\theta}_3 - \theta\)</span></li>
<li>and so on …</li>
</ul>
<p>So how do we measure the “size” of the estimation errors? The typical way to
quantify the amount of estimation error is by calculating the squared errors,
and then averaging over all the possible values of the estimators.
This is known as the <strong>Mean Squared Error</strong> (MSE) of <span class="math inline">\(\hat{\theta}\)</span>:</p>
<p><span class="math display">\[
\text{MSE}(\hat{\theta}) = \mathbb{E} [(\hat{\theta} - \theta)^2]
\]</span></p>
<p>MSE is the squared distance from our estimator <span class="math inline">\(\hat{\theta}\)</span> to the true value
<span class="math inline">\(\theta\)</span>, averaged over all possible samples.</p>
<p>It is convenient to regard the estimation error, <span class="math inline">\(\hat{\theta} - \theta\)</span>, with
respect to <span class="math inline">\(\mathbb{E}(\hat{\theta})\)</span>. In other words, the distance between <span class="math inline">\(\hat{\theta}\)</span>
and <span class="math inline">\(\theta\)</span> can be expressed with respect
to the expected value <span class="math inline">\(\mathbb{E}(\hat{\theta})\)</span>:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-71"></span>
<img src="images/theory/theta-hat-distribution3.svg" alt="Estimator, its mean, and the parameter" width="95%" />
<p class="caption">
Figure 8.6: Estimator, its mean, and the parameter
</p>
</div>
<p>Let’s rewrite <span class="math inline">\((\hat{\theta} - \theta)^2\)</span> as <span class="math inline">\(( \hat{\theta} - \mathbb{E}(\hat{\theta}) + \mathbb{E}(\hat{\theta}) - \theta)^2\)</span>, and let <span class="math inline">\(\mathbb{E}(\hat{\theta}) = \mu_{\hat{\theta}}\)</span>.
Then:</p>
<p><span class="math display">\[\begin{align*}
(\hat{\theta} - \theta)^2 &amp;= \left ( \hat{\theta} - \mathbb{E}(\hat{\theta}) + \mathbb{E}(\hat{\theta}) - \theta \right )^2 \\
&amp;= ( \hat{\theta} - \mu_{\hat{\theta}} + \mu_{\hat{\theta}} - \theta )^2 \\
&amp;= (\underbrace{\hat{\theta} - \mu_{\hat{\theta}}}_{a} + \underbrace{\mu_{\hat{\theta}} - \theta}_{b})^2 \\
&amp;= a^2 + b^2 + 2ab \\
\Longrightarrow \mathbb{E} \left [ (\hat{\theta} - \theta)^2 \right ] &amp;= \mathbb{E}[a^2 + b^2 + 2ab]
\end{align*}\]</span></p>
<p>We have that <span class="math inline">\(\text{MSE}(\hat{\theta}) = \mathbb{E} [(\hat{\theta} - \theta)^2]\)</span> can be decomposed as:</p>
<p><span class="math display">\[\begin{align*}
\mathbb{E} \left [ (\hat{\theta} - \theta)^2 \right ] &amp;= \mathbb{E}[a^2 + b^2 + 2ab] \\
&amp;= \mathbb{E}(a^2) + \mathbb{E}(b^2) + 2\mathbb{E}(ab) \\
&amp;= \mathbb{E} [ (\hat{\theta} - \mu_{\hat{\theta}})^2 ] +
\mathbb{E} [ (\mu_{\hat{\theta}} - \theta)^2 ] + 2\mathbb{E}(ab)
\end{align*}\]</span></p>
<p>Notice that <span class="math inline">\(\mathbb{E}(ab)\)</span>:</p>
<p><span class="math display">\[
\mathbb{E}(ab) = \mathbb{E}[ (\hat{\theta} - \mu_{\hat{\theta}}) (\mu_{\hat{\theta}} - \theta) ] = 0
\]</span></p>
<p>Consequently</p>
<p><span class="math display">\[\begin{align*}
\text{MSE}(\hat{\theta}) &amp;= \mathbb{E} \left [ (\hat{\theta} - \theta)^2 \right ] \\
&amp; \\
&amp;= \mathbb{E} [ (\hat{\theta} - \mu_{\hat{\theta}})^2 ] +
\mathbb{E} [ (\mu_{\hat{\theta}} - \theta)^2 ] \\
&amp; \\
&amp;= \mathbb{E} [(\hat{\theta} - \mu_{\hat{\theta}})^2] + 
\mathbb{E} [ (\mu_{\hat{\theta}} - \theta) ]^2 \\
&amp; \\
&amp;= \underbrace{\mathbb{E} [(\hat{\theta} - \mu_{\hat{\theta}})^2]}_{\text{Variance}} + 
(\underbrace{\mu_{\hat{\theta}} - \theta}_{\text{Bias}})^2 \\
&amp; \\
&amp;= \text{Var}(\hat{\theta}) + \text{Bias}^{2} (\hat{\theta})
\end{align*}\]</span></p>
<p>The MSE of an estimator can be decomposed in terms of Bias and Variance.</p>
<ul>
<li><p><strong>Bias</strong>, <span class="math inline">\(\mu_{\hat{\theta}} - \theta\)</span>, is the tendency of <span class="math inline">\(\hat{\theta}\)</span>
to overestimate or underestimate <span class="math inline">\(\theta\)</span> over all possible samples.</p></li>
<li><p><strong>Variance</strong>, <span class="math inline">\(\text{Var}(\hat{\theta})\)</span>, simply measures the average
variability of the estimators around their mean <span class="math inline">\(\mathbb{E}(\hat{\theta})\)</span>.</p></li>
</ul>
<div id="prototypical-cases-of-bias-and-variance" class="section level3">
<h3><span class="header-section-number">8.1.1</span> Prototypical Cases of Bias and Variance</h3>
<p>Depending on the type of estimator <span class="math inline">\(\hat{\theta}\)</span>, and the sample size <span class="math inline">\(m\)</span>,
we can get statistics having different behaviors. The following diagram illustrates
four classic scenarios contrasting low and high values for both the bias and
the variance.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-72"></span>
<img src="images/theory/bias-variance-targets.svg" alt="Prototypical scenarios for Bias-Variance" width="70%" />
<p class="caption">
Figure 8.7: Prototypical scenarios for Bias-Variance
</p>
</div>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="learning.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="biasvar.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["allmodelsarewrong.pdf", "allmodelsarewrong.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
