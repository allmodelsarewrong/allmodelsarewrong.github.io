<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>11 Overfitting | All Models Are Wrong: Concepts of Statistical Learning</title>
  <meta name="description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  <meta name="generator" content="bookdown 0.16 and GitBook 2.6.7" />

  <meta property="og:title" content="11 Overfitting | All Models Are Wrong: Concepts of Statistical Learning" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://allmodelsarewrong.github.io" />
  
  <meta property="og:description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  <meta name="github-repo" content="allmodelsarewrong/allmodelsarewrong.github.io" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="11 Overfitting | All Models Are Wrong: Concepts of Statistical Learning" />
  
  <meta name="twitter:description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  

<meta name="author" content="Gaston Sanchez Ethan Marzban" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="biasvar.html"/>
<link rel="next" href="phases.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><b>All Models Are Wrong (CSL)</b><br><small>G. Sanchez & E. Marzban</small></a></li>

<li class="divider"></li>
<li class="part"><span><b>I Welcome</b></span></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="about.html"><a href="about.html"><i class="fa fa-check"></i><b>1</b> About this book</a><ul>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#prerequisites"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="part"><span><b>II Intro</b></span></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#basic-notation"><i class="fa fa-check"></i><b>2.1</b> Basic Notation</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="duality.html"><a href="duality.html"><i class="fa fa-check"></i><b>3</b> Geometric Duality</a><ul>
<li class="chapter" data-level="3.1" data-path="duality.html"><a href="duality.html#rows-space"><i class="fa fa-check"></i><b>3.1</b> Rows Space</a></li>
<li class="chapter" data-level="3.2" data-path="duality.html"><a href="duality.html#columns-space"><i class="fa fa-check"></i><b>3.2</b> Columns Space</a></li>
<li class="chapter" data-level="3.3" data-path="duality.html"><a href="duality.html#cloud-of-individuals"><i class="fa fa-check"></i><b>3.3</b> Cloud of Individuals</a><ul>
<li class="chapter" data-level="3.3.1" data-path="duality.html"><a href="duality.html#average-individual"><i class="fa fa-check"></i><b>3.3.1</b> Average Individual</a></li>
<li class="chapter" data-level="3.3.2" data-path="duality.html"><a href="duality.html#centered-data"><i class="fa fa-check"></i><b>3.3.2</b> Centered Data</a></li>
<li class="chapter" data-level="3.3.3" data-path="duality.html"><a href="duality.html#distance-between-individuals"><i class="fa fa-check"></i><b>3.3.3</b> Distance between individuals</a></li>
<li class="chapter" data-level="3.3.4" data-path="duality.html"><a href="duality.html#distance-to-the-centroid"><i class="fa fa-check"></i><b>3.3.4</b> Distance to the centroid</a></li>
<li class="chapter" data-level="3.3.5" data-path="duality.html"><a href="duality.html#measures-of-dispersion"><i class="fa fa-check"></i><b>3.3.5</b> Measures of Dispersion</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="duality.html"><a href="duality.html#cloud-of-variables"><i class="fa fa-check"></i><b>3.4</b> Cloud of Variables</a><ul>
<li class="chapter" data-level="3.4.1" data-path="duality.html"><a href="duality.html#mean-of-a-variable"><i class="fa fa-check"></i><b>3.4.1</b> Mean of a Variable</a></li>
<li class="chapter" data-level="3.4.2" data-path="duality.html"><a href="duality.html#variance-of-a-variable"><i class="fa fa-check"></i><b>3.4.2</b> Variance of a Variable</a></li>
<li class="chapter" data-level="3.4.3" data-path="duality.html"><a href="duality.html#variance-with-vector-notation"><i class="fa fa-check"></i><b>3.4.3</b> Variance with Vector Notation</a></li>
<li class="chapter" data-level="3.4.4" data-path="duality.html"><a href="duality.html#standard-deviation-as-a-norm"><i class="fa fa-check"></i><b>3.4.4</b> Standard Deviation as a Norm</a></li>
<li class="chapter" data-level="3.4.5" data-path="duality.html"><a href="duality.html#covariance"><i class="fa fa-check"></i><b>3.4.5</b> Covariance</a></li>
<li class="chapter" data-level="3.4.6" data-path="duality.html"><a href="duality.html#correlation"><i class="fa fa-check"></i><b>3.4.6</b> Correlation</a></li>
<li class="chapter" data-level="3.4.7" data-path="duality.html"><a href="duality.html#geometry-of-correlation"><i class="fa fa-check"></i><b>3.4.7</b> Geometry of Correlation</a></li>
<li class="chapter" data-level="3.4.8" data-path="duality.html"><a href="duality.html#orthogonal-projections"><i class="fa fa-check"></i><b>3.4.8</b> Orthogonal Projections</a></li>
<li class="chapter" data-level="3.4.9" data-path="duality.html"><a href="duality.html#the-mean-as-an-orthogonal-projection"><i class="fa fa-check"></i><b>3.4.9</b> The mean as an orthogonal projection</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Unsupervised I: PCA</b></span></li>
<li class="chapter" data-level="4" data-path="pca.html"><a href="pca.html"><i class="fa fa-check"></i><b>4</b> Principal Components Analysis</a><ul>
<li class="chapter" data-level="4.1" data-path="pca.html"><a href="pca.html#low-dimensional-representations"><i class="fa fa-check"></i><b>4.1</b> Low-dimensional Representations</a></li>
<li class="chapter" data-level="4.2" data-path="pca.html"><a href="pca.html#projections"><i class="fa fa-check"></i><b>4.2</b> Projections</a><ul>
<li class="chapter" data-level="4.2.1" data-path="pca.html"><a href="pca.html#vector-and-scalar-projections"><i class="fa fa-check"></i><b>4.2.1</b> Vector and Scalar Projections</a></li>
<li class="chapter" data-level="4.2.2" data-path="pca.html"><a href="pca.html#projected-inertia"><i class="fa fa-check"></i><b>4.2.2</b> Projected Inertia</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="pca.html"><a href="pca.html#maximization-problem"><i class="fa fa-check"></i><b>4.3</b> Maximization Problem</a><ul>
<li class="chapter" data-level="4.3.1" data-path="pca.html"><a href="pca.html#eigenvectors-of-mathbfs"><i class="fa fa-check"></i><b>4.3.1</b> Eigenvectors of <span class="math inline">\(\mathbf{S}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="pca.html"><a href="pca.html#another-perspective-of-pca"><i class="fa fa-check"></i><b>4.4</b> Another Perspective of PCA</a><ul>
<li class="chapter" data-level="4.4.1" data-path="pca.html"><a href="pca.html#finding-principal-components"><i class="fa fa-check"></i><b>4.4.1</b> Finding Principal Components</a></li>
<li class="chapter" data-level="4.4.2" data-path="pca.html"><a href="pca.html#finding-the-first-pc"><i class="fa fa-check"></i><b>4.4.2</b> Finding the first PC</a></li>
<li class="chapter" data-level="4.4.3" data-path="pca.html"><a href="pca.html#finding-the-second-pc"><i class="fa fa-check"></i><b>4.4.3</b> Finding the second PC</a></li>
<li class="chapter" data-level="4.4.4" data-path="pca.html"><a href="pca.html#finding-all-pcs"><i class="fa fa-check"></i><b>4.4.4</b> Finding all PCs</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="pca.html"><a href="pca.html#data-decomposition-model"><i class="fa fa-check"></i><b>4.5</b> Data Decomposition Model</a><ul>
<li class="chapter" data-level="4.5.1" data-path="pca.html"><a href="pca.html#alternative-approaches"><i class="fa fa-check"></i><b>4.5.1</b> Alternative Approaches</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Linear Regression</b></span></li>
<li class="chapter" data-level="5" data-path="ols.html"><a href="ols.html"><i class="fa fa-check"></i><b>5</b> Linear Regression</a><ul>
<li class="chapter" data-level="5.1" data-path="ols.html"><a href="ols.html#motivation"><i class="fa fa-check"></i><b>5.1</b> Motivation</a></li>
<li class="chapter" data-level="5.2" data-path="ols.html"><a href="ols.html#the-ideaintuition-of-regression"><i class="fa fa-check"></i><b>5.2</b> The Idea/Intuition of Regression</a></li>
<li class="chapter" data-level="5.3" data-path="ols.html"><a href="ols.html#the-linear-regression-model"><i class="fa fa-check"></i><b>5.3</b> The Linear Regression Model</a></li>
<li class="chapter" data-level="5.4" data-path="ols.html"><a href="ols.html#the-error-measure"><i class="fa fa-check"></i><b>5.4</b> The Error Measure</a></li>
<li class="chapter" data-level="5.5" data-path="ols.html"><a href="ols.html#the-least-squares-algorithm"><i class="fa fa-check"></i><b>5.5</b> The Least Squares Algorithm</a></li>
<li class="chapter" data-level="5.6" data-path="ols.html"><a href="ols.html#geometries-of-ols"><i class="fa fa-check"></i><b>5.6</b> Geometries of OLS</a><ul>
<li class="chapter" data-level="5.6.1" data-path="ols.html"><a href="ols.html#rows-perspective"><i class="fa fa-check"></i><b>5.6.1</b> Rows Perspective</a></li>
<li class="chapter" data-level="5.6.2" data-path="ols.html"><a href="ols.html#columns-perspective"><i class="fa fa-check"></i><b>5.6.2</b> Columns Perspective</a></li>
<li class="chapter" data-level="5.6.3" data-path="ols.html"><a href="ols.html#parameters-perspective"><i class="fa fa-check"></i><b>5.6.3</b> Parameters Perspective</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="gradient.html"><a href="gradient.html"><i class="fa fa-check"></i><b>6</b> Gradient Descent</a><ul>
<li class="chapter" data-level="6.1" data-path="gradient.html"><a href="gradient.html#error-surface"><i class="fa fa-check"></i><b>6.1</b> Error Surface</a></li>
<li class="chapter" data-level="6.2" data-path="gradient.html"><a href="gradient.html#idea-of-gradient-descent"><i class="fa fa-check"></i><b>6.2</b> Idea of Gradient Descent</a></li>
<li class="chapter" data-level="6.3" data-path="gradient.html"><a href="gradient.html#moving-down-an-error-surface"><i class="fa fa-check"></i><b>6.3</b> Moving Down an Error Surface</a><ul>
<li class="chapter" data-level="6.3.1" data-path="gradient.html"><a href="gradient.html#the-direction-of-mathbfv"><i class="fa fa-check"></i><b>6.3.1</b> The direction of <span class="math inline">\(\mathbf{v}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="gradient.html"><a href="gradient.html#gd-algorithm-for-linear-regression"><i class="fa fa-check"></i><b>6.4</b> GD Algorithm for Linear Regression</a><ul>
<li class="chapter" data-level="6.4.1" data-path="gradient.html"><a href="gradient.html#gd-algorithm-in-vector-matrix-notation"><i class="fa fa-check"></i><b>6.4.1</b> GD Algorithm in vector-matrix notation</a></li>
<li class="chapter" data-level="6.4.2" data-path="gradient.html"><a href="gradient.html#gd-algorithm-in-pointwise-notation"><i class="fa fa-check"></i><b>6.4.2</b> GD algorithm in pointwise notation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="olsml.html"><a href="olsml.html"><i class="fa fa-check"></i><b>7</b> Regression via Maximum Likelihood</a><ul>
<li class="chapter" data-level="7.1" data-path="olsml.html"><a href="olsml.html#linear-regression-reminder"><i class="fa fa-check"></i><b>7.1</b> Linear Regression Reminder</a><ul>
<li class="chapter" data-level="7.1.1" data-path="olsml.html"><a href="olsml.html#maximum-likelihood"><i class="fa fa-check"></i><b>7.1.1</b> Maximum Likelihood</a></li>
<li class="chapter" data-level="7.1.2" data-path="olsml.html"><a href="olsml.html#ml-estimator-of-sigma2"><i class="fa fa-check"></i><b>7.1.2</b> ML Estimator of <span class="math inline">\(\sigma^2\)</span></a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>V Learning Concepts</b></span></li>
<li class="chapter" data-level="8" data-path="learning.html"><a href="learning.html"><i class="fa fa-check"></i><b>8</b> Theoretical Framework</a><ul>
<li class="chapter" data-level="8.1" data-path="learning.html"><a href="learning.html#mental-map"><i class="fa fa-check"></i><b>8.1</b> Mental Map</a></li>
<li class="chapter" data-level="8.2" data-path="learning.html"><a href="learning.html#kinds-of-predictions"><i class="fa fa-check"></i><b>8.2</b> Kinds of Predictions</a><ul>
<li class="chapter" data-level="8.2.1" data-path="learning.html"><a href="learning.html#two-types-of-data"><i class="fa fa-check"></i><b>8.2.1</b> Two Types of Data</a></li>
<li class="chapter" data-level="8.2.2" data-path="learning.html"><a href="learning.html#two-types-of-predictions"><i class="fa fa-check"></i><b>8.2.2</b> Two Types of Predictions</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="learning.html"><a href="learning.html#two-types-of-errors"><i class="fa fa-check"></i><b>8.3</b> Two Types of Errors</a><ul>
<li class="chapter" data-level="8.3.1" data-path="learning.html"><a href="learning.html#individual-errors"><i class="fa fa-check"></i><b>8.3.1</b> Individual Errors</a></li>
<li class="chapter" data-level="8.3.2" data-path="learning.html"><a href="learning.html#overall-errors"><i class="fa fa-check"></i><b>8.3.2</b> Overall Errors</a></li>
<li class="chapter" data-level="8.3.3" data-path="learning.html"><a href="learning.html#probability-as-an-auxiliary-technicality"><i class="fa fa-check"></i><b>8.3.3</b> Probability as an Auxiliary Technicality</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="learning.html"><a href="learning.html#noisy-targets"><i class="fa fa-check"></i><b>8.4</b> Noisy Targets</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="mse.html"><a href="mse.html"><i class="fa fa-check"></i><b>9</b> MSE of Estimator</a><ul>
<li class="chapter" data-level="9.1" data-path="mse.html"><a href="mse.html#mse-of-an-estimator"><i class="fa fa-check"></i><b>9.1</b> MSE of an Estimator</a><ul>
<li class="chapter" data-level="9.1.1" data-path="mse.html"><a href="mse.html#prototypical-cases-of-bias-and-variance"><i class="fa fa-check"></i><b>9.1.1</b> Prototypical Cases of Bias and Variance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="biasvar.html"><a href="biasvar.html"><i class="fa fa-check"></i><b>10</b> Bias-Variance Tradeoff</a><ul>
<li class="chapter" data-level="10.1" data-path="biasvar.html"><a href="biasvar.html#introduction"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="biasvar.html"><a href="biasvar.html#motivation-example"><i class="fa fa-check"></i><b>10.2</b> Motivation Example</a><ul>
<li class="chapter" data-level="10.2.1" data-path="biasvar.html"><a href="biasvar.html#two-hypotheses"><i class="fa fa-check"></i><b>10.2.1</b> Two Hypotheses</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="biasvar.html"><a href="biasvar.html#learning-from-two-points"><i class="fa fa-check"></i><b>10.3</b> Learning from two points</a></li>
<li class="chapter" data-level="10.4" data-path="biasvar.html"><a href="biasvar.html#bias-variance-derivation"><i class="fa fa-check"></i><b>10.4</b> Bias-Variance Derivation</a><ul>
<li class="chapter" data-level="10.4.1" data-path="biasvar.html"><a href="biasvar.html#out-of-sample-predictions"><i class="fa fa-check"></i><b>10.4.1</b> Out-of-Sample Predictions</a></li>
<li class="chapter" data-level="10.4.2" data-path="biasvar.html"><a href="biasvar.html#noisy-target"><i class="fa fa-check"></i><b>10.4.2</b> Noisy Target</a></li>
<li class="chapter" data-level="10.4.3" data-path="biasvar.html"><a href="biasvar.html#types-of-theoretical-mses"><i class="fa fa-check"></i><b>10.4.3</b> Types of Theoretical MSEs</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="biasvar.html"><a href="biasvar.html#the-tradeoff"><i class="fa fa-check"></i><b>10.5</b> The Tradeoff</a><ul>
<li class="chapter" data-level="10.5.1" data-path="biasvar.html"><a href="biasvar.html#bias-variance-tradeoff-picture"><i class="fa fa-check"></i><b>10.5.1</b> Bias-Variance Tradeoff Picture</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="overfit.html"><a href="overfit.html"><i class="fa fa-check"></i><b>11</b> Overfitting</a><ul>
<li class="chapter" data-level="11.1" data-path="overfit.html"><a href="overfit.html#introduction-1"><i class="fa fa-check"></i><b>11.1</b> Introduction</a><ul>
<li class="chapter" data-level="11.1.1" data-path="overfit.html"><a href="overfit.html#bias-variance-reminder-and-pitfalls"><i class="fa fa-check"></i><b>11.1.1</b> Bias-Variance Reminder and Pitfalls</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="overfit.html"><a href="overfit.html#simulation"><i class="fa fa-check"></i><b>11.2</b> Simulation</a><ul>
<li class="chapter" data-level="11.2.1" data-path="overfit.html"><a href="overfit.html#underfitting-overfitting-and-okayfitting"><i class="fa fa-check"></i><b>11.2.1</b> Underfitting, Overfitting and Okayfitting</a></li>
<li class="chapter" data-level="11.2.2" data-path="overfit.html"><a href="overfit.html#more-learning-sets"><i class="fa fa-check"></i><b>11.2.2</b> More learning sets</a></li>
<li class="chapter" data-level="11.2.3" data-path="overfit.html"><a href="overfit.html#when-does-overfitting-occurs"><i class="fa fa-check"></i><b>11.2.3</b> When does overfitting occurs?</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="overfit.html"><a href="overfit.html#in-summary"><i class="fa fa-check"></i><b>11.3</b> In Summary</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="phases.html"><a href="phases.html"><i class="fa fa-check"></i><b>12</b> Learning Phases</a><ul>
<li class="chapter" data-level="12.1" data-path="phases.html"><a href="phases.html#introduction-2"><i class="fa fa-check"></i><b>12.1</b> Introduction</a></li>
<li class="chapter" data-level="12.2" data-path="phases.html"><a href="phases.html#model-assessment"><i class="fa fa-check"></i><b>12.2</b> Model Assessment</a><ul>
<li class="chapter" data-level="12.2.1" data-path="phases.html"><a href="phases.html#holdout-test-set"><i class="fa fa-check"></i><b>12.2.1</b> Holdout Test Set</a></li>
<li class="chapter" data-level="12.2.2" data-path="phases.html"><a href="phases.html#why-does-a-test-set-work"><i class="fa fa-check"></i><b>12.2.2</b> Why does a test set work?</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="phases.html"><a href="phases.html#model-selection"><i class="fa fa-check"></i><b>12.3</b> Model Selection</a><ul>
<li class="chapter" data-level="12.3.1" data-path="phases.html"><a href="phases.html#three-way-holdout-method"><i class="fa fa-check"></i><b>12.3.1</b> Three-way Holdout Method</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="phases.html"><a href="phases.html#model-training"><i class="fa fa-check"></i><b>12.4</b> Model Training</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="resampling.html"><a href="resampling.html"><i class="fa fa-check"></i><b>13</b> Resampling Approaches</a><ul>
<li class="chapter" data-level="13.1" data-path="resampling.html"><a href="resampling.html#general-sampling-blueprint"><i class="fa fa-check"></i><b>13.1</b> General Sampling Blueprint</a></li>
<li class="chapter" data-level="13.2" data-path="resampling.html"><a href="resampling.html#monte-carlo-cross-validation"><i class="fa fa-check"></i><b>13.2</b> Monte Carlo Cross Validation</a></li>
<li class="chapter" data-level="13.3" data-path="resampling.html"><a href="resampling.html#bootstrap-method"><i class="fa fa-check"></i><b>13.3</b> Bootstrap Method</a></li>
<li class="chapter" data-level="13.4" data-path="resampling.html"><a href="resampling.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>13.4</b> <span class="math inline">\(K\)</span>-Fold Cross-Validation</a><ul>
<li class="chapter" data-level="13.4.1" data-path="resampling.html"><a href="resampling.html#leave-one-out-cross-validation-loocv"><i class="fa fa-check"></i><b>13.4.1</b> Leave-One-Out Cross Validation (LOOCV)</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>VI Regularization</b></span></li>
<li class="chapter" data-level="14" data-path="regular.html"><a href="regular.html"><i class="fa fa-check"></i><b>14</b> Regularization Techniques</a><ul>
<li class="chapter" data-level="14.1" data-path="regular.html"><a href="regular.html#multicollinearity-issues"><i class="fa fa-check"></i><b>14.1</b> Multicollinearity Issues</a><ul>
<li class="chapter" data-level="14.1.1" data-path="regular.html"><a href="regular.html#toy-example"><i class="fa fa-check"></i><b>14.1.1</b> Toy Example</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="regular.html"><a href="regular.html#irregular-coefficients"><i class="fa fa-check"></i><b>14.2</b> Irregular Coefficients</a></li>
<li class="chapter" data-level="14.3" data-path="regular.html"><a href="regular.html#connection-to-regularization"><i class="fa fa-check"></i><b>14.3</b> Connection to Regularization</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="pcr.html"><a href="pcr.html"><i class="fa fa-check"></i><b>15</b> Principal Components Regression</a><ul>
<li class="chapter" data-level="15.1" data-path="pcr.html"><a href="pcr.html#motivation-example-1"><i class="fa fa-check"></i><b>15.1</b> Motivation Example</a></li>
<li class="chapter" data-level="15.2" data-path="pcr.html"><a href="pcr.html#the-pcr-model"><i class="fa fa-check"></i><b>15.2</b> The PCR Model</a></li>
<li class="chapter" data-level="15.3" data-path="pcr.html"><a href="pcr.html#how-does-pcr-work"><i class="fa fa-check"></i><b>15.3</b> How does PCR work?</a><ul>
<li class="chapter" data-level="15.3.1" data-path="pcr.html"><a href="pcr.html#transition-formula"><i class="fa fa-check"></i><b>15.3.1</b> Transition Formula</a></li>
<li class="chapter" data-level="15.3.2" data-path="pcr.html"><a href="pcr.html#size-of-coefficients"><i class="fa fa-check"></i><b>15.3.2</b> Size of Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="pcr.html"><a href="pcr.html#selecting-number-of-pcs"><i class="fa fa-check"></i><b>15.4</b> Selecting Number of PCs</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="pls.html"><a href="pls.html"><i class="fa fa-check"></i><b>16</b> Partial Least Squares Regression</a><ul>
<li class="chapter" data-level="16.1" data-path="pls.html"><a href="pls.html#motivation-example-2"><i class="fa fa-check"></i><b>16.1</b> Motivation Example</a></li>
<li class="chapter" data-level="16.2" data-path="pls.html"><a href="pls.html#the-plsr-model"><i class="fa fa-check"></i><b>16.2</b> The PLSR Model</a></li>
<li class="chapter" data-level="16.3" data-path="pls.html"><a href="pls.html#how-does-plsr-work"><i class="fa fa-check"></i><b>16.3</b> How does PLSR work?</a></li>
<li class="chapter" data-level="16.4" data-path="pls.html"><a href="pls.html#plsr-algorithm"><i class="fa fa-check"></i><b>16.4</b> PLSR Algorithm</a><ul>
<li class="chapter" data-level="16.4.1" data-path="pls.html"><a href="pls.html#pls-solution-with-original-variables"><i class="fa fa-check"></i><b>16.4.1</b> PLS Solution with original variables</a></li>
<li class="chapter" data-level="16.4.2" data-path="pls.html"><a href="pls.html#some-properties"><i class="fa fa-check"></i><b>16.4.2</b> Some Properties</a></li>
<li class="chapter" data-level="16.4.3" data-path="pls.html"><a href="pls.html#pls-regression-for-price-of-cars"><i class="fa fa-check"></i><b>16.4.3</b> PLS Regression for Price of cars</a></li>
<li class="chapter" data-level="16.4.4" data-path="pls.html"><a href="pls.html#size-of-coefficients-1"><i class="fa fa-check"></i><b>16.4.4</b> Size of Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="16.5" data-path="pls.html"><a href="pls.html#selecting-number-of-pls-components"><i class="fa fa-check"></i><b>16.5</b> Selecting Number of PLS Components</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="ridge.html"><a href="ridge.html"><i class="fa fa-check"></i><b>17</b> Ridge Regression</a><ul>
<li class="chapter" data-level="17.1" data-path="ridge.html"><a href="ridge.html#a-new-minimization-problem"><i class="fa fa-check"></i><b>17.1</b> A New Minimization Problem</a><ul>
<li class="chapter" data-level="17.1.1" data-path="ridge.html"><a href="ridge.html#constraining-regression-coefficients"><i class="fa fa-check"></i><b>17.1.1</b> Constraining Regression Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="ridge.html"><a href="ridge.html#a-new-minimization-solution"><i class="fa fa-check"></i><b>17.2</b> A New Minimization Solution</a></li>
<li class="chapter" data-level="17.3" data-path="ridge.html"><a href="ridge.html#what-does-rr-accomplish"><i class="fa fa-check"></i><b>17.3</b> What does RR accomplish?</a><ul>
<li class="chapter" data-level="17.3.1" data-path="ridge.html"><a href="ridge.html#how-to-find-lambda"><i class="fa fa-check"></i><b>17.3.1</b> How to find <span class="math inline">\(\lambda\)</span>?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="lasso.html"><a href="lasso.html"><i class="fa fa-check"></i><b>18</b> Lasso Regression</a><ul>
<li class="chapter" data-level="18.1" data-path="lasso.html"><a href="lasso.html#mathematical-setup"><i class="fa fa-check"></i><b>18.1</b> Mathematical Setup</a><ul>
<li class="chapter" data-level="18.1.1" data-path="lasso.html"><a href="lasso.html#closed-form"><i class="fa fa-check"></i><b>18.1.1</b> Closed Form?</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="lasso.html"><a href="lasso.html#geometric-visualization"><i class="fa fa-check"></i><b>18.2</b> Geometric Visualization</a><ul>
<li class="chapter" data-level="18.2.1" data-path="lasso.html"><a href="lasso.html#some-more-math-variable-selection-in-action"><i class="fa fa-check"></i><b>18.2.1</b> Some More Math: Variable Selection in Action</a></li>
<li class="chapter" data-level="18.2.2" data-path="lasso.html"><a href="lasso.html#example-with-mtcars"><i class="fa fa-check"></i><b>18.2.2</b> Example with <code>mtcars</code></a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="lasso.html"><a href="lasso.html#going-beyond"><i class="fa fa-check"></i><b>18.3</b> Going Beyond</a></li>
</ul></li>
<li class="part"><span><b>VII Extending Linear Regression</b></span></li>
<li class="chapter" data-level="19" data-path="linear-extensions.html"><a href="linear-extensions.html"><i class="fa fa-check"></i><b>19</b> Beyond Linear Regression</a><ul>
<li class="chapter" data-level="19.1" data-path="linear-extensions.html"><a href="linear-extensions.html#introduction-3"><i class="fa fa-check"></i><b>19.1</b> Introduction</a></li>
<li class="chapter" data-level="19.2" data-path="linear-extensions.html"><a href="linear-extensions.html#expanding-the-regression-horizon"><i class="fa fa-check"></i><b>19.2</b> Expanding the Regression Horizon</a><ul>
<li class="chapter" data-level="19.2.1" data-path="linear-extensions.html"><a href="linear-extensions.html#linearity"><i class="fa fa-check"></i><b>19.2.1</b> Linearity</a></li>
<li class="chapter" data-level="19.2.2" data-path="linear-extensions.html"><a href="linear-extensions.html#parametric-and-nonparametric"><i class="fa fa-check"></i><b>19.2.2</b> Parametric and Nonparametric</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="linear-extensions.html"><a href="linear-extensions.html#transforming-features"><i class="fa fa-check"></i><b>19.3</b> Transforming Features</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="basis.html"><a href="basis.html"><i class="fa fa-check"></i><b>20</b> Basis Expansion</a><ul>
<li class="chapter" data-level="20.1" data-path="basis.html"><a href="basis.html#basis-functions"><i class="fa fa-check"></i><b>20.1</b> Basis Functions</a></li>
<li class="chapter" data-level="20.2" data-path="basis.html"><a href="basis.html#linear-regression"><i class="fa fa-check"></i><b>20.2</b> Linear Regression</a></li>
<li class="chapter" data-level="20.3" data-path="basis.html"><a href="basis.html#polynomial-regression"><i class="fa fa-check"></i><b>20.3</b> Polynomial Regression</a></li>
<li class="chapter" data-level="20.4" data-path="basis.html"><a href="basis.html#gaussian-rbfs"><i class="fa fa-check"></i><b>20.4</b> Gaussian RBF’s</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="nonparametric.html"><a href="nonparametric.html"><i class="fa fa-check"></i><b>21</b> Nonparametric Regression</a><ul>
<li class="chapter" data-level="21.1" data-path="nonparametric.html"><a href="nonparametric.html#conditional-averages"><i class="fa fa-check"></i><b>21.1</b> Conditional Averages</a></li>
<li class="chapter" data-level="21.2" data-path="nonparametric.html"><a href="nonparametric.html#looking-at-the-neighbors"><i class="fa fa-check"></i><b>21.2</b> Looking at the Neighbors</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="knn.html"><a href="knn.html"><i class="fa fa-check"></i><b>22</b> Nearest Neighbor Estimates</a><ul>
<li class="chapter" data-level="22.1" data-path="knn.html"><a href="knn.html#introduction-4"><i class="fa fa-check"></i><b>22.1</b> Introduction</a></li>
<li class="chapter" data-level="22.2" data-path="knn.html"><a href="knn.html#k-nearest-neighbors-knn"><i class="fa fa-check"></i><b>22.2</b> <span class="math inline">\(k\)</span> Nearest Neighbors (KNN)</a><ul>
<li class="chapter" data-level="22.2.1" data-path="knn.html"><a href="knn.html#distance-measures"><i class="fa fa-check"></i><b>22.2.1</b> Distance Measures</a></li>
<li class="chapter" data-level="22.2.2" data-path="knn.html"><a href="knn.html#knn-estimator"><i class="fa fa-check"></i><b>22.2.2</b> KNN Estimator</a></li>
<li class="chapter" data-level="22.2.3" data-path="knn.html"><a href="knn.html#how-to-find-k"><i class="fa fa-check"></i><b>22.2.3</b> How to find <span class="math inline">\(k\)</span>?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="23" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html"><i class="fa fa-check"></i><b>23</b> Kernel Smoothers</a><ul>
<li class="chapter" data-level="23.1" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#introduction-5"><i class="fa fa-check"></i><b>23.1</b> Introduction</a></li>
<li class="chapter" data-level="23.2" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#kernel-smoothers-1"><i class="fa fa-check"></i><b>23.2</b> Kernel Smoothers</a><ul>
<li class="chapter" data-level="23.2.1" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#kernel-functions"><i class="fa fa-check"></i><b>23.2.1</b> Kernel Functions</a></li>
<li class="chapter" data-level="23.2.2" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#weights-from-kernels"><i class="fa fa-check"></i><b>23.2.2</b> Weights from Kernels</a></li>
<li class="chapter" data-level="23.2.3" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#kernel-estimator"><i class="fa fa-check"></i><b>23.2.3</b> Kernel Estimator</a></li>
</ul></li>
<li class="chapter" data-level="23.3" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#local-polynomial-estimators"><i class="fa fa-check"></i><b>23.3</b> Local Polynomial Estimators</a></li>
</ul></li>
<li class="part"><span><b>VIII Classification</b></span></li>
<li class="chapter" data-level="24" data-path="classif.html"><a href="classif.html"><i class="fa fa-check"></i><b>24</b> Classification</a><ul>
<li class="chapter" data-level="24.1" data-path="classif.html"><a href="classif.html#introduction-6"><i class="fa fa-check"></i><b>24.1</b> Introduction</a><ul>
<li class="chapter" data-level="24.1.1" data-path="classif.html"><a href="classif.html#credit-score-example"><i class="fa fa-check"></i><b>24.1.1</b> Credit Score Example</a></li>
<li class="chapter" data-level="24.1.2" data-path="classif.html"><a href="classif.html#toy-example-1"><i class="fa fa-check"></i><b>24.1.2</b> Toy Example</a></li>
<li class="chapter" data-level="24.1.3" data-path="classif.html"><a href="classif.html#two-class-problem"><i class="fa fa-check"></i><b>24.1.3</b> Two-class Problem</a></li>
<li class="chapter" data-level="24.1.4" data-path="classif.html"><a href="classif.html#bayes-rule-reminder"><i class="fa fa-check"></i><b>24.1.4</b> Bayes’ Rule Reminder</a></li>
</ul></li>
<li class="chapter" data-level="24.2" data-path="classif.html"><a href="classif.html#bayes-classifier"><i class="fa fa-check"></i><b>24.2</b> Bayes Classifier</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="logistic.html"><a href="logistic.html"><i class="fa fa-check"></i><b>25</b> Logistic Regression</a><ul>
<li class="chapter" data-level="25.1" data-path="logistic.html"><a href="logistic.html#motivation-1"><i class="fa fa-check"></i><b>25.1</b> Motivation</a><ul>
<li class="chapter" data-level="25.1.1" data-path="logistic.html"><a href="logistic.html#first-approach-fitting-a-line"><i class="fa fa-check"></i><b>25.1.1</b> First Approach: Fitting a Line</a></li>
<li class="chapter" data-level="25.1.2" data-path="logistic.html"><a href="logistic.html#second-approach-harsh-thresholding"><i class="fa fa-check"></i><b>25.1.2</b> Second Approach: Harsh Thresholding</a></li>
<li class="chapter" data-level="25.1.3" data-path="logistic.html"><a href="logistic.html#third-approach-conditional-means"><i class="fa fa-check"></i><b>25.1.3</b> Third Approach: Conditional Means</a></li>
</ul></li>
<li class="chapter" data-level="25.2" data-path="logistic.html"><a href="logistic.html#logistic-regression-model"><i class="fa fa-check"></i><b>25.2</b> Logistic Regression Model</a><ul>
<li class="chapter" data-level="25.2.1" data-path="logistic.html"><a href="logistic.html#the-criterion-being-optimized"><i class="fa fa-check"></i><b>25.2.1</b> The Criterion Being Optimized</a></li>
<li class="chapter" data-level="25.2.2" data-path="logistic.html"><a href="logistic.html#another-way-to-solve-logistic-regression"><i class="fa fa-check"></i><b>25.2.2</b> Another Way to Solve Logistic Regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="26" data-path="discrim.html"><a href="discrim.html"><i class="fa fa-check"></i><b>26</b> Preamble for Discriminant Analysis</a><ul>
<li class="chapter" data-level="26.1" data-path="discrim.html"><a href="discrim.html#motivation-2"><i class="fa fa-check"></i><b>26.1</b> Motivation</a><ul>
<li class="chapter" data-level="26.1.1" data-path="discrim.html"><a href="discrim.html#distinguishing-species"><i class="fa fa-check"></i><b>26.1.1</b> Distinguishing Species</a></li>
<li class="chapter" data-level="26.1.2" data-path="discrim.html"><a href="discrim.html#sum-of-squares-decomposition"><i class="fa fa-check"></i><b>26.1.2</b> Sum of Squares Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="26.2" data-path="discrim.html"><a href="discrim.html#derived-ratios-from-sum-of-squares"><i class="fa fa-check"></i><b>26.2</b> Derived Ratios from Sum-of-Squares</a><ul>
<li class="chapter" data-level="26.2.1" data-path="discrim.html"><a href="discrim.html#correlation-ratio"><i class="fa fa-check"></i><b>26.2.1</b> Correlation Ratio</a></li>
<li class="chapter" data-level="26.2.2" data-path="discrim.html"><a href="discrim.html#f-ratio"><i class="fa fa-check"></i><b>26.2.2</b> F-Ratio</a></li>
</ul></li>
<li class="chapter" data-level="26.3" data-path="discrim.html"><a href="discrim.html#geometric-perspective"><i class="fa fa-check"></i><b>26.3</b> Geometric Perspective</a><ul>
<li class="chapter" data-level="26.3.1" data-path="discrim.html"><a href="discrim.html#clouds-from-class-structure"><i class="fa fa-check"></i><b>26.3.1</b> Clouds from Class Structure</a></li>
<li class="chapter" data-level="26.3.2" data-path="discrim.html"><a href="discrim.html#dispersion-decomposition"><i class="fa fa-check"></i><b>26.3.2</b> Dispersion Decomposition</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="27" data-path="cda.html"><a href="cda.html"><i class="fa fa-check"></i><b>27</b> Canonical Discriminant Analysis</a><ul>
<li class="chapter" data-level="27.1" data-path="cda.html"><a href="cda.html#cda-semi-supervised-aspect"><i class="fa fa-check"></i><b>27.1</b> CDA: Semi-Supervised Aspect</a></li>
<li class="chapter" data-level="27.2" data-path="cda.html"><a href="cda.html#looking-for-a-discriminant-axis"><i class="fa fa-check"></i><b>27.2</b> Looking for a discriminant axis</a></li>
<li class="chapter" data-level="27.3" data-path="cda.html"><a href="cda.html#looking-for-a-compromise-criterion"><i class="fa fa-check"></i><b>27.3</b> Looking for a Compromise Criterion</a><ul>
<li class="chapter" data-level="27.3.1" data-path="cda.html"><a href="cda.html#correlation-ratio-criterion"><i class="fa fa-check"></i><b>27.3.1</b> Correlation Ratio Criterion</a></li>
<li class="chapter" data-level="27.3.2" data-path="cda.html"><a href="cda.html#f-ratio-criterion"><i class="fa fa-check"></i><b>27.3.2</b> F-ratio Criterion</a></li>
<li class="chapter" data-level="27.3.3" data-path="cda.html"><a href="cda.html#a-special-pca"><i class="fa fa-check"></i><b>27.3.3</b> A Special PCA</a></li>
</ul></li>
<li class="chapter" data-level="27.4" data-path="cda.html"><a href="cda.html#cda-supervised-aspect"><i class="fa fa-check"></i><b>27.4</b> CDA: Supervised Aspect</a><ul>
<li class="chapter" data-level="27.4.1" data-path="cda.html"><a href="cda.html#distance-behind-cda"><i class="fa fa-check"></i><b>27.4.1</b> Distance behind CDA</a></li>
<li class="chapter" data-level="27.4.2" data-path="cda.html"><a href="cda.html#predictive-idea"><i class="fa fa-check"></i><b>27.4.2</b> Predictive Idea</a></li>
<li class="chapter" data-level="27.4.3" data-path="cda.html"><a href="cda.html#cda-classifier"><i class="fa fa-check"></i><b>27.4.3</b> CDA Classifier</a></li>
<li class="chapter" data-level="27.4.4" data-path="cda.html"><a href="cda.html#limitations-of-cda-classifier"><i class="fa fa-check"></i><b>27.4.4</b> Limitations of CDA classifier</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="28" data-path="discanalysis.html"><a href="discanalysis.html"><i class="fa fa-check"></i><b>28</b> Discriminant Analysis</a><ul>
<li class="chapter" data-level="28.1" data-path="discanalysis.html"><a href="discanalysis.html#probabilistic-da"><i class="fa fa-check"></i><b>28.1</b> Probabilistic DA</a><ul>
<li class="chapter" data-level="28.1.1" data-path="discanalysis.html"><a href="discanalysis.html#normal-distributions"><i class="fa fa-check"></i><b>28.1.1</b> Normal Distributions</a></li>
<li class="chapter" data-level="28.1.2" data-path="discanalysis.html"><a href="discanalysis.html#estimating-parameters-of-normal-distributions"><i class="fa fa-check"></i><b>28.1.2</b> Estimating Parameters of Normal Distributions</a></li>
</ul></li>
<li class="chapter" data-level="28.2" data-path="discanalysis.html"><a href="discanalysis.html#discriminant-functions"><i class="fa fa-check"></i><b>28.2</b> Discriminant Functions</a></li>
<li class="chapter" data-level="28.3" data-path="discanalysis.html"><a href="discanalysis.html#quadratic-discriminant-analysis-qda"><i class="fa fa-check"></i><b>28.3</b> Quadratic Discriminant Analysis (QDA)</a></li>
<li class="chapter" data-level="28.4" data-path="discanalysis.html"><a href="discanalysis.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>28.4</b> Linear Discriminant Analysis</a><ul>
<li class="chapter" data-level="28.4.1" data-path="discanalysis.html"><a href="discanalysis.html#canonical-discriminant-analysis"><i class="fa fa-check"></i><b>28.4.1</b> Canonical Discriminant Analysis</a></li>
<li class="chapter" data-level="28.4.2" data-path="discanalysis.html"><a href="discanalysis.html#naive-bayes"><i class="fa fa-check"></i><b>28.4.2</b> Naive Bayes</a></li>
<li class="chapter" data-level="28.4.3" data-path="discanalysis.html"><a href="discanalysis.html#fifth-case"><i class="fa fa-check"></i><b>28.4.3</b> Fifth Case</a></li>
</ul></li>
<li class="chapter" data-level="28.5" data-path="discanalysis.html"><a href="discanalysis.html#comparing-the-cases"><i class="fa fa-check"></i><b>28.5</b> Comparing the Cases</a></li>
</ul></li>
<li class="chapter" data-level="29" data-path="classperformance.html"><a href="classperformance.html"><i class="fa fa-check"></i><b>29</b> Performance of Classifiers</a><ul>
<li class="chapter" data-level="29.1" data-path="classperformance.html"><a href="classperformance.html#classification-error-measures"><i class="fa fa-check"></i><b>29.1</b> Classification Error Measures</a><ul>
<li class="chapter" data-level="29.1.1" data-path="classperformance.html"><a href="classperformance.html#errors-for-binary-response"><i class="fa fa-check"></i><b>29.1.1</b> Errors for Binary Response</a></li>
<li class="chapter" data-level="29.1.2" data-path="classperformance.html"><a href="classperformance.html#error-for-categorical-response"><i class="fa fa-check"></i><b>29.1.2</b> Error for Categorical Response</a></li>
</ul></li>
<li class="chapter" data-level="29.2" data-path="classperformance.html"><a href="classperformance.html#confusion-matrices"><i class="fa fa-check"></i><b>29.2</b> Confusion Matrices</a></li>
<li class="chapter" data-level="29.3" data-path="classperformance.html"><a href="classperformance.html#binary-response-example"><i class="fa fa-check"></i><b>29.3</b> Binary Response Example</a><ul>
<li class="chapter" data-level="29.3.1" data-path="classperformance.html"><a href="classperformance.html#application-for-checking-account"><i class="fa fa-check"></i><b>29.3.1</b> Application for Checking Account</a></li>
<li class="chapter" data-level="29.3.2" data-path="classperformance.html"><a href="classperformance.html#application-for-loan"><i class="fa fa-check"></i><b>29.3.2</b> Application for Loan</a></li>
</ul></li>
<li class="chapter" data-level="29.4" data-path="classperformance.html"><a href="classperformance.html#decision-rules-and-errors"><i class="fa fa-check"></i><b>29.4</b> Decision Rules and Errors</a></li>
<li class="chapter" data-level="29.5" data-path="classperformance.html"><a href="classperformance.html#roc-curves"><i class="fa fa-check"></i><b>29.5</b> ROC Curves</a><ul>
<li class="chapter" data-level="29.5.1" data-path="classperformance.html"><a href="classperformance.html#graphing-roc-curves"><i class="fa fa-check"></i><b>29.5.1</b> Graphing ROC curves</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IX Unsupervised II: Clustering</b></span></li>
<li class="chapter" data-level="30" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>30</b> Clustering</a><ul>
<li class="chapter" data-level="30.1" data-path="clustering.html"><a href="clustering.html#about-clustering"><i class="fa fa-check"></i><b>30.1</b> About Clustering</a><ul>
<li class="chapter" data-level="30.1.1" data-path="clustering.html"><a href="clustering.html#types-of-clustering"><i class="fa fa-check"></i><b>30.1.1</b> Types of Clustering</a></li>
<li class="chapter" data-level="30.1.2" data-path="clustering.html"><a href="clustering.html#hard-clustering"><i class="fa fa-check"></i><b>30.1.2</b> Hard Clustering</a></li>
</ul></li>
<li class="chapter" data-level="30.2" data-path="clustering.html"><a href="clustering.html#dispersion-measures"><i class="fa fa-check"></i><b>30.2</b> Dispersion Measures</a></li>
<li class="chapter" data-level="30.3" data-path="clustering.html"><a href="clustering.html#complexity-in-clustering"><i class="fa fa-check"></i><b>30.3</b> Complexity in Clustering</a></li>
</ul></li>
<li class="chapter" data-level="31" data-path="kmeans.html"><a href="kmeans.html"><i class="fa fa-check"></i><b>31</b> K-Means</a><ul>
<li class="chapter" data-level="31.1" data-path="kmeans.html"><a href="kmeans.html#toy-example-2"><i class="fa fa-check"></i><b>31.1</b> Toy Example</a></li>
<li class="chapter" data-level="31.2" data-path="kmeans.html"><a href="kmeans.html#what-does-k-means-do"><i class="fa fa-check"></i><b>31.2</b> What does K-means do?</a></li>
<li class="chapter" data-level="31.3" data-path="kmeans.html"><a href="kmeans.html#k-means-algorithms"><i class="fa fa-check"></i><b>31.3</b> K-Means Algorithms</a><ul>
<li class="chapter" data-level="31.3.1" data-path="kmeans.html"><a href="kmeans.html#classic-version"><i class="fa fa-check"></i><b>31.3.1</b> Classic Version</a></li>
<li class="chapter" data-level="31.3.2" data-path="kmeans.html"><a href="kmeans.html#moving-centers-algorithm"><i class="fa fa-check"></i><b>31.3.2</b> Moving Centers Algorithm</a></li>
<li class="chapter" data-level="31.3.3" data-path="kmeans.html"><a href="kmeans.html#dynamic-clouds"><i class="fa fa-check"></i><b>31.3.3</b> Dynamic Clouds</a></li>
<li class="chapter" data-level="31.3.4" data-path="kmeans.html"><a href="kmeans.html#choosing-k"><i class="fa fa-check"></i><b>31.3.4</b> Choosing <span class="math inline">\(K\)</span></a></li>
<li class="chapter" data-level="31.3.5" data-path="kmeans.html"><a href="kmeans.html#comments"><i class="fa fa-check"></i><b>31.3.5</b> Comments</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="32" data-path="hclus.html"><a href="hclus.html"><i class="fa fa-check"></i><b>32</b> Hierarchical Clustering</a><ul>
<li class="chapter" data-level="32.1" data-path="hclus.html"><a href="hclus.html#agglomerative-methods"><i class="fa fa-check"></i><b>32.1</b> Agglomerative Methods</a></li>
<li class="chapter" data-level="32.2" data-path="hclus.html"><a href="hclus.html#example-single-linkage"><i class="fa fa-check"></i><b>32.2</b> Example: Single Linkage</a><ul>
<li class="chapter" data-level="32.2.1" data-path="hclus.html"><a href="hclus.html#dendrogram"><i class="fa fa-check"></i><b>32.2.1</b> Dendrogram</a></li>
</ul></li>
<li class="chapter" data-level="32.3" data-path="hclus.html"><a href="hclus.html#example-complete-linkage"><i class="fa fa-check"></i><b>32.3</b> Example: Complete Linkage</a><ul>
<li class="chapter" data-level="32.3.1" data-path="hclus.html"><a href="hclus.html#cutting-dendograms"><i class="fa fa-check"></i><b>32.3.1</b> Cutting Dendograms</a></li>
<li class="chapter" data-level="32.3.2" data-path="hclus.html"><a href="hclus.html#pros-and-coons"><i class="fa fa-check"></i><b>32.3.2</b> Pros and COons</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>X Tree-based Methods</b></span></li>
<li class="chapter" data-level="33" data-path="trees.html"><a href="trees.html"><i class="fa fa-check"></i><b>33</b> Intro to Decision Trees</a><ul>
<li class="chapter" data-level="33.1" data-path="trees.html"><a href="trees.html#introduction-7"><i class="fa fa-check"></i><b>33.1</b> Introduction</a></li>
<li class="chapter" data-level="33.2" data-path="trees.html"><a href="trees.html#some-terminology"><i class="fa fa-check"></i><b>33.2</b> Some Terminology</a><ul>
<li class="chapter" data-level="33.2.1" data-path="trees.html"><a href="trees.html#binary-trees"><i class="fa fa-check"></i><b>33.2.1</b> Binary Trees</a></li>
</ul></li>
<li class="chapter" data-level="33.3" data-path="trees.html"><a href="trees.html#space-partitions"><i class="fa fa-check"></i><b>33.3</b> Space Partitions</a><ul>
<li class="chapter" data-level="33.3.1" data-path="trees.html"><a href="trees.html#the-process-of-building-a-tree"><i class="fa fa-check"></i><b>33.3.1</b> The Process of Building a Tree</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="34" data-path="tree-impurities.html"><a href="tree-impurities.html"><i class="fa fa-check"></i><b>34</b> Binary Splits and Impurity</a><ul>
<li class="chapter" data-level="34.1" data-path="tree-impurities.html"><a href="tree-impurities.html#binary-partitions"><i class="fa fa-check"></i><b>34.1</b> Binary Partitions</a><ul>
<li class="chapter" data-level="34.1.1" data-path="tree-impurities.html"><a href="tree-impurities.html#splits-of-binary-variables"><i class="fa fa-check"></i><b>34.1.1</b> Splits of Binary variables</a></li>
<li class="chapter" data-level="34.1.2" data-path="tree-impurities.html"><a href="tree-impurities.html#splits-of-nominal-variables"><i class="fa fa-check"></i><b>34.1.2</b> Splits of Nominal Variables</a></li>
<li class="chapter" data-level="34.1.3" data-path="tree-impurities.html"><a href="tree-impurities.html#splits-of-ordinal-variables"><i class="fa fa-check"></i><b>34.1.3</b> Splits of Ordinal Variables</a></li>
<li class="chapter" data-level="34.1.4" data-path="tree-impurities.html"><a href="tree-impurities.html#splits-continuous-variables"><i class="fa fa-check"></i><b>34.1.4</b> Splits Continuous variables</a></li>
</ul></li>
<li class="chapter" data-level="34.2" data-path="tree-impurities.html"><a href="tree-impurities.html#measures-of-impurity"><i class="fa fa-check"></i><b>34.2</b> Measures of Impurity</a><ul>
<li class="chapter" data-level="34.2.1" data-path="tree-impurities.html"><a href="tree-impurities.html#entropy"><i class="fa fa-check"></i><b>34.2.1</b> Entropy</a></li>
<li class="chapter" data-level="34.2.2" data-path="tree-impurities.html"><a href="tree-impurities.html#the-math-behind-entropy"><i class="fa fa-check"></i><b>34.2.2</b> The Math Behind Entropy</a></li>
<li class="chapter" data-level="34.2.3" data-path="tree-impurities.html"><a href="tree-impurities.html#gini-impurity"><i class="fa fa-check"></i><b>34.2.3</b> Gini Impurity</a></li>
<li class="chapter" data-level="34.2.4" data-path="tree-impurities.html"><a href="tree-impurities.html#variance-based-impurity"><i class="fa fa-check"></i><b>34.2.4</b> Variance-based Impurity</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="35" data-path="tree-splits.html"><a href="tree-splits.html"><i class="fa fa-check"></i><b>35</b> Splitting Nodes</a><ul>
<li class="chapter" data-level="35.1" data-path="tree-splits.html"><a href="tree-splits.html#entropy-based-splits"><i class="fa fa-check"></i><b>35.1</b> Entropy-based Splits</a></li>
<li class="chapter" data-level="35.2" data-path="tree-splits.html"><a href="tree-splits.html#gini-index-based-splits"><i class="fa fa-check"></i><b>35.2</b> Gini-index based Splits</a></li>
<li class="chapter" data-level="35.3" data-path="tree-splits.html"><a href="tree-splits.html#looking-for-the-best-split"><i class="fa fa-check"></i><b>35.3</b> Looking for the best split</a></li>
</ul></li>
<li class="chapter" data-level="36" data-path="tree-basics.html"><a href="tree-basics.html"><i class="fa fa-check"></i><b>36</b> Building Binary Trees</a><ul>
<li class="chapter" data-level="36.1" data-path="tree-basics.html"><a href="tree-basics.html#node-splitting-stopping-criteria"><i class="fa fa-check"></i><b>36.1</b> Node-Splitting Stopping Criteria</a></li>
<li class="chapter" data-level="36.2" data-path="tree-basics.html"><a href="tree-basics.html#issues-with-trees"><i class="fa fa-check"></i><b>36.2</b> Issues with Trees</a><ul>
<li class="chapter" data-level="36.2.1" data-path="tree-basics.html"><a href="tree-basics.html#bias-variance-of-trees"><i class="fa fa-check"></i><b>36.2.1</b> Bias-Variance of Trees</a></li>
</ul></li>
<li class="chapter" data-level="36.3" data-path="tree-basics.html"><a href="tree-basics.html#pruning-a-tree"><i class="fa fa-check"></i><b>36.3</b> Pruning a Tree</a></li>
<li class="chapter" data-level="36.4" data-path="tree-basics.html"><a href="tree-basics.html#pros-and-cons-of-trees"><i class="fa fa-check"></i><b>36.4</b> Pros and Cons of Trees</a><ul>
<li class="chapter" data-level="36.4.1" data-path="tree-basics.html"><a href="tree-basics.html#advantages-of-trees"><i class="fa fa-check"></i><b>36.4.1</b> Advantages of Trees</a></li>
<li class="chapter" data-level="36.4.2" data-path="tree-basics.html"><a href="tree-basics.html#disadvantages-of-trees"><i class="fa fa-check"></i><b>36.4.2</b> Disadvantages of Trees</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="37" data-path="bagging.html"><a href="bagging.html"><i class="fa fa-check"></i><b>37</b> Bagging</a><ul>
<li class="chapter" data-level="37.1" data-path="bagging.html"><a href="bagging.html#introduction-8"><i class="fa fa-check"></i><b>37.1</b> Introduction</a><ul>
<li class="chapter" data-level="37.1.1" data-path="bagging.html"><a href="bagging.html#idea-of-bagging"><i class="fa fa-check"></i><b>37.1.1</b> Idea of Bagging</a></li>
</ul></li>
<li class="chapter" data-level="37.2" data-path="bagging.html"><a href="bagging.html#why-bother-bagging"><i class="fa fa-check"></i><b>37.2</b> Why Bother Bagging?</a></li>
</ul></li>
<li class="chapter" data-level="38" data-path="forest.html"><a href="forest.html"><i class="fa fa-check"></i><b>38</b> Random Forests</a><ul>
<li class="chapter" data-level="38.1" data-path="forest.html"><a href="forest.html#introduction-9"><i class="fa fa-check"></i><b>38.1</b> Introduction</a></li>
<li class="chapter" data-level="38.2" data-path="forest.html"><a href="forest.html#algorithm-2"><i class="fa fa-check"></i><b>38.2</b> Algorithm</a><ul>
<li class="chapter" data-level="38.2.1" data-path="forest.html"><a href="forest.html#two-sources-of-randomness"><i class="fa fa-check"></i><b>38.2.1</b> Two Sources of Randomness</a></li>
<li class="chapter" data-level="38.2.2" data-path="forest.html"><a href="forest.html#regressions-and-classification-forests"><i class="fa fa-check"></i><b>38.2.2</b> Regressions and Classification Forests</a></li>
<li class="chapter" data-level="38.2.3" data-path="forest.html"><a href="forest.html#key-advantage-of-random-forests"><i class="fa fa-check"></i><b>38.2.3</b> Key Advantage of Random Forests</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Made with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">All Models Are Wrong: Concepts of Statistical Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="overfit" class="section level1">
<h1><span class="header-section-number">11</span> Overfitting</h1>
<p>In supervised learning, one of the major risks we run when fitting a model is
to overestimate how well it will do when we use it in the real world. This risk
is commonly known under the name of <em>overfitting</em>, and it also has its little
sibling <em>underfitting</em>. In this chapter we discuss these topics, or to be more
precise:</p>
<ul>
<li>What is overfitting? and why is it troublesome?</li>
<li>What causes overfitting?</li>
<li>What can be done to prevent/mitigate overfitting?</li>
</ul>
<div id="introduction-1" class="section level2">
<h2><span class="header-section-number">11.1</span> Introduction</h2>
<p>The phenomenon of overfitting is very similar to when we felt overconfident
before taking a test, and then finding out that we were not as well prepared
for the actual test as we presumptively thought.
Following on this metaphor of studying for a test, we are pretty sure you
have experienced the following situations:</p>
<ul>
<li><p><strong>Limited capacity</strong>: You studied for a test, and were able to grasp—in a
more or less superficial way—the general idea for some topics, but lacked
many of the important details. For instance, you read about simple linear
regression, understanding the idea of fitting a line, but not being able to
explain how to derive the formula of the regression line.</p></li>
<li><p><strong>Too much focus</strong>: You studied for a test, focusing too much on certain
topics, memorizing most or even all of their details (without necessarily
understanding the things that you memorized), at the expense of other topics.
For example, you memorized the derivation of the normal equations in a linear
regression model, and you even memorized the coefficients of the linear
regression example in the book, but at the expense of leaving out the role and
properties of the hat matrix.</p></li>
<li><p><strong>Distraction by “noise”</strong>: Despite your best disposition to study for a test,
you were not able to study properly because of major distractions. For example:
you got distracted with all the notifications in your phone, or an emergency
happened that affected your studying plans, or you got distracted with the noise
produced by those loud reparations taking place in your neighbor’s building.</p></li>
</ul>
<p>Those of us who have experienced these pitfalls, know very well that they
typically lead to poor performance on the test, or at least they result in a
lower than expected performance.</p>
<p>It turns out that these issues are also shared by supervised learning systems,
and they are directly related with the ingredients of the Bias-Variance
decomposition. So, in order to identify the three previous pitfalls, here’s a
reminder of the BV decomposition.</p>
<div id="bias-variance-reminder-and-pitfalls" class="section level3">
<h3><span class="header-section-number">11.1.1</span> Bias-Variance Reminder and Pitfalls</h3>
<p>We assume a response variable <span class="math inline">\(y = f(x) + \varepsilon\)</span>, and we seek to find a
model <span class="math inline">\(\widehat{h}\)</span> that is a good approximation to the target model <span class="math inline">\(f\)</span>. Given
a learning data set <span class="math inline">\(\mathcal{D}\)</span> of <span class="math inline">\(n\)</span> points, and a hypothesis <span class="math inline">\(h(x)\)</span>,
the expectation of the Squared Error for a given out-of-sample point <span class="math inline">\(x_0\)</span>,
over all possible learning sets, is expressed as:</p>
<p><span class="math display" id="eq:404-1">\[\begin{align}
\mathbb{E}_{\mathcal{D}} \left [ \left( h^{(\mathcal{D})}(x_0) - f(x_0) \right)^2 \right ] 
&amp;= 
\underbrace{\mathbb{E}_{\mathcal{D}} \left [ \left (h^{(\mathcal{D})}(x_0) - \bar{h}(x_0) \right)^2 \right ]}_{\text{variance}} \\
&amp;+
\underbrace{ \big( \ \bar{h}(x_0) - f(x_0) \big)^2 }_{\text{bias}^2} \\
&amp;+
\underbrace{ \sigma^2 }_{\text{noise}} \\
&amp;= \text{var} + \text{bias}^2 + \sigma^2
\tag{11.1}
\end{align}\]</span></p>
<p>where:</p>
<p><span class="math display" id="eq:404-2">\[
\bar{h}(x_0) = \mathbb{E}_{\mathcal{D}} \left[ h^{(\mathcal{D})}(x_0) \right] \quad \text{is the average hypothesis}
\tag{11.2}
\]</span></p>
<p>In summary:</p>
<ul>
<li><p>Bias: how much of the target model’s behavior your class of model
<span class="math inline">\(\bar{h}()\)</span> can approximate.</p></li>
<li><p>Variance: how much variation the fitted model <span class="math inline">\(h()\)</span> experiments
when exposed to a given learning data set.</p></li>
<li><p>Noise: random or irreducible noise <span class="math inline">\(\sigma^2\)</span> in the data.</p></li>
</ul>
<div id="pitfalls" class="section level4 unnumbered">
<h4>Pitfalls</h4>
<p>Having reviewed the BV-decomposition, we can now give names to the three kind
of pitfalls shared by supervised learning systems:</p>
<ul>
<li><p><strong>Large Bias</strong> is the equivalent of suffering from a “limited learning
capacity”. Models, or to be more precise <strong>class</strong> of models, that are largely biased have too little capacity to get close enough to the true model.</p></li>
<li><p><strong>Large Variance</strong> is the equivalent of focusing too much on certain details, that may well not be that important, at the expense of other equally or more
important details.</p></li>
<li><p><strong>Large Noise</strong> is the equivalent of getting distracted by the noise in the
data. This is especially serious when working with <em>bad</em> data, extremely messy,
perhaps having many missing values, or just simply being of poor quality.</p></li>
</ul>
<p>At this point, it would be nice to consider a simulation example in order to
illustrate each of the listed pitfalls, and how they are connected to the
so-called notions of overfitting and underfitting.</p>
</div>
</div>
</div>
<div id="simulation" class="section level2">
<h2><span class="header-section-number">11.2</span> Simulation</h2>
<p>Let’s carry out a simple simulation. We are going to consider a target function
with some noise given by the following expression:</p>
<p><span class="math display">\[
f(x) = sin(1 + x^2) + \varepsilon
\]</span></p>
<p>with the input variable <span class="math inline">\(x\)</span> in the interval <span class="math inline">\([0,1]\)</span>, and the noise term
<span class="math inline">\(\varepsilon \sim N(\mu=0, \sigma=0.03)\)</span>.</p>
<p>The function of the signal, <span class="math inline">\(sin(1 + x^2)\)</span>, is depicted in the figure below.
Keep in mind that in real life we will never have this knowledge: we won’t
really know the true form of the target function.</p>
<div class="figure" style="text-align: center"><span id="fig:plotoverfit1"></span>
<img src="allmodelsarewrong_files/figure-html/plotoverfit1-1.png" alt="Target signal function" width="70%" />
<p class="caption">
Figure 11.1: Target signal function
</p>
</div>
<p>We have a model to play with. But, what about the data? We are going to simulate
one <em>in-sample</em> set of 10 points <span class="math inline">\((x_i, y_i)\)</span>, and one <em>out-of-sample</em> set of
also 10 points <span class="math inline">\((x_0, y_0)\)</span>.</p>
<pre><code>       x_in     y_in    x_out    y_out
1    0.6364   0.9822   0.1313   0.8135
2    0.9293   0.9124   0.1515   0.8359
3    0.1919   0.8315   0.4444   0.9253
4    0.5354   0.9559   0.4141   0.9360
5    0.9091   0.9722   0.7879   1.0430
6    0.0808   0.8342   0.4545   0.9379
7    0.7374   1.0224   0.7980   0.9723
8    0.4040   0.8918   0.3232   0.8815
9    0.0101   0.8237   0.9394   0.8863
10   0.1717   0.8584   0.6869   0.9474</code></pre>
<p>In the figure below, the in-sample points are
depicted as blue circles, and the out-of-sample points are depicted as red
crosses. We should emphasize the fact that this out-of-sample set is just for
simulation purposes. A more formal out-of-sample set should include all
<span class="math inline">\(x\)</span>-values in the interval <span class="math inline">\([0,1]\)</span>. Likewise, we are including the curve of
the signal just for visualization (in practice you won’t have this luxury).</p>
<div class="figure" style="text-align: center"><span id="fig:plotoverfit2"></span>
<img src="allmodelsarewrong_files/figure-html/plotoverfit2-1.png" alt="Target signal with 10 in-sample points, and 10 out-of-sample points" width="70%" />
<p class="caption">
Figure 11.2: Target signal with 10 in-sample points, and 10 out-of-sample points
</p>
</div>
<p>In this simulation analysis, we are interested in fitting several regression
models taking the form of polynomials of various degrees. In theory, we could
start by fitting a polynomial of degree zero, that is, a constant model. But
we are going to skip this basic model and instead begin by fitting a linear
model (i.e. degree 1 polynomial). Then a second degree polynomial, a third
degree model, and so on and so forth. Having only 10 in-sample points, the
maximum degree polynomial that can be fitted is a 9-degree polynomial.</p>
<div id="linear-model" class="section level4 unnumbered">
<h4>Linear Model</h4>
<p>The first fitted model is a linear model of the form:</p>
<p><span class="math display">\[
h_1(x) = b_0 + b_1 x
\]</span></p>
<p>The fitted regression model is shown in the figure below (see blue line).</p>
<div class="figure" style="text-align: center"><span id="fig:plotoverfit3"></span>
<img src="allmodelsarewrong_files/figure-html/plotoverfit3-1.png" alt="Linear model fit on in-sample points" width="70%" />
<p class="caption">
Figure 11.3: Linear model fit on in-sample points
</p>
</div>
<p>With the obtained model, the corresponding in-sample error <span class="math inline">\(E_{in}\)</span> and
out-of-sample error <span class="math inline">\(E_{out}\)</span> are:</p>
<pre><code>    Ein      Eout   
0.00147   0.00215   </code></pre>
</div>
<div id="quadratic-model" class="section level4 unnumbered">
<h4>Quadratic Model</h4>
<p>The second fitted model is a quadratic model (i.e. polynomial of degree 2)</p>
<p><span class="math display">\[
h_2(x) = b_0 + b_1 x + b_2 x^2
\]</span></p>
<p>The following picture displays the fitted model (see blue curve).</p>
<div class="figure" style="text-align: center"><span id="fig:plotoverfit4"></span>
<img src="allmodelsarewrong_files/figure-html/plotoverfit4-1.png" alt="Quadratic model fit on in-sample points" width="70%" />
<p class="caption">
Figure 11.4: Quadratic model fit on in-sample points
</p>
</div>
<p>In this case, the corresponding in-sample error <span class="math inline">\(E_{in}\)</span> and
out-of-sample error <span class="math inline">\(E_{out}\)</span> are:</p>
<pre><code>    Ein      Eout   
0.00093   0.00137   </code></pre>
</div>
<div id="nonic-model" class="section level4 unnumbered">
<h4>Nonic Model</h4>
<p>Because we have <span class="math inline">\(n = 10\)</span> data points, we can fit a polynomial up to degree 9,
which is called a <em>nonic</em>:</p>
<p><span class="math display">\[
h_9(x) = b_0 + b_1 x + b_2 x^2 + \dots + b_8 x^8 + b_9 x^9
\]</span></p>
<div class="figure" style="text-align: center"><span id="fig:plotoverfit5"></span>
<img src="allmodelsarewrong_files/figure-html/plotoverfit5-1.png" alt="Nonic model fit on in-sample points" width="70%" />
<p class="caption">
Figure 11.5: Nonic model fit on in-sample points
</p>
</div>
<p>The corresponding in-sample error <span class="math inline">\(E_{in}\)</span> and out-of-sample error <span class="math inline">\(E_{out}\)</span> are:</p>
<pre><code>    Ein      Eout   
0.00000   0.00231   </code></pre>
</div>
<div id="so-far" class="section level4 unnumbered">
<h4>So far …</h4>
<p>With these three models fitted so far (linear, quadratic and nonic), the one
that best fits the in-sample data is the 9-degree polynomial which achieves
zero in-sample error. From this narrow perspective, it is very tempting to
select <span class="math inline">\(h_9(x)\)</span> as our winner model. After all, this is the model with a
perfect fit to the learning data. But is this a wise idea?</p>
<div class="figure" style="text-align: center"><span id="fig:plotoverfit6"></span>
<img src="allmodelsarewrong_files/figure-html/plotoverfit6-1.png" alt="Linear, quadratic and nonic regression lines with in-sample error values" width="90%" />
<p class="caption">
Figure 11.6: Linear, quadratic and nonic regression lines with in-sample error values
</p>
</div>
</div>
<div id="underfitting-overfitting-and-okayfitting" class="section level3">
<h3><span class="header-section-number">11.2.1</span> Underfitting, Overfitting and Okayfitting</h3>
<p>To better unerstand the behavor of low-order versus high-order polynomials, it
would be nice to compare all fitted polynomials from degree 1 to 9, and
evaluate not just their in-sample performance, but also their out-of-sample
behavior (see figure and table below). In fact, at the end of the day it is the
out-of-sample performace that we care about when deploying a model to the real
world.</p>
<p><img src="allmodelsarewrong_files/figure-html/plotoverfit7-1.png" width="90%" style="display: block; margin: auto;" /></p>
<p>The following table shows both the in-sample error and the out-of-sample error
for each of the nine regression models:</p>
<pre><code>    degree       Ein      Eout
1        1   0.00147   0.00215
2        2   0.00093   0.00137
3        3   0.00029   0.00081
4        4   0.00012   0.00116
5        5   0.00011   0.00110
6        6   0.00007   0.00108
7        7   0.00004   0.00118
8        8   0.00003   0.00151
9        9   0.00000   0.00231</code></pre>
<p>As you can tell, the linear model does not have enough capacity. Although it
does approximate the general positive trend between <span class="math inline">\(y\)</span> and most of the range
of <span class="math inline">\(x\)</span>-values, its in-sample error is the largest of all. Compared to the rest
of the models, we say that this polynomial underfits the data. This means that
we could increase a more complex model producing lower <span class="math inline">\(E_{in}\)</span> and <span class="math inline">\(E_{out}\)</span>.</p>
<p>The quadratic model does a slightly better job than the linear model, but still
misses the sigmoidal shape of the signal. Compared to the more flexibile
polynomials, we can also say that this model is underfitting.</p>
<p>Interestingly, the polynomials of degrees 3, 4, and 5 are able to approximate
most of the overall shape of the target model. And at least visually, we can
tell that the cubic polynomial is the model with an “okayfit”.</p>
<p>In turn, the degree-9 polynomial, while it does a terrific job of passing
through all the in-sample points, it is too wiggly. Formally, we say that this
model is too flexible.</p>
<p>To facilitate the comparison between in-sample error versus out-of-sample error,
we graph them as error curves along their level complexity in terms of the
polynomial degree.</p>
<p><img src="allmodelsarewrong_files/figure-html/plotoverfit8-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>As you might suspect, selecting the 9-order polynomial out of all nine models
because it is the one that has perfect in-sample prediction is not the best
decision. Compared to the rest of the models, we say that the 9-order regression
model overfits the data. And so do the 8-order and 7-order polynomial to a
lesser extent. Overfitting means that an attractively small <span class="math inline">\(E_{in}\)</span> value is
no longer a good indicator of a model’s out-of-sample performance.</p>
</div>
<div id="more-learning-sets" class="section level3">
<h3><span class="header-section-number">11.2.2</span> More learning sets</h3>
<p>Having the “view of the gods”, we can easily see how
wiggly the 9-degree polynomial model is compared to the true signal. It is not
hard to imagine that a different learning set of 10 points following the same target distribution, will result in completely different 9-order polynomials.
This situation is illustrated in the following diagram, in which three new
learning sets of size <span class="math inline">\(n = 10\)</span> have been generated, each with the fitted class
of polynomial.</p>
<div class="figure" style="text-align: center"><span id="fig:plotoverfit9"></span>
<img src="allmodelsarewrong_files/figure-html/plotoverfit9-1.png" alt="Three additional models based on three different learning sets" width="90%" />
<p class="caption">
Figure 11.7: Three additional models based on three different learning sets
</p>
</div>
<p>Notice again the high flexibility of the high-order polynomials, namely those
of degree 7, 8 and 9. Because each learning set contains 10 points, all the
9-order polynomials fit them perfectly. However, they all show an extremely
volatile behavior. In contrast, the low-order polynomials, namely the linear,
the quadratic and the cubic models are much more stable.</p>
</div>
<div id="when-does-overfitting-occurs" class="section level3">
<h3><span class="header-section-number">11.2.3</span> When does overfitting occurs?</h3>
<p>Simply put, overfitting happens when we fit the data more than is necessary.
More formally, overfitting is the phenomenon when we choose the model with
smaller <span class="math inline">\(E_{in}\)</span>, and it turns out in bigger <span class="math inline">\(E_{out}\)</span>. This implies that the
in-sample error <span class="math inline">\(E_{in}\)</span>, in and of itself, is no longer a good indicator for
the chosen model’s generalization power when predicting out-of-sample data.</p>
<p>What about underfitting? This occurs with models that perform poorly on unseen
data because of their little capacity or flexibility. Under-fit models are not
of the right class, and they suffer from large bias. In practice, of course,
it is generally impossible to know the true class of model for the target
function. How can we really know that <span class="math inline">\(f(x)\)</span> is a cubic model, or that
it is polynomial of a certain degree? Well, we can’t. However, we can get an
idea of the complexity of our hypothesis models. In general, low complexity
models tend to be biased, and viceversa, with more complex models the amount of
bias decreases.</p>
<p>The important thing, at least in theory, is that if none of the proposed
hypothesized models contain the truth, they will be biased. Keep in mind that
the amount of bias does not depend on the size of the in-sample set. This means
that increasing the number of learning points won’t give you a better chance to
approximate <span class="math inline">\(f()\)</span>.</p>
<p>The thing that does depend on the number of learning points is the variance
of the model. As <span class="math inline">\(n\)</span> increases, large-capacity models will experience a
reduction in variability, and their higher flexibility tends to become an
advantage.</p>
<p>There is nothing remarkable about polynomials here. All of the same comments
and remarks apply to other flexible classes of models, such as non-parametric
regression models (e.g. kernel models), k-nearest neighbors, locally weighted
regressions, dimension reduction techniques, penalized models, etc.</p>
</div>
</div>
<div id="in-summary" class="section level2">
<h2><span class="header-section-number">11.3</span> In Summary</h2>
<p>We have three characteristic situations:</p>
<ul>
<li><p>Underfitting: when the model has limited learning capacity, and gets only the
“general ideal”.</p></li>
<li><p>Overfitting: when the model shows good performance in-sample, but when applied
to out-of-sample data, it performs poorly.</p></li>
<li><p>Okayfitting: when the model reaches the “ritgh amount” of learning capacity,
with good performance both in-sample as well as out-of-sample.</p></li>
</ul>
<p>Overfitting is not just “bad generalization”, it is more complex than that.
Think of overfitting as a phenomenon or process in which as the complexity or
flexibility of the model increases, the in-sample error decreases while
the out-of-sample error increases. The trick is to find the sweet spot of
adequate complexity such that both in-sample and out-of-sample errors have an
okay low performance, as depicted in the following diagram.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-8"></span>
<img src="images/theory/underfit-overfit1.svg" alt="Moving from underfitting to okayfitting to overfitting" width="85%" />
<p class="caption">
Figure 11.8: Moving from underfitting to okayfitting to overfitting
</p>
</div>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="biasvar.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="phases.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["allmodelsarewrong.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
