<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4 Principal Components Analysis | All Models Are Wrong</title>
  <meta name="description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  <meta name="generator" content="bookdown 0.16 and GitBook 2.6.7" />

  <meta property="og:title" content="4 Principal Components Analysis | All Models Are Wrong" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://allmodelsarewrong.github.io" />
  
  <meta property="og:description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  <meta name="github-repo" content="allmodelsarewrong/allmodelsarewrong.github.io" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4 Principal Components Analysis | All Models Are Wrong" />
  
  <meta name="twitter:description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  

<meta name="author" content="by Gaston Sanchez, and Ethan Marzban" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="duality.html"/>
<link rel="next" href="ols.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><b>All Models Are Wrong (CSL)</b><br><small>by G. Sanchez & E. Marzban</small></a></li>

<li class="divider"></li>
<li class="part"><span><b>I Welcome</b></span></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="about.html"><a href="about.html"><i class="fa fa-check"></i><b>1</b> About this book</a><ul>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#prerequisites"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="part"><span><b>II Intro</b></span></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#basic-notation"><i class="fa fa-check"></i><b>2.1</b> Basic Notation</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="duality.html"><a href="duality.html"><i class="fa fa-check"></i><b>3</b> Geometric Duality</a><ul>
<li class="chapter" data-level="3.1" data-path="duality.html"><a href="duality.html#rows-space"><i class="fa fa-check"></i><b>3.1</b> Rows Space</a></li>
<li class="chapter" data-level="3.2" data-path="duality.html"><a href="duality.html#columns-space"><i class="fa fa-check"></i><b>3.2</b> Columns Space</a></li>
<li class="chapter" data-level="3.3" data-path="duality.html"><a href="duality.html#cloud-of-individuals"><i class="fa fa-check"></i><b>3.3</b> Cloud of Individuals</a><ul>
<li class="chapter" data-level="3.3.1" data-path="duality.html"><a href="duality.html#average-individual"><i class="fa fa-check"></i><b>3.3.1</b> Average Individual</a></li>
<li class="chapter" data-level="3.3.2" data-path="duality.html"><a href="duality.html#centered-data"><i class="fa fa-check"></i><b>3.3.2</b> Centered Data</a></li>
<li class="chapter" data-level="3.3.3" data-path="duality.html"><a href="duality.html#distance-between-individuals"><i class="fa fa-check"></i><b>3.3.3</b> Distance between individuals</a></li>
<li class="chapter" data-level="3.3.4" data-path="duality.html"><a href="duality.html#distance-to-the-centroid"><i class="fa fa-check"></i><b>3.3.4</b> Distance to the centroid</a></li>
<li class="chapter" data-level="3.3.5" data-path="duality.html"><a href="duality.html#measures-of-dispersion"><i class="fa fa-check"></i><b>3.3.5</b> Measures of Dispersion</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="duality.html"><a href="duality.html#cloud-of-variables"><i class="fa fa-check"></i><b>3.4</b> Cloud of Variables</a><ul>
<li class="chapter" data-level="3.4.1" data-path="duality.html"><a href="duality.html#mean-of-a-variable"><i class="fa fa-check"></i><b>3.4.1</b> Mean of a Variable</a></li>
<li class="chapter" data-level="3.4.2" data-path="duality.html"><a href="duality.html#variance-of-a-variable"><i class="fa fa-check"></i><b>3.4.2</b> Variance of a Variable</a></li>
<li class="chapter" data-level="3.4.3" data-path="duality.html"><a href="duality.html#variance-with-vector-notation"><i class="fa fa-check"></i><b>3.4.3</b> Variance with Vector Notation</a></li>
<li class="chapter" data-level="3.4.4" data-path="duality.html"><a href="duality.html#standard-deviation-as-a-norm"><i class="fa fa-check"></i><b>3.4.4</b> Standard Deviation as a Norm</a></li>
<li class="chapter" data-level="3.4.5" data-path="duality.html"><a href="duality.html#covariance"><i class="fa fa-check"></i><b>3.4.5</b> Covariance</a></li>
<li class="chapter" data-level="3.4.6" data-path="duality.html"><a href="duality.html#correlation"><i class="fa fa-check"></i><b>3.4.6</b> Correlation</a></li>
<li class="chapter" data-level="3.4.7" data-path="duality.html"><a href="duality.html#geometry-of-correlation"><i class="fa fa-check"></i><b>3.4.7</b> Geometry of Correlation</a></li>
<li class="chapter" data-level="3.4.8" data-path="duality.html"><a href="duality.html#orthogonal-projections"><i class="fa fa-check"></i><b>3.4.8</b> Orthogonal Projections</a></li>
<li class="chapter" data-level="3.4.9" data-path="duality.html"><a href="duality.html#the-mean-as-an-orthogonal-projection"><i class="fa fa-check"></i><b>3.4.9</b> The mean as an orthogonal projection</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Unsupervised I: PCA</b></span></li>
<li class="chapter" data-level="4" data-path="pca.html"><a href="pca.html"><i class="fa fa-check"></i><b>4</b> Principal Components Analysis</a><ul>
<li class="chapter" data-level="4.1" data-path="pca.html"><a href="pca.html#low-dimensional-representations"><i class="fa fa-check"></i><b>4.1</b> Low-dimensional Representations</a></li>
<li class="chapter" data-level="4.2" data-path="pca.html"><a href="pca.html#projections"><i class="fa fa-check"></i><b>4.2</b> Projections</a><ul>
<li class="chapter" data-level="4.2.1" data-path="pca.html"><a href="pca.html#vector-and-scalar-projections"><i class="fa fa-check"></i><b>4.2.1</b> Vector and Scalar Projections</a></li>
<li class="chapter" data-level="4.2.2" data-path="pca.html"><a href="pca.html#projected-inertia"><i class="fa fa-check"></i><b>4.2.2</b> Projected Inertia</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="pca.html"><a href="pca.html#maximization-problem"><i class="fa fa-check"></i><b>4.3</b> Maximization Problem</a><ul>
<li class="chapter" data-level="4.3.1" data-path="pca.html"><a href="pca.html#eigenvectors-of-mathbfs"><i class="fa fa-check"></i><b>4.3.1</b> Eigenvectors of <span class="math inline">\(\mathbf{S}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="pca.html"><a href="pca.html#another-perspective-of-pca"><i class="fa fa-check"></i><b>4.4</b> Another Perspective of PCA</a><ul>
<li class="chapter" data-level="4.4.1" data-path="pca.html"><a href="pca.html#finding-principal-components"><i class="fa fa-check"></i><b>4.4.1</b> Finding Principal Components</a></li>
<li class="chapter" data-level="4.4.2" data-path="pca.html"><a href="pca.html#finding-the-first-pc"><i class="fa fa-check"></i><b>4.4.2</b> Finding the first PC</a></li>
<li class="chapter" data-level="4.4.3" data-path="pca.html"><a href="pca.html#finding-the-second-pc"><i class="fa fa-check"></i><b>4.4.3</b> Finding the second PC</a></li>
<li class="chapter" data-level="4.4.4" data-path="pca.html"><a href="pca.html#finding-all-pcs"><i class="fa fa-check"></i><b>4.4.4</b> Finding all PCs</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="pca.html"><a href="pca.html#data-decomposition-model"><i class="fa fa-check"></i><b>4.5</b> Data Decomposition Model</a><ul>
<li class="chapter" data-level="4.5.1" data-path="pca.html"><a href="pca.html#alternative-approaches"><i class="fa fa-check"></i><b>4.5.1</b> Alternative Approaches</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Linear Regression</b></span></li>
<li class="chapter" data-level="5" data-path="ols.html"><a href="ols.html"><i class="fa fa-check"></i><b>5</b> Linear Regression</a><ul>
<li class="chapter" data-level="5.1" data-path="ols.html"><a href="ols.html#motivation"><i class="fa fa-check"></i><b>5.1</b> Motivation</a></li>
<li class="chapter" data-level="5.2" data-path="ols.html"><a href="ols.html#the-ideaintuition-of-regression"><i class="fa fa-check"></i><b>5.2</b> The Idea/Intuition of Regression</a></li>
<li class="chapter" data-level="5.3" data-path="ols.html"><a href="ols.html#the-linear-regression-model"><i class="fa fa-check"></i><b>5.3</b> The Linear Regression Model</a></li>
<li class="chapter" data-level="5.4" data-path="ols.html"><a href="ols.html#the-error-measure"><i class="fa fa-check"></i><b>5.4</b> The Error Measure</a></li>
<li class="chapter" data-level="5.5" data-path="ols.html"><a href="ols.html#the-least-squares-algorithm"><i class="fa fa-check"></i><b>5.5</b> The Least Squares Algorithm</a></li>
<li class="chapter" data-level="5.6" data-path="ols.html"><a href="ols.html#geometries-of-ols"><i class="fa fa-check"></i><b>5.6</b> Geometries of OLS</a><ul>
<li class="chapter" data-level="5.6.1" data-path="ols.html"><a href="ols.html#rows-perspective"><i class="fa fa-check"></i><b>5.6.1</b> Rows Perspective</a></li>
<li class="chapter" data-level="5.6.2" data-path="ols.html"><a href="ols.html#columns-perspective"><i class="fa fa-check"></i><b>5.6.2</b> Columns Perspective</a></li>
<li class="chapter" data-level="5.6.3" data-path="ols.html"><a href="ols.html#parameters-perspective"><i class="fa fa-check"></i><b>5.6.3</b> Parameters Perspective</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="gradient.html"><a href="gradient.html"><i class="fa fa-check"></i><b>6</b> Gradient Descent</a><ul>
<li class="chapter" data-level="6.1" data-path="gradient.html"><a href="gradient.html#error-surface"><i class="fa fa-check"></i><b>6.1</b> Error Surface</a></li>
<li class="chapter" data-level="6.2" data-path="gradient.html"><a href="gradient.html#idea-of-gradient-descent"><i class="fa fa-check"></i><b>6.2</b> Idea of Gradient Descent</a></li>
<li class="chapter" data-level="6.3" data-path="gradient.html"><a href="gradient.html#moving-down-an-error-surface"><i class="fa fa-check"></i><b>6.3</b> Moving Down an Error Surface</a><ul>
<li class="chapter" data-level="6.3.1" data-path="gradient.html"><a href="gradient.html#the-direction-of-mathbfv"><i class="fa fa-check"></i><b>6.3.1</b> The direction of <span class="math inline">\(\mathbf{v}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="gradient.html"><a href="gradient.html#gradient-descent-and-our-model"><i class="fa fa-check"></i><b>6.4</b> Gradient Descent and our Model</a></li>
</ul></li>
<li class="part"><span><b>V Learning Concepts</b></span></li>
<li class="chapter" data-level="7" data-path="learning.html"><a href="learning.html"><i class="fa fa-check"></i><b>7</b> Theoretical Framework</a><ul>
<li class="chapter" data-level="7.1" data-path="learning.html"><a href="learning.html#mental-map"><i class="fa fa-check"></i><b>7.1</b> Mental Map</a></li>
<li class="chapter" data-level="7.2" data-path="learning.html"><a href="learning.html#kinds-of-predictions"><i class="fa fa-check"></i><b>7.2</b> Kinds of Predictions</a><ul>
<li class="chapter" data-level="7.2.1" data-path="learning.html"><a href="learning.html#two-types-of-data"><i class="fa fa-check"></i><b>7.2.1</b> Two Types of Data</a></li>
<li class="chapter" data-level="7.2.2" data-path="learning.html"><a href="learning.html#two-types-of-predictions"><i class="fa fa-check"></i><b>7.2.2</b> Two Types of Predictions</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="learning.html"><a href="learning.html#two-types-of-errors"><i class="fa fa-check"></i><b>7.3</b> Two Types of Errors</a><ul>
<li class="chapter" data-level="7.3.1" data-path="learning.html"><a href="learning.html#individual-errors"><i class="fa fa-check"></i><b>7.3.1</b> Individual Errors</a></li>
<li class="chapter" data-level="7.3.2" data-path="learning.html"><a href="learning.html#overall-errors"><i class="fa fa-check"></i><b>7.3.2</b> Overall Errors</a></li>
<li class="chapter" data-level="7.3.3" data-path="learning.html"><a href="learning.html#auxiliary-technicality"><i class="fa fa-check"></i><b>7.3.3</b> Auxiliary Technicality</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="learning.html"><a href="learning.html#noisy-targets"><i class="fa fa-check"></i><b>7.4</b> Noisy Targets</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="mse.html"><a href="mse.html"><i class="fa fa-check"></i><b>8</b> MSE of Estimator</a><ul>
<li class="chapter" data-level="8.1" data-path="mse.html"><a href="mse.html#mse-of-an-estimator"><i class="fa fa-check"></i><b>8.1</b> MSE of an Estimator</a><ul>
<li class="chapter" data-level="8.1.1" data-path="mse.html"><a href="mse.html#prototypical-cases-of-bias-and-variance"><i class="fa fa-check"></i><b>8.1.1</b> Prototypical Cases of Bias and Variance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="biasvar.html"><a href="biasvar.html"><i class="fa fa-check"></i><b>9</b> Bias-Variance Tradeoff</a><ul>
<li class="chapter" data-level="9.1" data-path="biasvar.html"><a href="biasvar.html#introduction"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="biasvar.html"><a href="biasvar.html#motivation-example"><i class="fa fa-check"></i><b>9.2</b> Motivation Example</a><ul>
<li class="chapter" data-level="9.2.1" data-path="biasvar.html"><a href="biasvar.html#two-hypotheses"><i class="fa fa-check"></i><b>9.2.1</b> Two Hypotheses</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="biasvar.html"><a href="biasvar.html#learning-from-two-points"><i class="fa fa-check"></i><b>9.3</b> Learning from two points</a></li>
<li class="chapter" data-level="9.4" data-path="biasvar.html"><a href="biasvar.html#bias-variance-derivation"><i class="fa fa-check"></i><b>9.4</b> Bias-Variance Derivation</a><ul>
<li class="chapter" data-level="9.4.1" data-path="biasvar.html"><a href="biasvar.html#out-of-sample-predictions"><i class="fa fa-check"></i><b>9.4.1</b> Out-of-Sample Predictions</a></li>
<li class="chapter" data-level="9.4.2" data-path="biasvar.html"><a href="biasvar.html#noisy-target"><i class="fa fa-check"></i><b>9.4.2</b> Noisy Target</a></li>
<li class="chapter" data-level="9.4.3" data-path="biasvar.html"><a href="biasvar.html#types-of-theoretical-mses"><i class="fa fa-check"></i><b>9.4.3</b> Types of Theoretical MSEs</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="biasvar.html"><a href="biasvar.html#the-tradeoff"><i class="fa fa-check"></i><b>9.5</b> The Tradeoff</a><ul>
<li class="chapter" data-level="9.5.1" data-path="biasvar.html"><a href="biasvar.html#bias-variance-tradeoff-picture"><i class="fa fa-check"></i><b>9.5.1</b> Bias-Variance Tradeoff Picture</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="phases.html"><a href="phases.html"><i class="fa fa-check"></i><b>10</b> Learning Phases</a><ul>
<li class="chapter" data-level="10.1" data-path="phases.html"><a href="phases.html#introduction-1"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="phases.html"><a href="phases.html#model-assessment"><i class="fa fa-check"></i><b>10.2</b> Model Assessment</a><ul>
<li class="chapter" data-level="10.2.1" data-path="phases.html"><a href="phases.html#holdout-test-set"><i class="fa fa-check"></i><b>10.2.1</b> Holdout Test Set</a></li>
<li class="chapter" data-level="10.2.2" data-path="phases.html"><a href="phases.html#why-does-a-test-set-work"><i class="fa fa-check"></i><b>10.2.2</b> Why does a test set work?</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="phases.html"><a href="phases.html#model-selection"><i class="fa fa-check"></i><b>10.3</b> Model Selection</a><ul>
<li class="chapter" data-level="10.3.1" data-path="phases.html"><a href="phases.html#three-way-holdout-method"><i class="fa fa-check"></i><b>10.3.1</b> Three-way Holdout Method</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="phases.html"><a href="phases.html#model-training"><i class="fa fa-check"></i><b>10.4</b> Model Training</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="resampling.html"><a href="resampling.html"><i class="fa fa-check"></i><b>11</b> Resample Approaches</a><ul>
<li class="chapter" data-level="11.1" data-path="resampling.html"><a href="resampling.html#general-sampling-blueprint"><i class="fa fa-check"></i><b>11.1</b> General Sampling Blueprint</a></li>
<li class="chapter" data-level="11.2" data-path="resampling.html"><a href="resampling.html#monte-carlo-cross-validation"><i class="fa fa-check"></i><b>11.2</b> Monte Carlo Cross Validation</a></li>
<li class="chapter" data-level="11.3" data-path="resampling.html"><a href="resampling.html#bootstrap-method"><i class="fa fa-check"></i><b>11.3</b> Bootstrap Method</a></li>
<li class="chapter" data-level="11.4" data-path="resampling.html"><a href="resampling.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>11.4</b> <span class="math inline">\(K\)</span>-Fold Cross-Validation</a><ul>
<li class="chapter" data-level="11.4.1" data-path="resampling.html"><a href="resampling.html#leave-one-out-cross-validation-loocv"><i class="fa fa-check"></i><b>11.4.1</b> Leave-One-Out Cross Validation (LOOCV)</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>VI Regularization</b></span></li>
<li class="chapter" data-level="12" data-path="regular.html"><a href="regular.html"><i class="fa fa-check"></i><b>12</b> Regularization Techniques</a><ul>
<li class="chapter" data-level="12.1" data-path="regular.html"><a href="regular.html#multicollinearity-issues"><i class="fa fa-check"></i><b>12.1</b> Multicollinearity Issues</a><ul>
<li class="chapter" data-level="12.1.1" data-path="regular.html"><a href="regular.html#toy-example"><i class="fa fa-check"></i><b>12.1.1</b> Toy Example</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="regular.html"><a href="regular.html#irregular-coefficients"><i class="fa fa-check"></i><b>12.2</b> Irregular Coefficients</a></li>
<li class="chapter" data-level="12.3" data-path="regular.html"><a href="regular.html#connection-to-regularization"><i class="fa fa-check"></i><b>12.3</b> Connection to Regularization</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="pcr.html"><a href="pcr.html"><i class="fa fa-check"></i><b>13</b> Principal Components Regression</a><ul>
<li class="chapter" data-level="13.1" data-path="pcr.html"><a href="pcr.html#motivation-example-1"><i class="fa fa-check"></i><b>13.1</b> Motivation Example</a></li>
<li class="chapter" data-level="13.2" data-path="pcr.html"><a href="pcr.html#the-pcr-model"><i class="fa fa-check"></i><b>13.2</b> The PCR Model</a></li>
<li class="chapter" data-level="13.3" data-path="pcr.html"><a href="pcr.html#how-does-pcr-work"><i class="fa fa-check"></i><b>13.3</b> How does PCR work?</a><ul>
<li class="chapter" data-level="13.3.1" data-path="pcr.html"><a href="pcr.html#transition-formula"><i class="fa fa-check"></i><b>13.3.1</b> Transition Formula</a></li>
<li class="chapter" data-level="13.3.2" data-path="pcr.html"><a href="pcr.html#size-of-coefficients"><i class="fa fa-check"></i><b>13.3.2</b> Size of Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="pcr.html"><a href="pcr.html#selecting-number-of-pcs"><i class="fa fa-check"></i><b>13.4</b> Selecting Number of PCs</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="pls.html"><a href="pls.html"><i class="fa fa-check"></i><b>14</b> Partial Least Squares Regression</a><ul>
<li class="chapter" data-level="14.1" data-path="pls.html"><a href="pls.html#motivation-example-2"><i class="fa fa-check"></i><b>14.1</b> Motivation Example</a></li>
<li class="chapter" data-level="14.2" data-path="pls.html"><a href="pls.html#the-plsr-model"><i class="fa fa-check"></i><b>14.2</b> The PLSR Model</a></li>
<li class="chapter" data-level="14.3" data-path="pls.html"><a href="pls.html#how-does-plsr-work"><i class="fa fa-check"></i><b>14.3</b> How does PLSR work?</a></li>
<li class="chapter" data-level="14.4" data-path="pls.html"><a href="pls.html#plsr-algorithm"><i class="fa fa-check"></i><b>14.4</b> PLSR Algorithm</a><ul>
<li class="chapter" data-level="14.4.1" data-path="pls.html"><a href="pls.html#pls-solution-with-original-variables"><i class="fa fa-check"></i><b>14.4.1</b> PLS Solution with original variables</a></li>
<li class="chapter" data-level="14.4.2" data-path="pls.html"><a href="pls.html#size-of-coefficients-1"><i class="fa fa-check"></i><b>14.4.2</b> Size of Coefficients</a></li>
<li class="chapter" data-level="14.4.3" data-path="pls.html"><a href="pls.html#some-properties"><i class="fa fa-check"></i><b>14.4.3</b> Some Properties</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="pls.html"><a href="pls.html#selecting-number-of-pls-components"><i class="fa fa-check"></i><b>14.5</b> Selecting Number of PLS Components</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="ridge.html"><a href="ridge.html"><i class="fa fa-check"></i><b>15</b> Ridge Regression</a><ul>
<li class="chapter" data-level="15.1" data-path="ridge.html"><a href="ridge.html#a-new-minimization-problem"><i class="fa fa-check"></i><b>15.1</b> A New Minimization Problem</a><ul>
<li class="chapter" data-level="15.1.1" data-path="ridge.html"><a href="ridge.html#constraining-regression-coefficients"><i class="fa fa-check"></i><b>15.1.1</b> Constraining Regression Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="ridge.html"><a href="ridge.html#a-new-minimization-solution"><i class="fa fa-check"></i><b>15.2</b> A New Minimization Solution</a></li>
<li class="chapter" data-level="15.3" data-path="ridge.html"><a href="ridge.html#what-does-rr-accomplish"><i class="fa fa-check"></i><b>15.3</b> What does RR accomplish?</a><ul>
<li class="chapter" data-level="15.3.1" data-path="ridge.html"><a href="ridge.html#how-to-find-lambda"><i class="fa fa-check"></i><b>15.3.1</b> How to find <span class="math inline">\(\lambda\)</span>?</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>VII Extending Linear Regression</b></span></li>
<li class="chapter" data-level="16" data-path="linear-extensions.html"><a href="linear-extensions.html"><i class="fa fa-check"></i><b>16</b> Beyond Linear Regression</a><ul>
<li class="chapter" data-level="16.1" data-path="linear-extensions.html"><a href="linear-extensions.html#introduction-2"><i class="fa fa-check"></i><b>16.1</b> Introduction</a></li>
<li class="chapter" data-level="16.2" data-path="linear-extensions.html"><a href="linear-extensions.html#expanding-the-regression-horizon"><i class="fa fa-check"></i><b>16.2</b> Expanding the Regression Horizon</a><ul>
<li class="chapter" data-level="16.2.1" data-path="linear-extensions.html"><a href="linear-extensions.html#linearity"><i class="fa fa-check"></i><b>16.2.1</b> Linearity</a></li>
<li class="chapter" data-level="16.2.2" data-path="linear-extensions.html"><a href="linear-extensions.html#parametric-and-nonparametric"><i class="fa fa-check"></i><b>16.2.2</b> Parametric and Nonparametric</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="linear-extensions.html"><a href="linear-extensions.html#transforming-features"><i class="fa fa-check"></i><b>16.3</b> Transforming Features</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="nonparametric.html"><a href="nonparametric.html"><i class="fa fa-check"></i><b>17</b> Nonparametric Regression</a><ul>
<li class="chapter" data-level="17.1" data-path="nonparametric.html"><a href="nonparametric.html#conditional-averages"><i class="fa fa-check"></i><b>17.1</b> Conditional Averages</a></li>
<li class="chapter" data-level="17.2" data-path="nonparametric.html"><a href="nonparametric.html#looking-at-the-neighbors"><i class="fa fa-check"></i><b>17.2</b> Looking at the Neighbors</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="knn.html"><a href="knn.html"><i class="fa fa-check"></i><b>18</b> Nearest Neighbor Estimates</a><ul>
<li class="chapter" data-level="18.1" data-path="knn.html"><a href="knn.html#introduction-3"><i class="fa fa-check"></i><b>18.1</b> Introduction</a></li>
<li class="chapter" data-level="18.2" data-path="knn.html"><a href="knn.html#k-nearest-neighbors-knn"><i class="fa fa-check"></i><b>18.2</b> <span class="math inline">\(k\)</span> Nearest Neighbors (KNN)</a><ul>
<li class="chapter" data-level="18.2.1" data-path="knn.html"><a href="knn.html#distance-measures"><i class="fa fa-check"></i><b>18.2.1</b> Distance Measures</a></li>
<li class="chapter" data-level="18.2.2" data-path="knn.html"><a href="knn.html#knn-estimator"><i class="fa fa-check"></i><b>18.2.2</b> KNN Estimator</a></li>
<li class="chapter" data-level="18.2.3" data-path="knn.html"><a href="knn.html#how-to-find-k"><i class="fa fa-check"></i><b>18.2.3</b> How to find <span class="math inline">\(k\)</span>?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html"><i class="fa fa-check"></i><b>19</b> Kernel Smoothers</a><ul>
<li class="chapter" data-level="19.1" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#introduction-4"><i class="fa fa-check"></i><b>19.1</b> Introduction</a></li>
<li class="chapter" data-level="19.2" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#kernel-smoothers-1"><i class="fa fa-check"></i><b>19.2</b> Kernel Smoothers</a><ul>
<li class="chapter" data-level="19.2.1" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#kernel-functions"><i class="fa fa-check"></i><b>19.2.1</b> Kernel Functions</a></li>
<li class="chapter" data-level="19.2.2" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#weights-from-kernels"><i class="fa fa-check"></i><b>19.2.2</b> Weights from Kernels</a></li>
<li class="chapter" data-level="19.2.3" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#kernel-estimator"><i class="fa fa-check"></i><b>19.2.3</b> Kernel Estimator</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#local-polynomial-estimators"><i class="fa fa-check"></i><b>19.3</b> Local Polynomial Estimators</a></li>
</ul></li>
<li class="part"><span><b>VIII Classification</b></span></li>
<li class="chapter" data-level="20" data-path="classif.html"><a href="classif.html"><i class="fa fa-check"></i><b>20</b> Classification Methods</a></li>
<li class="chapter" data-level="21" data-path="logistic.html"><a href="logistic.html"><i class="fa fa-check"></i><b>21</b> Logistic Regression</a><ul>
<li class="chapter" data-level="21.1" data-path="logistic.html"><a href="logistic.html#motivation-1"><i class="fa fa-check"></i><b>21.1</b> Motivation</a><ul>
<li class="chapter" data-level="21.1.1" data-path="logistic.html"><a href="logistic.html#first-approach-fitting-a-line"><i class="fa fa-check"></i><b>21.1.1</b> First Approach: Fitting a Line</a></li>
<li class="chapter" data-level="21.1.2" data-path="logistic.html"><a href="logistic.html#second-approach-harsh-thresholding"><i class="fa fa-check"></i><b>21.1.2</b> Second Approach: Harsh Thresholding</a></li>
<li class="chapter" data-level="21.1.3" data-path="logistic.html"><a href="logistic.html#third-approach-conditional-means"><i class="fa fa-check"></i><b>21.1.3</b> Third Approach: Conditional Means</a></li>
</ul></li>
<li class="chapter" data-level="21.2" data-path="logistic.html"><a href="logistic.html#logistic-regression-model"><i class="fa fa-check"></i><b>21.2</b> Logistic Regression Model</a><ul>
<li class="chapter" data-level="21.2.1" data-path="logistic.html"><a href="logistic.html#the-criterion-being-optimized"><i class="fa fa-check"></i><b>21.2.1</b> The Criterion Being Optimized</a></li>
<li class="chapter" data-level="21.2.2" data-path="logistic.html"><a href="logistic.html#another-way-to-solve-logistic-regression"><i class="fa fa-check"></i><b>21.2.2</b> Another Way to Solve Logistic Regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="22" data-path="discrim.html"><a href="discrim.html"><i class="fa fa-check"></i><b>22</b> Preamble for Discriminant Analysis</a><ul>
<li class="chapter" data-level="22.1" data-path="discrim.html"><a href="discrim.html#motivation-2"><i class="fa fa-check"></i><b>22.1</b> Motivation</a><ul>
<li class="chapter" data-level="22.1.1" data-path="discrim.html"><a href="discrim.html#distinguishing-species"><i class="fa fa-check"></i><b>22.1.1</b> Distinguishing Species</a></li>
<li class="chapter" data-level="22.1.2" data-path="discrim.html"><a href="discrim.html#sum-of-squares-decomposition"><i class="fa fa-check"></i><b>22.1.2</b> Sum of Squares Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="22.2" data-path="discrim.html"><a href="discrim.html#derived-ratios-from-sum-of-squares"><i class="fa fa-check"></i><b>22.2</b> Derived Ratios from Sum-of-Squares</a><ul>
<li class="chapter" data-level="22.2.1" data-path="discrim.html"><a href="discrim.html#correlation-ratio"><i class="fa fa-check"></i><b>22.2.1</b> Correlation Ratio</a></li>
<li class="chapter" data-level="22.2.2" data-path="discrim.html"><a href="discrim.html#f-ratio"><i class="fa fa-check"></i><b>22.2.2</b> F-Ratio</a></li>
</ul></li>
<li class="chapter" data-level="22.3" data-path="discrim.html"><a href="discrim.html#geometric-perspective"><i class="fa fa-check"></i><b>22.3</b> Geometric Perspective</a><ul>
<li class="chapter" data-level="22.3.1" data-path="discrim.html"><a href="discrim.html#clouds-from-class-structure"><i class="fa fa-check"></i><b>22.3.1</b> Clouds from Class Structure</a></li>
<li class="chapter" data-level="22.3.2" data-path="discrim.html"><a href="discrim.html#dispersion-decomposition"><i class="fa fa-check"></i><b>22.3.2</b> Dispersion Decomposition</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="23" data-path="cda.html"><a href="cda.html"><i class="fa fa-check"></i><b>23</b> Canonical Discriminant Analysis</a><ul>
<li class="chapter" data-level="23.1" data-path="cda.html"><a href="cda.html#cda-semi-supervised-aspect"><i class="fa fa-check"></i><b>23.1</b> CDA: Semi-Supervised Aspect</a></li>
<li class="chapter" data-level="23.2" data-path="cda.html"><a href="cda.html#looking-for-a-discriminant-axis"><i class="fa fa-check"></i><b>23.2</b> Looking for a discriminant axis</a></li>
<li class="chapter" data-level="23.3" data-path="cda.html"><a href="cda.html#looking-for-a-compromise-criterion"><i class="fa fa-check"></i><b>23.3</b> Looking for a Compromise Criterion</a><ul>
<li class="chapter" data-level="23.3.1" data-path="cda.html"><a href="cda.html#correlation-ratio-criterion"><i class="fa fa-check"></i><b>23.3.1</b> Correlation Ratio Criterion</a></li>
<li class="chapter" data-level="23.3.2" data-path="cda.html"><a href="cda.html#f-ratio-criterion"><i class="fa fa-check"></i><b>23.3.2</b> F-ratio Criterion</a></li>
</ul></li>
<li class="chapter" data-level="23.4" data-path="cda.html"><a href="cda.html#cda-supervised-aspect"><i class="fa fa-check"></i><b>23.4</b> CDA: Supervised Aspect</a><ul>
<li class="chapter" data-level="23.4.1" data-path="cda.html"><a href="cda.html#a-special-pca"><i class="fa fa-check"></i><b>23.4.1</b> A Special PCA</a></li>
<li class="chapter" data-level="23.4.2" data-path="cda.html"><a href="cda.html#predictive-idea"><i class="fa fa-check"></i><b>23.4.2</b> Predictive Idea</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="24" data-path="discanalysis.html"><a href="discanalysis.html"><i class="fa fa-check"></i><b>24</b> Discriminant Analysis</a><ul>
<li class="chapter" data-level="24.1" data-path="discanalysis.html"><a href="discanalysis.html#probabilistic-da"><i class="fa fa-check"></i><b>24.1</b> Probabilistic DA</a></li>
<li class="chapter" data-level="24.2" data-path="discanalysis.html"><a href="discanalysis.html#estimations"><i class="fa fa-check"></i><b>24.2</b> Estimations</a><ul>
<li class="chapter" data-level="24.2.1" data-path="discanalysis.html"><a href="discanalysis.html#univariate-data-boldsymbolp-1"><i class="fa fa-check"></i><b>24.2.1</b> Univariate Data (<span class="math inline">\(\boldsymbol{p = 1}\)</span>)</a></li>
<li class="chapter" data-level="24.2.2" data-path="discanalysis.html"><a href="discanalysis.html#multivariate-data-boldsymbolp-1"><i class="fa fa-check"></i><b>24.2.2</b> Multivariate Data (<span class="math inline">\(\boldsymbol{p &gt; 1}\)</span>)</a></li>
<li class="chapter" data-level="24.2.3" data-path="discanalysis.html"><a href="discanalysis.html#multivariate-estimations"><i class="fa fa-check"></i><b>24.2.3</b> Multivariate Estimations</a></li>
</ul></li>
<li class="chapter" data-level="24.3" data-path="discanalysis.html"><a href="discanalysis.html#discriminant-functions"><i class="fa fa-check"></i><b>24.3</b> Discriminant Functions</a></li>
<li class="chapter" data-level="24.4" data-path="discanalysis.html"><a href="discanalysis.html#quadratic-discriminant-analysis-qda"><i class="fa fa-check"></i><b>24.4</b> Quadratic Discriminant Analysis (QDA)</a></li>
<li class="chapter" data-level="24.5" data-path="discanalysis.html"><a href="discanalysis.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>24.5</b> Linear Discriminant Analysis</a></li>
<li class="chapter" data-level="24.6" data-path="discanalysis.html"><a href="discanalysis.html#canonical-discriminant-analysis"><i class="fa fa-check"></i><b>24.6</b> Canonical Discriminant Analysis</a></li>
<li class="chapter" data-level="24.7" data-path="discanalysis.html"><a href="discanalysis.html#naive-bayes"><i class="fa fa-check"></i><b>24.7</b> Naive Bayes</a></li>
<li class="chapter" data-level="24.8" data-path="discanalysis.html"><a href="discanalysis.html#fifth-case"><i class="fa fa-check"></i><b>24.8</b> Fifth Case</a></li>
<li class="chapter" data-level="24.9" data-path="discanalysis.html"><a href="discanalysis.html#comparing-the-cases"><i class="fa fa-check"></i><b>24.9</b> Comparing the Cases</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="classperformance.html"><a href="classperformance.html"><i class="fa fa-check"></i><b>25</b> Performance of Classifiers</a><ul>
<li class="chapter" data-level="25.1" data-path="classperformance.html"><a href="classperformance.html#error-for-binary-response"><i class="fa fa-check"></i><b>25.1</b> Error for Binary Response</a></li>
<li class="chapter" data-level="25.2" data-path="classperformance.html"><a href="classperformance.html#categorical-response"><i class="fa fa-check"></i><b>25.2</b> Categorical Response</a></li>
<li class="chapter" data-level="25.3" data-path="classperformance.html"><a href="classperformance.html#confusion-matrices"><i class="fa fa-check"></i><b>25.3</b> Confusion Matrices</a></li>
<li class="chapter" data-level="25.4" data-path="classperformance.html"><a href="classperformance.html#binary-response-example"><i class="fa fa-check"></i><b>25.4</b> Binary Response: Example</a><ul>
<li class="chapter" data-level="25.4.1" data-path="classperformance.html"><a href="classperformance.html#types-of-errors-application-for-savings-account"><i class="fa fa-check"></i><b>25.4.1</b> Types of Errors: Application for Savings Account</a></li>
<li class="chapter" data-level="25.4.2" data-path="classperformance.html"><a href="classperformance.html#another-example-loans"><i class="fa fa-check"></i><b>25.4.2</b> Another Example: Loans</a></li>
<li class="chapter" data-level="25.4.3" data-path="classperformance.html"><a href="classperformance.html#the-ideal-situation"><i class="fa fa-check"></i><b>25.4.3</b> The Ideal Situation</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IX Unsupervised II: Clustering</b></span></li>
<li class="chapter" data-level="26" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>26</b> Clustering</a><ul>
<li class="chapter" data-level="26.1" data-path="clustering.html"><a href="clustering.html#about-clustering"><i class="fa fa-check"></i><b>26.1</b> About Clustering</a><ul>
<li class="chapter" data-level="26.1.1" data-path="clustering.html"><a href="clustering.html#types-of-clustering"><i class="fa fa-check"></i><b>26.1.1</b> Types of Clustering</a></li>
<li class="chapter" data-level="26.1.2" data-path="clustering.html"><a href="clustering.html#hard-clustering"><i class="fa fa-check"></i><b>26.1.2</b> Hard Clustering</a></li>
</ul></li>
<li class="chapter" data-level="26.2" data-path="clustering.html"><a href="clustering.html#dispersion-measures"><i class="fa fa-check"></i><b>26.2</b> Dispersion Measures</a></li>
<li class="chapter" data-level="26.3" data-path="clustering.html"><a href="clustering.html#complexity-in-clustering"><i class="fa fa-check"></i><b>26.3</b> Complexity in Clustering</a></li>
</ul></li>
<li class="chapter" data-level="27" data-path="kmeans.html"><a href="kmeans.html"><i class="fa fa-check"></i><b>27</b> K-Means</a><ul>
<li class="chapter" data-level="27.1" data-path="kmeans.html"><a href="kmeans.html#toy-example-1"><i class="fa fa-check"></i><b>27.1</b> Toy Example</a></li>
<li class="chapter" data-level="27.2" data-path="kmeans.html"><a href="kmeans.html#what-does-k-means-do"><i class="fa fa-check"></i><b>27.2</b> What does K-means do?</a></li>
<li class="chapter" data-level="27.3" data-path="kmeans.html"><a href="kmeans.html#k-means-algorithms"><i class="fa fa-check"></i><b>27.3</b> K-Means Algorithms</a><ul>
<li class="chapter" data-level="27.3.1" data-path="kmeans.html"><a href="kmeans.html#classic-version"><i class="fa fa-check"></i><b>27.3.1</b> Classic Version</a></li>
<li class="chapter" data-level="27.3.2" data-path="kmeans.html"><a href="kmeans.html#moving-centers-algorithm"><i class="fa fa-check"></i><b>27.3.2</b> Moving Centers Algorithm</a></li>
<li class="chapter" data-level="27.3.3" data-path="kmeans.html"><a href="kmeans.html#dynamic-clouds"><i class="fa fa-check"></i><b>27.3.3</b> Dynamic Clouds</a></li>
<li class="chapter" data-level="27.3.4" data-path="kmeans.html"><a href="kmeans.html#choosing-k"><i class="fa fa-check"></i><b>27.3.4</b> Choosing <span class="math inline">\(k\)</span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="28" data-path="hclus.html"><a href="hclus.html"><i class="fa fa-check"></i><b>28</b> Hierarchical Clustering</a><ul>
<li class="chapter" data-level="28.1" data-path="hclus.html"><a href="hclus.html#agglomerative-methods"><i class="fa fa-check"></i><b>28.1</b> Agglomerative Methods</a></li>
<li class="chapter" data-level="28.2" data-path="hclus.html"><a href="hclus.html#example-single-linkage"><i class="fa fa-check"></i><b>28.2</b> Example: Single Linkage</a><ul>
<li class="chapter" data-level="28.2.1" data-path="hclus.html"><a href="hclus.html#dendrogram"><i class="fa fa-check"></i><b>28.2.1</b> Dendrogram</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>X Tree-based Methods</b></span></li>
<li class="chapter" data-level="29" data-path="trees.html"><a href="trees.html"><i class="fa fa-check"></i><b>29</b> Intro to Decision Trees</a><ul>
<li class="chapter" data-level="29.1" data-path="trees.html"><a href="trees.html#introduction-5"><i class="fa fa-check"></i><b>29.1</b> Introduction</a></li>
<li class="chapter" data-level="29.2" data-path="trees.html"><a href="trees.html#binary-trees"><i class="fa fa-check"></i><b>29.2</b> Binary Trees</a><ul>
<li class="chapter" data-level="29.2.1" data-path="trees.html"><a href="trees.html#the-process-of-building-a-tree"><i class="fa fa-check"></i><b>29.2.1</b> The Process of Building a Tree</a></li>
<li class="chapter" data-level="29.2.2" data-path="trees.html"><a href="trees.html#binary-partitions"><i class="fa fa-check"></i><b>29.2.2</b> Binary Partitions</a></li>
</ul></li>
<li class="chapter" data-level="29.3" data-path="trees.html"><a href="trees.html#measures-of-heterogeneity"><i class="fa fa-check"></i><b>29.3</b> Measures of Heterogeneity</a><ul>
<li class="chapter" data-level="29.3.1" data-path="trees.html"><a href="trees.html#entropy"><i class="fa fa-check"></i><b>29.3.1</b> Entropy</a></li>
<li class="chapter" data-level="29.3.2" data-path="trees.html"><a href="trees.html#the-math-behind-entropy"><i class="fa fa-check"></i><b>29.3.2</b> The Math Behind Entropy</a></li>
<li class="chapter" data-level="29.3.3" data-path="trees.html"><a href="trees.html#gini-impurity"><i class="fa fa-check"></i><b>29.3.3</b> Gini Impurity</a></li>
<li class="chapter" data-level="29.3.4" data-path="trees.html"><a href="trees.html#toy-example-2"><i class="fa fa-check"></i><b>29.3.4</b> Toy Example</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Made with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">All Models Are Wrong</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="pca" class="section level1">
<h1><span class="header-section-number">4</span> Principal Components Analysis</h1>
<p>Our first unsupervised method of the book is Principal Components Analysis,
commonly referred to as PCA.</p>
<p>Principal Components Analysis (PCA) is the workhorse method of multivariate data
analysis. Simply put, PCA helps us study and explore a data set of quantitative
variables measured on a set of objects. One way to look at the purpose of principal
components analysis is to get the <em>best</em> low-dimensional representation of the
variation in data. Among the various appealing features of PCA is that it allows
us to obtain a visualization of the objects in order to see their proximities.
Likewise, it also provides us results to get a graphic representation of the
variables in terms of their correlations. Overall, PCA is a multivariate technique
that allows us to summarize the systematic patterns of variations in a data set.</p>
<p>The classic reference for PCA is the work by the eminent British biostatistician
Karl Pearson “On Lines and Planes of Closest Fit to Systems of Points in Space,”
from 1901. This publication presents the PCA problem under a purely geometric
standpoint, describing how to find low-dimensional subspaces that best
fit—in the least squares sense—a cloud of points. The other seminal work of
PCA is the one by the American mathematician and economic theorist Harold Hotelling
with “Analysis of a Complex of Statistical Variables into Principal Components,”
from 1933. Unlike Pearson, Hotelling finds the principal components as
orthogonal linear combinations of the variables of maximum variance.</p>
<p>PCA is one of those methods that can be approached from multiple, seemingly
unrelated, perspectives. The way we are going to introduce PCA is not the typical
way in which PCA is discussed in most books published in English. However, our
introduction is actually based on the ideas and concepts originally published
in <a href="https://en.wikipedia.org/wiki/Karl_Pearson">Karl Pearson’s</a> 1901 paper
<em>On lines and planes of closest fit to systems of points in space</em>. This is what
can be considered to be the first paper on PCA, although keep in mind that
Karl Pearson never used the term <em>principal components analysis</em>. That term was
coined by Harold Hotelling, who formalized the method by giving it a more
mature statistical perspective.</p>
<div id="low-dimensional-representations" class="section level2">
<h2><span class="header-section-number">4.1</span> Low-dimensional Representations</h2>
<p>Let’s play the following game. Imagine for a minute that you have the superpower
to see any type of multidimensional space (not just three-dimensions). As we
mentioned before, we think of the individuals as forming a cloud of points in a
<span class="math inline">\(p\)</span>-dim space, and the variables forming a cloud of arrows in an <span class="math inline">\(n\)</span>-dim space.</p>
<p>Pretend that you have some data in which its cloud of points has the shape of
a mug, like in the following diagram:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-25"></span>
<img src="images/pca/mug-data.png" alt="Cloud of points in the form of a mug" width="30%" />
<p class="caption">
Figure 4.1: Cloud of points in the form of a mug
</p>
</div>
<p>This mug is supposed to be high-dimensional, and something that you are not
supposed to ever see in real life. So the question is: Is there a way in which
we can get a low-dimensional representation of this data?</p>
<p>Luckily, the answer is: YES, we can!</p>
<p>How? Well, the name of the game is <strong>projections</strong>: we can look for projections
of the data into sub-spaces of lower dimension, like in the diagram below.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-26"></span>
<img src="images/pca/mug-projections.png" alt="Various projections onto subspaces" width="70%" />
<p class="caption">
Figure 4.2: Various projections onto subspaces
</p>
</div>
<p>Think of <em>projections</em> as taking photographs or x-rays of the mug. You can take
a photo of the mug from different angles. For instance, you can take a picture
in which the lens
of the camera is on the top of the mug, or another picture in which the lens is
below the mug (from the bottom), and so on.</p>
<p>If you have to take the “best” photograph of the mug, from what angle would
you take such picture? To answer this question we need to be more precise
about what do we mean by “best”. Here, we are talking about getting a picture
in which the image of the mug is as similar as possible to the original object.</p>
<p>As you can tell from the above figure, we have three candidate subspaces:
<span class="math inline">\(\mathbb{H}_A\)</span>, <span class="math inline">\(\mathbb{H}_B\)</span>, and <span class="math inline">\(\mathbb{H}_C\)</span>. Among the three possible
projections, subspace <span class="math inline">\(\mathbb{H}_C\)</span> is the one that provides the best low
dimensional representation, in the sense that the projected silhouette is the
most similar to the original mug shape.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-27"></span>
<img src="images/pca/mug-subspace.png" alt="The shape of the projection is similar to the original mug shape." width="70%" />
<p class="caption">
Figure 4.3: The shape of the projection is similar to the original mug shape.
</p>
</div>
<p>We can say that the “photo” from projecting onto subspace <span class="math inline">\(\mathbb{H}_C\)</span> is the
one that most resembles the original object. Now, keep in mind that the resulting
image in the low-dimensional space is not capturing the whole pattern. In other
words, there is some loss of information. However, by chosing the right
projection, we hope to minimize such loss.</p>
</div>
<div id="projections" class="section level2">
<h2><span class="header-section-number">4.2</span> Projections</h2>
<p>Following the idea of projections, let’s now discuss with more detail this
concept, and its implications.</p>
<p>Pretend that we zoom in to see some of the individuals of the cloud of points
that form the mug (see figure below). Keep in mind that these data points are
in a <span class="math inline">\(p\)</span>-dimensional space, which will also have its centroid (i.e. average
individual).</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-28"></span>
<img src="images/pca/pca-scalar-projection0.svg" alt="Set of individuals in p-dim space." width="55%" />
<p class="caption">
Figure 4.4: Set of individuals in p-dim space.
</p>
</div>
<p>Because our goal is to look for a low-dimensional represention,
we can start by considering a one-dimensional space, that is, some axis.
In the above diagram (as well as the one below) this dimension is depicted with
the yellow line, labeled as <span class="math inline">\(dim_{\mathbf{v}}\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-29"></span>
<img src="images/pca/pca-scalar-projection1.svg" alt="Set of individuals in p-dim space." width="70%" />
<p class="caption">
Figure 4.5: Set of individuals in p-dim space.
</p>
</div>
<p>We should note that we don’t really manipulate <span class="math inline">\(dim_{\mathbf{v}}\)</span> directly.
Instead, what we manipulate is a vector <span class="math inline">\(\mathbf{v}\)</span> along this dimension.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-30"></span>
<img src="images/pca/pca-scalar-projection2.svg" alt="Dimension spanned by vector v" width="70%" />
<p class="caption">
Figure 4.6: Dimension spanned by vector v
</p>
</div>
<p>At the end of the day, we want to project the individuals onto this dimension.
In particular, the type of projections that we are interested in are orthogonal
projections.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-31"></span>
<img src="images/pca/pca-scalar-projection3.svg" alt="Projections onto one dimension" width="70%" />
<p class="caption">
Figure 4.7: Projections onto one dimension
</p>
</div>
<div id="vector-and-scalar-projections" class="section level3">
<h3><span class="header-section-number">4.2.1</span> Vector and Scalar Projections</h3>
<p>Let’s consider a specific individual, for example the <span class="math inline">\(i\)</span>-th individual.
And let’s take the centroid as the origin of the cloud of points. In this way,
the dimension that we are looking for has to pass thorugh the centroid of this
cloud. Obtaining the orthogonal projection of the <span class="math inline">\(i\)</span>-th individual
onto the dimension <span class="math inline">\(dim_{\mathbf{v}}\)</span> involves projecting <span class="math inline">\(\mathbf{x_i}\)</span> onto
<span class="math inline">\(\mathbf{v}\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-32"></span>
<img src="images/pca/pca-scalar-projection4.svg" alt="Projection of an individual onto one dimension" width="70%" />
<p class="caption">
Figure 4.8: Projection of an individual onto one dimension
</p>
</div>
<p>Recall that an orthogonal projection can be split into two types of projections:
(1) the vector projection, and (2) the scalar projection.</p>
<p>The <strong>vector projection</strong> of <span class="math inline">\(\mathbf{x_i}\)</span> onto the <span class="math inline">\(\mathbf{v}\)</span>
is defined as:</p>
<p><span class="math display">\[
proj_{\mathbf{v}} (\mathbf{x_i}) = \frac{\mathbf{x_{i}^\mathsf{T} v}}{\mathbf{v^\mathsf{T}v}} \hspace{1mm} \mathbf{v} = \mathbf{\hat{v}}
\]</span></p>
<p>The <strong>scalar projection</strong> of <span class="math inline">\(\mathbf{x_i}\)</span> onto <span class="math inline">\(\mathbf{v}\)</span>
is defined as:</p>
<p><span class="math display">\[
comp_{\mathbf{v}} (\mathbf{x_i}) = \frac{\mathbf{x_{i}^\mathsf{T}v}}{\|\mathbf{v}\|} = z_{ik}
\]</span></p>
<p>The following diagram displays both types of projections:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-33"></span>
<img src="images/pca/pca-scalar-projection5.svg" alt="Scalar and vector projections of i-th individual onto vector v" width="70%" />
<p class="caption">
Figure 4.9: Scalar and vector projections of i-th individual onto vector v
</p>
</div>
<p>We are not really interested in obtaining the vector projection
<span class="math inline">\(proj_{\mathbf{v}} (\mathbf{x_i})\)</span>. Instead, what we care about is the scalar
projection <span class="math inline">\(comp_{\mathbf{v}} (\mathbf{x_i})\)</span>. In other words, we just want to
obtain the coordinate of the <span class="math inline">\(i\)</span>-th individual along this axis.</p>
</div>
<div id="projected-inertia" class="section level3">
<h3><span class="header-section-number">4.2.2</span> Projected Inertia</h3>
<p>As we said before, our goal is to find the angle that gives us the “best”
photo of the mug (i.e. cloud of points). This can be translated as finding a
subspace in which the distances between the points is most similar to those on
the actual mug.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-34"></span>
<img src="images/pca/pca-projection-goal.svg" alt="Projection Goal" width="90%" />
<p class="caption">
Figure 4.10: Projection Goal
</p>
</div>
<p>Now, we need to define what we mean by “similar” distances in <span class="math inline">\(\mathbb{H}\)</span>. Consider the overall dispersion of our original system (the one with all <span class="math inline">\(p\)</span>
dimensions): <span class="math inline">\(\sum_{j} \sum_{\ell} d^2 (i, \ell)\)</span>. This is a fixed number;
and we cannot change it. However, we can try fine-tune our subset <span class="math inline">\(\mathbb{H}\)</span>
such that
<span class="math inline">\(\sum_{j} \sum_{\ell} d^2 (i, \ell) \approx \sum_{i} \sum_{\ell} d_H^2(i, \ell)\)</span>.</p>
<p><span class="math display">\[ 
\sum_{j} \sum_{\ell} d^2 (i, \ell) = \ 2n \sum_i d^2 (i, g)
\]</span></p>
<p>However, it will be easier to simply consider the <em>inertia</em> of the system
(as opposed to the overall dispersion).</p>
<p><span class="math display">\[
2n \sum_i d^2 (i, g) \  \to  \ \frac{1}{n} \sum_{i=1}^{n} d^2 (i, g)
\]</span></p>
<p>In mathematical terms, finding the subspace <span class="math inline">\(\mathbb{H}\)</span> that gives us “similar”
distances to those of the original space corresponds to maximizing the
projected inertia:</p>
<p><span class="math display">\[
\max_{\mathbb{H}} \left\{ \frac{1}{n} \sum_{i=1}^{n} d_{\mathbb{H}}^2 (i, g) \right\}
\]</span></p>
<p>Now, as we discuss before, the simplest subspace will be one dimension,
that is, we consider the case where <span class="math inline">\(\mathbb{H} \subseteq \mathbb{R}^1\)</span>.
So the first approach is to project our data points onto a vector <span class="math inline">\(\mathbf{v}\)</span>.
Hence, the projected inertia becomes:</p>
<p><span class="math display">\[
\frac{1}{n} \sum_{i=1}^{n} d_\mathbb{H}^2 (i, g) = \frac{1}{n} \sum_{i=1}^{n} \left( \mathbf{x_i}^\mathsf{T} \mathbf{v}  \right)^2 = \frac{1}{n} \sum_{i=1}^{n} z_i^2
\]</span></p>
<p>and our maximization problem becomes</p>
<p><span class="math display">\[
\max_{\mathbf{v}} \left\{ \frac{1}{n} \sum_{i=1}^{n} \left( \mathbf{x_i}^\mathsf{T}  \mathbf{v}  \right)^2   \right\} \quad \mathrm{s.t.} \quad \mathbf{v}^\mathsf{T} \mathbf{v} = 1
\]</span></p>
<p>For convenience, we add the restriction that <span class="math inline">\(\mathbf{v}\)</span> is a unit-vector
(vector of unit norm). Why do we need the constraint? If we didn’t have the
constraint, we could always find a <span class="math inline">\(\mathbf{v}\)</span> that obtains higher inertia,
and things would explode. Furthermore, as we will see in the next sectoin,
this helps in the algebra we will use when we actually perform the maximization.</p>
</div>
</div>
<div id="maximization-problem" class="section level2">
<h2><span class="header-section-number">4.3</span> Maximization Problem</h2>
<p>We now need to compute the projected Inertia. Since we have projected onto a
line spanned by <span class="math inline">\(\mathbf{v}\)</span>, the projected inertia <span class="math inline">\(I_{\mathbb{H}}\)</span> will simply
be the variance of the projected data points:</p>
<p><span class="math display">\[
I_{\mathbb{H}} = \frac{1}{n} \sum_{i=1}^{n} d_\mathbb{H}^2(i, 0) = \frac{1}{n} \sum_{i=1}^{n} z_i^2
\]</span></p>
<p>(note that <span class="math inline">\(g = 0\)</span> since we are assuming our data is mean-centered).
We can simplify notation as follows:</p>
<p><span class="math display">\[
\mathbf{z} := \begin{pmatrix} z_1 \\ z_2 \\ \vdots \\ z_n \\ \end{pmatrix} \ \Longrightarrow \ I_\mathbb{H} = \frac{1}{n} \mathbf{z}^\mathsf{T} \mathbf{z} = \frac{1}{n} \mathbf{v}^\mathsf{T} \mathbf{X}^\mathsf{T} \mathbf{X} \mathbf{v}
\]</span></p>
<p>Hence, the maximization problem becomes</p>
<p><span class="math display">\[
\max_{\mathbf{v}} \left\{ \frac{1}{n} \mathbf{v}^\mathsf{T} \mathbf{X}^\mathsf{T} \mathbf{X} \mathbf{v} \right\} \qquad \text{s.t.} \qquad \mathbf{v}^\mathsf{T} \mathbf{v} = 1
\]</span></p>
<p>To solve this maximization problem, we utilize the method of
Lagrange Multipliers:</p>
<p><span class="math display">\[
\mathcal{L} = \frac{1}{n} \mathbf{v}^\mathsf{T} \mathbf{X}^\mathsf{T} \mathbf{X} \mathbf{v} - \lambda \left( \mathbf{v}^\mathsf{T} \mathbf{v}  - 1 \right)
\]</span></p>
<p>Calculating the derivative of the lagrangian <span class="math inline">\(\mathcal{L}\)</span> with respect to
<span class="math inline">\(\mathbf{v}\)</span>, and equating to zero we get:</p>
<p><span class="math display">\[
\frac{\partial \mathcal{L}}{\partial \mathbf{v}} = \frac{2}{n} \mathbf{X}^\mathsf{T} \mathbf{X} \mathbf{v} - 2 \lambda \mathbf{v} = \mathbf{0} \ \Rightarrow \ \underbrace{  \frac{1}{n} \mathbf{X}^\mathsf{T} \mathbf{X} \mathbf{v} }_{:= \mathbf{S} } = \lambda \mathbf{v}  \ \Rightarrow \ \mathbf{S} \mathbf{v} = \lambda \mathbf{v}
\]</span></p>
<p>That is, we obtain the equation <span class="math inline">\(\mathbf{S} \mathbf{v} = \lambda \mathbf{v}\)</span>
where <span class="math inline">\(\mathbf{S} \in \mathbb{R}^{p \times p}\)</span> is symmetric.
This means that <span class="math inline">\(\mathbf{v}\)</span> is an eigenvector (with eigenvalue <span class="math inline">\(\lambda\)</span>)
of <span class="math inline">\(\mathbf{S}\)</span>. Moreover, <span class="math inline">\(\lambda\)</span> is the value of the projected inertia
<span class="math inline">\(I_\mathbb{H}\)</span>—which is the thing that we want to maximize.</p>
<div id="eigenvectors-of-mathbfs" class="section level3">
<h3><span class="header-section-number">4.3.1</span> Eigenvectors of <span class="math inline">\(\mathbf{S}\)</span></h3>
<p>Assume that <span class="math inline">\(\mathbf{X}\)</span> has full rank (i.e. <span class="math inline">\(\mathrm{rank}(\mathbf{X}) = p\)</span>).
We then obtain <span class="math inline">\(p\)</span> eigenvectors, which together form the matrix <span class="math inline">\(\mathbf{V}\)</span>:</p>
<p><span class="math display">\[
\mathbf{V} = \begin{bmatrix} \mathbf{v_1} &amp; \mathbf{v_2} &amp; \dots &amp; \mathbf{v_k} &amp; \dots &amp; \mathbf{v_p} \\ \end{bmatrix}
\]</span></p>
<p>We can also obtain <span class="math inline">\(\mathbf{\Lambda}_{p \times p} := \mathrm{diag}\left\{ \lambda_i \right\}_{i=1}^{n}\)</span>
(i.e. the matrix of eigenvalues): that is,</p>
<p><span class="math display">\[
\mathbf{\Lambda} = \begin{bmatrix} \lambda_1 &amp; 0 &amp; \dots &amp; 0 \\ 0 &amp; \lambda_2 &amp; \dots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \dots &amp; \lambda_p \\ \end{bmatrix}
\]</span></p>
<p>Finally, we can also obtain the matrix <span class="math inline">\(\mathbf{Z}_{n \times p}\)</span> which is the
matrix consisting of the vectors <span class="math inline">\(\mathbf{z_i}\)</span>:</p>
<p><span class="math display">\[
\mathbf{Z} = \begin{bmatrix} \mathbf{z_1} &amp; \mathbf{z_2} &amp; \dots &amp; \mathbf{z_k} &amp; \dots &amp; \mathbf{z_p} \\ \end{bmatrix}
\]</span></p>
<ul>
<li><p>The matrix <span class="math inline">\(\mathbf{V}\)</span> consists of the <strong>loadings</strong></p></li>
<li><p>The matrix <span class="math inline">\(\mathbf{Z}\)</span> contains the <strong>principal components</strong> (PC’s),
also known as <strong>scores</strong>.</p></li>
</ul>
<p>Let us examine the <span class="math inline">\(k\)</span>th principal component <span class="math inline">\(\mathbf{z_k}\)</span>:</p>
<p><span class="math display">\[
\mathbf{z_k} = \mathbf{X v_k} = v_{1k} \mathbf{x_1} + v_{2k} \mathbf{x_2} + \dots + v_{pk} \mathbf{x_p}
\]</span></p>
<p>(where <span class="math inline">\(\mathbf{x_k}\)</span> denotes columns of <span class="math inline">\(\mathbf{X}\)</span>). Note that
<span class="math inline">\(\mathrm{mean}(\mathbf{z_k}) = 0\)</span>; since we are assuming that the data is
mean-centered, we have that <span class="math inline">\(\mathrm{mean}(\mathbf{x_1}) = 0\)</span>.
What about variance?</p>
<p><span class="math display">\[\begin{align*}
Var(\mathbf{z_k}) &amp; = \frac{1}{n} \mathbf{z_{k}^\mathsf{T}} \mathbf{z_k} \\
&amp;  = \frac{1}{n}\left( \mathbf{X} \mathbf{v_k} \right)^{\mathsf{T}} \left( \mathbf{X} \mathbf{v_k} \right) \\
&amp; = \frac{1}{n} \mathbf{v_{k}^\mathsf{T}} \mathbf{X}^\mathsf{T} \mathbf{X} \mathbf{v_k} \\
&amp; = \mathbf{v_k}^\mathsf{T} \mathbf{S} \mathbf{v_k} \\
&amp; = \mathbf{v_k}^\mathsf{T} \left( \lambda_k \mathbf{v_k} \right) = \lambda_k \left( \mathbf{v_k}^\mathsf{T} \mathbf{v_k} \right) \\
&amp; = \lambda_k 
\end{align*}\]</span></p>
<p>Hence, we obtain the following interesting result: the <span class="math inline">\(k\)</span>-th eigenvalue of <span class="math inline">\(\mathbf{S}\)</span> is simply the variance of the <span class="math inline">\(k\)</span>-th principal component.</p>
<p>If <span class="math inline">\(\mathbf{X}\)</span> is mean centered, then
<span class="math inline">\(\mathbf{S} = \frac{1}{n} \mathbf{X^\mathsf{T} X}\)</span> is nothing but the
variance-covariance matrix of our data. If <span class="math inline">\(\mathbf{X}\)</span> is <em>standardized</em>
(i.e. mean-centered and scaled by the variance), then <span class="math inline">\(\mathbf{S}\)</span> becomes the
correlation matrix.</p>
<p>In summary:</p>
<ul>
<li><p><span class="math inline">\(\mathrm{mean}(\mathbf{z_k}) = 0\)</span></p></li>
<li><p><span class="math inline">\(Var(\mathbf{z_k}) = \lambda_k\)</span></p></li>
<li><p><span class="math inline">\(\mathrm{sd}(\mathbf{z_k}) = \sqrt{\lambda_k}\)</span></p></li>
</ul>
<p>We have the following fact (the proof is omitted, and may be assigned as
homework or as a test question):</p>
<p><span class="math display">\[
I = \frac{1}{n} \sum_{i=1}^{n} d^2(i, g) = \sum_{k} \lambda_k = \mathrm{tr}\left( \frac{1}{n} \mathbf{X^\mathsf{T} X} \right)
\]</span></p>
<ul>
<li><p>The dimensions that we find in our analysis (through <span class="math inline">\(\mathbf{v_k}\)</span>)
relates directly to <span class="math inline">\(\mathbf{z_k}\)</span>.</p></li>
<li><p><span class="math inline">\(\sum_{k=1}^{p} \lambda_k\)</span> relates directly to the total amount of variability
of our data.</p></li>
</ul>
<p><strong>Remark</strong>: The principal components capture different parts of the variability
of the full data.</p>
</div>
</div>
<div id="another-perspective-of-pca" class="section level2">
<h2><span class="header-section-number">4.4</span> Another Perspective of PCA</h2>
<p>Let us present another way to approach PCA. Given a set of <span class="math inline">\(p\)</span> variables
<span class="math inline">\(\mathbf{x_1}, \mathbf{x_2}, \dots, \mathbf{x_p}\)</span>, we want to obtain new <span class="math inline">\(k\)</span> variables
<span class="math inline">\(\mathbf{z_1}, \mathbf{z_2}, \dots, \mathbf{z_k}\)</span>, called the <strong>Principal Components</strong> (PCs).</p>
<p>A principal component is a linear combination of the <span class="math inline">\(p\)</span> variables:
<span class="math inline">\(\mathbf{z} = \mathbf{Xv}\)</span>. The first PC is a linear mix:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-35"></span>
<img src="images/pca/pca-path-diag1.svg" alt="PCs as linear combinations of X-variables" width="55%" />
<p class="caption">
Figure 4.11: PCs as linear combinations of X-variables
</p>
</div>
<p>The second PC is another linear mix:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-36"></span>
<img src="images/pca/pca-path-diag2.svg" alt="PCs as linear combinations of X-variables" width="55%" />
<p class="caption">
Figure 4.12: PCs as linear combinations of X-variables
</p>
</div>
<p>We want to compute the <strong>PCs as linear combinations</strong> of the original variables.</p>
<p><span class="math display">\[ 
\begin{array}{c}
\mathbf{z_1} = v_{11} \mathbf{x_1} + \dots + v_{p1} \mathbf{x_p} \\
\mathbf{z_2} = v_{12} \mathbf{x_1} + \dots + v_{p2} \mathbf{x_p} \\
\vdots \\
\mathbf{z_k} = v_{1k} \mathbf{x_1} + \dots + v_{pk} \mathbf{x_p} \\
\end{array}
\]</span></p>
<p>Or in matrix notation:</p>
<p><span class="math display">\[
\mathbf{Z} = \mathbf{X V}
\]</span></p>
<p>where <span class="math inline">\(\mathbf{Z}\)</span> is an <span class="math inline">\(n \times k\)</span> matrix of principal components, and
<span class="math inline">\(\mathbf{V}\)</span> is a <span class="math inline">\(p \times k\)</span> matrix of weights, also known as directional
vectors of the principal axes. The following figure shows a graphical
representation of a PCA problem in diagram notation:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-37"></span>
<img src="images/pca/pca-path-diag4.svg" alt="PCs as linear combinations of X-variables" width="70%" />
<p class="caption">
Figure 4.13: PCs as linear combinations of X-variables
</p>
</div>
<p>We look to transform the original variables into a smaller set of new variables,
the Principal Components (PCs), that summarize the variation in data.
The PCs are obtained as linear combinations (i.e. weighted sums) of the
original variables. We look for PCs is in such a way that they have maximum
variance, and being mutually orthogonal.</p>
<div id="finding-principal-components" class="section level3">
<h3><span class="header-section-number">4.4.1</span> Finding Principal Components</h3>
<p>The way to find principal components is to construct them as weighted sums of
the original variables, looking to optimize some criterion and following some
constraints. One way in which we can express the criterion is to require components
<span class="math inline">\(\mathbf{z_1}, \mathbf{z_2}, \dots, \mathbf{z_k}\)</span> that capture most of the variation
in the data <span class="math inline">\(\mathbf{X}\)</span>. “Capturing most of the variation,” implies looking
for a vector <span class="math inline">\(\mathbf{v_h}\)</span> such that a component <span class="math inline">\(\mathbf{z_h} = \mathbf{X v_h}\)</span>
has maximum variance:</p>
<p><span class="math display">\[
\max_{\mathbf{v_h}} \; var(\mathbf{z_h}) \quad \Rightarrow \quad \max_{\mathbf{v_h}} \; var(\mathbf{X v_h})
\]</span></p>
<p>that is</p>
<p><span class="math display">\[
\max_{\mathbf{v_h}} \; \frac{1}{n} \mathbf{v_{h}^\mathsf{T} X^\mathsf{T} X v_h}
\]</span></p>
<p>As you can tell, this is a maximization problem. Without any constraints, this
problem is unbounded, not to mention useless. We could take <span class="math inline">\(\mathbf{v_h}\)</span> as
bigger as we want without being able to reach any maximum. To get a feasible
solution we need to impose some kind of restriction. The standard adopted
constraint is to require <span class="math inline">\(\mathbf{v_h}\)</span> to be of unit norm:</p>
<p><span class="math display">\[
\| \mathbf{v_h} \| = 1 \; \hspace{1mm} \Rightarrow \; \hspace{1mm} \mathbf{v_{h}^\mathsf{T} v_h} = 1 
\]</span></p>
<p>Note that <span class="math inline">\((1/n) \mathbf{X^\mathsf{T} X}\)</span> is the variance-covariance matrix.
If we denote <span class="math inline">\(\mathbf{S} = (1/n) \mathbf{X^\mathsf{T} X}\)</span> then the criterion to
be maximized is:</p>
<p><span class="math display">\[
\max_{\mathbf{v_h}} \; \mathbf{v_{h}^\mathsf{T} S v_h}
\]</span></p>
<p>subject to <span class="math inline">\(\mathbf{v_{h}^\mathsf{T} v_h} = 1\)</span></p>
<p>To avoid a PC <span class="math inline">\(\mathbf{z_h}\)</span> from capturing the same variation as other PCs
<span class="math inline">\(\mathbf{z_l}\)</span> (i.e. avoiding redundant information), we also require them to be
<strong>mutually orthogonal</strong> so they are uncorrelated with each other. Formally,
we impose the restriction <span class="math inline">\(\mathbf{z_h}\)</span> to be perpendicular to other components:
<span class="math inline">\(\mathbf{z_{h}^\mathsf{T} z_l} = 0; (h \neq l)\)</span>.</p>
</div>
<div id="finding-the-first-pc" class="section level3">
<h3><span class="header-section-number">4.4.2</span> Finding the first PC</h3>
<p>In order to get the first principal component <span class="math inline">\(\mathbf{z_1} = \mathbf{X v_1}\)</span>,
we need to find <span class="math inline">\(\mathbf{v_1}\)</span> such that:</p>
<p><span class="math display">\[
\max_{\mathbf{v_1}} \; \mathbf{v_{1}^\mathsf{T} S v_1}
\]</span></p>
<p>subject to <span class="math inline">\(\mathbf{v_{1}^\mathsf{T} v_1} = 1\)</span></p>
<p>Being a maximization problem, the typical procedure to find the solution is by
using the <strong>Lagrangian multiplier</strong> method. Using Lagrange multipliers we get:</p>
<p><span class="math display">\[
\mathbf{v_{1}^\mathsf{T} S v_1} - \lambda (\mathbf{v_{1}^\mathsf{T} v_1} - 1) = 0
\]</span></p>
<p>Differentiation with respect to <span class="math inline">\(\mathbf{v_1}\)</span>, and equating to zero gives:</p>
<p><span class="math display">\[
\mathbf{S v_1} - \lambda_1 \mathbf{v_1} = \mathbf{0}
\]</span></p>
<p>Rearranging some terms we get:</p>
<p><span class="math display">\[
\mathbf{S v_1} = \lambda_1 \mathbf{v_1}
\]</span></p>
<p>What does this mean? It means that <span class="math inline">\(\lambda_1\)</span> is an eigenvalue of <span class="math inline">\(\mathbf{S}\)</span>,
and <span class="math inline">\(\mathbf{v_1}\)</span> is the corresponding eigenvector.</p>
</div>
<div id="finding-the-second-pc" class="section level3">
<h3><span class="header-section-number">4.4.3</span> Finding the second PC</h3>
<p>In order to find the second principal component <span class="math inline">\(\mathbf{z_2} = \mathbf{X v_2}\)</span>,
we need to find <span class="math inline">\(\mathbf{v_2}\)</span> such that</p>
<p><span class="math display">\[
\max_{\mathbf{v_2}} \; \mathbf{v_{2}^\mathsf{T} S v_2}
\]</span></p>
<p>subject to <span class="math inline">\(\| \mathbf{v_2} \| = 1\)</span> and <span class="math inline">\(\mathbf{z_{1}^\mathsf{T} z_2} = 0\)</span>.
Remember that <span class="math inline">\(\mathbf{z_2}\)</span> must be uncorrelated to <span class="math inline">\(\mathbf{z_1}\)</span>.
Applying the Lagrange multipliers, it can be shown that the desired
<span class="math inline">\(\mathbf{v_2}\)</span> is such that</p>
<p><span class="math display">\[
\mathbf{S v_2} = \lambda_2 \mathbf{v_2}
\]</span></p>
<p>In other words. <span class="math inline">\(\lambda_2\)</span> is an eigenvalue of <span class="math inline">\(\mathbf{S}\)</span> and <span class="math inline">\(\mathbf{v_2}\)</span>
is the corresponding eigenvector.</p>
</div>
<div id="finding-all-pcs" class="section level3">
<h3><span class="header-section-number">4.4.4</span> Finding all PCs</h3>
<p>All PCs can be found simultaneously by <strong>diagonalizing</strong> <span class="math inline">\(\mathbf{S}\)</span>.
Diagonalizing <span class="math inline">\(\mathbf{S}\)</span> involves expressing it as the product:</p>
<p><span class="math display">\[ 
\mathbf{S} = \mathbf{V \Lambda V^\mathsf{T}}
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\mathbf{D}\)</span> is a diagonal matrix</li>
<li>the elements in the diagonal of <span class="math inline">\(\mathbf{D}\)</span> are the eigenvalues of <span class="math inline">\(\mathbf{S}\)</span></li>
<li>the columns of <span class="math inline">\(\mathbf{V}\)</span> are orthonormal: <span class="math inline">\(\mathbf{V^\mathsf{T} V= I}\)</span></li>
<li>the columns of <span class="math inline">\(\mathbf{V}\)</span> are the eigenvectors of <span class="math inline">\(\mathbf{S}\)</span></li>
<li><span class="math inline">\(\mathbf{V^\mathsf{T}} = \mathbf{V^{-1}}\)</span></li>
</ul>
<p>Diagonalizing a symmetric matrix is nothing more than obtaining its
<strong>eigenvalue decomposition</strong> (a.k.a. spectral decomposition).
A <span class="math inline">\(p \times p\)</span> symmetric matrix <span class="math inline">\(\mathbf{S}\)</span> has the following properties:</p>
<ul>
<li><span class="math inline">\(\mathbf{S}\)</span> has <span class="math inline">\(p\)</span> real eigenvalues (counting multiplicites)</li>
<li>the eigenvectors corresponding to different eigenvalues are orthogonal</li>
<li><span class="math inline">\(\mathbf{S}\)</span> is orthogonally diagonalizable (<span class="math inline">\(\mathbf{S} = \mathbf{V \Lambda V^\mathsf{T}}\)</span>)</li>
<li>the set of eigenvalues of <span class="math inline">\(\mathbf{S}\)</span> is called the <strong>spectrum</strong> of <span class="math inline">\(\mathbf{S}\)</span></li>
</ul>
<p>In summary: The PCA solution can be obtained with an Eigenvalue Decomposition
of the matrix <span class="math inline">\(\mathbf{S} = (1/n) \mathbf{X^\mathsf{T}X}\)</span></p>
</div>
</div>
<div id="data-decomposition-model" class="section level2">
<h2><span class="header-section-number">4.5</span> Data Decomposition Model</h2>
<p>Formally, PCA involves finding scores and loadings such that the data can be
expressed as a product of two matrices:</p>
<p><span class="math display">\[
\underset{n \times p}{\mathbf{X}} = \underset{n \times r}{\mathbf{Z}} \underset{r \times p}{\mathbf{V^\mathsf{T}}}
\]</span></p>
<p>where <span class="math inline">\(\mathbf{Z}\)</span> is the matrix of PCs or <em>scores</em>, and <span class="math inline">\(\mathbf{V}\)</span> is the
matrix of <em>loadings</em>. We can obtain as many different eigenvalues as the rank of
<span class="math inline">\(\mathbf{S}\)</span> denoted by <span class="math inline">\(r\)</span>. Ideally, we expect <span class="math inline">\(r\)</span> to be smaller than <span class="math inline">\(p\)</span>
so we get a convenient data reduction. But usually we will only retain just a
few PCs (i.e. <span class="math inline">\(k \ll p\)</span>) expecting not to lose too much information:</p>
<p><span class="math display">\[
\underset{n \times p}{\mathbf{X}} \approx \underset{n \times k}{\mathbf{Z}} \hspace{1mm} \underset{k \times p}{\mathbf{V^\mathsf{T}}} + \text{Residual}
\]</span></p>
<p>The previous expression means that just a few PCs will <em>optimally</em> summarize the main structure of the data</p>
<div id="alternative-approaches" class="section level3">
<h3><span class="header-section-number">4.5.1</span> Alternative Approaches</h3>
<p>Finding <span class="math inline">\(\mathbf{z_h} = \mathbf{X v_h}\)</span> with maximum variance has another
important property that it is not always mentioned in multivariate textbooks
but that we find worth mentioning. <span class="math inline">\(\mathbf{z_h}\)</span> is such that</p>
<p><span class="math display">\[
\max \sum_{j = 1}^{p} cor^2(\mathbf{z_h, x_j})
\]</span></p>
<p>What this expression implies is that principal components <span class="math inline">\(\mathbf{z_h}\)</span> are
computed to be the <em>best</em> representants in terms of maximizing the sum of
squared correlations with the variables
<span class="math inline">\(\mathbf{x_1}, \mathbf{x_2}, \dots, \mathbf{x_p}\)</span>.
Interestingly, you can think of PCs as predictors
of the variables in <span class="math inline">\(\mathbf{X}\)</span>. Under this perspective, we can reverse the
relations and see PCA from a regression-like model perspective:</p>
<p><span class="math display">\[
\mathbf{x_j} = v_{jh} \mathbf{z_h} + \mathbf{e_h}
\]</span></p>
<p>Notice that the regression coefficient is the <span class="math inline">\(j\)</span>-th element of the <span class="math inline">\(h\)</span>-th
eigenvector.</p>

</div>
</div>
</div>



            </section>

          </div>
        </div>
      </div>
<a href="duality.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ols.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["allmodelsarewrong.pdf", "allmodelsarewrong.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
