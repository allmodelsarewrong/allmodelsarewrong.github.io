<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>23 Canonical Discriminant Analysis | All Models Are Wrong: Concepts of Statistical Learning</title>
  <meta name="description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  <meta name="generator" content="bookdown 0.16 and GitBook 2.6.7" />

  <meta property="og:title" content="23 Canonical Discriminant Analysis | All Models Are Wrong: Concepts of Statistical Learning" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://allmodelsarewrong.github.io" />
  
  <meta property="og:description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  <meta name="github-repo" content="allmodelsarewrong/allmodelsarewrong.github.io" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="23 Canonical Discriminant Analysis | All Models Are Wrong: Concepts of Statistical Learning" />
  
  <meta name="twitter:description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  

<meta name="author" content="Gaston Sanchez Ethan Marzban" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="discrim.html"/>
<link rel="next" href="discanalysis.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><b>All Models Are Wrong (CSL)</b><br><small>by G. Sanchez & E. Marzban</small></a></li>

<li class="divider"></li>
<li class="part"><span><b>I Welcome</b></span></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="about.html"><a href="about.html"><i class="fa fa-check"></i><b>1</b> About this book</a><ul>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#prerequisites"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="part"><span><b>II Intro</b></span></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#basic-notation"><i class="fa fa-check"></i><b>2.1</b> Basic Notation</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="duality.html"><a href="duality.html"><i class="fa fa-check"></i><b>3</b> Geometric Duality</a><ul>
<li class="chapter" data-level="3.1" data-path="duality.html"><a href="duality.html#rows-space"><i class="fa fa-check"></i><b>3.1</b> Rows Space</a></li>
<li class="chapter" data-level="3.2" data-path="duality.html"><a href="duality.html#columns-space"><i class="fa fa-check"></i><b>3.2</b> Columns Space</a></li>
<li class="chapter" data-level="3.3" data-path="duality.html"><a href="duality.html#cloud-of-individuals"><i class="fa fa-check"></i><b>3.3</b> Cloud of Individuals</a><ul>
<li class="chapter" data-level="3.3.1" data-path="duality.html"><a href="duality.html#average-individual"><i class="fa fa-check"></i><b>3.3.1</b> Average Individual</a></li>
<li class="chapter" data-level="3.3.2" data-path="duality.html"><a href="duality.html#centered-data"><i class="fa fa-check"></i><b>3.3.2</b> Centered Data</a></li>
<li class="chapter" data-level="3.3.3" data-path="duality.html"><a href="duality.html#distance-between-individuals"><i class="fa fa-check"></i><b>3.3.3</b> Distance between individuals</a></li>
<li class="chapter" data-level="3.3.4" data-path="duality.html"><a href="duality.html#distance-to-the-centroid"><i class="fa fa-check"></i><b>3.3.4</b> Distance to the centroid</a></li>
<li class="chapter" data-level="3.3.5" data-path="duality.html"><a href="duality.html#measures-of-dispersion"><i class="fa fa-check"></i><b>3.3.5</b> Measures of Dispersion</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="duality.html"><a href="duality.html#cloud-of-variables"><i class="fa fa-check"></i><b>3.4</b> Cloud of Variables</a><ul>
<li class="chapter" data-level="3.4.1" data-path="duality.html"><a href="duality.html#mean-of-a-variable"><i class="fa fa-check"></i><b>3.4.1</b> Mean of a Variable</a></li>
<li class="chapter" data-level="3.4.2" data-path="duality.html"><a href="duality.html#variance-of-a-variable"><i class="fa fa-check"></i><b>3.4.2</b> Variance of a Variable</a></li>
<li class="chapter" data-level="3.4.3" data-path="duality.html"><a href="duality.html#variance-with-vector-notation"><i class="fa fa-check"></i><b>3.4.3</b> Variance with Vector Notation</a></li>
<li class="chapter" data-level="3.4.4" data-path="duality.html"><a href="duality.html#standard-deviation-as-a-norm"><i class="fa fa-check"></i><b>3.4.4</b> Standard Deviation as a Norm</a></li>
<li class="chapter" data-level="3.4.5" data-path="duality.html"><a href="duality.html#covariance"><i class="fa fa-check"></i><b>3.4.5</b> Covariance</a></li>
<li class="chapter" data-level="3.4.6" data-path="duality.html"><a href="duality.html#correlation"><i class="fa fa-check"></i><b>3.4.6</b> Correlation</a></li>
<li class="chapter" data-level="3.4.7" data-path="duality.html"><a href="duality.html#geometry-of-correlation"><i class="fa fa-check"></i><b>3.4.7</b> Geometry of Correlation</a></li>
<li class="chapter" data-level="3.4.8" data-path="duality.html"><a href="duality.html#orthogonal-projections"><i class="fa fa-check"></i><b>3.4.8</b> Orthogonal Projections</a></li>
<li class="chapter" data-level="3.4.9" data-path="duality.html"><a href="duality.html#the-mean-as-an-orthogonal-projection"><i class="fa fa-check"></i><b>3.4.9</b> The mean as an orthogonal projection</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Unsupervised I: PCA</b></span></li>
<li class="chapter" data-level="4" data-path="pca.html"><a href="pca.html"><i class="fa fa-check"></i><b>4</b> Principal Components Analysis</a><ul>
<li class="chapter" data-level="4.1" data-path="pca.html"><a href="pca.html#low-dimensional-representations"><i class="fa fa-check"></i><b>4.1</b> Low-dimensional Representations</a></li>
<li class="chapter" data-level="4.2" data-path="pca.html"><a href="pca.html#projections"><i class="fa fa-check"></i><b>4.2</b> Projections</a><ul>
<li class="chapter" data-level="4.2.1" data-path="pca.html"><a href="pca.html#vector-and-scalar-projections"><i class="fa fa-check"></i><b>4.2.1</b> Vector and Scalar Projections</a></li>
<li class="chapter" data-level="4.2.2" data-path="pca.html"><a href="pca.html#projected-inertia"><i class="fa fa-check"></i><b>4.2.2</b> Projected Inertia</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="pca.html"><a href="pca.html#maximization-problem"><i class="fa fa-check"></i><b>4.3</b> Maximization Problem</a><ul>
<li class="chapter" data-level="4.3.1" data-path="pca.html"><a href="pca.html#eigenvectors-of-mathbfs"><i class="fa fa-check"></i><b>4.3.1</b> Eigenvectors of <span class="math inline">\(\mathbf{S}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="pca.html"><a href="pca.html#another-perspective-of-pca"><i class="fa fa-check"></i><b>4.4</b> Another Perspective of PCA</a><ul>
<li class="chapter" data-level="4.4.1" data-path="pca.html"><a href="pca.html#finding-principal-components"><i class="fa fa-check"></i><b>4.4.1</b> Finding Principal Components</a></li>
<li class="chapter" data-level="4.4.2" data-path="pca.html"><a href="pca.html#finding-the-first-pc"><i class="fa fa-check"></i><b>4.4.2</b> Finding the first PC</a></li>
<li class="chapter" data-level="4.4.3" data-path="pca.html"><a href="pca.html#finding-the-second-pc"><i class="fa fa-check"></i><b>4.4.3</b> Finding the second PC</a></li>
<li class="chapter" data-level="4.4.4" data-path="pca.html"><a href="pca.html#finding-all-pcs"><i class="fa fa-check"></i><b>4.4.4</b> Finding all PCs</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="pca.html"><a href="pca.html#data-decomposition-model"><i class="fa fa-check"></i><b>4.5</b> Data Decomposition Model</a><ul>
<li class="chapter" data-level="4.5.1" data-path="pca.html"><a href="pca.html#alternative-approaches"><i class="fa fa-check"></i><b>4.5.1</b> Alternative Approaches</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Linear Regression</b></span></li>
<li class="chapter" data-level="5" data-path="ols.html"><a href="ols.html"><i class="fa fa-check"></i><b>5</b> Linear Regression</a><ul>
<li class="chapter" data-level="5.1" data-path="ols.html"><a href="ols.html#motivation"><i class="fa fa-check"></i><b>5.1</b> Motivation</a></li>
<li class="chapter" data-level="5.2" data-path="ols.html"><a href="ols.html#the-ideaintuition-of-regression"><i class="fa fa-check"></i><b>5.2</b> The Idea/Intuition of Regression</a></li>
<li class="chapter" data-level="5.3" data-path="ols.html"><a href="ols.html#the-linear-regression-model"><i class="fa fa-check"></i><b>5.3</b> The Linear Regression Model</a></li>
<li class="chapter" data-level="5.4" data-path="ols.html"><a href="ols.html#the-error-measure"><i class="fa fa-check"></i><b>5.4</b> The Error Measure</a></li>
<li class="chapter" data-level="5.5" data-path="ols.html"><a href="ols.html#the-least-squares-algorithm"><i class="fa fa-check"></i><b>5.5</b> The Least Squares Algorithm</a></li>
<li class="chapter" data-level="5.6" data-path="ols.html"><a href="ols.html#geometries-of-ols"><i class="fa fa-check"></i><b>5.6</b> Geometries of OLS</a><ul>
<li class="chapter" data-level="5.6.1" data-path="ols.html"><a href="ols.html#rows-perspective"><i class="fa fa-check"></i><b>5.6.1</b> Rows Perspective</a></li>
<li class="chapter" data-level="5.6.2" data-path="ols.html"><a href="ols.html#columns-perspective"><i class="fa fa-check"></i><b>5.6.2</b> Columns Perspective</a></li>
<li class="chapter" data-level="5.6.3" data-path="ols.html"><a href="ols.html#parameters-perspective"><i class="fa fa-check"></i><b>5.6.3</b> Parameters Perspective</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="gradient.html"><a href="gradient.html"><i class="fa fa-check"></i><b>6</b> Gradient Descent</a><ul>
<li class="chapter" data-level="6.1" data-path="gradient.html"><a href="gradient.html#error-surface"><i class="fa fa-check"></i><b>6.1</b> Error Surface</a></li>
<li class="chapter" data-level="6.2" data-path="gradient.html"><a href="gradient.html#idea-of-gradient-descent"><i class="fa fa-check"></i><b>6.2</b> Idea of Gradient Descent</a></li>
<li class="chapter" data-level="6.3" data-path="gradient.html"><a href="gradient.html#moving-down-an-error-surface"><i class="fa fa-check"></i><b>6.3</b> Moving Down an Error Surface</a><ul>
<li class="chapter" data-level="6.3.1" data-path="gradient.html"><a href="gradient.html#the-direction-of-mathbfv"><i class="fa fa-check"></i><b>6.3.1</b> The direction of <span class="math inline">\(\mathbf{v}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="gradient.html"><a href="gradient.html#gradient-descent-and-our-model"><i class="fa fa-check"></i><b>6.4</b> Gradient Descent and our Model</a></li>
</ul></li>
<li class="part"><span><b>V Learning Concepts</b></span></li>
<li class="chapter" data-level="7" data-path="learning.html"><a href="learning.html"><i class="fa fa-check"></i><b>7</b> Theoretical Framework</a><ul>
<li class="chapter" data-level="7.1" data-path="learning.html"><a href="learning.html#mental-map"><i class="fa fa-check"></i><b>7.1</b> Mental Map</a></li>
<li class="chapter" data-level="7.2" data-path="learning.html"><a href="learning.html#kinds-of-predictions"><i class="fa fa-check"></i><b>7.2</b> Kinds of Predictions</a><ul>
<li class="chapter" data-level="7.2.1" data-path="learning.html"><a href="learning.html#two-types-of-data"><i class="fa fa-check"></i><b>7.2.1</b> Two Types of Data</a></li>
<li class="chapter" data-level="7.2.2" data-path="learning.html"><a href="learning.html#two-types-of-predictions"><i class="fa fa-check"></i><b>7.2.2</b> Two Types of Predictions</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="learning.html"><a href="learning.html#two-types-of-errors"><i class="fa fa-check"></i><b>7.3</b> Two Types of Errors</a><ul>
<li class="chapter" data-level="7.3.1" data-path="learning.html"><a href="learning.html#individual-errors"><i class="fa fa-check"></i><b>7.3.1</b> Individual Errors</a></li>
<li class="chapter" data-level="7.3.2" data-path="learning.html"><a href="learning.html#overall-errors"><i class="fa fa-check"></i><b>7.3.2</b> Overall Errors</a></li>
<li class="chapter" data-level="7.3.3" data-path="learning.html"><a href="learning.html#auxiliary-technicality"><i class="fa fa-check"></i><b>7.3.3</b> Auxiliary Technicality</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="learning.html"><a href="learning.html#noisy-targets"><i class="fa fa-check"></i><b>7.4</b> Noisy Targets</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="mse.html"><a href="mse.html"><i class="fa fa-check"></i><b>8</b> MSE of Estimator</a><ul>
<li class="chapter" data-level="8.1" data-path="mse.html"><a href="mse.html#mse-of-an-estimator"><i class="fa fa-check"></i><b>8.1</b> MSE of an Estimator</a><ul>
<li class="chapter" data-level="8.1.1" data-path="mse.html"><a href="mse.html#prototypical-cases-of-bias-and-variance"><i class="fa fa-check"></i><b>8.1.1</b> Prototypical Cases of Bias and Variance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="biasvar.html"><a href="biasvar.html"><i class="fa fa-check"></i><b>9</b> Bias-Variance Tradeoff</a><ul>
<li class="chapter" data-level="9.1" data-path="biasvar.html"><a href="biasvar.html#introduction"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="biasvar.html"><a href="biasvar.html#motivation-example"><i class="fa fa-check"></i><b>9.2</b> Motivation Example</a><ul>
<li class="chapter" data-level="9.2.1" data-path="biasvar.html"><a href="biasvar.html#two-hypotheses"><i class="fa fa-check"></i><b>9.2.1</b> Two Hypotheses</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="biasvar.html"><a href="biasvar.html#learning-from-two-points"><i class="fa fa-check"></i><b>9.3</b> Learning from two points</a></li>
<li class="chapter" data-level="9.4" data-path="biasvar.html"><a href="biasvar.html#bias-variance-derivation"><i class="fa fa-check"></i><b>9.4</b> Bias-Variance Derivation</a><ul>
<li class="chapter" data-level="9.4.1" data-path="biasvar.html"><a href="biasvar.html#out-of-sample-predictions"><i class="fa fa-check"></i><b>9.4.1</b> Out-of-Sample Predictions</a></li>
<li class="chapter" data-level="9.4.2" data-path="biasvar.html"><a href="biasvar.html#noisy-target"><i class="fa fa-check"></i><b>9.4.2</b> Noisy Target</a></li>
<li class="chapter" data-level="9.4.3" data-path="biasvar.html"><a href="biasvar.html#types-of-theoretical-mses"><i class="fa fa-check"></i><b>9.4.3</b> Types of Theoretical MSEs</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="biasvar.html"><a href="biasvar.html#the-tradeoff"><i class="fa fa-check"></i><b>9.5</b> The Tradeoff</a><ul>
<li class="chapter" data-level="9.5.1" data-path="biasvar.html"><a href="biasvar.html#bias-variance-tradeoff-picture"><i class="fa fa-check"></i><b>9.5.1</b> Bias-Variance Tradeoff Picture</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="phases.html"><a href="phases.html"><i class="fa fa-check"></i><b>10</b> Learning Phases</a><ul>
<li class="chapter" data-level="10.1" data-path="phases.html"><a href="phases.html#introduction-1"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="phases.html"><a href="phases.html#model-assessment"><i class="fa fa-check"></i><b>10.2</b> Model Assessment</a><ul>
<li class="chapter" data-level="10.2.1" data-path="phases.html"><a href="phases.html#holdout-test-set"><i class="fa fa-check"></i><b>10.2.1</b> Holdout Test Set</a></li>
<li class="chapter" data-level="10.2.2" data-path="phases.html"><a href="phases.html#why-does-a-test-set-work"><i class="fa fa-check"></i><b>10.2.2</b> Why does a test set work?</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="phases.html"><a href="phases.html#model-selection"><i class="fa fa-check"></i><b>10.3</b> Model Selection</a><ul>
<li class="chapter" data-level="10.3.1" data-path="phases.html"><a href="phases.html#three-way-holdout-method"><i class="fa fa-check"></i><b>10.3.1</b> Three-way Holdout Method</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="phases.html"><a href="phases.html#model-training"><i class="fa fa-check"></i><b>10.4</b> Model Training</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="resampling.html"><a href="resampling.html"><i class="fa fa-check"></i><b>11</b> Resample Approaches</a><ul>
<li class="chapter" data-level="11.1" data-path="resampling.html"><a href="resampling.html#general-sampling-blueprint"><i class="fa fa-check"></i><b>11.1</b> General Sampling Blueprint</a></li>
<li class="chapter" data-level="11.2" data-path="resampling.html"><a href="resampling.html#monte-carlo-cross-validation"><i class="fa fa-check"></i><b>11.2</b> Monte Carlo Cross Validation</a></li>
<li class="chapter" data-level="11.3" data-path="resampling.html"><a href="resampling.html#bootstrap-method"><i class="fa fa-check"></i><b>11.3</b> Bootstrap Method</a></li>
<li class="chapter" data-level="11.4" data-path="resampling.html"><a href="resampling.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>11.4</b> <span class="math inline">\(K\)</span>-Fold Cross-Validation</a><ul>
<li class="chapter" data-level="11.4.1" data-path="resampling.html"><a href="resampling.html#leave-one-out-cross-validation-loocv"><i class="fa fa-check"></i><b>11.4.1</b> Leave-One-Out Cross Validation (LOOCV)</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>VI Regularization</b></span></li>
<li class="chapter" data-level="12" data-path="regular.html"><a href="regular.html"><i class="fa fa-check"></i><b>12</b> Regularization Techniques</a><ul>
<li class="chapter" data-level="12.1" data-path="regular.html"><a href="regular.html#multicollinearity-issues"><i class="fa fa-check"></i><b>12.1</b> Multicollinearity Issues</a><ul>
<li class="chapter" data-level="12.1.1" data-path="regular.html"><a href="regular.html#toy-example"><i class="fa fa-check"></i><b>12.1.1</b> Toy Example</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="regular.html"><a href="regular.html#irregular-coefficients"><i class="fa fa-check"></i><b>12.2</b> Irregular Coefficients</a></li>
<li class="chapter" data-level="12.3" data-path="regular.html"><a href="regular.html#connection-to-regularization"><i class="fa fa-check"></i><b>12.3</b> Connection to Regularization</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="pcr.html"><a href="pcr.html"><i class="fa fa-check"></i><b>13</b> Principal Components Regression</a><ul>
<li class="chapter" data-level="13.1" data-path="pcr.html"><a href="pcr.html#motivation-example-1"><i class="fa fa-check"></i><b>13.1</b> Motivation Example</a></li>
<li class="chapter" data-level="13.2" data-path="pcr.html"><a href="pcr.html#the-pcr-model"><i class="fa fa-check"></i><b>13.2</b> The PCR Model</a></li>
<li class="chapter" data-level="13.3" data-path="pcr.html"><a href="pcr.html#how-does-pcr-work"><i class="fa fa-check"></i><b>13.3</b> How does PCR work?</a><ul>
<li class="chapter" data-level="13.3.1" data-path="pcr.html"><a href="pcr.html#transition-formula"><i class="fa fa-check"></i><b>13.3.1</b> Transition Formula</a></li>
<li class="chapter" data-level="13.3.2" data-path="pcr.html"><a href="pcr.html#size-of-coefficients"><i class="fa fa-check"></i><b>13.3.2</b> Size of Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="pcr.html"><a href="pcr.html#selecting-number-of-pcs"><i class="fa fa-check"></i><b>13.4</b> Selecting Number of PCs</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="pls.html"><a href="pls.html"><i class="fa fa-check"></i><b>14</b> Partial Least Squares Regression</a><ul>
<li class="chapter" data-level="14.1" data-path="pls.html"><a href="pls.html#motivation-example-2"><i class="fa fa-check"></i><b>14.1</b> Motivation Example</a></li>
<li class="chapter" data-level="14.2" data-path="pls.html"><a href="pls.html#the-plsr-model"><i class="fa fa-check"></i><b>14.2</b> The PLSR Model</a></li>
<li class="chapter" data-level="14.3" data-path="pls.html"><a href="pls.html#how-does-plsr-work"><i class="fa fa-check"></i><b>14.3</b> How does PLSR work?</a></li>
<li class="chapter" data-level="14.4" data-path="pls.html"><a href="pls.html#plsr-algorithm"><i class="fa fa-check"></i><b>14.4</b> PLSR Algorithm</a><ul>
<li class="chapter" data-level="14.4.1" data-path="pls.html"><a href="pls.html#pls-solution-with-original-variables"><i class="fa fa-check"></i><b>14.4.1</b> PLS Solution with original variables</a></li>
<li class="chapter" data-level="14.4.2" data-path="pls.html"><a href="pls.html#size-of-coefficients-1"><i class="fa fa-check"></i><b>14.4.2</b> Size of Coefficients</a></li>
<li class="chapter" data-level="14.4.3" data-path="pls.html"><a href="pls.html#some-properties"><i class="fa fa-check"></i><b>14.4.3</b> Some Properties</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="pls.html"><a href="pls.html#selecting-number-of-pls-components"><i class="fa fa-check"></i><b>14.5</b> Selecting Number of PLS Components</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="ridge.html"><a href="ridge.html"><i class="fa fa-check"></i><b>15</b> Ridge Regression</a><ul>
<li class="chapter" data-level="15.1" data-path="ridge.html"><a href="ridge.html#a-new-minimization-problem"><i class="fa fa-check"></i><b>15.1</b> A New Minimization Problem</a><ul>
<li class="chapter" data-level="15.1.1" data-path="ridge.html"><a href="ridge.html#constraining-regression-coefficients"><i class="fa fa-check"></i><b>15.1.1</b> Constraining Regression Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="ridge.html"><a href="ridge.html#a-new-minimization-solution"><i class="fa fa-check"></i><b>15.2</b> A New Minimization Solution</a></li>
<li class="chapter" data-level="15.3" data-path="ridge.html"><a href="ridge.html#what-does-rr-accomplish"><i class="fa fa-check"></i><b>15.3</b> What does RR accomplish?</a><ul>
<li class="chapter" data-level="15.3.1" data-path="ridge.html"><a href="ridge.html#how-to-find-lambda"><i class="fa fa-check"></i><b>15.3.1</b> How to find <span class="math inline">\(\lambda\)</span>?</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>VII Extending Linear Regression</b></span></li>
<li class="chapter" data-level="16" data-path="linear-extensions.html"><a href="linear-extensions.html"><i class="fa fa-check"></i><b>16</b> Beyond Linear Regression</a><ul>
<li class="chapter" data-level="16.1" data-path="linear-extensions.html"><a href="linear-extensions.html#introduction-2"><i class="fa fa-check"></i><b>16.1</b> Introduction</a></li>
<li class="chapter" data-level="16.2" data-path="linear-extensions.html"><a href="linear-extensions.html#expanding-the-regression-horizon"><i class="fa fa-check"></i><b>16.2</b> Expanding the Regression Horizon</a><ul>
<li class="chapter" data-level="16.2.1" data-path="linear-extensions.html"><a href="linear-extensions.html#linearity"><i class="fa fa-check"></i><b>16.2.1</b> Linearity</a></li>
<li class="chapter" data-level="16.2.2" data-path="linear-extensions.html"><a href="linear-extensions.html#parametric-and-nonparametric"><i class="fa fa-check"></i><b>16.2.2</b> Parametric and Nonparametric</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="linear-extensions.html"><a href="linear-extensions.html#transforming-features"><i class="fa fa-check"></i><b>16.3</b> Transforming Features</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="nonparametric.html"><a href="nonparametric.html"><i class="fa fa-check"></i><b>17</b> Nonparametric Regression</a><ul>
<li class="chapter" data-level="17.1" data-path="nonparametric.html"><a href="nonparametric.html#conditional-averages"><i class="fa fa-check"></i><b>17.1</b> Conditional Averages</a></li>
<li class="chapter" data-level="17.2" data-path="nonparametric.html"><a href="nonparametric.html#looking-at-the-neighbors"><i class="fa fa-check"></i><b>17.2</b> Looking at the Neighbors</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="knn.html"><a href="knn.html"><i class="fa fa-check"></i><b>18</b> Nearest Neighbor Estimates</a><ul>
<li class="chapter" data-level="18.1" data-path="knn.html"><a href="knn.html#introduction-3"><i class="fa fa-check"></i><b>18.1</b> Introduction</a></li>
<li class="chapter" data-level="18.2" data-path="knn.html"><a href="knn.html#k-nearest-neighbors-knn"><i class="fa fa-check"></i><b>18.2</b> <span class="math inline">\(k\)</span> Nearest Neighbors (KNN)</a><ul>
<li class="chapter" data-level="18.2.1" data-path="knn.html"><a href="knn.html#distance-measures"><i class="fa fa-check"></i><b>18.2.1</b> Distance Measures</a></li>
<li class="chapter" data-level="18.2.2" data-path="knn.html"><a href="knn.html#knn-estimator"><i class="fa fa-check"></i><b>18.2.2</b> KNN Estimator</a></li>
<li class="chapter" data-level="18.2.3" data-path="knn.html"><a href="knn.html#how-to-find-k"><i class="fa fa-check"></i><b>18.2.3</b> How to find <span class="math inline">\(k\)</span>?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html"><i class="fa fa-check"></i><b>19</b> Kernel Smoothers</a><ul>
<li class="chapter" data-level="19.1" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#introduction-4"><i class="fa fa-check"></i><b>19.1</b> Introduction</a></li>
<li class="chapter" data-level="19.2" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#kernel-smoothers-1"><i class="fa fa-check"></i><b>19.2</b> Kernel Smoothers</a><ul>
<li class="chapter" data-level="19.2.1" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#kernel-functions"><i class="fa fa-check"></i><b>19.2.1</b> Kernel Functions</a></li>
<li class="chapter" data-level="19.2.2" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#weights-from-kernels"><i class="fa fa-check"></i><b>19.2.2</b> Weights from Kernels</a></li>
<li class="chapter" data-level="19.2.3" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#kernel-estimator"><i class="fa fa-check"></i><b>19.2.3</b> Kernel Estimator</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#local-polynomial-estimators"><i class="fa fa-check"></i><b>19.3</b> Local Polynomial Estimators</a></li>
</ul></li>
<li class="part"><span><b>VIII Classification</b></span></li>
<li class="chapter" data-level="20" data-path="classif.html"><a href="classif.html"><i class="fa fa-check"></i><b>20</b> Classification</a><ul>
<li class="chapter" data-level="20.1" data-path="classif.html"><a href="classif.html#introduction-5"><i class="fa fa-check"></i><b>20.1</b> Introduction</a><ul>
<li class="chapter" data-level="20.1.1" data-path="classif.html"><a href="classif.html#credit-score-example"><i class="fa fa-check"></i><b>20.1.1</b> Credit Score Example</a></li>
<li class="chapter" data-level="20.1.2" data-path="classif.html"><a href="classif.html#toy-example-1"><i class="fa fa-check"></i><b>20.1.2</b> Toy Example</a></li>
<li class="chapter" data-level="20.1.3" data-path="classif.html"><a href="classif.html#two-class-problem"><i class="fa fa-check"></i><b>20.1.3</b> Two-class Problem</a></li>
<li class="chapter" data-level="20.1.4" data-path="classif.html"><a href="classif.html#bayes-rule-reminder"><i class="fa fa-check"></i><b>20.1.4</b> Bayes’ Rule Reminder</a></li>
</ul></li>
<li class="chapter" data-level="20.2" data-path="classif.html"><a href="classif.html#bayes-classifier"><i class="fa fa-check"></i><b>20.2</b> Bayes Classifier</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="logistic.html"><a href="logistic.html"><i class="fa fa-check"></i><b>21</b> Logistic Regression</a><ul>
<li class="chapter" data-level="21.1" data-path="logistic.html"><a href="logistic.html#motivation-1"><i class="fa fa-check"></i><b>21.1</b> Motivation</a><ul>
<li class="chapter" data-level="21.1.1" data-path="logistic.html"><a href="logistic.html#first-approach-fitting-a-line"><i class="fa fa-check"></i><b>21.1.1</b> First Approach: Fitting a Line</a></li>
<li class="chapter" data-level="21.1.2" data-path="logistic.html"><a href="logistic.html#second-approach-harsh-thresholding"><i class="fa fa-check"></i><b>21.1.2</b> Second Approach: Harsh Thresholding</a></li>
<li class="chapter" data-level="21.1.3" data-path="logistic.html"><a href="logistic.html#third-approach-conditional-means"><i class="fa fa-check"></i><b>21.1.3</b> Third Approach: Conditional Means</a></li>
</ul></li>
<li class="chapter" data-level="21.2" data-path="logistic.html"><a href="logistic.html#logistic-regression-model"><i class="fa fa-check"></i><b>21.2</b> Logistic Regression Model</a><ul>
<li class="chapter" data-level="21.2.1" data-path="logistic.html"><a href="logistic.html#the-criterion-being-optimized"><i class="fa fa-check"></i><b>21.2.1</b> The Criterion Being Optimized</a></li>
<li class="chapter" data-level="21.2.2" data-path="logistic.html"><a href="logistic.html#another-way-to-solve-logistic-regression"><i class="fa fa-check"></i><b>21.2.2</b> Another Way to Solve Logistic Regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="22" data-path="discrim.html"><a href="discrim.html"><i class="fa fa-check"></i><b>22</b> Preamble for Discriminant Analysis</a><ul>
<li class="chapter" data-level="22.1" data-path="discrim.html"><a href="discrim.html#motivation-2"><i class="fa fa-check"></i><b>22.1</b> Motivation</a><ul>
<li class="chapter" data-level="22.1.1" data-path="discrim.html"><a href="discrim.html#distinguishing-species"><i class="fa fa-check"></i><b>22.1.1</b> Distinguishing Species</a></li>
<li class="chapter" data-level="22.1.2" data-path="discrim.html"><a href="discrim.html#sum-of-squares-decomposition"><i class="fa fa-check"></i><b>22.1.2</b> Sum of Squares Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="22.2" data-path="discrim.html"><a href="discrim.html#derived-ratios-from-sum-of-squares"><i class="fa fa-check"></i><b>22.2</b> Derived Ratios from Sum-of-Squares</a><ul>
<li class="chapter" data-level="22.2.1" data-path="discrim.html"><a href="discrim.html#correlation-ratio"><i class="fa fa-check"></i><b>22.2.1</b> Correlation Ratio</a></li>
<li class="chapter" data-level="22.2.2" data-path="discrim.html"><a href="discrim.html#f-ratio"><i class="fa fa-check"></i><b>22.2.2</b> F-Ratio</a></li>
</ul></li>
<li class="chapter" data-level="22.3" data-path="discrim.html"><a href="discrim.html#geometric-perspective"><i class="fa fa-check"></i><b>22.3</b> Geometric Perspective</a><ul>
<li class="chapter" data-level="22.3.1" data-path="discrim.html"><a href="discrim.html#clouds-from-class-structure"><i class="fa fa-check"></i><b>22.3.1</b> Clouds from Class Structure</a></li>
<li class="chapter" data-level="22.3.2" data-path="discrim.html"><a href="discrim.html#dispersion-decomposition"><i class="fa fa-check"></i><b>22.3.2</b> Dispersion Decomposition</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="23" data-path="cda.html"><a href="cda.html"><i class="fa fa-check"></i><b>23</b> Canonical Discriminant Analysis</a><ul>
<li class="chapter" data-level="23.1" data-path="cda.html"><a href="cda.html#cda-semi-supervised-aspect"><i class="fa fa-check"></i><b>23.1</b> CDA: Semi-Supervised Aspect</a></li>
<li class="chapter" data-level="23.2" data-path="cda.html"><a href="cda.html#looking-for-a-discriminant-axis"><i class="fa fa-check"></i><b>23.2</b> Looking for a discriminant axis</a></li>
<li class="chapter" data-level="23.3" data-path="cda.html"><a href="cda.html#looking-for-a-compromise-criterion"><i class="fa fa-check"></i><b>23.3</b> Looking for a Compromise Criterion</a><ul>
<li class="chapter" data-level="23.3.1" data-path="cda.html"><a href="cda.html#correlation-ratio-criterion"><i class="fa fa-check"></i><b>23.3.1</b> Correlation Ratio Criterion</a></li>
<li class="chapter" data-level="23.3.2" data-path="cda.html"><a href="cda.html#f-ratio-criterion"><i class="fa fa-check"></i><b>23.3.2</b> F-ratio Criterion</a></li>
<li class="chapter" data-level="23.3.3" data-path="cda.html"><a href="cda.html#a-special-pca"><i class="fa fa-check"></i><b>23.3.3</b> A Special PCA</a></li>
</ul></li>
<li class="chapter" data-level="23.4" data-path="cda.html"><a href="cda.html#cda-supervised-aspect"><i class="fa fa-check"></i><b>23.4</b> CDA: Supervised Aspect</a><ul>
<li class="chapter" data-level="23.4.1" data-path="cda.html"><a href="cda.html#distance-behind-cda"><i class="fa fa-check"></i><b>23.4.1</b> Distance behind CDA</a></li>
<li class="chapter" data-level="23.4.2" data-path="cda.html"><a href="cda.html#predictive-idea"><i class="fa fa-check"></i><b>23.4.2</b> Predictive Idea</a></li>
<li class="chapter" data-level="23.4.3" data-path="cda.html"><a href="cda.html#cda-classifier"><i class="fa fa-check"></i><b>23.4.3</b> CDA Classifier</a></li>
<li class="chapter" data-level="23.4.4" data-path="cda.html"><a href="cda.html#limitations-of-cda-classifier"><i class="fa fa-check"></i><b>23.4.4</b> Limitations of CDA classifier</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="24" data-path="discanalysis.html"><a href="discanalysis.html"><i class="fa fa-check"></i><b>24</b> Discriminant Analysis</a><ul>
<li class="chapter" data-level="24.1" data-path="discanalysis.html"><a href="discanalysis.html#probabilistic-da"><i class="fa fa-check"></i><b>24.1</b> Probabilistic DA</a><ul>
<li class="chapter" data-level="24.1.1" data-path="discanalysis.html"><a href="discanalysis.html#normal-distributions"><i class="fa fa-check"></i><b>24.1.1</b> Normal Distributions</a></li>
<li class="chapter" data-level="24.1.2" data-path="discanalysis.html"><a href="discanalysis.html#estimating-parameters-of-normal-distributions"><i class="fa fa-check"></i><b>24.1.2</b> Estimating Parameters of Normal Distributions</a></li>
</ul></li>
<li class="chapter" data-level="24.2" data-path="discanalysis.html"><a href="discanalysis.html#discriminant-functions"><i class="fa fa-check"></i><b>24.2</b> Discriminant Functions</a></li>
<li class="chapter" data-level="24.3" data-path="discanalysis.html"><a href="discanalysis.html#quadratic-discriminant-analysis-qda"><i class="fa fa-check"></i><b>24.3</b> Quadratic Discriminant Analysis (QDA)</a></li>
<li class="chapter" data-level="24.4" data-path="discanalysis.html"><a href="discanalysis.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>24.4</b> Linear Discriminant Analysis</a><ul>
<li class="chapter" data-level="24.4.1" data-path="discanalysis.html"><a href="discanalysis.html#canonical-discriminant-analysis"><i class="fa fa-check"></i><b>24.4.1</b> Canonical Discriminant Analysis</a></li>
<li class="chapter" data-level="24.4.2" data-path="discanalysis.html"><a href="discanalysis.html#naive-bayes"><i class="fa fa-check"></i><b>24.4.2</b> Naive Bayes</a></li>
<li class="chapter" data-level="24.4.3" data-path="discanalysis.html"><a href="discanalysis.html#fifth-case"><i class="fa fa-check"></i><b>24.4.3</b> Fifth Case</a></li>
</ul></li>
<li class="chapter" data-level="24.5" data-path="discanalysis.html"><a href="discanalysis.html#comparing-the-cases"><i class="fa fa-check"></i><b>24.5</b> Comparing the Cases</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="classperformance.html"><a href="classperformance.html"><i class="fa fa-check"></i><b>25</b> Performance of Classifiers</a><ul>
<li class="chapter" data-level="25.1" data-path="classperformance.html"><a href="classperformance.html#classification-error-measures"><i class="fa fa-check"></i><b>25.1</b> Classification Error Measures</a><ul>
<li class="chapter" data-level="25.1.1" data-path="classperformance.html"><a href="classperformance.html#errors-for-binary-response"><i class="fa fa-check"></i><b>25.1.1</b> Errors for Binary Response</a></li>
<li class="chapter" data-level="25.1.2" data-path="classperformance.html"><a href="classperformance.html#error-for-categorical-response"><i class="fa fa-check"></i><b>25.1.2</b> Error for Categorical Response</a></li>
</ul></li>
<li class="chapter" data-level="25.2" data-path="classperformance.html"><a href="classperformance.html#confusion-matrices"><i class="fa fa-check"></i><b>25.2</b> Confusion Matrices</a></li>
<li class="chapter" data-level="25.3" data-path="classperformance.html"><a href="classperformance.html#binary-response-example"><i class="fa fa-check"></i><b>25.3</b> Binary Response Example</a><ul>
<li class="chapter" data-level="25.3.1" data-path="classperformance.html"><a href="classperformance.html#application-for-checking-account"><i class="fa fa-check"></i><b>25.3.1</b> Application for Checking Account</a></li>
<li class="chapter" data-level="25.3.2" data-path="classperformance.html"><a href="classperformance.html#application-for-loan"><i class="fa fa-check"></i><b>25.3.2</b> Application for Loan</a></li>
</ul></li>
<li class="chapter" data-level="25.4" data-path="classperformance.html"><a href="classperformance.html#decision-rules-and-errors"><i class="fa fa-check"></i><b>25.4</b> Decision Rules and Errors</a></li>
<li class="chapter" data-level="25.5" data-path="classperformance.html"><a href="classperformance.html#roc-curves"><i class="fa fa-check"></i><b>25.5</b> ROC Curves</a><ul>
<li class="chapter" data-level="25.5.1" data-path="classperformance.html"><a href="classperformance.html#graphing-roc-curves"><i class="fa fa-check"></i><b>25.5.1</b> Graphing ROC curves</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IX Unsupervised II: Clustering</b></span></li>
<li class="chapter" data-level="26" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>26</b> Clustering</a><ul>
<li class="chapter" data-level="26.1" data-path="clustering.html"><a href="clustering.html#about-clustering"><i class="fa fa-check"></i><b>26.1</b> About Clustering</a><ul>
<li class="chapter" data-level="26.1.1" data-path="clustering.html"><a href="clustering.html#types-of-clustering"><i class="fa fa-check"></i><b>26.1.1</b> Types of Clustering</a></li>
<li class="chapter" data-level="26.1.2" data-path="clustering.html"><a href="clustering.html#hard-clustering"><i class="fa fa-check"></i><b>26.1.2</b> Hard Clustering</a></li>
</ul></li>
<li class="chapter" data-level="26.2" data-path="clustering.html"><a href="clustering.html#dispersion-measures"><i class="fa fa-check"></i><b>26.2</b> Dispersion Measures</a></li>
<li class="chapter" data-level="26.3" data-path="clustering.html"><a href="clustering.html#complexity-in-clustering"><i class="fa fa-check"></i><b>26.3</b> Complexity in Clustering</a></li>
</ul></li>
<li class="chapter" data-level="27" data-path="kmeans.html"><a href="kmeans.html"><i class="fa fa-check"></i><b>27</b> K-Means</a><ul>
<li class="chapter" data-level="27.1" data-path="kmeans.html"><a href="kmeans.html#toy-example-2"><i class="fa fa-check"></i><b>27.1</b> Toy Example</a></li>
<li class="chapter" data-level="27.2" data-path="kmeans.html"><a href="kmeans.html#what-does-k-means-do"><i class="fa fa-check"></i><b>27.2</b> What does K-means do?</a></li>
<li class="chapter" data-level="27.3" data-path="kmeans.html"><a href="kmeans.html#k-means-algorithms"><i class="fa fa-check"></i><b>27.3</b> K-Means Algorithms</a><ul>
<li class="chapter" data-level="27.3.1" data-path="kmeans.html"><a href="kmeans.html#classic-version"><i class="fa fa-check"></i><b>27.3.1</b> Classic Version</a></li>
<li class="chapter" data-level="27.3.2" data-path="kmeans.html"><a href="kmeans.html#moving-centers-algorithm"><i class="fa fa-check"></i><b>27.3.2</b> Moving Centers Algorithm</a></li>
<li class="chapter" data-level="27.3.3" data-path="kmeans.html"><a href="kmeans.html#dynamic-clouds"><i class="fa fa-check"></i><b>27.3.3</b> Dynamic Clouds</a></li>
<li class="chapter" data-level="27.3.4" data-path="kmeans.html"><a href="kmeans.html#choosing-k"><i class="fa fa-check"></i><b>27.3.4</b> Choosing <span class="math inline">\(K\)</span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="28" data-path="hclus.html"><a href="hclus.html"><i class="fa fa-check"></i><b>28</b> Hierarchical Clustering</a><ul>
<li class="chapter" data-level="28.1" data-path="hclus.html"><a href="hclus.html#agglomerative-methods"><i class="fa fa-check"></i><b>28.1</b> Agglomerative Methods</a></li>
<li class="chapter" data-level="28.2" data-path="hclus.html"><a href="hclus.html#example-single-linkage"><i class="fa fa-check"></i><b>28.2</b> Example: Single Linkage</a><ul>
<li class="chapter" data-level="28.2.1" data-path="hclus.html"><a href="hclus.html#dendrogram"><i class="fa fa-check"></i><b>28.2.1</b> Dendrogram</a></li>
</ul></li>
<li class="chapter" data-level="28.3" data-path="hclus.html"><a href="hclus.html#example-complete-linkage"><i class="fa fa-check"></i><b>28.3</b> Example: Complete Linkage</a><ul>
<li class="chapter" data-level="28.3.1" data-path="hclus.html"><a href="hclus.html#cutting-dendograms"><i class="fa fa-check"></i><b>28.3.1</b> Cutting Dendograms</a></li>
<li class="chapter" data-level="28.3.2" data-path="hclus.html"><a href="hclus.html#pros-and-coons"><i class="fa fa-check"></i><b>28.3.2</b> Pros and COons</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>X Tree-based Methods</b></span></li>
<li class="chapter" data-level="29" data-path="trees.html"><a href="trees.html"><i class="fa fa-check"></i><b>29</b> Intro to Decision Trees</a><ul>
<li class="chapter" data-level="29.1" data-path="trees.html"><a href="trees.html#introduction-6"><i class="fa fa-check"></i><b>29.1</b> Introduction</a></li>
<li class="chapter" data-level="29.2" data-path="trees.html"><a href="trees.html#some-terminology"><i class="fa fa-check"></i><b>29.2</b> Some Terminology</a></li>
<li class="chapter" data-level="29.3" data-path="trees.html"><a href="trees.html#binary-trees"><i class="fa fa-check"></i><b>29.3</b> Binary Trees</a></li>
<li class="chapter" data-level="29.4" data-path="trees.html"><a href="trees.html#space-partitions"><i class="fa fa-check"></i><b>29.4</b> Space Partitions</a><ul>
<li class="chapter" data-level="29.4.1" data-path="trees.html"><a href="trees.html#the-process-of-building-a-tree"><i class="fa fa-check"></i><b>29.4.1</b> The Process of Building a Tree</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="30" data-path="tree-basics.html"><a href="tree-basics.html"><i class="fa fa-check"></i><b>30</b> Building Trees</a><ul>
<li class="chapter" data-level="30.1" data-path="tree-basics.html"><a href="tree-basics.html#binary-partitions"><i class="fa fa-check"></i><b>30.1</b> Binary Partitions</a><ul>
<li class="chapter" data-level="30.1.1" data-path="tree-basics.html"><a href="tree-basics.html#splits-of-binary-variables"><i class="fa fa-check"></i><b>30.1.1</b> Splits of Binary variables</a></li>
<li class="chapter" data-level="30.1.2" data-path="tree-basics.html"><a href="tree-basics.html#splits-of-nominal-variables"><i class="fa fa-check"></i><b>30.1.2</b> Splits of Nominal Variables</a></li>
<li class="chapter" data-level="30.1.3" data-path="tree-basics.html"><a href="tree-basics.html#splits-of-ordinal-variables"><i class="fa fa-check"></i><b>30.1.3</b> Splits of Ordinal Variables</a></li>
<li class="chapter" data-level="30.1.4" data-path="tree-basics.html"><a href="tree-basics.html#splits-continuous-variables"><i class="fa fa-check"></i><b>30.1.4</b> Splits Continuous variables</a></li>
</ul></li>
<li class="chapter" data-level="30.2" data-path="tree-basics.html"><a href="tree-basics.html#measures-of-heterogeneity"><i class="fa fa-check"></i><b>30.2</b> Measures of Heterogeneity</a><ul>
<li class="chapter" data-level="30.2.1" data-path="tree-basics.html"><a href="tree-basics.html#entropy"><i class="fa fa-check"></i><b>30.2.1</b> Entropy</a></li>
<li class="chapter" data-level="30.2.2" data-path="tree-basics.html"><a href="tree-basics.html#the-math-behind-entropy"><i class="fa fa-check"></i><b>30.2.2</b> The Math Behind Entropy</a></li>
<li class="chapter" data-level="30.2.3" data-path="tree-basics.html"><a href="tree-basics.html#gini-impurity"><i class="fa fa-check"></i><b>30.2.3</b> Gini Impurity</a></li>
<li class="chapter" data-level="30.2.4" data-path="tree-basics.html"><a href="tree-basics.html#toy-example-3"><i class="fa fa-check"></i><b>30.2.4</b> Toy Example</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Made with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">All Models Are Wrong: Concepts of Statistical Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="cda" class="section level1">
<h1><span class="header-section-number">23</span> Canonical Discriminant Analysis</h1>
<p>In this chapter we talk about <strong>Canonical Discriminant Analysis</strong> (CDA),
which is a special case of Linear Discriminant Analysis (LDA). The main reason
why we introduce CDA separately, is because this method has a somewhat
hybrid learning nature with two aspects:</p>
<ul>
<li><p><strong>Semi-supervised</strong>: On one hand, CDA has an unsupervised or descriptive
aspect that aims to tackle the following question. How to find a representation
of the objects which provides the best separation between classes?</p></li>
<li><p><strong>Supervised</strong>: On the other hand, CDA also has a decisively supervised
aspect that tackles the question: How to find the rules for assigning a class
to a given object?</p></li>
</ul>
<p>We begin with the semi-supervised part of CDA, approaching its exploratory
nature from a purely geometric approach. This discussion covers most of the
material in this chapter. And then at the end, we present its supervised aspect, describing how to use CDA for classification purposes.</p>
<div id="cda-semi-supervised-aspect" class="section level2">
<h2><span class="header-section-number">23.1</span> CDA: Semi-Supervised Aspect</h2>
<p>We can formulate the classfication problem behind CDA in a geometric way. For
sake of illustration, let’s consider three classes in a 2-dimensional space,
like depicted in the figure below.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-222"></span>
<img src="images/discrim/discrim_axes1.svg" alt="Three classes in 2-dim space" width="50%" />
<p class="caption">
Figure 23.1: Three classes in 2-dim space
</p>
</div>
<p>From an exploratory/descriptive perspective, we are looking for a good low
dimensional representation that separates the three classes. One option is to
consider the axis associated with the predictor <span class="math inline">\(X_1\)</span>, that is, the horizontal
axis in the next figure:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-223"></span>
<img src="images/discrim/discrim_axes2.svg" alt="Axis $X_1$ separates class 1 from groups 2 and 3" width="50%" />
<p class="caption">
Figure 23.2: Axis <span class="math inline">\(X_1\)</span> separates class 1 from groups 2 and 3
</p>
</div>
<p>If we project the individuals onto the <span class="math inline">\(X_1\)</span> axis, class 1 is well separated
from classes 2 and 3. However, class 2 is largely confounded with class 3.</p>
<p>Another option is to consider the axis associated with the predictor <span class="math inline">\(X_2\)</span>,
that is, the vertical axis in the figure below:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-224"></span>
<img src="images/discrim/discrim_axes3.svg" alt="Axis $X_1$ separates class 3 from groups 1 and 2" width="50%" />
<p class="caption">
Figure 23.3: Axis <span class="math inline">\(X_1\)</span> separates class 3 from groups 1 and 2
</p>
</div>
<p>As you can tell, if we project the individuals onto this axis <span class="math inline">\(X_2\)</span>, class 3 is
well separated from classes 1 and 2. However, class 1 is largely confounded with
class 2.</p>
<p>Is there an axis or dimension that “best” separates the three clouds?</p>
<p>We can try to look for an optimal representation in the sense of finding
an axis that best separates the clouds, like in the following diagram:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-225"></span>
<img src="images/discrim/discrim_axes4.svg" alt="An axis the best separates all three classes" width="50%" />
<p class="caption">
Figure 23.4: An axis the best separates all three classes
</p>
</div>
<p>To summarize, the exploratory aspect of canonical discriminant analysis has to
do with seeking a low dimensional representation in which the class of objects
are well separated.</p>
</div>
<div id="looking-for-a-discriminant-axis" class="section level2">
<h2><span class="header-section-number">23.2</span> Looking for a discriminant axis</h2>
<p>How do we find a low dimensional representation of the objects which provides
the best separation between classes? To answer this question, we can look for
an axis <span class="math inline">\(\Delta_u\)</span>, spanned by some vector <span class="math inline">\(\mathbf{u}\)</span>, such that the linear
combination</p>
<p><span class="math display">\[
Z = u_1 X_1 + u_2 X_2
\]</span></p>
<p>separates all three groups in an adequate way.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-226"></span>
<img src="images/discrim/discrim_axes5.svg" alt="Looking for a discriminant axis" width="50%" />
<p class="caption">
Figure 23.5: Looking for a discriminant axis
</p>
</div>
<p>Algebraically, the idea is to look for a linear combination of the predictors:</p>
<p><span class="math display">\[
\mathbf{z} = \mathbf{Xu}
\]</span></p>
<p>that <em>ideally</em> could achieve the following two goals:</p>
<ul>
<li><p>Minimize within-class dispersion (<span class="math inline">\(\text{wss}\)</span>): <span class="math inline">\(\quad min \{ \mathbf{u^\mathsf{T} W u \}}\)</span></p></li>
<li><p>Maximize between-class dispersion (<span class="math inline">\(\text{bss}\)</span>): <span class="math inline">\(\quad max \{ \mathbf{u^\mathsf{T} B u \}}\)</span></p></li>
</ul>
<p>On one hand, it would be nice to have <span class="math inline">\(\mathbf{u}\)</span>, such that the between-class
dispersion is maximized. This corresponds to a situation in which the class
centroids are well separated:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-227"></span>
<img src="images/discrim/discrim_maximize_between.svg" alt="Maximize between-class dispersion" width="60%" />
<p class="caption">
Figure 23.6: Maximize between-class dispersion
</p>
</div>
<p>On the other hand, it would also make sense to have <span class="math inline">\(\mathbf{u}\)</span>, such that
the within-class dispersion is minimized. This implies having classes in which,
on average, the “inner” variation is small (i.e. concentrated local dispersion)</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-228"></span>
<img src="images/discrim/discrim_minimize_within.svg" alt="Minimize within-class dispersion" width="60%" />
<p class="caption">
Figure 23.7: Minimize within-class dispersion
</p>
</div>
<p>Can we find such a mythical vector <span class="math inline">\(\mathbf{u}\)</span>?</p>
<p>We have not so good news. It is generally impossible to find an axis
<span class="math inline">\(\Delta_u\)</span>, spanned by <span class="math inline">\(\mathbf{u}\)</span>, which simoultaneously:</p>
<ul>
<li>maximizes the between-groups variance</li>
<li>minimizes the within-groups variance</li>
</ul>
<p>Let’s see a cartoon picture of this issue. Say we have two classes, each
one with its centroid.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-229"></span>
<img src="images/discrim/discrim_double_goal1.svg" alt="Minimize within-class dispersion" width="40%" />
<p class="caption">
Figure 23.8: Minimize within-class dispersion
</p>
</div>
<p>Maximizing the between-class separation, involves choosing <span class="math inline">\(\mathbf{u}_a\)</span>
parallel to the segment linking the centroids. In other words, the direction
of <span class="math inline">\(\mathbf{u}\)</span> is the one that crosses the centroids:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-230"></span>
<img src="images/discrim/discrim_double_goal2.svg" alt="Maximize between-class dispersion" width="40%" />
<p class="caption">
Figure 23.9: Maximize between-class dispersion
</p>
</div>
<p>Minimizing the within-class separation, involves finding <span class="math inline">\(\mathbf{u}_b\)</span>
perpendicular to the principal axis of the ellipses. This type of <span class="math inline">\(\mathbf{u}\)</span>
does not necessarily crosses through the centroids:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-231"></span>
<img src="images/discrim/discrim_double_goal3.svg" alt="Minimize within-class dispersion" width="40%" />
<p class="caption">
Figure 23.10: Minimize within-class dispersion
</p>
</div>
<p>In general, we end up with two possibilities <span class="math inline">\(\mathbf{u}_a \neq \mathbf{u}_b\)</span>:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-232"></span>
<img src="images/discrim/discrim_double_goal4.svg" alt="Double goal of discriminant analysis ... generally impossible" width="40%" />
<p class="caption">
Figure 23.11: Double goal of discriminant analysis … generally impossible
</p>
</div>
<p>Let’s summarize the geometric motivation behind canonical discriminant analysis.
We seek to find the linear combination of the predictors such
that the between-class variance is maximized relative to the within-class
variance. In other words, we want to find the combination of the predictors
that gives maximum separation between the centroids of the data while at the
same time minimizing the variation within each class of data. In general,
accomplishing these two goals in a simultaneous way is impossible.</p>
</div>
<div id="looking-for-a-compromise-criterion" class="section level2">
<h2><span class="header-section-number">23.3</span> Looking for a Compromise Criterion</h2>
<p>So far we have an impossible simultaneity involving a minimization criterion,
as well as a maximization criterion:</p>
<p><span class="math display">\[
min \{ \mathbf{u^\mathsf{T} W u} \} \Longrightarrow  \mathbf{W u} = \alpha \mathbf{u} \\
\textsf{and} \\
max \{ \mathbf{u^\mathsf{T} B u} \} \Longrightarrow  \mathbf{B u} = \beta \mathbf{u}
\]</span></p>
<p>What can we do about this simultaneous impossibility?</p>
<p>Well, we need to look for a compromise. This is where the variance
decomposition discussed in the <a href="discrim.html#discrim">previous chapter</a> comes handy:</p>
<p><span class="math display">\[
\mathbf{V} = \mathbf{W} + \mathbf{B}
\]</span></p>
<p>Doing some algebra, it can be shown that the quadratic form
<span class="math inline">\(\mathbf{u^\mathsf{T} V u}\)</span> can be decomposed as:</p>
<p><span class="math display">\[
\mathbf{u^\mathsf{T} V u} = \mathbf{u^\mathsf{T} W u} + \mathbf{u^\mathsf{T} B u}
\]</span></p>
<p>Again, we are pursuing a dual goal that is, in general, hard to accomplish:</p>
<p><span class="math display">\[
\mathbf{u^\mathsf{T} V u} = \underbrace{\mathbf{u^\mathsf{T} W u}}_{\text{minimize}} + \underbrace{\mathbf{u^\mathsf{T} B u}}_{\text{maximize}}
\]</span></p>
<p>We have two options for the compromise:</p>
<p><span class="math display">\[
max \left \{ \frac{\mathbf{u^\mathsf{T} B u}}{\mathbf{u^\mathsf{T} V u}} \right \} 
\quad \textsf{OR} \quad
max \left \{ \frac{\mathbf{u^\mathsf{T} B u}}{\mathbf{u^\mathsf{T} W u}} \right \}
\]</span></p>
<p>which are actually associated to the two ratios described in the previous
chapter: correlation ratio <span class="math inline">\(\eta^2\)</span>, and <span class="math inline">\(F\)</span>-ratio:</p>
<p><span class="math display">\[
\eta^2 \leftrightarrow \frac{\mathbf{u^\mathsf{T} B u}}{\mathbf{u^\mathsf{T} V u}}, 
\qquad
F \leftrightarrow\frac{\mathbf{u^\mathsf{T} B u}}{\mathbf{u^\mathsf{T} W u}} 
\]</span></p>
<p>Which option to choose? The good news is that both options are equivalent, so
using any one of them should give you similar—although not exactly identical—results.</p>
<div id="correlation-ratio-criterion" class="section level3">
<h3><span class="header-section-number">23.3.1</span> Correlation Ratio Criterion</h3>
<p>Suppose we decide to work with the first criterion. We look for <span class="math inline">\(\mathbf{u}\)</span>
such that:</p>
<p><span class="math display">\[
max \left \{ \frac{\mathbf{u^\mathsf{T} B u}}{\mathbf{u^\mathsf{T} V u}} \right \} 
\]</span></p>
<p>This criterion is scale invariant, meaning that we use any scale variation of
<span class="math inline">\(\mathbf{u}\)</span>: i.e. <span class="math inline">\(\alpha \mathbf{u}\)</span>. For convenience, we can impose a
normalizing restriction: <span class="math inline">\(\mathbf{u^\mathsf{T} V u} = 1\)</span>. Consequently:</p>
<p><span class="math display">\[
max \left \{ \frac{\mathbf{u^\mathsf{T} B u}}{\mathbf{u^\mathsf{T} V u}} \right \} 
\Longleftrightarrow max \{\mathbf{u^\mathsf{T} B u}\} \quad \text{s.t.} \quad 
\mathbf{u^\mathsf{T} V u} = 1
\]</span></p>
<p>Using the method of Lagrangian multiplier:</p>
<p><span class="math display">\[
L(\mathbf{u}) = \mathbf{u^\mathsf{T} B u} - \lambda (\mathbf{u^\mathsf{T} V u} - 1)
\]</span></p>
<p>Deriving w.r.t <span class="math inline">\(\mathbf{u}\)</span> and equating to zero:</p>
<p><span class="math display">\[
\frac{\partial L(\mathbf{u})}{\partial \mathbf{u}} = 2 \mathbf{Bu} - 2 \lambda \mathbf{Vu} = \mathbf{0}
\]</span></p>
<p>The optimal vector <span class="math inline">\(\mathbf{u}\)</span> is such that:</p>
<p><span class="math display">\[
\mathbf{Bu} = \lambda \mathbf{Vu}
\]</span></p>
<p>If the matrix <span class="math inline">\(\mathbf{V}\)</span> is inversible (which it is in general) then:</p>
<p><span class="math display">\[
\mathbf{V^{-1}Bu} = \lambda \mathbf{u}
\]</span></p>
<p>that is, the optimal vector <span class="math inline">\(\mathbf{u}\)</span> is eigenvector of <span class="math inline">\(\mathbf{V^{-1}B}\)</span>.
Keep in mind that, in general, <span class="math inline">\(\mathbf{V^{-1}B}\)</span> is not symmetric.</p>
</div>
<div id="f-ratio-criterion" class="section level3">
<h3><span class="header-section-number">23.3.2</span> F-ratio Criterion</h3>
<p>If we decide to work with the criterion associated to the <span class="math inline">\(F\)</span>-ratio, then the
criterion to be maximized is:</p>
<p><span class="math display">\[
max \left \{ \frac{\mathbf{u^\mathsf{T} B u}}{\mathbf{u^\mathsf{T} W u}} \right \} 
\Longleftrightarrow max \{\mathbf{u^\mathsf{T} B u}\} \quad \text{s.t.} \quad 
\mathbf{u^\mathsf{T} W u} = 1
\]</span></p>
<p>Applying the same Lagrangian procedure, with a multiplier <span class="math inline">\(\rho\)</span>, we have that
<span class="math inline">\(\mathbf{u}\)</span> is such a vector that:</p>
<p><span class="math display">\[
\mathbf{Bu} = \rho \mathbf{Wu}
\]</span></p>
<p>and if <span class="math inline">\(\mathbf{W}\)</span> is invertible (which it is in most cases), then it can be
shown that <span class="math inline">\(\mathbf{u}\)</span> is also eigenvector of <span class="math inline">\(\mathbf{W^{-1}B}\)</span>, associated
to eigenvalue <span class="math inline">\(\rho\)</span>:</p>
<p><span class="math display">\[
\mathbf{W^{-1}Bu} = \rho \mathbf{u}
\]</span></p>
<p>Likewise, keep in mind that, in general, <span class="math inline">\(\mathbf{W^{-1}B}\)</span> is not symmetric.</p>
<p>The relationship between the eigenvalues <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(\rho\)</span> is:</p>
<p><span class="math display">\[
\rho = \frac{\lambda}{1 - \lambda}
\]</span></p>
<p>Why is it important to keep in mind that, in general, both matrices
<span class="math inline">\(\mathbf{V^{-1}B}\)</span> and <span class="math inline">\(\mathbf{W^{-1}B}\)</span> are not symmetric? Because most
software routines that diagonilize a matrix (to find its eigenvalue
decomposition) use the
<a href="https://en.wikipedia.org/wiki/Jacobi_eigenvalue_algorithm">Jacobi method</a>,
which works with symmetric matrices.</p>
</div>
<div id="a-special-pca" class="section level3">
<h3><span class="header-section-number">23.3.3</span> A Special PCA</h3>
<p>What do <span class="math inline">\(\mathbf{u}\)</span> and <span class="math inline">\(\mathbf{W^{-1}}\)</span> correspond in geometric terms?
The vector <span class="math inline">\(\mathbf{u}\)</span> is the axis from the PCA on the cloud of centroids
<span class="math inline">\(\mathbf{g_1}, \mathbf{g_2}, \dots, \mathbf{g_K}\)</span>, but it is an axis on which
the points are projected <strong>obliquely</strong>, not orthogonally.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-233"></span>
<img src="images/discrim/discrim_metrics1.svg" alt="Oblique Projection" width="50%" />
<p class="caption">
Figure 23.12: Oblique Projection
</p>
</div>
<p>Without this obliqueness, corresponding to the equivalent metrics
<span class="math inline">\(\mathbf{V^{-1}}\)</span> and <span class="math inline">\(\mathbf{W^{-1}}\)</span>, this would be a simple PCA performed
on the centroids, in which the classes would be less well separated, because of
an orthogonal projection.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-234"></span>
<img src="images/discrim/discrim_metrics2.svg" alt="Orthogonal Projection" width="50%" />
<p class="caption">
Figure 23.13: Orthogonal Projection
</p>
</div>
<p>The implication of using a metric matrix such as <span class="math inline">\(\mathbf{W^{-1}}\)</span>
(or <span class="math inline">\(\mathbf{V^{-1}}\)</span>), is that the separation of two points depends not only on
a Euclidean measurement, but also on the variance and correlation of the
variables. So, in summary:</p>
<ul>
<li><span class="math inline">\(\mathbf{u}\)</span> is the vector associated to the so-called <strong>canonical axis</strong></li>
<li>When the first canonical axis has been determined, we search for a 2nd one</li>
<li>The second axis should be the most discriminant and uncorrelated with the
first one</li>
<li>This procedure is repeated until the number of axis reaches the minimum of:
<span class="math inline">\(K - 1\)</span> and <span class="math inline">\(p\)</span></li>
</ul>
<p>In fact, it is not the canonical axes that are manipulated directly, but the
<em>canonical variables</em> or vectors associated to the canonical axes.</p>
</div>
</div>
<div id="cda-supervised-aspect" class="section level2">
<h2><span class="header-section-number">23.4</span> CDA: Supervised Aspect</h2>
<p>The supervised learning aspect of CDA has to do with the question: how do we
use it for classification purposes? This involves establishing a decision rule
that let us predict the class of an object.</p>
<p>CDA proposes a geometric rule of classification. As we will see, such rule has
a linear form. This is the reason why we consider CDA to be a special member
of the family of Linear Discriminant Analysis.</p>
<div id="distance-behind-cda" class="section level3">
<h3><span class="header-section-number">23.4.1</span> Distance behind CDA</h3>
<p>The first thing we need to discuss is the notion of distance used in CDA. For
this, consider two vectors <span class="math inline">\(\mathbf{a}, \mathbf{b} \in \mathbb{R}^{p}\)</span>. Recall
that the inner product of <span class="math inline">\(\mathbf{a}\)</span> and <span class="math inline">\(\mathbf{b}\)</span> is defined to be:</p>
<p><span class="math display">\[
\langle \mathbf{a}, \mathbf{b} \rangle = \mathbf{a}^\mathsf{T} \mathbf{b}
\]</span></p>
<p>Note that we can equivalently write the inner product as:</p>
<p><span class="math display">\[
\langle \mathbf{a}, \mathbf{b} \rangle = \mathbf{a}^\mathsf{T} \mathbf{I_p} \mathbf{b}
\]</span></p>
<p>where <span class="math inline">\(\mathbf{I_p}\)</span> denotes the <span class="math inline">\(p \times p\)</span> identity matrix. This notation
allows us to generalize inner products: for a symmetric matrix <span class="math inline">\(\mathbf{M}\)</span>
(called the <strong>metric matrix</strong>) of conformable dimensions, we define the
inner product of <span class="math inline">\(\mathbf{a}\)</span> and <span class="math inline">\(\mathbf{b}\)</span>, under metric <span class="math inline">\(\mathbf{M}\)</span>, to be</p>
<p><span class="math display">\[
\langle \mathbf{a}, \mathbf{b} \rangle_{\mathbf{M}} = \mathbf{a}^\mathsf{T} \mathbf{M} \mathbf{b}
\]</span></p>
<p>With this new notion of an inner product, we can also expand our definition of
<strong>distance</strong>. Recall that the squared Euclidean distance between two vectors
<span class="math inline">\(\mathbf{x_i}\)</span> and <span class="math inline">\(\mathbf{x_\ell}\)</span> is defined as:</p>
<p><span class="math display">\[\begin{align*}
d^2(i, \ell) &amp;= \langle \mathbf{x_i} - \mathbf{x_\ell}, \mathbf{x_i} - \mathbf{x_\ell} \rangle \\
&amp;= (\mathbf{x_i} - \mathbf{x_\ell})^{\mathsf{T}} (\mathbf{x_i} - \mathbf{x_\ell}) \\
&amp;= (\mathbf{x_i} - \mathbf{x_\ell})^{\mathsf{T}}  \mathbf{I_p} (\mathbf{x_i} - \mathbf{x_\ell})
\end{align*}\]</span></p>
<p>Now, we can play the same game as we did for the inner product and replace
<span class="math inline">\(\mathbf{I_p}\)</span> with any metric matrix <span class="math inline">\(\mathbf{M}\)</span> to obtain a generalized
distance metric:</p>
<p><span class="math display">\[
d^2_{\mathbf{M}} (i, \ell)  = (\mathbf{x_i} - \mathbf{x_\ell})^{\mathsf{T}}  \mathbf{M} (\mathbf{x_i} - \mathbf{x_\ell})
\]</span></p>
</div>
<div id="predictive-idea" class="section level3">
<h3><span class="header-section-number">23.4.2</span> Predictive Idea</h3>
<p>The classification rule used in CDA consists of assigning each individual
<span class="math inline">\(\mathbf{x_i}\)</span> to the class <span class="math inline">\(\mathcal{C_k}\)</span> for which the distance to the centroid
<span class="math inline">\(\mathbf{g_k}\)</span> is minimal.</p>
<p><span class="math display">\[
\text{assign object } i \text{ to the class for which } d^2(\mathbf{x_i},\mathbf{g_k}) \text{ is minimal}
\]</span></p>
<p>But here we don’t use the typical Euclidean distance. Instead, in CDA we use a
different distance: the <strong>Mahalanobis</strong> distance. This other type of distance
is based on the Mahalanobis metric matrix <span class="math inline">\(\mathbf{W^{-1}}\)</span>. Consequently,
the formula of the (squared) distance is given by:</p>
<p><span class="math display">\[
\textsf{Mahalanobis:} \quad d^2(\mathbf{x_i, g_k}) = (\mathbf{x_i} - \mathbf{g_k})^\mathsf{T} \mathbf{W}^{-1} (\mathbf{x_i} - \mathbf{g_k})
\]</span></p>
<p>What does this distance do? The Mahalanobis distance measures the (squared)
distance of a point <span class="math inline">\(\mathbf{x_i}\)</span> to a centroid <span class="math inline">\(\mathbf{g_k}\)</span> by taking
into account the correlational structure of the variables.</p>
<p>The following figure illustrates what this distance is doing compared to the
Euclidean distance. Suppose we have two points <span class="math inline">\(\mathbf{x_i}\)</span> and
<span class="math inline">\(\mathbf{x_\ell}\)</span> lying on the same density ellipsoid.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-235"></span>
<img src="images/discrim/discrim-euclidean-mahalanobis.svg" alt="Euclidean -vs- Mahalanobis distances" width="70%" />
<p class="caption">
Figure 23.14: Euclidean -vs- Mahalanobis distances
</p>
</div>
<p>Without taking into account the correlational structure of the features,
the points <span class="math inline">\(\mathbf{x_i}\)</span> and <span class="math inline">\(\mathbf{x_\ell}\)</span> are at different distances from
the centroid <span class="math inline">\(\mathbf{g_k}\)</span>. This is the case of the Euclidean distance which
does not take into account such correlational structure.</p>
<p>However, if we use a metric such as the Mahalanobis metric based on
<span class="math inline">\(\mathbf{W}^{-1}\)</span> (or an equivalent metric <span class="math inline">\(\mathbf{V}^{-1}\)</span>), the distance of
a particular point <span class="math inline">\(\mathbf{x_i}\)</span> to the centroid <span class="math inline">\(\mathbf{g_k}\)</span> depends
on how spread out is the cloud of points in class <span class="math inline">\(k\)</span>. Moreover, if two points
<span class="math inline">\(i\)</span> and <span class="math inline">\(\ell\)</span> are on the same density ellipsoid, then they are
equidistant to the centroid:</p>
<p><span class="math display">\[
d^{2}_{\mathbf{W}^{-1}} (\mathbf{x_i}, \mathbf{g_k}) = d^{2}_{\mathbf{W}^{-1}} (\mathbf{x_\ell}, \mathbf{g_k})
\]</span></p>
</div>
<div id="cda-classifier" class="section level3">
<h3><span class="header-section-number">23.4.3</span> CDA Classifier</h3>
<p>As we said before, the classification rule behind CDA says that we should assign
an object <span class="math inline">\(\mathbf{x_i}\)</span> to the class it is nearest to, using the
<span class="math inline">\(\mathbf{W^{-1}}\)</span> metric to calculate the distance of the object from the
centroid of the group. Expanding the Mahalanobis distance of <span class="math inline">\(\mathbf{x_i}\)</span> to
centroid <span class="math inline">\(\mathbf{g_k}\)</span> we get:</p>
<p><span class="math display">\[\begin{align*}
d^2(\mathbf{x_i, g_k}) &amp;= (\mathbf{x_i} - \mathbf{g_k})^\mathsf{T} \mathbf{W}^{-1} (\mathbf{x_i} - \mathbf{g_k}) \\
&amp;= \mathbf{x_i}^\mathsf{T} \mathbf{W}^{-1} \mathbf{x_i} - 2 \mathbf{g_k}^\mathsf{T} \mathbf{W}^{-1} \mathbf{x_i} + \mathbf{g_k}^\mathsf{T} \mathbf{W}^{-1} \mathbf{g_k}
\end{align*}\]</span></p>
<p>As you can tell, there are three terms: the first one does not depend on <span class="math inline">\(k\)</span>,
but the second and third terms do depend on <span class="math inline">\(k\)</span>:</p>
<p><span class="math display">\[
d^2(\mathbf{x_i, g_k}) = \underbrace{\mathbf{x_i}^\mathsf{T} \mathbf{W}^{-1} \mathbf{x_i}}_{constant} - \underbrace{2 \mathbf{g_k}^\mathsf{T} \mathbf{W}^{-1} \mathbf{x_i}}_{\text{depends on } k} + \underbrace{ \mathbf{g_k}^\mathsf{T} \mathbf{W}^{-1} \mathbf{g_k} }_{\text{depends on } k}
\]</span></p>
<p>Note that minimizing <span class="math inline">\(d^2(\mathbf{x_i, g_k})\)</span> is equivalent to maximizing the
negative of those terms that depend on <span class="math inline">\(k\)</span>:</p>
<p><span class="math display">\[
2 \mathbf{g_k}^\mathsf{T} \mathbf{W}^{-1} \mathbf{x_i} - \mathbf{g_k}^\mathsf{T} \mathbf{W}^{-1} \mathbf{g_k}
\]</span></p>
<p>Let’s go back to our hypothetical example at the beginning of the chapter. Now
suppose that we have three unclassified individuals A, B, and C. In canonical
discriminant analysis we compute the Mahalanobis distances of the unclassified
objects to the three centroids.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-236"></span>
<img src="images/discrim/discrim-cda-prediction1.svg" alt="Mahalanobis distance of an object to the class centroids" width="60%" />
<p class="caption">
Figure 23.15: Mahalanobis distance of an object to the class centroids
</p>
</div>
<p>For instance, with individual A, we would have to compute:</p>
<p><span class="math display">\[
d^2(\mathbf{x_A}, \mathbf{g_1}), \quad d^2(\mathbf{x_A}, \mathbf{g_2}), \quad and \quad d^2(\mathbf{x_A}, \mathbf{g_3})
\]</span></p>
<p>and then select the class, in this case class <span class="math inline">\(C_2\)</span>, for which the Mahalanobis
distance is minimal. An equivalent approach is to look for the maximum of:</p>
<ul>
<li><span class="math inline">\(2 \mathbf{g_1}^\mathsf{T} \mathbf{W}^{-1} \mathbf{x_A} - \mathbf{g_1}^\mathsf{T} \mathbf{W}^{-1} \mathbf{g_1}\)</span></li>
<li><span class="math inline">\(2 \mathbf{g_2}^\mathsf{T} \mathbf{W}^{-1} \mathbf{x_A} - \mathbf{g_2}^\mathsf{T} \mathbf{W}^{-1} \mathbf{g_2}\)</span></li>
<li><span class="math inline">\(2 \mathbf{g_3}^\mathsf{T} \mathbf{W}^{-1} \mathbf{x_A} - \mathbf{g_3}^\mathsf{T} \mathbf{W}^{-1} \mathbf{g_3}\)</span></li>
</ul>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-237"></span>
<img src="images/discrim/discrim-cda-prediction2.svg" alt="Assign class for which Mahalanobis distance is minimal" width="60%" />
<p class="caption">
Figure 23.16: Assign class for which Mahalanobis distance is minimal
</p>
</div>
</div>
<div id="limitations-of-cda-classifier" class="section level3">
<h3><span class="header-section-number">23.4.4</span> Limitations of CDA classifier</h3>
<p>There is an underlying assumption behind the geometric rule of CDA. This rule
should not be used if the two classes have different <em>a priori</em> probabilities
or variances (we discuss this in the next chapter).</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-238"></span>
<img src="images/discrim/discrim_limitations.svg" alt="To which class should we assign the new object?" width="50%" />
<p class="caption">
Figure 23.17: To which class should we assign the new object?
</p>
</div>
<p>In the next chapter we shift gears by introducing a more generic framework
for discriminant analysis based on a probabilistic approach,
as opposed to the geometric distance-based approach of CDA.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="discrim.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="discanalysis.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["allmodelsarewrong.pdf", "allmodelsarewrong.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
