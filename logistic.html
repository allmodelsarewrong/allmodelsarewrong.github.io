<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>17 Logistic Regression | All Models Are Wrong</title>
  <meta name="description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  <meta name="generator" content="bookdown 0.16 and GitBook 2.6.7" />

  <meta property="og:title" content="17 Logistic Regression | All Models Are Wrong" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://allmodelsarewrong.github.io" />
  
  <meta property="og:description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  <meta name="github-repo" content="allmodelsarewrong/allmodelsarewrong.github.io" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="17 Logistic Regression | All Models Are Wrong" />
  
  <meta name="twitter:description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  

<meta name="author" content="by Gaston Sanchez, and Ethan Marzban" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="classif.html"/>
<link rel="next" href="discrim.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><b>All Models Are Wrong (CSL)</b><br><small>by G. Sanchez & E. Marzban</small></a></li>

<li class="divider"></li>
<li class="part"><span><b>I Welcome</b></span></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="about.html"><a href="about.html"><i class="fa fa-check"></i><b>1</b> About this book</a><ul>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#prerequisites"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="part"><span><b>II Intro</b></span></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#basic-notation"><i class="fa fa-check"></i><b>2.1</b> Basic Notation</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="duality.html"><a href="duality.html"><i class="fa fa-check"></i><b>3</b> Geometric Duality</a><ul>
<li class="chapter" data-level="3.1" data-path="duality.html"><a href="duality.html#rows-space"><i class="fa fa-check"></i><b>3.1</b> Rows Space</a></li>
<li class="chapter" data-level="3.2" data-path="duality.html"><a href="duality.html#columns-space"><i class="fa fa-check"></i><b>3.2</b> Columns Space</a></li>
<li class="chapter" data-level="3.3" data-path="duality.html"><a href="duality.html#cloud-of-individuals"><i class="fa fa-check"></i><b>3.3</b> Cloud of Individuals</a><ul>
<li class="chapter" data-level="3.3.1" data-path="duality.html"><a href="duality.html#average-individual"><i class="fa fa-check"></i><b>3.3.1</b> Average Individual</a></li>
<li class="chapter" data-level="3.3.2" data-path="duality.html"><a href="duality.html#centered-data"><i class="fa fa-check"></i><b>3.3.2</b> Centered Data</a></li>
<li class="chapter" data-level="3.3.3" data-path="duality.html"><a href="duality.html#distance-between-individuals"><i class="fa fa-check"></i><b>3.3.3</b> Distance between individuals</a></li>
<li class="chapter" data-level="3.3.4" data-path="duality.html"><a href="duality.html#distance-to-the-centroid"><i class="fa fa-check"></i><b>3.3.4</b> Distance to the centroid</a></li>
<li class="chapter" data-level="3.3.5" data-path="duality.html"><a href="duality.html#measures-of-dispersion"><i class="fa fa-check"></i><b>3.3.5</b> Measures of Dispersion</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="duality.html"><a href="duality.html#cloud-of-variables"><i class="fa fa-check"></i><b>3.4</b> Cloud of Variables</a><ul>
<li class="chapter" data-level="3.4.1" data-path="duality.html"><a href="duality.html#mean-of-a-variable"><i class="fa fa-check"></i><b>3.4.1</b> Mean of a Variable</a></li>
<li class="chapter" data-level="3.4.2" data-path="duality.html"><a href="duality.html#variance-of-a-variable"><i class="fa fa-check"></i><b>3.4.2</b> Variance of a Variable</a></li>
<li class="chapter" data-level="3.4.3" data-path="duality.html"><a href="duality.html#variance-with-vector-notation"><i class="fa fa-check"></i><b>3.4.3</b> Variance with Vector Notation</a></li>
<li class="chapter" data-level="3.4.4" data-path="duality.html"><a href="duality.html#standard-deviation-as-a-norm"><i class="fa fa-check"></i><b>3.4.4</b> Standard Deviation as a Norm</a></li>
<li class="chapter" data-level="3.4.5" data-path="duality.html"><a href="duality.html#covariance"><i class="fa fa-check"></i><b>3.4.5</b> Covariance</a></li>
<li class="chapter" data-level="3.4.6" data-path="duality.html"><a href="duality.html#correlation"><i class="fa fa-check"></i><b>3.4.6</b> Correlation</a></li>
<li class="chapter" data-level="3.4.7" data-path="duality.html"><a href="duality.html#geometry-of-correlation"><i class="fa fa-check"></i><b>3.4.7</b> Geometry of Correlation</a></li>
<li class="chapter" data-level="3.4.8" data-path="duality.html"><a href="duality.html#orthogonal-projections"><i class="fa fa-check"></i><b>3.4.8</b> Orthogonal Projections</a></li>
<li class="chapter" data-level="3.4.9" data-path="duality.html"><a href="duality.html#the-mean-as-an-orthogonal-projection"><i class="fa fa-check"></i><b>3.4.9</b> The mean as an orthogonal projection</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Unsupervised I: PCA</b></span></li>
<li class="chapter" data-level="4" data-path="pca.html"><a href="pca.html"><i class="fa fa-check"></i><b>4</b> Principal Components Analysis</a><ul>
<li class="chapter" data-level="4.1" data-path="pca.html"><a href="pca.html#low-dimensional-representations"><i class="fa fa-check"></i><b>4.1</b> Low-dimensional Representations</a></li>
<li class="chapter" data-level="4.2" data-path="pca.html"><a href="pca.html#projections"><i class="fa fa-check"></i><b>4.2</b> Projections</a><ul>
<li class="chapter" data-level="4.2.1" data-path="pca.html"><a href="pca.html#vector-and-scalar-projections"><i class="fa fa-check"></i><b>4.2.1</b> Vector and Scalar Projections</a></li>
<li class="chapter" data-level="4.2.2" data-path="pca.html"><a href="pca.html#projected-inertia"><i class="fa fa-check"></i><b>4.2.2</b> Projected Inertia</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="pca.html"><a href="pca.html#maximization-problem"><i class="fa fa-check"></i><b>4.3</b> Maximization Problem</a><ul>
<li class="chapter" data-level="4.3.1" data-path="pca.html"><a href="pca.html#eigenvectors-of-mathbfs"><i class="fa fa-check"></i><b>4.3.1</b> Eigenvectors of <span class="math inline">\(\mathbf{S}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="pca.html"><a href="pca.html#another-perspective-of-pca"><i class="fa fa-check"></i><b>4.4</b> Another Perspective of PCA</a><ul>
<li class="chapter" data-level="4.4.1" data-path="pca.html"><a href="pca.html#finding-principal-components"><i class="fa fa-check"></i><b>4.4.1</b> Finding Principal Components</a></li>
<li class="chapter" data-level="4.4.2" data-path="pca.html"><a href="pca.html#finding-the-first-pc"><i class="fa fa-check"></i><b>4.4.2</b> Finding the first PC</a></li>
<li class="chapter" data-level="4.4.3" data-path="pca.html"><a href="pca.html#finding-the-second-pc"><i class="fa fa-check"></i><b>4.4.3</b> Finding the second PC</a></li>
<li class="chapter" data-level="4.4.4" data-path="pca.html"><a href="pca.html#finding-all-pcs"><i class="fa fa-check"></i><b>4.4.4</b> Finding all PCs</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="pca.html"><a href="pca.html#data-decomposition-model"><i class="fa fa-check"></i><b>4.5</b> Data Decomposition Model</a><ul>
<li class="chapter" data-level="4.5.1" data-path="pca.html"><a href="pca.html#alternative-approaches"><i class="fa fa-check"></i><b>4.5.1</b> Alternative Approaches</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Linear Regression</b></span></li>
<li class="chapter" data-level="5" data-path="ols.html"><a href="ols.html"><i class="fa fa-check"></i><b>5</b> Linear Regression</a><ul>
<li class="chapter" data-level="5.1" data-path="ols.html"><a href="ols.html#motivation"><i class="fa fa-check"></i><b>5.1</b> Motivation</a></li>
<li class="chapter" data-level="5.2" data-path="ols.html"><a href="ols.html#the-ideaintuition-of-regression"><i class="fa fa-check"></i><b>5.2</b> The Idea/Intuition of Regression</a></li>
<li class="chapter" data-level="5.3" data-path="ols.html"><a href="ols.html#the-linear-regression-model"><i class="fa fa-check"></i><b>5.3</b> The Linear Regression Model</a></li>
<li class="chapter" data-level="5.4" data-path="ols.html"><a href="ols.html#the-error-measure"><i class="fa fa-check"></i><b>5.4</b> The Error Measure</a></li>
<li class="chapter" data-level="5.5" data-path="ols.html"><a href="ols.html#the-least-squares-algorithm"><i class="fa fa-check"></i><b>5.5</b> The Least Squares Algorithm</a></li>
<li class="chapter" data-level="5.6" data-path="ols.html"><a href="ols.html#geometries-of-ols"><i class="fa fa-check"></i><b>5.6</b> Geometries of OLS</a><ul>
<li class="chapter" data-level="5.6.1" data-path="ols.html"><a href="ols.html#rows-perspective"><i class="fa fa-check"></i><b>5.6.1</b> Rows Perspective</a></li>
<li class="chapter" data-level="5.6.2" data-path="ols.html"><a href="ols.html#columns-perspective"><i class="fa fa-check"></i><b>5.6.2</b> Columns Perspective</a></li>
<li class="chapter" data-level="5.6.3" data-path="ols.html"><a href="ols.html#parameters-perspective"><i class="fa fa-check"></i><b>5.6.3</b> Parameters Perspective</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="gradient.html"><a href="gradient.html"><i class="fa fa-check"></i><b>6</b> Gradient Descent</a><ul>
<li class="chapter" data-level="6.1" data-path="gradient.html"><a href="gradient.html#error-surface"><i class="fa fa-check"></i><b>6.1</b> Error Surface</a></li>
<li class="chapter" data-level="6.2" data-path="gradient.html"><a href="gradient.html#idea-of-gradient-descent"><i class="fa fa-check"></i><b>6.2</b> Idea of Gradient Descent</a></li>
<li class="chapter" data-level="6.3" data-path="gradient.html"><a href="gradient.html#moving-down-an-error-surface"><i class="fa fa-check"></i><b>6.3</b> Moving Down an Error Surface</a><ul>
<li class="chapter" data-level="6.3.1" data-path="gradient.html"><a href="gradient.html#the-direction-of-mathbfv"><i class="fa fa-check"></i><b>6.3.1</b> The direction of <span class="math inline">\(\mathbf{v}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="gradient.html"><a href="gradient.html#gradient-descent-and-our-model"><i class="fa fa-check"></i><b>6.4</b> Gradient Descent and our Model</a></li>
</ul></li>
<li class="part"><span><b>V Learning Concepts</b></span></li>
<li class="chapter" data-level="7" data-path="learning.html"><a href="learning.html"><i class="fa fa-check"></i><b>7</b> Theoretical Framework</a><ul>
<li class="chapter" data-level="7.1" data-path="learning.html"><a href="learning.html#mental-map"><i class="fa fa-check"></i><b>7.1</b> Mental Map</a></li>
<li class="chapter" data-level="7.2" data-path="learning.html"><a href="learning.html#kinds-of-predictions"><i class="fa fa-check"></i><b>7.2</b> Kinds of Predictions</a><ul>
<li class="chapter" data-level="7.2.1" data-path="learning.html"><a href="learning.html#two-types-of-data"><i class="fa fa-check"></i><b>7.2.1</b> Two Types of Data</a></li>
<li class="chapter" data-level="7.2.2" data-path="learning.html"><a href="learning.html#two-types-of-predictions"><i class="fa fa-check"></i><b>7.2.2</b> Two Types of Predictions</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="learning.html"><a href="learning.html#two-types-of-errors"><i class="fa fa-check"></i><b>7.3</b> Two Types of Errors</a><ul>
<li class="chapter" data-level="7.3.1" data-path="learning.html"><a href="learning.html#individual-errors"><i class="fa fa-check"></i><b>7.3.1</b> Individual Errors</a></li>
<li class="chapter" data-level="7.3.2" data-path="learning.html"><a href="learning.html#overall-errors"><i class="fa fa-check"></i><b>7.3.2</b> Overall Errors</a></li>
<li class="chapter" data-level="7.3.3" data-path="learning.html"><a href="learning.html#auxiliary-technicality"><i class="fa fa-check"></i><b>7.3.3</b> Auxiliary Technicality</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="learning.html"><a href="learning.html#noisy-targets"><i class="fa fa-check"></i><b>7.4</b> Noisy Targets</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="mse.html"><a href="mse.html"><i class="fa fa-check"></i><b>8</b> MSE of Estimator</a><ul>
<li class="chapter" data-level="8.1" data-path="mse.html"><a href="mse.html#mse-of-an-estimator"><i class="fa fa-check"></i><b>8.1</b> MSE of an Estimator</a><ul>
<li class="chapter" data-level="8.1.1" data-path="mse.html"><a href="mse.html#prototypical-cases-of-bias-and-variance"><i class="fa fa-check"></i><b>8.1.1</b> Prototypical Cases of Bias and Variance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="biasvar.html"><a href="biasvar.html"><i class="fa fa-check"></i><b>9</b> Bias-Variance Tradeoff</a><ul>
<li class="chapter" data-level="9.1" data-path="biasvar.html"><a href="biasvar.html#introduction"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="biasvar.html"><a href="biasvar.html#motivation-example"><i class="fa fa-check"></i><b>9.2</b> Motivation Example</a><ul>
<li class="chapter" data-level="9.2.1" data-path="biasvar.html"><a href="biasvar.html#two-hypotheses"><i class="fa fa-check"></i><b>9.2.1</b> Two Hypotheses</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="biasvar.html"><a href="biasvar.html#learning-from-two-points"><i class="fa fa-check"></i><b>9.3</b> Learning from two points</a></li>
<li class="chapter" data-level="9.4" data-path="biasvar.html"><a href="biasvar.html#bias-variance-derivation"><i class="fa fa-check"></i><b>9.4</b> Bias-Variance Derivation</a><ul>
<li class="chapter" data-level="9.4.1" data-path="biasvar.html"><a href="biasvar.html#out-of-sample-predictions"><i class="fa fa-check"></i><b>9.4.1</b> Out-of-Sample Predictions</a></li>
<li class="chapter" data-level="9.4.2" data-path="biasvar.html"><a href="biasvar.html#noisy-target"><i class="fa fa-check"></i><b>9.4.2</b> Noisy Target</a></li>
<li class="chapter" data-level="9.4.3" data-path="biasvar.html"><a href="biasvar.html#types-of-theoretical-mses"><i class="fa fa-check"></i><b>9.4.3</b> Types of Theoretical MSEs</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="biasvar.html"><a href="biasvar.html#the-tradeoff"><i class="fa fa-check"></i><b>9.5</b> The Tradeoff</a><ul>
<li class="chapter" data-level="9.5.1" data-path="biasvar.html"><a href="biasvar.html#bias-variance-tradeoff-picture"><i class="fa fa-check"></i><b>9.5.1</b> Bias-Variance Tradeoff Picture</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="phases.html"><a href="phases.html"><i class="fa fa-check"></i><b>10</b> Learning Phases</a><ul>
<li class="chapter" data-level="10.1" data-path="phases.html"><a href="phases.html#introduction-1"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="phases.html"><a href="phases.html#model-assessment"><i class="fa fa-check"></i><b>10.2</b> Model Assessment</a><ul>
<li class="chapter" data-level="10.2.1" data-path="phases.html"><a href="phases.html#holdout-test-set"><i class="fa fa-check"></i><b>10.2.1</b> Holdout Test Set</a></li>
<li class="chapter" data-level="10.2.2" data-path="phases.html"><a href="phases.html#why-does-a-test-set-work"><i class="fa fa-check"></i><b>10.2.2</b> Why does a test set work?</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="phases.html"><a href="phases.html#model-selection"><i class="fa fa-check"></i><b>10.3</b> Model Selection</a><ul>
<li class="chapter" data-level="10.3.1" data-path="phases.html"><a href="phases.html#three-way-holdout-method"><i class="fa fa-check"></i><b>10.3.1</b> Three-way Holdout Method</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="phases.html"><a href="phases.html#model-training"><i class="fa fa-check"></i><b>10.4</b> Model Training</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="resampling.html"><a href="resampling.html"><i class="fa fa-check"></i><b>11</b> Resample Approaches</a><ul>
<li class="chapter" data-level="11.1" data-path="resampling.html"><a href="resampling.html#general-sampling-blueprint"><i class="fa fa-check"></i><b>11.1</b> General Sampling Blueprint</a></li>
<li class="chapter" data-level="11.2" data-path="resampling.html"><a href="resampling.html#monte-carlo-cross-validation"><i class="fa fa-check"></i><b>11.2</b> Monte Carlo Cross Validation</a></li>
<li class="chapter" data-level="11.3" data-path="resampling.html"><a href="resampling.html#bootstrap-method"><i class="fa fa-check"></i><b>11.3</b> Bootstrap Method</a></li>
<li class="chapter" data-level="11.4" data-path="resampling.html"><a href="resampling.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>11.4</b> <span class="math inline">\(K\)</span>-Fold Cross-Validation</a><ul>
<li class="chapter" data-level="11.4.1" data-path="resampling.html"><a href="resampling.html#leave-one-out-cross-validation-loocv"><i class="fa fa-check"></i><b>11.4.1</b> Leave-One-Out Cross Validation (LOOCV)</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>VI Regularization</b></span></li>
<li class="chapter" data-level="12" data-path="regular.html"><a href="regular.html"><i class="fa fa-check"></i><b>12</b> Regularization Techniques</a><ul>
<li class="chapter" data-level="12.1" data-path="regular.html"><a href="regular.html#multicollinearity-issues"><i class="fa fa-check"></i><b>12.1</b> Multicollinearity Issues</a><ul>
<li class="chapter" data-level="12.1.1" data-path="regular.html"><a href="regular.html#toy-example"><i class="fa fa-check"></i><b>12.1.1</b> Toy Example</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="regular.html"><a href="regular.html#irregular-coefficients"><i class="fa fa-check"></i><b>12.2</b> Irregular Coefficients</a></li>
<li class="chapter" data-level="12.3" data-path="regular.html"><a href="regular.html#connection-to-regularization"><i class="fa fa-check"></i><b>12.3</b> Connection to Regularization</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="pcr.html"><a href="pcr.html"><i class="fa fa-check"></i><b>13</b> Principal Components Regression</a><ul>
<li class="chapter" data-level="13.1" data-path="pcr.html"><a href="pcr.html#motivation-example-1"><i class="fa fa-check"></i><b>13.1</b> Motivation Example</a></li>
<li class="chapter" data-level="13.2" data-path="pcr.html"><a href="pcr.html#the-pcr-model"><i class="fa fa-check"></i><b>13.2</b> The PCR Model</a></li>
<li class="chapter" data-level="13.3" data-path="pcr.html"><a href="pcr.html#how-does-pcr-work"><i class="fa fa-check"></i><b>13.3</b> How does PCR work?</a><ul>
<li class="chapter" data-level="13.3.1" data-path="pcr.html"><a href="pcr.html#transition-formula"><i class="fa fa-check"></i><b>13.3.1</b> Transition Formula</a></li>
<li class="chapter" data-level="13.3.2" data-path="pcr.html"><a href="pcr.html#size-of-coefficients"><i class="fa fa-check"></i><b>13.3.2</b> Size of Coefficients</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="pls.html"><a href="pls.html"><i class="fa fa-check"></i><b>14</b> Partial Least Squares Regression</a><ul>
<li class="chapter" data-level="14.1" data-path="pls.html"><a href="pls.html#motivation-example-2"><i class="fa fa-check"></i><b>14.1</b> Motivation Example</a></li>
<li class="chapter" data-level="14.2" data-path="pls.html"><a href="pls.html#the-plsr-model"><i class="fa fa-check"></i><b>14.2</b> The PLSR Model</a></li>
<li class="chapter" data-level="14.3" data-path="pls.html"><a href="pls.html#how-does-plsr-work"><i class="fa fa-check"></i><b>14.3</b> How does PLSR work?</a></li>
<li class="chapter" data-level="14.4" data-path="pls.html"><a href="pls.html#plsr-algorithm"><i class="fa fa-check"></i><b>14.4</b> PLSR Algorithm</a><ul>
<li class="chapter" data-level="14.4.1" data-path="pls.html"><a href="pls.html#pls-solution-with-original-variables"><i class="fa fa-check"></i><b>14.4.1</b> PLS Solution with original variables</a></li>
<li class="chapter" data-level="14.4.2" data-path="pls.html"><a href="pls.html#size-of-coefficients-1"><i class="fa fa-check"></i><b>14.4.2</b> Size of Coefficients</a></li>
<li class="chapter" data-level="14.4.3" data-path="pls.html"><a href="pls.html#some-properties"><i class="fa fa-check"></i><b>14.4.3</b> Some Properties</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="ridge.html"><a href="ridge.html"><i class="fa fa-check"></i><b>15</b> Ridge Regression</a></li>
<li class="part"><span><b>VII Classification</b></span></li>
<li class="chapter" data-level="16" data-path="classif.html"><a href="classif.html"><i class="fa fa-check"></i><b>16</b> Classification Methods</a></li>
<li class="chapter" data-level="17" data-path="logistic.html"><a href="logistic.html"><i class="fa fa-check"></i><b>17</b> Logistic Regression</a><ul>
<li class="chapter" data-level="17.1" data-path="logistic.html"><a href="logistic.html#motivation-1"><i class="fa fa-check"></i><b>17.1</b> Motivation</a><ul>
<li class="chapter" data-level="17.1.1" data-path="logistic.html"><a href="logistic.html#first-approach-fitting-a-line"><i class="fa fa-check"></i><b>17.1.1</b> First Approach: Fitting a Line</a></li>
<li class="chapter" data-level="17.1.2" data-path="logistic.html"><a href="logistic.html#secodn-approach-harsh-thresholding"><i class="fa fa-check"></i><b>17.1.2</b> Secodn Approach: Harsh Thresholding</a></li>
<li class="chapter" data-level="17.1.3" data-path="logistic.html"><a href="logistic.html#third-approach-conditional-means"><i class="fa fa-check"></i><b>17.1.3</b> Third Approach: Conditional Means</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="logistic.html"><a href="logistic.html#logistic-regression-model"><i class="fa fa-check"></i><b>17.2</b> Logistic Regression Model</a><ul>
<li class="chapter" data-level="17.2.1" data-path="logistic.html"><a href="logistic.html#the-criterion-being-optimized"><i class="fa fa-check"></i><b>17.2.1</b> The Criterion Being Optimized</a></li>
<li class="chapter" data-level="17.2.2" data-path="logistic.html"><a href="logistic.html#another-way-to-solve-logistic-regression"><i class="fa fa-check"></i><b>17.2.2</b> Another Way to Solve Logistic Regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="discrim.html"><a href="discrim.html"><i class="fa fa-check"></i><b>18</b> Preamble for Discriminant Analysis</a><ul>
<li class="chapter" data-level="18.1" data-path="discrim.html"><a href="discrim.html#motivation-2"><i class="fa fa-check"></i><b>18.1</b> Motivation</a><ul>
<li class="chapter" data-level="18.1.1" data-path="discrim.html"><a href="discrim.html#distinguishing-species"><i class="fa fa-check"></i><b>18.1.1</b> Distinguishing Species</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="discrim.html"><a href="discrim.html#derived-ratios-from-sum-of-squares"><i class="fa fa-check"></i><b>18.2</b> Derived Ratios from Sum-of-Squares</a><ul>
<li class="chapter" data-level="18.2.1" data-path="discrim.html"><a href="discrim.html#correlation-ratio"><i class="fa fa-check"></i><b>18.2.1</b> Correlation Ratio</a></li>
<li class="chapter" data-level="18.2.2" data-path="discrim.html"><a href="discrim.html#f-ratio"><i class="fa fa-check"></i><b>18.2.2</b> F-Ratio</a></li>
<li class="chapter" data-level="18.2.3" data-path="discrim.html"><a href="discrim.html#example-with-iris-data"><i class="fa fa-check"></i><b>18.2.3</b> Example with Iris data</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="discrim.html"><a href="discrim.html#geometric-perspective"><i class="fa fa-check"></i><b>18.3</b> Geometric Perspective</a><ul>
<li class="chapter" data-level="18.3.1" data-path="discrim.html"><a href="discrim.html#clouds-from-class-structure"><i class="fa fa-check"></i><b>18.3.1</b> Clouds from Class Structure</a></li>
<li class="chapter" data-level="18.3.2" data-path="discrim.html"><a href="discrim.html#dispersion-decomposition"><i class="fa fa-check"></i><b>18.3.2</b> Dispersion Decomposition</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="cda.html"><a href="cda.html"><i class="fa fa-check"></i><b>19</b> Canonical Discriminant Analysis</a><ul>
<li class="chapter" data-level="19.0.1" data-path="cda.html"><a href="cda.html#looking-for-a-discriminant-axis"><i class="fa fa-check"></i><b>19.0.1</b> Looking for a discriminant axis</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="discanalysis.html"><a href="discanalysis.html"><i class="fa fa-check"></i><b>20</b> Discriminant Analysis</a><ul>
<li class="chapter" data-level="20.1" data-path="discanalysis.html"><a href="discanalysis.html#probabilistic-da"><i class="fa fa-check"></i><b>20.1</b> Probabilistic DA</a></li>
<li class="chapter" data-level="20.2" data-path="discanalysis.html"><a href="discanalysis.html#estimations"><i class="fa fa-check"></i><b>20.2</b> Estimations</a><ul>
<li class="chapter" data-level="20.2.1" data-path="discanalysis.html"><a href="discanalysis.html#univariate-data-boldsymbolp-1"><i class="fa fa-check"></i><b>20.2.1</b> Univariate Data (<span class="math inline">\(\boldsymbol{p = 1}\)</span>)</a></li>
<li class="chapter" data-level="20.2.2" data-path="discanalysis.html"><a href="discanalysis.html#multivariate-data-boldsymbolp-1"><i class="fa fa-check"></i><b>20.2.2</b> Multivariate Data (<span class="math inline">\(\boldsymbol{p &gt; 1}\)</span>)</a></li>
<li class="chapter" data-level="20.2.3" data-path="discanalysis.html"><a href="discanalysis.html#multivariate-estimations"><i class="fa fa-check"></i><b>20.2.3</b> Multivariate Estimations</a></li>
</ul></li>
<li class="chapter" data-level="20.3" data-path="discanalysis.html"><a href="discanalysis.html#discriminant-functions"><i class="fa fa-check"></i><b>20.3</b> Discriminant Functions</a></li>
<li class="chapter" data-level="20.4" data-path="discanalysis.html"><a href="discanalysis.html#quadratic-discriminant-analysis-qda"><i class="fa fa-check"></i><b>20.4</b> Quadratic Discriminant Analysis (QDA)</a></li>
<li class="chapter" data-level="20.5" data-path="discanalysis.html"><a href="discanalysis.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>20.5</b> Linear Discriminant Analysis</a></li>
<li class="chapter" data-level="20.6" data-path="discanalysis.html"><a href="discanalysis.html#canonical-discriminant-analysis"><i class="fa fa-check"></i><b>20.6</b> Canonical Discriminant Analysis</a></li>
<li class="chapter" data-level="20.7" data-path="discanalysis.html"><a href="discanalysis.html#naive-bayes"><i class="fa fa-check"></i><b>20.7</b> Naive Bayes</a></li>
<li class="chapter" data-level="20.8" data-path="discanalysis.html"><a href="discanalysis.html#fifth-case"><i class="fa fa-check"></i><b>20.8</b> Fifth Case</a></li>
<li class="chapter" data-level="20.9" data-path="discanalysis.html"><a href="discanalysis.html#comparing-the-cases"><i class="fa fa-check"></i><b>20.9</b> Comparing the Cases</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="classperformance.html"><a href="classperformance.html"><i class="fa fa-check"></i><b>21</b> Performance of Classifiers</a><ul>
<li class="chapter" data-level="21.1" data-path="classperformance.html"><a href="classperformance.html#error-for-binary-response"><i class="fa fa-check"></i><b>21.1</b> Error for Binary Response</a></li>
<li class="chapter" data-level="21.2" data-path="classperformance.html"><a href="classperformance.html#categorical-response"><i class="fa fa-check"></i><b>21.2</b> Categorical Response</a></li>
<li class="chapter" data-level="21.3" data-path="classperformance.html"><a href="classperformance.html#confusion-matrices"><i class="fa fa-check"></i><b>21.3</b> Confusion Matrices</a></li>
<li class="chapter" data-level="21.4" data-path="classperformance.html"><a href="classperformance.html#binary-response-example"><i class="fa fa-check"></i><b>21.4</b> Binary Response: Example</a><ul>
<li class="chapter" data-level="21.4.1" data-path="classperformance.html"><a href="classperformance.html#types-of-errors-application-for-savings-account"><i class="fa fa-check"></i><b>21.4.1</b> Types of Errors: Application for Savings Account</a></li>
<li class="chapter" data-level="21.4.2" data-path="classperformance.html"><a href="classperformance.html#another-example-loans"><i class="fa fa-check"></i><b>21.4.2</b> Another Example: Loans</a></li>
<li class="chapter" data-level="21.4.3" data-path="classperformance.html"><a href="classperformance.html#the-ideal-situation"><i class="fa fa-check"></i><b>21.4.3</b> The Ideal Situation</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>VIII Unsupervised II: Clustering</b></span></li>
<li class="chapter" data-level="22" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>22</b> Clustering</a><ul>
<li class="chapter" data-level="22.1" data-path="clustering.html"><a href="clustering.html#about-clustering"><i class="fa fa-check"></i><b>22.1</b> About Clustering</a><ul>
<li class="chapter" data-level="22.1.1" data-path="clustering.html"><a href="clustering.html#types-of-clustering"><i class="fa fa-check"></i><b>22.1.1</b> Types of Clustering</a></li>
<li class="chapter" data-level="22.1.2" data-path="clustering.html"><a href="clustering.html#hard-clustering"><i class="fa fa-check"></i><b>22.1.2</b> Hard Clustering</a></li>
</ul></li>
<li class="chapter" data-level="22.2" data-path="clustering.html"><a href="clustering.html#dispersion-measures"><i class="fa fa-check"></i><b>22.2</b> Dispersion Measures</a></li>
<li class="chapter" data-level="22.3" data-path="clustering.html"><a href="clustering.html#complexity-in-clustering"><i class="fa fa-check"></i><b>22.3</b> Complexity in Clustering</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="kmeans.html"><a href="kmeans.html"><i class="fa fa-check"></i><b>23</b> K-Means</a><ul>
<li class="chapter" data-level="23.1" data-path="kmeans.html"><a href="kmeans.html#toy-example-1"><i class="fa fa-check"></i><b>23.1</b> Toy Example</a></li>
<li class="chapter" data-level="23.2" data-path="kmeans.html"><a href="kmeans.html#what-does-k-means-do"><i class="fa fa-check"></i><b>23.2</b> What does K-means do?</a></li>
<li class="chapter" data-level="23.3" data-path="kmeans.html"><a href="kmeans.html#k-means-algorithms"><i class="fa fa-check"></i><b>23.3</b> K-Means Algorithms</a><ul>
<li class="chapter" data-level="23.3.1" data-path="kmeans.html"><a href="kmeans.html#classic-version"><i class="fa fa-check"></i><b>23.3.1</b> Classic Version</a></li>
<li class="chapter" data-level="23.3.2" data-path="kmeans.html"><a href="kmeans.html#moving-centers-algorithm"><i class="fa fa-check"></i><b>23.3.2</b> Moving Centers Algorithm</a></li>
<li class="chapter" data-level="23.3.3" data-path="kmeans.html"><a href="kmeans.html#dynamic-clouds"><i class="fa fa-check"></i><b>23.3.3</b> Dynamic Clouds</a></li>
<li class="chapter" data-level="23.3.4" data-path="kmeans.html"><a href="kmeans.html#choosing-k"><i class="fa fa-check"></i><b>23.3.4</b> Choosing <span class="math inline">\(k\)</span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="24" data-path="hclus.html"><a href="hclus.html"><i class="fa fa-check"></i><b>24</b> Hierarchical Clustering</a><ul>
<li class="chapter" data-level="24.1" data-path="hclus.html"><a href="hclus.html#agglomerative-methods"><i class="fa fa-check"></i><b>24.1</b> Agglomerative Methods</a></li>
<li class="chapter" data-level="24.2" data-path="hclus.html"><a href="hclus.html#example-single-linkage"><i class="fa fa-check"></i><b>24.2</b> Example: Single Linkage</a><ul>
<li class="chapter" data-level="24.2.1" data-path="hclus.html"><a href="hclus.html#dendrogram"><i class="fa fa-check"></i><b>24.2.1</b> Dendrogram</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Made with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">All Models Are Wrong</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="logistic" class="section level1">
<h1><span class="header-section-number">17</span> Logistic Regression</h1>
<p>So far we’ve been dealing with predicting a quantitative response, using mostly
linear models. But what about predicting a qualitative or categorical response?
We now turn our attention to predicting a discrete (aka categorical) response.</p>
<p>To have a gentle transition from regression models into classification models,
we’ll start with the famous logistic regression.</p>
<div id="motivation-1" class="section level2">
<h2><span class="header-section-number">17.1</span> Motivation</h2>
<p>Let’s consider a classic example: predicting heart attack. The data consists of
a number of individuals (patients) with some medical variables.</p>
<p>Consider the following (very famous) example, found in Hosmer and Lemeshow
(edition from 2000) in which we wish to predict coronary heart disease (<em>chd</em>).
The data is relatively small (<span class="math inline">\(n = 100\)</span> patients), and we start with considering
only one predictor: <span class="math inline">\(age\)</span>.</p>
<pre><code>#&gt;   age chd
#&gt; 1  20   0
#&gt; 2  23   0
#&gt; 3  24   0
#&gt; 4  25   0
#&gt; 5  25   1
#&gt; 6  26   0</code></pre>
<p>For illustration purposes, let’s consider the response <span class="math inline">\(\mathbf{y}\)</span> and only one predictor <span class="math inline">\(\mathbf{x}\)</span>. If we graph a scatterplot we get:</p>
<p><img src="allmodelsarewrong_files/figure-html/chd_scatter3-1.png" width=".95\linewidth" height=".65\linewidth" style="display: block; margin: auto;" /></p>
<p>With respect to the x-axis, we have values ranging from small x’s to large x’s.
In contrast, the response is a binary response, so there are only 0’s or 1’s. As you can tell, the distribution of points does not seem to uniform along the x-axis. Moreover, for a given response value, say <span class="math inline">\(y_i = 0\)</span> there are more small values <span class="math inline">\(x\)</span> than large ones. And viceversa, for <span class="math inline">\(y_i = 1\)</span>, there are more large values <span class="math inline">\(x\)</span> than small ones.</p>
<p>In other words, we can see that (in general) younger people are less likely to
have CHD than older people. Hence, there seems to be some information about
<em>chd</em> in <em>age</em>. The goal is to fit a model that predicts <em>chd</em> from <em>age</em>:</p>
<p><span class="math display">\[
\text{chd} = f(\text{age}) + \varepsilon
\]</span></p>
<div id="first-approach-fitting-a-line" class="section level3">
<h3><span class="header-section-number">17.1.1</span> First Approach: Fitting a Line</h3>
<p>As a naive first approach, we could try to fit a linear model:
$ = b_0 + b_1  =  $ where:</p>
<p><span class="math display">\[
\mathbf{X} = 
\begin{pmatrix} 
1 &amp; x_1 \\ 
1 &amp; x_2 \\ 
\vdots &amp; \vdots \\ 
1 &amp; x_n \\ 
\end{pmatrix}; \hspace{5mm} 
\mathbf{b} = 
\begin{pmatrix} 
b_0 \\ b_1 \\ 
\end{pmatrix}
\]</span></p>
<p>So let’s see what happens if we use least squares to fit such line.</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="logistic.html#cb32-1"></a><span class="co"># fit line</span></span>
<span id="cb32-2"><a href="logistic.html#cb32-2"></a>reg =<span class="st"> </span><span class="kw">lm</span>(chd <span class="op">~</span><span class="st"> </span>age, <span class="dt">data =</span> dat)</span>
<span id="cb32-3"><a href="logistic.html#cb32-3"></a><span class="kw">summary</span>(reg)</span>
<span id="cb32-4"><a href="logistic.html#cb32-4"></a><span class="co">#&gt; </span></span>
<span id="cb32-5"><a href="logistic.html#cb32-5"></a><span class="co">#&gt; Call:</span></span>
<span id="cb32-6"><a href="logistic.html#cb32-6"></a><span class="co">#&gt; lm(formula = chd ~ age, data = dat)</span></span>
<span id="cb32-7"><a href="logistic.html#cb32-7"></a><span class="co">#&gt; </span></span>
<span id="cb32-8"><a href="logistic.html#cb32-8"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb32-9"><a href="logistic.html#cb32-9"></a><span class="co">#&gt;      Min       1Q   Median       3Q      Max </span></span>
<span id="cb32-10"><a href="logistic.html#cb32-10"></a><span class="co">#&gt; -0.85793 -0.33992 -0.07274  0.31656  0.99269 </span></span>
<span id="cb32-11"><a href="logistic.html#cb32-11"></a><span class="co">#&gt; </span></span>
<span id="cb32-12"><a href="logistic.html#cb32-12"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb32-13"><a href="logistic.html#cb32-13"></a><span class="co">#&gt;              Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb32-14"><a href="logistic.html#cb32-14"></a><span class="co">#&gt; (Intercept) -0.537960   0.168809  -3.187  0.00193 ** </span></span>
<span id="cb32-15"><a href="logistic.html#cb32-15"></a><span class="co">#&gt; age          0.021811   0.003679   5.929 4.57e-08 ***</span></span>
<span id="cb32-16"><a href="logistic.html#cb32-16"></a><span class="co">#&gt; ---</span></span>
<span id="cb32-17"><a href="logistic.html#cb32-17"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb32-18"><a href="logistic.html#cb32-18"></a><span class="co">#&gt; </span></span>
<span id="cb32-19"><a href="logistic.html#cb32-19"></a><span class="co">#&gt; Residual standard error: 0.429 on 98 degrees of freedom</span></span>
<span id="cb32-20"><a href="logistic.html#cb32-20"></a><span class="co">#&gt; Multiple R-squared:  0.264,  Adjusted R-squared:  0.2565 </span></span>
<span id="cb32-21"><a href="logistic.html#cb32-21"></a><span class="co">#&gt; F-statistic: 35.15 on 1 and 98 DF,  p-value: 4.575e-08</span></span></code></pre></div>
<p>There seems to be some sort of positive relation between <em>age</em> and <em>chd</em>. To
better see this, let’s take a look at the scatterplot, and see how the fitted
line looks like:</p>
<p><img src="allmodelsarewrong_files/figure-html/chd_reg_line-1.png" width=".95\linewidth" height=".65\linewidth" style="display: block; margin: auto;" /></p>
<p>This model yields a very awkward fit, with a couple of issues going on.
For one thing, the line, and consequently the predicted values <span class="math inline">\(\hat{y}_i\)</span>,
extend beyond the range <span class="math inline">\([0,1]\)</span>. Think about it: we could obtain fitted values
<span class="math inline">\(\hat{y}_i\)</span> taking any number between 0 and 1 (which, in this context, makes no
sense). We could also get negative predicted values, or even predicted values
greater than 1! On the other hand, if we examine the residuals, then things don’t look great for the linear regression.</p>
<p><img src="allmodelsarewrong_files/figure-html/unnamed-chunk-157-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>So, we need a way to fix these problems.</p>
</div>
<div id="secodn-approach-harsh-thresholding" class="section level3">
<h3><span class="header-section-number">17.1.2</span> Secodn Approach: Harsh Thresholding</h3>
<p>One idea to try to solve the issues from the regression line would be to set
some threshold <span class="math inline">\(c\)</span> (for example, <span class="math inline">\(c = 0.5\)</span>) and look at how the signal compares
to it, which will allow us to create a decision rule. That is, for a given
<em>age</em> value <span class="math inline">\(x_0\)</span>, compare its predicted value to the threshold.
If <span class="math inline">\(\hat{y}_0 \geq 0.5\)</span>, classify it as “1,” otherwise classify it as “0.”</p>
<p><span class="math display">\[
\hat{y}_i =
\begin{cases}
1 \quad \text{if} &amp; b_0 + b_1 x_i \geq c \\
0 \quad \text{if} &amp; b_0 + b_1 x_i &lt; c
\end{cases}
\]</span></p>
<p>We can further arrange the terms above: <span class="math inline">\(\hat{f}(x_i) = b_0 + b_1 x_i - c\)</span></p>
<p><span class="math display">\[\begin{align*}
\hat{f}(x_i) &amp;= b_0 + b_1 x_i - c \\
&amp;= b_{0}^{&#39;} + b_1 x_i, \qquad b_{0}^{&#39;} = b_0 - c 
\end{align*}\]</span></p>
<p>By paying attention to the sign of the signal, we can transform our fitted model into: <span class="math inline">\(\text{sign}(\mathbf{b^\mathsf{T}x})\)</span>.</p>
<p><span class="math display">\[
\hat{y}_i =
\begin{cases}
1 \quad \text{if} &amp; \text{sign}(b_{0}^{&#39;} + b_1 x_i) \geq 0 \\
0 \quad \text{if} &amp; \text{sign}(b_{0}^{&#39;} + b_1 x_i) &lt; 0
\end{cases}
\]</span></p>
<p>This transformation imposes a harsh threshold on the signal. Notice that the signal is still linear but we apply a non-linear transformation <span class="math inline">\(\phi(x) = \text{sign}(x)\)</span>.</p>
<p><strong>Insert image of harsh threshold</strong></p>
<p>I like to think about this transformation is a quick fix. It’s definitely not something to be proud of, but you could use it to get the job done—although in a quick-dirty fashion.</p>
</div>
<div id="third-approach-conditional-means" class="section level3">
<h3><span class="header-section-number">17.1.3</span> Third Approach: Conditional Means</h3>
<p>Using a sign-transformation allows us to overcome some of the limitations of the
linear regression model, but it’s far from ideal.</p>
<p>An alternative approach involves calculating conditional means. How does that work?
The idea is very simple and clever. Say you are looking at patients <span class="math inline">\(x = 24\)</span>
years old, and you count the relative frequency of chd cases. In other words,
you count the proportion of chd cases among individuals 24 years old. This is
nothing else than computing the conditional mean: <span class="math inline">\(avg(y_i | x_i = 24)\)</span>.</p>
<p>Following this idea, we could compute all conditional means for all age values:
<span class="math inline">\((\bar{y}|x_i = 25), (\bar{y}|x_i = 26), \dots, (\bar{y}|x_i = 69)\)</span>.</p>
<pre><code>#&gt; age_group
#&gt; 20-29 30-34 35-39 40-44 45-49 50-54 55-59 60-69 
#&gt;    10    15    12    15    13     8    17    10</code></pre>
<p>Now that we have age by groups, we can get the proportion of
coronary heart disease cases in each age group</p>
<pre><code>#&gt; # A tibble: 8 x 3
#&gt;   age_group count_chd prop_chd
#&gt;   &lt;fct&gt;         &lt;int&gt;    &lt;dbl&gt;
#&gt; 1 20-29            10    0.1  
#&gt; 2 30-34            15    0.133
#&gt; 3 35-39            12    0.25 
#&gt; 4 40-44            15    0.333
#&gt; 5 45-49            13    0.462
#&gt; 6 50-54             8    0.625
#&gt; 7 55-59            17    0.765
#&gt; 8 60-69            10    0.8</code></pre>
<p>And then graph these averages on the scatterplot, connecting the dots with a
line:</p>
<p><img src="allmodelsarewrong_files/figure-html/unnamed-chunk-160-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Sometimes, however, you may not have data points for a specific <span class="math inline">\(x\)</span>-value. So instead we can use groups of ages. For example, say we define a first group of ages to be 24 - 29 years. The corresponding average will be: <span class="math inline">\(avg(y_i | x_i = \text{group}_1)\)</span></p>
<p>Theoretically, we are modeling the conditional expectations: <span class="math inline">\(\mathbb{E}(y|x)\)</span>. Which is exactly the <strong>regression function</strong>.</p>
<p>By connecting the averages, we get an interesting sigmoid pattern</p>
<p><img src="allmodelsarewrong_files/figure-html/unnamed-chunk-161-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>This pattern can be approximated by some mathematical functions, the most popular being the so-called <strong>logistic</strong> function:</p>
<p><span class="math display">\[
\text{logistic function:} \qquad f(s) = \frac{e^{s}}{1 + e^{s}}
\]</span></p>
<p><img src="allmodelsarewrong_files/figure-html/logistic-curves-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Sometimes you may also find the logistic equation in an alternative form:</p>
<p><span class="math display">\[\begin{align*}
f(x) &amp;= \frac{e^{\beta_0 + \beta_1 x}}{1 + e^{\beta_0 + \beta_1 x}} \\
&amp;= \frac{1}{\frac{1 + e^{\beta_0 + \beta_1 x}}{e^{\beta_0 + \beta_1 x}}} \\
&amp;= \frac{1}{\frac{1}{e^{\beta_0 + \beta_1 x}} + 1} \\
&amp;= \frac{1}{1 + e^{-(\beta_0 + \beta_1 x})}
\end{align*}\]</span></p>
<p>Since probability values range inside <span class="math inline">\([0,1]\)</span>, instead of using a line to try to approximate these values, we should use a more adequate curve. This is the reason why sigmoid-like curves, such as the logistic function, are preferred for this purpose.</p>
</div>
</div>
<div id="logistic-regression-model" class="section level2">
<h2><span class="header-section-number">17.2</span> Logistic Regression Model</h2>
<p>We consider the following model:</p>
<p><span class="math display">\[
Prob(y \mid x; \mathbf{b}) = f(x)
\]</span></p>
<p>We don’t get to observe the true probability; rather, we observe the noisy target
<span class="math inline">\(y_i\)</span>, <em>generated/affected</em> by the probability <span class="math inline">\(f(x)\)</span>. How will we model the
probability? We would ideally like a mathematical function that looks like the
sigmoid shape from the toy example above; that is, use a <strong>sigmoid</strong> function.
The most famous function—and the function we will use—is the
<strong>logistic function</strong> defined in the previous section</p>
<p><span class="math display">\[
\text{logistic function:} \qquad f(s) = \frac{e^{s}}{1 + e^{s}}
\]</span></p>
<p>In other words, we have the following model:</p>
<p><span class="math display">\[
Prob(y _i \mid \mathbf{X} ; \boldsymbol{\beta} ) = \frac{e^{\mathbf{b^\mathsf{T} x_i}}}{1 + e^{\mathbf{b^\mathsf{T} x_i}}}
\]</span>
where here <span class="math inline">\(\mathbf{x_i}\)</span> represents the vector of observations for individual <span class="math inline">\(i\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-162"></span>
<img src="images/logistic/logistic-pathdiag1.svg" alt="Regression Model Diagram" width="70%" />
<p class="caption">
Figure 17.1: Regression Model Diagram
</p>
</div>
<p>Hence we have:</p>
<p><span class="math display">\[
Prob(y_i \mid \mathbf{x_i}, \mathbf{b}) = \begin{cases} h(\mathbf{x_i})&amp;  \to y_i = 1 \\ 1 - h(\mathbf{x_i}) &amp; \to y_i = 0 \\ \end{cases}
\]</span></p>
<p>where <span class="math inline">\(h\)</span> denotes the logistic function.</p>
<div id="the-criterion-being-optimized" class="section level3">
<h3><span class="header-section-number">17.2.1</span> The Criterion Being Optimized</h3>
<p>We will not be using the MSE as our error measure (since doing so would make no
sense in this particular context). Instead, we will use an “error” based on
Maximum Likelihood Estimation. In order to do so, we must assume that our model
is true; that is, that <span class="math inline">\(h(x) = f(x)\)</span>; given that assumption, we ask “how likely
is it that we observe the data we already observed (i.e. <span class="math inline">\(y_i\)</span>)?”</p>
<p>We start with the likelihood function <span class="math inline">\(L(\mathbf{b})\)</span>. Note that <span class="math inline">\(L\)</span> <em>implicitly</em>
also depends on <span class="math inline">\(\mathbf{X}\)</span>, and also compute the log-likelihood
<span class="math inline">\(\ell(\mathbf{b})\)</span>:</p>
<p><span class="math display">\[\begin{align*}
Prob(\mathbf{y} \mid x_1, x_2, \dots, x_p; \mathbf{b}) &amp; = \prod_{i=1}^{n} P(y_i \mid \mathbf{b^\mathsf{T} x_i} ) \\
&amp; = \prod_{i=1}^{n} h(\mathbf{b^\mathsf{T} x_i})^{y_i} \left[ 1 - h(\mathbf{b^\mathsf{T} x_i} ) \right]^{1 - y_i} \\
\ell(\mathbf{b}) := \ln[L(\mathbf{b}) ]&amp; = \sum_{i=1}^{n} \ln\left[ P(y_i \mid \mathbf{b^\mathsf{T} x_i} ) \right] \\
&amp; = \sum_{i=1}^{n} \left\{ y_i \ln[h(\mathbf{b^\mathsf{T} x_i})] + (1 - y_i) \ln[ 1 - h(\mathbf{b^\mathsf{T} x_i} ) ]  \right\} \\
&amp; = \sum_{i=1}^{n} \left[ y_i \ln\left( \frac{e^{\mathbf{b^\mathsf{T} x_i}} }{1 + e^{\mathbf{b^\mathsf{T} x_i} } } \right) + (1 - y_i) \ln \left( 1 -  \frac{e^{\mathbf{b^\mathsf{T} x_i}} }{1 + e^{\mathbf{b^\mathsf{T} x_i} } }  \right) \right] \\
&amp; = \sum_{i=1}^{n} \left[ y_i \mathbf{b^\mathsf{T} x_i} - \ln \left( 1 + e ^{\mathbf{b^\mathsf{T} x_i} } \right) \right] 
\end{align*}\]</span></p>
<p>Now, here is the bad news: differentiating and setting equal to 0 yields and
equation for which no closed-form solution exists. To find the maximum likelihood
estimate of <span class="math inline">\(\mathbf{b}\)</span>, we require an iterative method: for example, the <strong>Newton-Raphson method</strong>, or <strong>gradient ascent</strong>.</p>
<p><span class="math display">\[\begin{align*}
\nabla \ell(\mathbf{b}) &amp; = \sum_{i=1}^{n} \left[ y_i \mathbf{x_i} - \left( \frac{e^{\mathbf{b^\mathsf{T} x_i}} }{1 + e^{\mathbf{b^\mathsf{T} x_i} } } \right) \mathbf{x_i} \right] \\
&amp; = \sum_{i=1}^{n} \left[ y_i \mathbf{x_i} - \phi(\mathbf{b^\mathsf{T} x_i} ) \mathbf{x_i}  \right]  \\
&amp; = \sum_{i=1}^{n} \left[ y_i - \phi(\mathbf{b^\mathsf{T} x_i} )  \right] \mathbf{x_i} 
\end{align*}\]</span></p>
<p>Hence, in gradient ascent, we would use:</p>
<p><span class="math display">\[
\mathbf{b}^{(s + 1)} = \mathbf{b}^{(s)} + \alpha \nabla \ell(\mathbf{b}^{(s)})
\]</span></p>
</div>
<div id="another-way-to-solve-logistic-regression" class="section level3">
<h3><span class="header-section-number">17.2.2</span> Another Way to Solve Logistic Regression</h3>
<p>The overall picture stills stands:</p>
<p><img src="images/logistic/logistic-pathdiag1.svg" width="70%" style="display: block; margin: auto;" /></p>
<p>Now, however, we change our probability expression a bit:</p>
<p><span class="math display">\[
Prob(y_i \mid \mathbf{x_i}, \mathbf{b}) = \begin{cases} h(\mathbf{x_i})&amp;  \to y_i = 1 \\ 1 - h(\mathbf{x_i}) &amp; \to y_i = -1 \\ \end{cases}
\]</span></p>
<p>We also need a new way of combining these expression (like we did inside the
likelihood expression). They key observation is to note the following:
<span class="math inline">\(h(-s) = 1 - h(s)\)</span>. With this, we begin computing the likelihood:</p>
<p><span class="math display">\[\begin{align*}
Prob(\mathbf{y} \mid x_1, x_2, \dots, x_p; \mathbf{b}) &amp; = \prod_{i=1}^{n} P(y_i \mid \mathbf{b^\mathsf{T} x_i} ) \\
&amp; = \prod_{i=1}^{n} \phi\left( y_i \mathbf{b^\mathsf{T} x_i} \right) \\
\ell(\mathbf{b}) : = \ln[L(\mathbf{\beta})] &amp; = \sum_{i=1}^{n} \ln\left[ \phi\left( y_i \mathbf{b^\mathsf{T} x_i} \right)  \right] \\
&amp; \Rightarrow \frac{1}{n} \sum_{i=1}^{n} \underbrace{ \ln\left[ \phi\left( y_i \mathbf{b^\mathsf{T} x_i} \right)  \right] }_{\text{logit}}
\end{align*}\]</span></p>
<p>In other words, we can perform the following <strong>minimization</strong> problem:</p>
<p><span class="math display">\[\begin{align*}
&amp; \min_{\mathbf{b}} \left\{ - \frac{1}{n} \sum_{i=1}^{n}  \ln\left[ \phi\left( y_i \mathbf{b^\mathsf{T} x_i} \right)  \right]   \right\} \  \Leftrightarrow  \ \boxed{  \min_{\mathbf{b}} \left\{ \underbrace{ \frac{1}{n} \sum_{i=1}^{n} \underbrace{ \ln\left( 1 + e^{-y_i \mathbf{b^\mathsf{T} x_i} } \right) }_{\text{pointwise error}} }_{E_{in}(\mathbf{b})} \right\}  }
\end{align*}\]</span></p>
<p>The product <span class="math inline">\(y_i \mathbf{b^\mathsf{T} x_i} =: y_i \mathbf{s}\)</span>
(where <span class="math inline">\(\mathbf{s}\)</span> represents “signal”) can be understood using the following
table:</p>
<p>We can think of the term <span class="math inline">\(\mathbf{b^\mathsf{T} x_i}\)</span> as a numeric value ranging
from “very small” to “very large”. A small signal means that the probability
<span class="math inline">\(\phi(\mathbf{b^\mathsf{T} x_i})\)</span> will be small. Conversely, a large signal
means that <span class="math inline">\(\phi(\mathbf{b^\mathsf{T} x_i})\)</span> will be large.
In turn, the term <span class="math inline">\(y_i\)</span> can be either <span class="math inline">\(-1\)</span> or <span class="math inline">\(+1\)</span>.</p>
<p>If we have a correct prediction, we would expect a small probability
<span class="math inline">\(\phi(\mathbf{b^\mathsf{T} x_i})\)</span> to agree with <span class="math inline">\(y_i = -1\)</span>. Likewise, we would
also expect a large probability <span class="math inline">\(\phi(\mathbf{b^\mathsf{T} x_i})\)</span> to agree with
<span class="math inline">\(y_i = +1\)</span>.</p>
<p>In turn, if we have an incorrect prediction, we would expect a small probability
<span class="math inline">\(\phi(\mathbf{b^\mathsf{T} x_i})\)</span> to mismatch <span class="math inline">\(y_i = +1\)</span>. Likewise, we would
also expect a large probability <span class="math inline">\(\phi(\mathbf{b^\mathsf{T} x_i})\)</span> to disagree
with an observed <span class="math inline">\(y_i = -1\)</span>.</p>
<ul>
<li><p>With agreements, <span class="math inline">\(e^{-y_i \mathbf{b^\mathsf{T} x_i} }\)</span> will be small, and
will consequently give you a small error.</p></li>
<li><p>With disagreements, <span class="math inline">\(e^{-y_i \mathbf{b^\mathsf{T} x_i} }\)</span> will be large, and
will consequently give you a large error.</p></li>
</ul>
<p>We will also refer to our <span class="math inline">\(E_{in}(\mathbf{b})\)</span> quantity as the
<strong>mean cross-entropy error</strong>. Technically it isn’t a “cross-entropy” measure in
the classification sense, however we will ignore that technicality for now.
We find:</p>
<p><span class="math display">\[
\nabla E_{in}(\mathbf{b}) = - \frac{1}{n} \sum_{i=1}^{n} \left( \frac{1}{1 + e^{-y_i \mathbf{b^\mathsf{T} x_i} } }  \right) y_i \mathbf{x_i}
\]</span></p>
<p>and we would use <a href="gradient.html#gradient">gradient descent</a> to compute the minimum.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="classif.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="discrim.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["allmodelsarewrong.pdf", "allmodelsarewrong.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
