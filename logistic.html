<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>21 Logistic Regression | All Models Are Wrong</title>
  <meta name="description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  <meta name="generator" content="bookdown 0.16 and GitBook 2.6.7" />

  <meta property="og:title" content="21 Logistic Regression | All Models Are Wrong" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://allmodelsarewrong.github.io" />
  
  <meta property="og:description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  <meta name="github-repo" content="allmodelsarewrong/allmodelsarewrong.github.io" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="21 Logistic Regression | All Models Are Wrong" />
  
  <meta name="twitter:description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  

<meta name="author" content="by Gaston Sanchez, and Ethan Marzban" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="classif.html"/>
<link rel="next" href="discrim.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><b>All Models Are Wrong (CSL)</b><br><small>by G. Sanchez & E. Marzban</small></a></li>

<li class="divider"></li>
<li class="part"><span><b>I Welcome</b></span></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="about.html"><a href="about.html"><i class="fa fa-check"></i><b>1</b> About this book</a><ul>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#prerequisites"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="part"><span><b>II Intro</b></span></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#basic-notation"><i class="fa fa-check"></i><b>2.1</b> Basic Notation</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="duality.html"><a href="duality.html"><i class="fa fa-check"></i><b>3</b> Geometric Duality</a><ul>
<li class="chapter" data-level="3.1" data-path="duality.html"><a href="duality.html#rows-space"><i class="fa fa-check"></i><b>3.1</b> Rows Space</a></li>
<li class="chapter" data-level="3.2" data-path="duality.html"><a href="duality.html#columns-space"><i class="fa fa-check"></i><b>3.2</b> Columns Space</a></li>
<li class="chapter" data-level="3.3" data-path="duality.html"><a href="duality.html#cloud-of-individuals"><i class="fa fa-check"></i><b>3.3</b> Cloud of Individuals</a><ul>
<li class="chapter" data-level="3.3.1" data-path="duality.html"><a href="duality.html#average-individual"><i class="fa fa-check"></i><b>3.3.1</b> Average Individual</a></li>
<li class="chapter" data-level="3.3.2" data-path="duality.html"><a href="duality.html#centered-data"><i class="fa fa-check"></i><b>3.3.2</b> Centered Data</a></li>
<li class="chapter" data-level="3.3.3" data-path="duality.html"><a href="duality.html#distance-between-individuals"><i class="fa fa-check"></i><b>3.3.3</b> Distance between individuals</a></li>
<li class="chapter" data-level="3.3.4" data-path="duality.html"><a href="duality.html#distance-to-the-centroid"><i class="fa fa-check"></i><b>3.3.4</b> Distance to the centroid</a></li>
<li class="chapter" data-level="3.3.5" data-path="duality.html"><a href="duality.html#measures-of-dispersion"><i class="fa fa-check"></i><b>3.3.5</b> Measures of Dispersion</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="duality.html"><a href="duality.html#cloud-of-variables"><i class="fa fa-check"></i><b>3.4</b> Cloud of Variables</a><ul>
<li class="chapter" data-level="3.4.1" data-path="duality.html"><a href="duality.html#mean-of-a-variable"><i class="fa fa-check"></i><b>3.4.1</b> Mean of a Variable</a></li>
<li class="chapter" data-level="3.4.2" data-path="duality.html"><a href="duality.html#variance-of-a-variable"><i class="fa fa-check"></i><b>3.4.2</b> Variance of a Variable</a></li>
<li class="chapter" data-level="3.4.3" data-path="duality.html"><a href="duality.html#variance-with-vector-notation"><i class="fa fa-check"></i><b>3.4.3</b> Variance with Vector Notation</a></li>
<li class="chapter" data-level="3.4.4" data-path="duality.html"><a href="duality.html#standard-deviation-as-a-norm"><i class="fa fa-check"></i><b>3.4.4</b> Standard Deviation as a Norm</a></li>
<li class="chapter" data-level="3.4.5" data-path="duality.html"><a href="duality.html#covariance"><i class="fa fa-check"></i><b>3.4.5</b> Covariance</a></li>
<li class="chapter" data-level="3.4.6" data-path="duality.html"><a href="duality.html#correlation"><i class="fa fa-check"></i><b>3.4.6</b> Correlation</a></li>
<li class="chapter" data-level="3.4.7" data-path="duality.html"><a href="duality.html#geometry-of-correlation"><i class="fa fa-check"></i><b>3.4.7</b> Geometry of Correlation</a></li>
<li class="chapter" data-level="3.4.8" data-path="duality.html"><a href="duality.html#orthogonal-projections"><i class="fa fa-check"></i><b>3.4.8</b> Orthogonal Projections</a></li>
<li class="chapter" data-level="3.4.9" data-path="duality.html"><a href="duality.html#the-mean-as-an-orthogonal-projection"><i class="fa fa-check"></i><b>3.4.9</b> The mean as an orthogonal projection</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Unsupervised I: PCA</b></span></li>
<li class="chapter" data-level="4" data-path="pca.html"><a href="pca.html"><i class="fa fa-check"></i><b>4</b> Principal Components Analysis</a><ul>
<li class="chapter" data-level="4.1" data-path="pca.html"><a href="pca.html#low-dimensional-representations"><i class="fa fa-check"></i><b>4.1</b> Low-dimensional Representations</a></li>
<li class="chapter" data-level="4.2" data-path="pca.html"><a href="pca.html#projections"><i class="fa fa-check"></i><b>4.2</b> Projections</a><ul>
<li class="chapter" data-level="4.2.1" data-path="pca.html"><a href="pca.html#vector-and-scalar-projections"><i class="fa fa-check"></i><b>4.2.1</b> Vector and Scalar Projections</a></li>
<li class="chapter" data-level="4.2.2" data-path="pca.html"><a href="pca.html#projected-inertia"><i class="fa fa-check"></i><b>4.2.2</b> Projected Inertia</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="pca.html"><a href="pca.html#maximization-problem"><i class="fa fa-check"></i><b>4.3</b> Maximization Problem</a><ul>
<li class="chapter" data-level="4.3.1" data-path="pca.html"><a href="pca.html#eigenvectors-of-mathbfs"><i class="fa fa-check"></i><b>4.3.1</b> Eigenvectors of <span class="math inline">\(\mathbf{S}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="pca.html"><a href="pca.html#another-perspective-of-pca"><i class="fa fa-check"></i><b>4.4</b> Another Perspective of PCA</a><ul>
<li class="chapter" data-level="4.4.1" data-path="pca.html"><a href="pca.html#finding-principal-components"><i class="fa fa-check"></i><b>4.4.1</b> Finding Principal Components</a></li>
<li class="chapter" data-level="4.4.2" data-path="pca.html"><a href="pca.html#finding-the-first-pc"><i class="fa fa-check"></i><b>4.4.2</b> Finding the first PC</a></li>
<li class="chapter" data-level="4.4.3" data-path="pca.html"><a href="pca.html#finding-the-second-pc"><i class="fa fa-check"></i><b>4.4.3</b> Finding the second PC</a></li>
<li class="chapter" data-level="4.4.4" data-path="pca.html"><a href="pca.html#finding-all-pcs"><i class="fa fa-check"></i><b>4.4.4</b> Finding all PCs</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="pca.html"><a href="pca.html#data-decomposition-model"><i class="fa fa-check"></i><b>4.5</b> Data Decomposition Model</a><ul>
<li class="chapter" data-level="4.5.1" data-path="pca.html"><a href="pca.html#alternative-approaches"><i class="fa fa-check"></i><b>4.5.1</b> Alternative Approaches</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Linear Regression</b></span></li>
<li class="chapter" data-level="5" data-path="ols.html"><a href="ols.html"><i class="fa fa-check"></i><b>5</b> Linear Regression</a><ul>
<li class="chapter" data-level="5.1" data-path="ols.html"><a href="ols.html#motivation"><i class="fa fa-check"></i><b>5.1</b> Motivation</a></li>
<li class="chapter" data-level="5.2" data-path="ols.html"><a href="ols.html#the-ideaintuition-of-regression"><i class="fa fa-check"></i><b>5.2</b> The Idea/Intuition of Regression</a></li>
<li class="chapter" data-level="5.3" data-path="ols.html"><a href="ols.html#the-linear-regression-model"><i class="fa fa-check"></i><b>5.3</b> The Linear Regression Model</a></li>
<li class="chapter" data-level="5.4" data-path="ols.html"><a href="ols.html#the-error-measure"><i class="fa fa-check"></i><b>5.4</b> The Error Measure</a></li>
<li class="chapter" data-level="5.5" data-path="ols.html"><a href="ols.html#the-least-squares-algorithm"><i class="fa fa-check"></i><b>5.5</b> The Least Squares Algorithm</a></li>
<li class="chapter" data-level="5.6" data-path="ols.html"><a href="ols.html#geometries-of-ols"><i class="fa fa-check"></i><b>5.6</b> Geometries of OLS</a><ul>
<li class="chapter" data-level="5.6.1" data-path="ols.html"><a href="ols.html#rows-perspective"><i class="fa fa-check"></i><b>5.6.1</b> Rows Perspective</a></li>
<li class="chapter" data-level="5.6.2" data-path="ols.html"><a href="ols.html#columns-perspective"><i class="fa fa-check"></i><b>5.6.2</b> Columns Perspective</a></li>
<li class="chapter" data-level="5.6.3" data-path="ols.html"><a href="ols.html#parameters-perspective"><i class="fa fa-check"></i><b>5.6.3</b> Parameters Perspective</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="gradient.html"><a href="gradient.html"><i class="fa fa-check"></i><b>6</b> Gradient Descent</a><ul>
<li class="chapter" data-level="6.1" data-path="gradient.html"><a href="gradient.html#error-surface"><i class="fa fa-check"></i><b>6.1</b> Error Surface</a></li>
<li class="chapter" data-level="6.2" data-path="gradient.html"><a href="gradient.html#idea-of-gradient-descent"><i class="fa fa-check"></i><b>6.2</b> Idea of Gradient Descent</a></li>
<li class="chapter" data-level="6.3" data-path="gradient.html"><a href="gradient.html#moving-down-an-error-surface"><i class="fa fa-check"></i><b>6.3</b> Moving Down an Error Surface</a><ul>
<li class="chapter" data-level="6.3.1" data-path="gradient.html"><a href="gradient.html#the-direction-of-mathbfv"><i class="fa fa-check"></i><b>6.3.1</b> The direction of <span class="math inline">\(\mathbf{v}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="gradient.html"><a href="gradient.html#gradient-descent-and-our-model"><i class="fa fa-check"></i><b>6.4</b> Gradient Descent and our Model</a></li>
</ul></li>
<li class="part"><span><b>V Learning Concepts</b></span></li>
<li class="chapter" data-level="7" data-path="learning.html"><a href="learning.html"><i class="fa fa-check"></i><b>7</b> Theoretical Framework</a><ul>
<li class="chapter" data-level="7.1" data-path="learning.html"><a href="learning.html#mental-map"><i class="fa fa-check"></i><b>7.1</b> Mental Map</a></li>
<li class="chapter" data-level="7.2" data-path="learning.html"><a href="learning.html#kinds-of-predictions"><i class="fa fa-check"></i><b>7.2</b> Kinds of Predictions</a><ul>
<li class="chapter" data-level="7.2.1" data-path="learning.html"><a href="learning.html#two-types-of-data"><i class="fa fa-check"></i><b>7.2.1</b> Two Types of Data</a></li>
<li class="chapter" data-level="7.2.2" data-path="learning.html"><a href="learning.html#two-types-of-predictions"><i class="fa fa-check"></i><b>7.2.2</b> Two Types of Predictions</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="learning.html"><a href="learning.html#two-types-of-errors"><i class="fa fa-check"></i><b>7.3</b> Two Types of Errors</a><ul>
<li class="chapter" data-level="7.3.1" data-path="learning.html"><a href="learning.html#individual-errors"><i class="fa fa-check"></i><b>7.3.1</b> Individual Errors</a></li>
<li class="chapter" data-level="7.3.2" data-path="learning.html"><a href="learning.html#overall-errors"><i class="fa fa-check"></i><b>7.3.2</b> Overall Errors</a></li>
<li class="chapter" data-level="7.3.3" data-path="learning.html"><a href="learning.html#auxiliary-technicality"><i class="fa fa-check"></i><b>7.3.3</b> Auxiliary Technicality</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="learning.html"><a href="learning.html#noisy-targets"><i class="fa fa-check"></i><b>7.4</b> Noisy Targets</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="mse.html"><a href="mse.html"><i class="fa fa-check"></i><b>8</b> MSE of Estimator</a><ul>
<li class="chapter" data-level="8.1" data-path="mse.html"><a href="mse.html#mse-of-an-estimator"><i class="fa fa-check"></i><b>8.1</b> MSE of an Estimator</a><ul>
<li class="chapter" data-level="8.1.1" data-path="mse.html"><a href="mse.html#prototypical-cases-of-bias-and-variance"><i class="fa fa-check"></i><b>8.1.1</b> Prototypical Cases of Bias and Variance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="biasvar.html"><a href="biasvar.html"><i class="fa fa-check"></i><b>9</b> Bias-Variance Tradeoff</a><ul>
<li class="chapter" data-level="9.1" data-path="biasvar.html"><a href="biasvar.html#introduction"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="biasvar.html"><a href="biasvar.html#motivation-example"><i class="fa fa-check"></i><b>9.2</b> Motivation Example</a><ul>
<li class="chapter" data-level="9.2.1" data-path="biasvar.html"><a href="biasvar.html#two-hypotheses"><i class="fa fa-check"></i><b>9.2.1</b> Two Hypotheses</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="biasvar.html"><a href="biasvar.html#learning-from-two-points"><i class="fa fa-check"></i><b>9.3</b> Learning from two points</a></li>
<li class="chapter" data-level="9.4" data-path="biasvar.html"><a href="biasvar.html#bias-variance-derivation"><i class="fa fa-check"></i><b>9.4</b> Bias-Variance Derivation</a><ul>
<li class="chapter" data-level="9.4.1" data-path="biasvar.html"><a href="biasvar.html#out-of-sample-predictions"><i class="fa fa-check"></i><b>9.4.1</b> Out-of-Sample Predictions</a></li>
<li class="chapter" data-level="9.4.2" data-path="biasvar.html"><a href="biasvar.html#noisy-target"><i class="fa fa-check"></i><b>9.4.2</b> Noisy Target</a></li>
<li class="chapter" data-level="9.4.3" data-path="biasvar.html"><a href="biasvar.html#types-of-theoretical-mses"><i class="fa fa-check"></i><b>9.4.3</b> Types of Theoretical MSEs</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="biasvar.html"><a href="biasvar.html#the-tradeoff"><i class="fa fa-check"></i><b>9.5</b> The Tradeoff</a><ul>
<li class="chapter" data-level="9.5.1" data-path="biasvar.html"><a href="biasvar.html#bias-variance-tradeoff-picture"><i class="fa fa-check"></i><b>9.5.1</b> Bias-Variance Tradeoff Picture</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="phases.html"><a href="phases.html"><i class="fa fa-check"></i><b>10</b> Learning Phases</a><ul>
<li class="chapter" data-level="10.1" data-path="phases.html"><a href="phases.html#introduction-1"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="phases.html"><a href="phases.html#model-assessment"><i class="fa fa-check"></i><b>10.2</b> Model Assessment</a><ul>
<li class="chapter" data-level="10.2.1" data-path="phases.html"><a href="phases.html#holdout-test-set"><i class="fa fa-check"></i><b>10.2.1</b> Holdout Test Set</a></li>
<li class="chapter" data-level="10.2.2" data-path="phases.html"><a href="phases.html#why-does-a-test-set-work"><i class="fa fa-check"></i><b>10.2.2</b> Why does a test set work?</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="phases.html"><a href="phases.html#model-selection"><i class="fa fa-check"></i><b>10.3</b> Model Selection</a><ul>
<li class="chapter" data-level="10.3.1" data-path="phases.html"><a href="phases.html#three-way-holdout-method"><i class="fa fa-check"></i><b>10.3.1</b> Three-way Holdout Method</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="phases.html"><a href="phases.html#model-training"><i class="fa fa-check"></i><b>10.4</b> Model Training</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="resampling.html"><a href="resampling.html"><i class="fa fa-check"></i><b>11</b> Resample Approaches</a><ul>
<li class="chapter" data-level="11.1" data-path="resampling.html"><a href="resampling.html#general-sampling-blueprint"><i class="fa fa-check"></i><b>11.1</b> General Sampling Blueprint</a></li>
<li class="chapter" data-level="11.2" data-path="resampling.html"><a href="resampling.html#monte-carlo-cross-validation"><i class="fa fa-check"></i><b>11.2</b> Monte Carlo Cross Validation</a></li>
<li class="chapter" data-level="11.3" data-path="resampling.html"><a href="resampling.html#bootstrap-method"><i class="fa fa-check"></i><b>11.3</b> Bootstrap Method</a></li>
<li class="chapter" data-level="11.4" data-path="resampling.html"><a href="resampling.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>11.4</b> <span class="math inline">\(K\)</span>-Fold Cross-Validation</a><ul>
<li class="chapter" data-level="11.4.1" data-path="resampling.html"><a href="resampling.html#leave-one-out-cross-validation-loocv"><i class="fa fa-check"></i><b>11.4.1</b> Leave-One-Out Cross Validation (LOOCV)</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>VI Regularization</b></span></li>
<li class="chapter" data-level="12" data-path="regular.html"><a href="regular.html"><i class="fa fa-check"></i><b>12</b> Regularization Techniques</a><ul>
<li class="chapter" data-level="12.1" data-path="regular.html"><a href="regular.html#multicollinearity-issues"><i class="fa fa-check"></i><b>12.1</b> Multicollinearity Issues</a><ul>
<li class="chapter" data-level="12.1.1" data-path="regular.html"><a href="regular.html#toy-example"><i class="fa fa-check"></i><b>12.1.1</b> Toy Example</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="regular.html"><a href="regular.html#irregular-coefficients"><i class="fa fa-check"></i><b>12.2</b> Irregular Coefficients</a></li>
<li class="chapter" data-level="12.3" data-path="regular.html"><a href="regular.html#connection-to-regularization"><i class="fa fa-check"></i><b>12.3</b> Connection to Regularization</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="pcr.html"><a href="pcr.html"><i class="fa fa-check"></i><b>13</b> Principal Components Regression</a><ul>
<li class="chapter" data-level="13.1" data-path="pcr.html"><a href="pcr.html#motivation-example-1"><i class="fa fa-check"></i><b>13.1</b> Motivation Example</a></li>
<li class="chapter" data-level="13.2" data-path="pcr.html"><a href="pcr.html#the-pcr-model"><i class="fa fa-check"></i><b>13.2</b> The PCR Model</a></li>
<li class="chapter" data-level="13.3" data-path="pcr.html"><a href="pcr.html#how-does-pcr-work"><i class="fa fa-check"></i><b>13.3</b> How does PCR work?</a><ul>
<li class="chapter" data-level="13.3.1" data-path="pcr.html"><a href="pcr.html#transition-formula"><i class="fa fa-check"></i><b>13.3.1</b> Transition Formula</a></li>
<li class="chapter" data-level="13.3.2" data-path="pcr.html"><a href="pcr.html#size-of-coefficients"><i class="fa fa-check"></i><b>13.3.2</b> Size of Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="pcr.html"><a href="pcr.html#selecting-number-of-pcs"><i class="fa fa-check"></i><b>13.4</b> Selecting Number of PCs</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="pls.html"><a href="pls.html"><i class="fa fa-check"></i><b>14</b> Partial Least Squares Regression</a><ul>
<li class="chapter" data-level="14.1" data-path="pls.html"><a href="pls.html#motivation-example-2"><i class="fa fa-check"></i><b>14.1</b> Motivation Example</a></li>
<li class="chapter" data-level="14.2" data-path="pls.html"><a href="pls.html#the-plsr-model"><i class="fa fa-check"></i><b>14.2</b> The PLSR Model</a></li>
<li class="chapter" data-level="14.3" data-path="pls.html"><a href="pls.html#how-does-plsr-work"><i class="fa fa-check"></i><b>14.3</b> How does PLSR work?</a></li>
<li class="chapter" data-level="14.4" data-path="pls.html"><a href="pls.html#plsr-algorithm"><i class="fa fa-check"></i><b>14.4</b> PLSR Algorithm</a><ul>
<li class="chapter" data-level="14.4.1" data-path="pls.html"><a href="pls.html#pls-solution-with-original-variables"><i class="fa fa-check"></i><b>14.4.1</b> PLS Solution with original variables</a></li>
<li class="chapter" data-level="14.4.2" data-path="pls.html"><a href="pls.html#size-of-coefficients-1"><i class="fa fa-check"></i><b>14.4.2</b> Size of Coefficients</a></li>
<li class="chapter" data-level="14.4.3" data-path="pls.html"><a href="pls.html#some-properties"><i class="fa fa-check"></i><b>14.4.3</b> Some Properties</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="pls.html"><a href="pls.html#selecting-number-of-pls-components"><i class="fa fa-check"></i><b>14.5</b> Selecting Number of PLS Components</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="ridge.html"><a href="ridge.html"><i class="fa fa-check"></i><b>15</b> Ridge Regression</a><ul>
<li class="chapter" data-level="15.1" data-path="ridge.html"><a href="ridge.html#a-new-minimization-problem"><i class="fa fa-check"></i><b>15.1</b> A New Minimization Problem</a><ul>
<li class="chapter" data-level="15.1.1" data-path="ridge.html"><a href="ridge.html#constraining-regression-coefficients"><i class="fa fa-check"></i><b>15.1.1</b> Constraining Regression Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="ridge.html"><a href="ridge.html#a-new-minimization-solution"><i class="fa fa-check"></i><b>15.2</b> A New Minimization Solution</a></li>
<li class="chapter" data-level="15.3" data-path="ridge.html"><a href="ridge.html#what-does-rr-accomplish"><i class="fa fa-check"></i><b>15.3</b> What does RR accomplish?</a><ul>
<li class="chapter" data-level="15.3.1" data-path="ridge.html"><a href="ridge.html#how-to-find-lambda"><i class="fa fa-check"></i><b>15.3.1</b> How to find <span class="math inline">\(\lambda\)</span>?</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>VII Extending Linear Regression</b></span></li>
<li class="chapter" data-level="16" data-path="linear-extensions.html"><a href="linear-extensions.html"><i class="fa fa-check"></i><b>16</b> Beyond Linear Regression</a><ul>
<li class="chapter" data-level="16.1" data-path="linear-extensions.html"><a href="linear-extensions.html#introduction-2"><i class="fa fa-check"></i><b>16.1</b> Introduction</a></li>
<li class="chapter" data-level="16.2" data-path="linear-extensions.html"><a href="linear-extensions.html#expanding-the-regression-horizon"><i class="fa fa-check"></i><b>16.2</b> Expanding the Regression Horizon</a><ul>
<li class="chapter" data-level="16.2.1" data-path="linear-extensions.html"><a href="linear-extensions.html#linearity"><i class="fa fa-check"></i><b>16.2.1</b> Linearity</a></li>
<li class="chapter" data-level="16.2.2" data-path="linear-extensions.html"><a href="linear-extensions.html#parametric-and-nonparametric"><i class="fa fa-check"></i><b>16.2.2</b> Parametric and Nonparametric</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="linear-extensions.html"><a href="linear-extensions.html#transforming-features"><i class="fa fa-check"></i><b>16.3</b> Transforming Features</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="nonparametric.html"><a href="nonparametric.html"><i class="fa fa-check"></i><b>17</b> Nonparametric Regression</a><ul>
<li class="chapter" data-level="17.1" data-path="nonparametric.html"><a href="nonparametric.html#conditional-averages"><i class="fa fa-check"></i><b>17.1</b> Conditional Averages</a></li>
<li class="chapter" data-level="17.2" data-path="nonparametric.html"><a href="nonparametric.html#looking-at-the-neighbors"><i class="fa fa-check"></i><b>17.2</b> Looking at the Neighbors</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="knn.html"><a href="knn.html"><i class="fa fa-check"></i><b>18</b> Nearest Neighbor Estimates</a><ul>
<li class="chapter" data-level="18.1" data-path="knn.html"><a href="knn.html#introduction-3"><i class="fa fa-check"></i><b>18.1</b> Introduction</a></li>
<li class="chapter" data-level="18.2" data-path="knn.html"><a href="knn.html#k-nearest-neighbors-knn"><i class="fa fa-check"></i><b>18.2</b> <span class="math inline">\(k\)</span> Nearest Neighbors (KNN)</a><ul>
<li class="chapter" data-level="18.2.1" data-path="knn.html"><a href="knn.html#distance-measures"><i class="fa fa-check"></i><b>18.2.1</b> Distance Measures</a></li>
<li class="chapter" data-level="18.2.2" data-path="knn.html"><a href="knn.html#knn-estimator"><i class="fa fa-check"></i><b>18.2.2</b> KNN Estimator</a></li>
<li class="chapter" data-level="18.2.3" data-path="knn.html"><a href="knn.html#how-to-find-k"><i class="fa fa-check"></i><b>18.2.3</b> How to find <span class="math inline">\(k\)</span>?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html"><i class="fa fa-check"></i><b>19</b> Kernel Smoothers</a><ul>
<li class="chapter" data-level="19.1" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#introduction-4"><i class="fa fa-check"></i><b>19.1</b> Introduction</a></li>
<li class="chapter" data-level="19.2" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#kernel-smoothers-1"><i class="fa fa-check"></i><b>19.2</b> Kernel Smoothers</a><ul>
<li class="chapter" data-level="19.2.1" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#kernel-functions"><i class="fa fa-check"></i><b>19.2.1</b> Kernel Functions</a></li>
<li class="chapter" data-level="19.2.2" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#weights-from-kernels"><i class="fa fa-check"></i><b>19.2.2</b> Weights from Kernels</a></li>
<li class="chapter" data-level="19.2.3" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#kernel-estimator"><i class="fa fa-check"></i><b>19.2.3</b> Kernel Estimator</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#local-polynomial-estimators"><i class="fa fa-check"></i><b>19.3</b> Local Polynomial Estimators</a></li>
</ul></li>
<li class="part"><span><b>VIII Classification</b></span></li>
<li class="chapter" data-level="20" data-path="classif.html"><a href="classif.html"><i class="fa fa-check"></i><b>20</b> Classification Methods</a></li>
<li class="chapter" data-level="21" data-path="logistic.html"><a href="logistic.html"><i class="fa fa-check"></i><b>21</b> Logistic Regression</a><ul>
<li class="chapter" data-level="21.1" data-path="logistic.html"><a href="logistic.html#motivation-1"><i class="fa fa-check"></i><b>21.1</b> Motivation</a><ul>
<li class="chapter" data-level="21.1.1" data-path="logistic.html"><a href="logistic.html#first-approach-fitting-a-line"><i class="fa fa-check"></i><b>21.1.1</b> First Approach: Fitting a Line</a></li>
<li class="chapter" data-level="21.1.2" data-path="logistic.html"><a href="logistic.html#second-approach-harsh-thresholding"><i class="fa fa-check"></i><b>21.1.2</b> Second Approach: Harsh Thresholding</a></li>
<li class="chapter" data-level="21.1.3" data-path="logistic.html"><a href="logistic.html#third-approach-conditional-means"><i class="fa fa-check"></i><b>21.1.3</b> Third Approach: Conditional Means</a></li>
</ul></li>
<li class="chapter" data-level="21.2" data-path="logistic.html"><a href="logistic.html#logistic-regression-model"><i class="fa fa-check"></i><b>21.2</b> Logistic Regression Model</a><ul>
<li class="chapter" data-level="21.2.1" data-path="logistic.html"><a href="logistic.html#the-criterion-being-optimized"><i class="fa fa-check"></i><b>21.2.1</b> The Criterion Being Optimized</a></li>
<li class="chapter" data-level="21.2.2" data-path="logistic.html"><a href="logistic.html#another-way-to-solve-logistic-regression"><i class="fa fa-check"></i><b>21.2.2</b> Another Way to Solve Logistic Regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="22" data-path="discrim.html"><a href="discrim.html"><i class="fa fa-check"></i><b>22</b> Preamble for Discriminant Analysis</a><ul>
<li class="chapter" data-level="22.1" data-path="discrim.html"><a href="discrim.html#motivation-2"><i class="fa fa-check"></i><b>22.1</b> Motivation</a><ul>
<li class="chapter" data-level="22.1.1" data-path="discrim.html"><a href="discrim.html#distinguishing-species"><i class="fa fa-check"></i><b>22.1.1</b> Distinguishing Species</a></li>
<li class="chapter" data-level="22.1.2" data-path="discrim.html"><a href="discrim.html#sum-of-squares-decomposition"><i class="fa fa-check"></i><b>22.1.2</b> Sum of Squares Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="22.2" data-path="discrim.html"><a href="discrim.html#derived-ratios-from-sum-of-squares"><i class="fa fa-check"></i><b>22.2</b> Derived Ratios from Sum-of-Squares</a><ul>
<li class="chapter" data-level="22.2.1" data-path="discrim.html"><a href="discrim.html#correlation-ratio"><i class="fa fa-check"></i><b>22.2.1</b> Correlation Ratio</a></li>
<li class="chapter" data-level="22.2.2" data-path="discrim.html"><a href="discrim.html#f-ratio"><i class="fa fa-check"></i><b>22.2.2</b> F-Ratio</a></li>
</ul></li>
<li class="chapter" data-level="22.3" data-path="discrim.html"><a href="discrim.html#geometric-perspective"><i class="fa fa-check"></i><b>22.3</b> Geometric Perspective</a><ul>
<li class="chapter" data-level="22.3.1" data-path="discrim.html"><a href="discrim.html#clouds-from-class-structure"><i class="fa fa-check"></i><b>22.3.1</b> Clouds from Class Structure</a></li>
<li class="chapter" data-level="22.3.2" data-path="discrim.html"><a href="discrim.html#dispersion-decomposition"><i class="fa fa-check"></i><b>22.3.2</b> Dispersion Decomposition</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="23" data-path="cda.html"><a href="cda.html"><i class="fa fa-check"></i><b>23</b> Canonical Discriminant Analysis</a><ul>
<li class="chapter" data-level="23.1" data-path="cda.html"><a href="cda.html#cda-semi-supervised-aspect"><i class="fa fa-check"></i><b>23.1</b> CDA: Semi-Supervised Aspect</a></li>
<li class="chapter" data-level="23.2" data-path="cda.html"><a href="cda.html#looking-for-a-discriminant-axis"><i class="fa fa-check"></i><b>23.2</b> Looking for a discriminant axis</a></li>
<li class="chapter" data-level="23.3" data-path="cda.html"><a href="cda.html#looking-for-a-compromise-criterion"><i class="fa fa-check"></i><b>23.3</b> Looking for a Compromise Criterion</a><ul>
<li class="chapter" data-level="23.3.1" data-path="cda.html"><a href="cda.html#correlation-ratio-criterion"><i class="fa fa-check"></i><b>23.3.1</b> Correlation Ratio Criterion</a></li>
<li class="chapter" data-level="23.3.2" data-path="cda.html"><a href="cda.html#f-ratio-criterion"><i class="fa fa-check"></i><b>23.3.2</b> F-ratio Criterion</a></li>
</ul></li>
<li class="chapter" data-level="23.4" data-path="cda.html"><a href="cda.html#cda-supervised-aspect"><i class="fa fa-check"></i><b>23.4</b> CDA: Supervised Aspect</a><ul>
<li class="chapter" data-level="23.4.1" data-path="cda.html"><a href="cda.html#a-special-pca"><i class="fa fa-check"></i><b>23.4.1</b> A Special PCA</a></li>
<li class="chapter" data-level="23.4.2" data-path="cda.html"><a href="cda.html#predictive-idea"><i class="fa fa-check"></i><b>23.4.2</b> Predictive Idea</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="24" data-path="discanalysis.html"><a href="discanalysis.html"><i class="fa fa-check"></i><b>24</b> Discriminant Analysis</a><ul>
<li class="chapter" data-level="24.1" data-path="discanalysis.html"><a href="discanalysis.html#probabilistic-da"><i class="fa fa-check"></i><b>24.1</b> Probabilistic DA</a></li>
<li class="chapter" data-level="24.2" data-path="discanalysis.html"><a href="discanalysis.html#estimations"><i class="fa fa-check"></i><b>24.2</b> Estimations</a><ul>
<li class="chapter" data-level="24.2.1" data-path="discanalysis.html"><a href="discanalysis.html#univariate-data-boldsymbolp-1"><i class="fa fa-check"></i><b>24.2.1</b> Univariate Data (<span class="math inline">\(\boldsymbol{p = 1}\)</span>)</a></li>
<li class="chapter" data-level="24.2.2" data-path="discanalysis.html"><a href="discanalysis.html#multivariate-data-boldsymbolp-1"><i class="fa fa-check"></i><b>24.2.2</b> Multivariate Data (<span class="math inline">\(\boldsymbol{p &gt; 1}\)</span>)</a></li>
<li class="chapter" data-level="24.2.3" data-path="discanalysis.html"><a href="discanalysis.html#multivariate-estimations"><i class="fa fa-check"></i><b>24.2.3</b> Multivariate Estimations</a></li>
</ul></li>
<li class="chapter" data-level="24.3" data-path="discanalysis.html"><a href="discanalysis.html#discriminant-functions"><i class="fa fa-check"></i><b>24.3</b> Discriminant Functions</a></li>
<li class="chapter" data-level="24.4" data-path="discanalysis.html"><a href="discanalysis.html#quadratic-discriminant-analysis-qda"><i class="fa fa-check"></i><b>24.4</b> Quadratic Discriminant Analysis (QDA)</a></li>
<li class="chapter" data-level="24.5" data-path="discanalysis.html"><a href="discanalysis.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>24.5</b> Linear Discriminant Analysis</a></li>
<li class="chapter" data-level="24.6" data-path="discanalysis.html"><a href="discanalysis.html#canonical-discriminant-analysis"><i class="fa fa-check"></i><b>24.6</b> Canonical Discriminant Analysis</a></li>
<li class="chapter" data-level="24.7" data-path="discanalysis.html"><a href="discanalysis.html#naive-bayes"><i class="fa fa-check"></i><b>24.7</b> Naive Bayes</a></li>
<li class="chapter" data-level="24.8" data-path="discanalysis.html"><a href="discanalysis.html#fifth-case"><i class="fa fa-check"></i><b>24.8</b> Fifth Case</a></li>
<li class="chapter" data-level="24.9" data-path="discanalysis.html"><a href="discanalysis.html#comparing-the-cases"><i class="fa fa-check"></i><b>24.9</b> Comparing the Cases</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="classperformance.html"><a href="classperformance.html"><i class="fa fa-check"></i><b>25</b> Performance of Classifiers</a><ul>
<li class="chapter" data-level="25.1" data-path="classperformance.html"><a href="classperformance.html#error-for-binary-response"><i class="fa fa-check"></i><b>25.1</b> Error for Binary Response</a></li>
<li class="chapter" data-level="25.2" data-path="classperformance.html"><a href="classperformance.html#categorical-response"><i class="fa fa-check"></i><b>25.2</b> Categorical Response</a></li>
<li class="chapter" data-level="25.3" data-path="classperformance.html"><a href="classperformance.html#confusion-matrices"><i class="fa fa-check"></i><b>25.3</b> Confusion Matrices</a></li>
<li class="chapter" data-level="25.4" data-path="classperformance.html"><a href="classperformance.html#binary-response-example"><i class="fa fa-check"></i><b>25.4</b> Binary Response: Example</a><ul>
<li class="chapter" data-level="25.4.1" data-path="classperformance.html"><a href="classperformance.html#types-of-errors-application-for-savings-account"><i class="fa fa-check"></i><b>25.4.1</b> Types of Errors: Application for Savings Account</a></li>
<li class="chapter" data-level="25.4.2" data-path="classperformance.html"><a href="classperformance.html#another-example-loans"><i class="fa fa-check"></i><b>25.4.2</b> Another Example: Loans</a></li>
<li class="chapter" data-level="25.4.3" data-path="classperformance.html"><a href="classperformance.html#the-ideal-situation"><i class="fa fa-check"></i><b>25.4.3</b> The Ideal Situation</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IX Unsupervised II: Clustering</b></span></li>
<li class="chapter" data-level="26" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>26</b> Clustering</a><ul>
<li class="chapter" data-level="26.1" data-path="clustering.html"><a href="clustering.html#about-clustering"><i class="fa fa-check"></i><b>26.1</b> About Clustering</a><ul>
<li class="chapter" data-level="26.1.1" data-path="clustering.html"><a href="clustering.html#types-of-clustering"><i class="fa fa-check"></i><b>26.1.1</b> Types of Clustering</a></li>
<li class="chapter" data-level="26.1.2" data-path="clustering.html"><a href="clustering.html#hard-clustering"><i class="fa fa-check"></i><b>26.1.2</b> Hard Clustering</a></li>
</ul></li>
<li class="chapter" data-level="26.2" data-path="clustering.html"><a href="clustering.html#dispersion-measures"><i class="fa fa-check"></i><b>26.2</b> Dispersion Measures</a></li>
<li class="chapter" data-level="26.3" data-path="clustering.html"><a href="clustering.html#complexity-in-clustering"><i class="fa fa-check"></i><b>26.3</b> Complexity in Clustering</a></li>
</ul></li>
<li class="chapter" data-level="27" data-path="kmeans.html"><a href="kmeans.html"><i class="fa fa-check"></i><b>27</b> K-Means</a><ul>
<li class="chapter" data-level="27.1" data-path="kmeans.html"><a href="kmeans.html#toy-example-1"><i class="fa fa-check"></i><b>27.1</b> Toy Example</a></li>
<li class="chapter" data-level="27.2" data-path="kmeans.html"><a href="kmeans.html#what-does-k-means-do"><i class="fa fa-check"></i><b>27.2</b> What does K-means do?</a></li>
<li class="chapter" data-level="27.3" data-path="kmeans.html"><a href="kmeans.html#k-means-algorithms"><i class="fa fa-check"></i><b>27.3</b> K-Means Algorithms</a><ul>
<li class="chapter" data-level="27.3.1" data-path="kmeans.html"><a href="kmeans.html#classic-version"><i class="fa fa-check"></i><b>27.3.1</b> Classic Version</a></li>
<li class="chapter" data-level="27.3.2" data-path="kmeans.html"><a href="kmeans.html#moving-centers-algorithm"><i class="fa fa-check"></i><b>27.3.2</b> Moving Centers Algorithm</a></li>
<li class="chapter" data-level="27.3.3" data-path="kmeans.html"><a href="kmeans.html#dynamic-clouds"><i class="fa fa-check"></i><b>27.3.3</b> Dynamic Clouds</a></li>
<li class="chapter" data-level="27.3.4" data-path="kmeans.html"><a href="kmeans.html#choosing-k"><i class="fa fa-check"></i><b>27.3.4</b> Choosing <span class="math inline">\(k\)</span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="28" data-path="hclus.html"><a href="hclus.html"><i class="fa fa-check"></i><b>28</b> Hierarchical Clustering</a><ul>
<li class="chapter" data-level="28.1" data-path="hclus.html"><a href="hclus.html#agglomerative-methods"><i class="fa fa-check"></i><b>28.1</b> Agglomerative Methods</a></li>
<li class="chapter" data-level="28.2" data-path="hclus.html"><a href="hclus.html#example-single-linkage"><i class="fa fa-check"></i><b>28.2</b> Example: Single Linkage</a><ul>
<li class="chapter" data-level="28.2.1" data-path="hclus.html"><a href="hclus.html#dendrogram"><i class="fa fa-check"></i><b>28.2.1</b> Dendrogram</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>X Tree-based Methods</b></span></li>
<li class="chapter" data-level="29" data-path="trees.html"><a href="trees.html"><i class="fa fa-check"></i><b>29</b> Intro to Decision Trees</a><ul>
<li class="chapter" data-level="29.1" data-path="trees.html"><a href="trees.html#introduction-5"><i class="fa fa-check"></i><b>29.1</b> Introduction</a></li>
<li class="chapter" data-level="29.2" data-path="trees.html"><a href="trees.html#binary-trees"><i class="fa fa-check"></i><b>29.2</b> Binary Trees</a><ul>
<li class="chapter" data-level="29.2.1" data-path="trees.html"><a href="trees.html#the-process-of-building-a-tree"><i class="fa fa-check"></i><b>29.2.1</b> The Process of Building a Tree</a></li>
<li class="chapter" data-level="29.2.2" data-path="trees.html"><a href="trees.html#binary-partitions"><i class="fa fa-check"></i><b>29.2.2</b> Binary Partitions</a></li>
</ul></li>
<li class="chapter" data-level="29.3" data-path="trees.html"><a href="trees.html#measures-of-heterogeneity"><i class="fa fa-check"></i><b>29.3</b> Measures of Heterogeneity</a><ul>
<li class="chapter" data-level="29.3.1" data-path="trees.html"><a href="trees.html#entropy"><i class="fa fa-check"></i><b>29.3.1</b> Entropy</a></li>
<li class="chapter" data-level="29.3.2" data-path="trees.html"><a href="trees.html#the-math-behind-entropy"><i class="fa fa-check"></i><b>29.3.2</b> The Math Behind Entropy</a></li>
<li class="chapter" data-level="29.3.3" data-path="trees.html"><a href="trees.html#gini-impurity"><i class="fa fa-check"></i><b>29.3.3</b> Gini Impurity</a></li>
<li class="chapter" data-level="29.3.4" data-path="trees.html"><a href="trees.html#toy-example-2"><i class="fa fa-check"></i><b>29.3.4</b> Toy Example</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Made with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">All Models Are Wrong</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="logistic" class="section level1">
<h1><span class="header-section-number">21</span> Logistic Regression</h1>
<p>So far we’ve been dealing with predicting a quantitative response, using mostly
linear models. But what about predicting a qualitative or categorical response?
We now turn our attention to predicting a discrete (aka categorical) response.</p>
<p>To have a gentle transition from regression models into classification models,
we’ll start with the famous logistic regression.</p>
<div id="motivation-1" class="section level2">
<h2><span class="header-section-number">21.1</span> Motivation</h2>
<p>Let’s consider a classic example: predicting heart attack. The data consists of
a number of individuals (patients) with some medical variables.</p>
<p>The following (very famous) example is based on data found in Hosmer and Lemeshow
(edition from 2000) in which we wish to predict coronary heart disease (<em>chd</em>).
The data is relatively small (<span class="math inline">\(n = 100\)</span> patients), and we start by considering
only one predictor: <em>age</em>.</p>
<p>The following display shows a few data points in the data set. The predictor
variable <em>age</em> is expressed in whole years. The response <em>chd</em> is a binary
variable with 0 indicating absence of <em>chd</em>, and 1 indicating presence of <em>chd</em>.</p>
<pre><code>#&gt;   age chd
#&gt; 1  20   0
#&gt; 2  23   0
#&gt; 3  24   0
#&gt; 4  25   0
#&gt; 5  25   1
#&gt; 6  26   0</code></pre>
<p>If we graph a scatterplot we get the following image. We have added a little
bit of <em>jitter</em> effect to the dots, in order to better visualize their spread.</p>
<p><img src="allmodelsarewrong_files/figure-html/chd_scatter3-1.png" width=".95\linewidth" height=".65\linewidth" style="display: block; margin: auto;" /></p>
<p>With respect to the x-axis (age), we have values ranging from small x’s to
large x’s. In contrast, the response is a binary response, so there are only 0’s
or 1’s. As you can tell, the distribution of points does not seem uniform
along the x-axis. Moreover, for a given response value, say <span class="math inline">\(y_i = 0\)</span> there are
more small values <span class="math inline">\(x\)</span> than large ones. And viceversa, for <span class="math inline">\(y_i = 1\)</span>, there are
more large values <span class="math inline">\(x\)</span> than small ones.</p>
<p>In other words, we can see that (in general) younger people are less likely to
have <em>chd</em> than older people. Hence, there seems to be some information about
<em>chd</em> in <em>age</em>. The goal is to fit a model that predicts <em>chd</em> from <em>age</em>:</p>
<p><span class="math display" id="eq:702-01">\[
\textsf{chd} = f(\textsf{age}) + \varepsilon
\tag{21.1}
\]</span></p>
<div id="first-approach-fitting-a-line" class="section level3">
<h3><span class="header-section-number">21.1.1</span> First Approach: Fitting a Line</h3>
<p>As a naive first approach, we could try to fit a linear model:</p>
<p><span class="math display" id="eq:702-02">\[
\mathbf{\hat{y}} = b_0 + b_1 \mathbf{x} = \mathbf{Xb}
\tag{21.2}
\]</span></p>
<p>where:</p>
<p><span class="math display">\[
\mathbf{X} = 
\begin{pmatrix} 
1 &amp; x_1 \\ 
1 &amp; x_2 \\ 
\vdots &amp; \vdots \\ 
1 &amp; x_n \\ 
\end{pmatrix}; \hspace{5mm} 
\mathbf{b} = 
\begin{pmatrix} 
b_0 \\ b_1 \\ 
\end{pmatrix}
\]</span></p>
<p>So let’s see what happens if we use least squares to fit a line for this model.</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="logistic.html#cb32-1"></a><span class="co"># fit line via OLS</span></span>
<span id="cb32-2"><a href="logistic.html#cb32-2"></a>reg =<span class="st"> </span><span class="kw">lm</span>(chd <span class="op">~</span><span class="st"> </span>age, <span class="dt">data =</span> dat)</span>
<span id="cb32-3"><a href="logistic.html#cb32-3"></a><span class="kw">summary</span>(reg)</span>
<span id="cb32-4"><a href="logistic.html#cb32-4"></a><span class="co">#&gt; </span></span>
<span id="cb32-5"><a href="logistic.html#cb32-5"></a><span class="co">#&gt; Call:</span></span>
<span id="cb32-6"><a href="logistic.html#cb32-6"></a><span class="co">#&gt; lm(formula = chd ~ age, data = dat)</span></span>
<span id="cb32-7"><a href="logistic.html#cb32-7"></a><span class="co">#&gt; </span></span>
<span id="cb32-8"><a href="logistic.html#cb32-8"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb32-9"><a href="logistic.html#cb32-9"></a><span class="co">#&gt;      Min       1Q   Median       3Q      Max </span></span>
<span id="cb32-10"><a href="logistic.html#cb32-10"></a><span class="co">#&gt; -0.85793 -0.33992 -0.07274  0.31656  0.99269 </span></span>
<span id="cb32-11"><a href="logistic.html#cb32-11"></a><span class="co">#&gt; </span></span>
<span id="cb32-12"><a href="logistic.html#cb32-12"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb32-13"><a href="logistic.html#cb32-13"></a><span class="co">#&gt;              Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb32-14"><a href="logistic.html#cb32-14"></a><span class="co">#&gt; (Intercept) -0.537960   0.168809  -3.187  0.00193 ** </span></span>
<span id="cb32-15"><a href="logistic.html#cb32-15"></a><span class="co">#&gt; age          0.021811   0.003679   5.929 4.57e-08 ***</span></span>
<span id="cb32-16"><a href="logistic.html#cb32-16"></a><span class="co">#&gt; ---</span></span>
<span id="cb32-17"><a href="logistic.html#cb32-17"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb32-18"><a href="logistic.html#cb32-18"></a><span class="co">#&gt; </span></span>
<span id="cb32-19"><a href="logistic.html#cb32-19"></a><span class="co">#&gt; Residual standard error: 0.429 on 98 degrees of freedom</span></span>
<span id="cb32-20"><a href="logistic.html#cb32-20"></a><span class="co">#&gt; Multiple R-squared:  0.264,  Adjusted R-squared:  0.2565 </span></span>
<span id="cb32-21"><a href="logistic.html#cb32-21"></a><span class="co">#&gt; F-statistic: 35.15 on 1 and 98 DF,  p-value: 4.575e-08</span></span></code></pre></div>
<p>Looking at the estimated coefficients, in particular the coefficient of <em>age</em>,
there seems to be some sort of positive relation between <em>age</em> and <em>chd</em>.
Which, in this example, it does make sense and matches what we previously
stated about younger patients are less likely to have <em>chd</em> (<span class="math inline">\(y = 0\)</span>),
whereas older patients are more likely to have <em>chd</em> (<span class="math inline">\(y = 1\)</span>).
To better see this, let’s take a look at the scatterplot, and see how the
fitted line looks like:</p>
<p><img src="allmodelsarewrong_files/figure-html/chd_reg_line-1.png" width=".95\linewidth" height=".65\linewidth" style="display: block; margin: auto;" /></p>
<p>This model yields a very awkward fit, with a couple of issues going on.
For one thing, the regression line, and consequently the predicted values
<span class="math inline">\(\hat{y}_i\)</span>, extend beyond the range <span class="math inline">\([0,1]\)</span>.
Think about it: we could obtain fitted values
<span class="math inline">\(\hat{y}_i\)</span> taking any number between 0 and 1 (which, in this context, makes no
sense). We could also get negative predicted values, or even predicted values
greater than 1! (which also does not make sense). On the other hand, if we
examine the residuals, then things don’t look great for the linear regression.</p>
<p><img src="allmodelsarewrong_files/figure-html/unnamed-chunk-180-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>So, we need a way to fix these problems.</p>
</div>
<div id="second-approach-harsh-thresholding" class="section level3">
<h3><span class="header-section-number">21.1.2</span> Second Approach: Harsh Thresholding</h3>
<p>One idea to try to solve the issues from the regression line would be to set
some threshold <span class="math inline">\(c\)</span> (for example, <span class="math inline">\(c = 0.5\)</span>) and look at how the predict
<span class="math inline">\(\hat{y}_i\)</span> compares to it:</p>
<p><span class="math display">\[
\hat{y}_i = b_0 + b_1 x_i \quad -vs- \quad c
\]</span></p>
<p>If we think of the linear expression <span class="math inline">\(b_0 + b_1 x_i\)</span> as the <strong>signal</strong>,
we are basically comparing such signal against a specific threshold <span class="math inline">\(c\)</span>.
Which in turn will allow us to create a decision rule. That is, for a given
<em>age</em> value <span class="math inline">\(x_0\)</span>, compare its predicted value to the threshold.
If <span class="math inline">\(\hat{y}_0 \geq 0.5\)</span>, classify it as “1,” otherwise classify it as “0.”</p>
<p><span class="math display" id="eq:702-03">\[
\hat{y}_i =
\begin{cases}
1 \quad \text{if} &amp; b_0 + b_1 x_i \geq c \\
&amp; \\
0 \quad \text{if} &amp; b_0 + b_1 x_i &lt; c
\end{cases}
\tag{21.3}
\]</span></p>
<p>We can further arrange the terms above to get the following expression:</p>
<p><span class="math display" id="eq:702-04">\[
\hat{f}(x_i) = b_0 + b_1 x_i - c = \hat{y}_i
\tag{21.4}
\]</span></p>
<p>and then merge the two constant terms <span class="math inline">\(b_0\)</span> and <span class="math inline">\(c\)</span> into a single constant
denoted as <span class="math inline">\(b_{0}^{&#39;}\)</span>:</p>
<p><span class="math display" id="eq:702-05">\[\begin{align*}
\hat{f}(x_i) &amp;= b_0 + b_1 x_i - c \\
&amp;= (b_0 - c) +b_1 x_i \\ 
&amp;= b_{0}^{&#39;} + b_1 x_i
\tag{21.5}
\end{align*}\]</span></p>
<p>In vector notation, the above equation—the so-called <em>signal</em>—becomes:</p>
<p><span class="math display" id="eq:702-06">\[
\hat{y} = \mathbf{b^\mathsf{T}x}
\tag{21.6}
\]</span></p>
<p>By paying attention to the sign of the signal, we can transform our fitted
model into:</p>
<p><span class="math display" id="eq:702-07">\[
\hat{y} = \text{sign}(\mathbf{b^\mathsf{T}x})
\tag{21.7}
\]</span></p>
<p>that is:</p>
<p><span class="math display" id="eq:702-08">\[
\hat{y}_i =
\begin{cases}
1 \quad \text{if} &amp; \text{sign}(b_{0}^{&#39;} + b_1 x_i) \geq 0 \\
&amp; \\
0 \quad \text{if} &amp; \text{sign}(b_{0}^{&#39;} + b_1 x_i) &lt; 0
\end{cases}
\tag{21.8}
\]</span></p>
<p>This transformation imposes a harsh threshold on the signal. Notice that the
signal is still linear but we apply a non-linear transformation to it:</p>
<p><span class="math display" id="eq:702-09">\[
\phi(x) = \text{sign}(x)
\tag{21.9}
\]</span></p>
<p><img src="allmodelsarewrong_files/figure-html/chd_sign_line-1.png" width=".95\linewidth" height=".65\linewidth" style="display: block; margin: auto;" /></p>
<p>Think of this transformation as a quick fix. It’s definitely not something to be proud of, but we could use it to get the job done—although in a quick-dirty fashion.</p>
</div>
<div id="third-approach-conditional-means" class="section level3">
<h3><span class="header-section-number">21.1.3</span> Third Approach: Conditional Means</h3>
<p>Using a sign-transformation allows us to overcome some of the limitations of the
linear regression model, but it’s far from ideal.</p>
<p>An alternative approach involves calculating conditional means. How does that work?
The idea is very simple and clever. Say you are looking at patients <span class="math inline">\(x = 24\)</span>
years old, and you count the relative frequency of <em>chd</em> cases. In other words,
you count the proportion of <em>chd</em> cases among individuals 24 years old. This is
nothing else than computing the conditional mean:</p>
<p><span class="math display" id="eq:702-10">\[
avg(y_i | x_i = 24)
\tag{21.10}
\]</span></p>
<p>Following this idea, we could compute all conditional means for all age values:</p>
<p><span class="math display">\[
(\bar{y}|x_i = 25), \quad (\bar{y}|x_i = 26), \quad \dots, \quad (\bar{y}|x_i = 69)
\]</span></p>
<p>Sometimes, however, we may not have data points for a specific <span class="math inline">\(x\)</span>-value. So
instead we can use groups of ages. For example, say we define a first group of
ages to be 20 - 29 years. And then calculate the proportion of <em>chd</em> cases in
this group. The corresponding average will be:</p>
<p><span class="math display" id="eq:702-11">\[
avg(y_i | x_i \in \{ 20 - 29 \text{ years}\})
\tag{21.11}
\]</span></p>
<p>In general, for a given group of age, we calculate the proportion of <em>chd</em>
cases as:</p>
<p><span class="math display" id="eq:702-12">\[
avg(y_i | x_i = \text{age group})
\tag{21.12}
\]</span></p>
<p>Following our example, we can specify the following age groups:</p>
<pre><code>#&gt; age_group
#&gt; 20-29 30-34 35-39 40-44 45-49 50-54 55-59 60-69 
#&gt;    10    15    12    15    13     8    17    10</code></pre>
<p>Now that we have age by groups, we can get the proportion of coronary heart
disease cases in each age group. The results are displayed in the table below.</p>
<pre><code>#&gt; # A tibble: 8 x 3
#&gt;   age_group count_chd prop_chd
#&gt;   &lt;fct&gt;         &lt;int&gt;    &lt;dbl&gt;
#&gt; 1 20-29            10    0.1  
#&gt; 2 30-34            15    0.133
#&gt; 3 35-39            12    0.25 
#&gt; 4 40-44            15    0.333
#&gt; 5 45-49            13    0.462
#&gt; 6 50-54             8    0.625
#&gt; 7 55-59            17    0.765
#&gt; 8 60-69            10    0.8</code></pre>
<p>Likewise, we can graph these averages on a scatterplot:</p>
<p><img src="allmodelsarewrong_files/figure-html/unnamed-chunk-183-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Theoretically, we are modeling the conditional expectations: <span class="math inline">\(\mathbb{E}(y|x)\)</span>.
Which is exactly the <strong>regression function</strong>.</p>
<p>By connecting the averages, we get an interesting sigmoid pattern</p>
<p><img src="allmodelsarewrong_files/figure-html/unnamed-chunk-184-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>This pattern can be approximated by some mathematical functions, the most popular being the so-called <strong>logistic</strong> function:</p>
<p><span class="math display" id="eq:702-13">\[
\text{logistic function:} \qquad f(s) = \frac{e^{s}}{1 + e^{s}}
\tag{21.13}
\]</span></p>
<p><img src="allmodelsarewrong_files/figure-html/unnamed-chunk-185-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Sometimes you may also find the logistic equation in an alternative form:</p>
<p><span class="math display">\[
f(s) = \frac{e^{s}}{1 + e^{s}} \quad \longleftrightarrow \quad f(s) = \frac{1}{1 + e^{-s}}
\]</span></p>
<p>Replacing the the signal <span class="math inline">\(s\)</span> by a linear model <span class="math inline">\(\beta_0 + \beta_1 x\)</span>, we have:</p>
<p><span class="math display" id="eq:702-14">\[\begin{align*}
f(x) &amp;= \frac{e^{\beta_0 + \beta_1 x}}{1 + e^{\beta_0 + \beta_1 x}} \\
&amp;= \frac{1}{\frac{1 + e^{\beta_0 + \beta_1 x}}{e^{\beta_0 + \beta_1 x}}} \\
&amp;= \frac{1}{\frac{1}{e^{\beta_0 + \beta_1 x}} + 1} \\
&amp;= \frac{1}{1 + e^{-(\beta_0 + \beta_1 x})}
\tag{21.14}
\end{align*}\]</span></p>
<p>The following figure shows different logistic functions for different <span class="math inline">\(\beta\)</span>
values:</p>
<p><img src="allmodelsarewrong_files/figure-html/logistic-curves-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Since probability values range inside <span class="math inline">\([0,1]\)</span>, instead of using a line to try
to approximate these values, we should use a more adequate curve. This is the
reason why sigmoid-like curves, such as the logistic function, are preferred
for this purpose.</p>
</div>
</div>
<div id="logistic-regression-model" class="section level2">
<h2><span class="header-section-number">21.2</span> Logistic Regression Model</h2>
<p>We consider the following model:</p>
<p><span class="math display" id="eq:702-15">\[
Prob(y \mid \mathbf{x}; \mathbf{b}) = f(\mathbf{x})
\tag{21.15}
\]</span></p>
<p>We don’t get to observe the true probability; rather, we observe the noisy
target <span class="math inline">\(y_i\)</span>, that is generated (or affected) by the probability <span class="math inline">\(f(x)\)</span>.
How will we model the probability? We would ideally like to use a mathematical
function that looks like the sigmoid shape from the toy example above; that is,
use a sigmoid function. The most famous function—and the function we will
use—is the <strong>logistic function</strong>:</p>
<p><span class="math display" id="eq:702-16">\[
\text{logistic function:} \qquad f(s) = \frac{e^{s}}{1 + e^{s}}
\tag{21.16}
\]</span></p>
<p>In other words, we have the following model:</p>
<p><span class="math display" id="eq:702-17">\[
Prob(y _i \mid \mathbf{x} ; \mathbf{b} ) = \frac{e^{\mathbf{b^\mathsf{T} x_i}}}{1 + e^{\mathbf{b^\mathsf{T} x_i}}}
\tag{21.17}
\]</span></p>
<p>where here <span class="math inline">\(\mathbf{x_i}\)</span> represents the vector of features for individual <span class="math inline">\(i\)</span>.</p>
<p>Using the same kind of diagrams depicted for various regression models, we
can put logistic regression in this visual format as follows:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-186"></span>
<img src="images/logistic/logistic-pathdiag1.svg" alt="Regression Model Diagram" width="80%" />
<p class="caption">
Figure 21.1: Regression Model Diagram
</p>
</div>
<p>Starting with the input features, we form a linear combination, the so-called
linear signal <span class="math inline">\(\mathbf{s} = \mathbf{Xb}\)</span>, that gets passed to a
logistic function <span class="math inline">\(\phi()\)</span>. This is a nonlinear transformation of the linear
signal, which allows us to interpret the response as a true probability value.</p>
<p>Hence we have that the probabilities are given by:</p>
<p><span class="math display" id="eq:702-18">\[
Prob(y_i \mid \mathbf{x_i}, \mathbf{b}) = 
\begin{cases} 
h(\mathbf{x_i})&amp;  \textsf{for } y_i = 1 \\ 
&amp; \\
1 - h(\mathbf{x_i}) &amp; \textsf{for } y_i = 0 \\ \end{cases}
\tag{21.18}
\]</span></p>
<p>where <span class="math inline">\(h()\)</span> denotes the logistic function <span class="math inline">\(\phi()\)</span>:</p>
<p><span class="math display">\[
h(\mathbf{x}) = \phi(\mathbf{b^\mathsf{T} x})
\]</span></p>
<div id="the-criterion-being-optimized" class="section level3">
<h3><span class="header-section-number">21.2.1</span> The Criterion Being Optimized</h3>
<p>We will not be using the MSE as our error measure (since doing so would make no
sense in this particular context). Instead, we will use an “error” based on
Maximum Likelihood Estimation. In order to do so, we must assume that our model
is true; that is:</p>
<p><span class="math display">\[
\textsf{assuming this is true} \quad h(x) = f(x)
\]</span></p>
<p>we ask “how likely is it that we observe the data we already observed (i.e. <span class="math inline">\(y_i\)</span>)?”</p>
<p>We start with the likelihood function <span class="math inline">\(L(\mathbf{b})\)</span>. Note that <span class="math inline">\(L\)</span> <em>implicitly</em>
also depends on all <span class="math inline">\(X\)</span>-inputs, and also compute the log-likelihood
<span class="math inline">\(\ell(\mathbf{b})\)</span>:</p>
<p><span class="math display">\[\begin{align*}
Prob(\mathbf{y} \mid x_1, x_2, \dots, x_p; \mathbf{b}) &amp; = \prod_{i=1}^{n} P(y_i \mid \mathbf{b^\mathsf{T} x_i} ) \\
&amp; = \prod_{i=1}^{n} h(\mathbf{b^\mathsf{T} x_i})^{y_i} \left[ 1 - h(\mathbf{b^\mathsf{T} x_i} ) \right]^{1 - y_i}
\end{align*}\]</span></p>
<p>Taking the logarithm (log-likelihood):</p>
<p><span class="math display" id="eq:702-19">\[\begin{align*}
\ell(\mathbf{b}) &amp;= \ln[L(\mathbf{b}) ] \\
&amp;= \sum_{i=1}^{n} \ln\left[ P(y_i \mid \mathbf{b^\mathsf{T} x_i} ) \right] \\
&amp; = \sum_{i=1}^{n} \left\{ y_i \ln[h(\mathbf{b^\mathsf{T} x_i})] + (1 - y_i) \ln[ 1 - h(\mathbf{b^\mathsf{T} x_i} ) ]  \right\} \\
&amp; = \sum_{i=1}^{n} \left[ y_i \ln\left( \frac{e^{\mathbf{b^\mathsf{T} x_i}} }{1 + e^{\mathbf{b^\mathsf{T} x_i} } } \right) + (1 - y_i) \ln \left( 1 -  \frac{e^{\mathbf{b^\mathsf{T} x_i}} }{1 + e^{\mathbf{b^\mathsf{T} x_i} } }  \right) \right] \\
&amp; = \sum_{i=1}^{n} \left[ y_i \mathbf{b^\mathsf{T} x_i} - \ln \left( 1 + e ^{\mathbf{b^\mathsf{T} x_i} } \right) \right]
\tag{21.19}
\end{align*}\]</span></p>
<p>Now, here is the not so good news: differentiating and setting equal to 0 yields
and equation for which no closed-form solution exists. To find the maximum
likelihood estimate of <span class="math inline">\(\mathbf{b}\)</span>, we require an iterative method:
for example, the <strong>Newton-Raphson method</strong>, or <strong>gradient ascent</strong>.</p>
<p><span class="math display" id="eq:702-20">\[\begin{align*}
\nabla \ell(\mathbf{b}) &amp; = \sum_{i=1}^{n} \left[ y_i \mathbf{x_i} - \left( \frac{e^{\mathbf{b^\mathsf{T} x_i}} }{1 + e^{\mathbf{b^\mathsf{T} x_i} } } \right) \mathbf{x_i} \right] \\
&amp; = \sum_{i=1}^{n} \left[ y_i \mathbf{x_i} - \phi(\mathbf{b^\mathsf{T} x_i} ) \mathbf{x_i}  \right]  \\
&amp; = \sum_{i=1}^{n} \left[ y_i - \phi(\mathbf{b^\mathsf{T} x_i} )  \right] \mathbf{x_i} 
\tag{21.20}
\end{align*}\]</span></p>
<p>Hence, in gradient ascent, we would use:</p>
<p><span class="math display" id="eq:702-21">\[
\mathbf{b}^{(s + 1)} = \mathbf{b}^{(s)} + \alpha \nabla \ell(\mathbf{b}^{(s)})
\tag{21.21}
\]</span></p>
</div>
<div id="another-way-to-solve-logistic-regression" class="section level3">
<h3><span class="header-section-number">21.2.2</span> Another Way to Solve Logistic Regression</h3>
<p>The overall picture stills stands:</p>
<p><img src="images/logistic/logistic-pathdiag1.svg" width="80%" style="display: block; margin: auto;" /></p>
<p>Now, however, we will change our probability expression a bit. How? By recoding
the response variable <span class="math inline">\(y_i\)</span>. Specifically, what is used to be <span class="math inline">\(y_i = 0\)</span>
let’s now encode it as <span class="math inline">\(y_i = -1\)</span>.</p>
<p><span class="math display" id="eq:702-22">\[
Prob(y_i \mid \mathbf{x_i}, \mathbf{b}) = 
\begin{cases} 
h(\mathbf{x_i}) &amp;  \textsf{for } y_i = 1 \\ 
&amp; \\
1 - h(\mathbf{x_i}) &amp; \textsf{for } y_i = -1 \\ 
\end{cases}
\tag{21.22}
\]</span></p>
<p>Recall that <span class="math inline">\(h()\)</span> is the logistic function <span class="math inline">\(\phi()\)</span>:</p>
<p><span class="math display">\[
h(\mathbf{x}) = \phi(\mathbf{b^\mathsf{T} x})
\]</span></p>
<p>They key observation is to note the following property of the logistic function:</p>
<p><span class="math display" id="eq:702-23">\[
\phi(-s) = 1 - \phi(s)
\tag{21.23}
\]</span></p>
<p>Taking into account this property, we can update the expression of the
conditional probability:</p>
<p><span class="math display" id="eq:702-24">\[
Prob(y_i \mid \mathbf{x_i}, \mathbf{b}) = 
\begin{cases} 
h(\mathbf{x_i}) = \phi(y_i \mathbf{b^\mathsf{T} x}) &amp;  \textsf{for } y_i = 1 \\ 
&amp; \\
1 - h(\mathbf{x_i}) = \phi(y_i \mathbf{b^\mathsf{T} x}) &amp; \textsf{for } y_i = -1 \\ 
\end{cases}
\tag{21.24}
\]</span></p>
<p>Notice that this allows us to simplify things in a very convenient way. This
implies that we only need one case regardless of the value of <span class="math inline">\(y_i\)</span>:</p>
<p><span class="math display" id="eq:702-25">\[
Prob(y_i \mid \mathbf{x_i}, \mathbf{b}) = \phi(y_i \mathbf{b^\mathsf{T} x})
\tag{21.25}
\]</span></p>
<p>With this, we begin computing the likelihood as we previously did:</p>
<p><span class="math display" id="eq:702-26">\[\begin{align*}
Prob(\mathbf{y} \mid x_1, x_2, \dots, x_p; \mathbf{b}) &amp; = \prod_{i=1}^{n} P(y_i \mid \mathbf{b^\mathsf{T} x_i} ) \\
&amp; = \prod_{i=1}^{n} \phi\left( y_i \mathbf{b^\mathsf{T} x_i} \right) \\
\tag{21.26}
\end{align*}\]</span></p>
<p>and then the log-likelihood:</p>
<p><span class="math display" id="eq:702-27">\[\begin{align*}
\ell(\mathbf{b}) : = \ln[L(\mathbf{\beta})] &amp; = \sum_{i=1}^{n} \ln\left[ \phi\left( y_i \mathbf{b^\mathsf{T} x_i} \right)  \right] \\
&amp; \Rightarrow \frac{1}{n} \sum_{i=1}^{n} \underbrace{ \ln\left[ \phi\left( y_i \mathbf{b^\mathsf{T} x_i} \right)  \right] }_{\text{logit}}
\tag{21.27}
\end{align*}\]</span></p>
<p>Here comes an important trick. Instead of maximizing the log-likelihood,
we can minimize the negative of the loglikelihood.
In other words, we can perform the following <strong>minimization</strong> problem:</p>
<p><span class="math display" id="eq:702-28">\[
\min_{\mathbf{b}} \left\{ - \frac{1}{n} \sum_{i=1}^{n}  \ln\left[ \phi\left( y_i \mathbf{b^\mathsf{T} x_i} \right)  \right]   \right\}
\tag{21.28}
\]</span></p>
<p>Doing some algebra, it can be proved that the above criterion is equivalent
to the right hand side of the expression below:</p>
<p><span class="math display">\[\begin{align*}
&amp; \min_{\mathbf{b}} \left\{ - \frac{1}{n} \sum_{i=1}^{n}  \ln\left[ \phi\left( y_i \mathbf{b^\mathsf{T} x_i} \right)  \right]   \right\} \  \Leftrightarrow  \  \min_{\mathbf{b}} \left\{ \underbrace{ \frac{1}{n} \sum_{i=1}^{n} \underbrace{ \ln\left( 1 + e^{-y_i \mathbf{b^\mathsf{T} x_i} } \right) }_{\text{pointwise error}} }_{E_{in}(\mathbf{b})} \right\}
\end{align*}\]</span></p>
<p>Focus on the product term between the response and the linear signal:</p>
<p><span class="math display" id="eq:702-29">\[
y_i \mathbf{b^\mathsf{T} x_i} =: y_i \mathbf{s}
\tag{21.29}
\]</span></p>
<p>where <span class="math inline">\(\mathbf{s}\)</span> represents “signal”. This term can be understood using the
following table:</p>
<p><img src="images/logistic/logistic-signal-table.svg" width="65%" style="display: block; margin: auto;" /></p>
<p>We can think of the signal term <span class="math inline">\(\mathbf{b^\mathsf{T} x_i}\)</span> as a numeric value
or “score” ranging from “very small” to “very large”. A small signal means that
the probability <span class="math inline">\(\phi(\mathbf{b^\mathsf{T} x_i})\)</span> will be small. Conversely, a
large signal means that <span class="math inline">\(\phi(\mathbf{b^\mathsf{T} x_i})\)</span> will be large.
In turn, the term <span class="math inline">\(y_i\)</span> can be either <span class="math inline">\(-1\)</span> or <span class="math inline">\(+1\)</span>.</p>
<p>If we have a correct prediction, we would expect a small probability
<span class="math inline">\(\phi(\mathbf{b^\mathsf{T} x_i})\)</span> to agree with <span class="math inline">\(y_i = -1\)</span>. Likewise, we would
also expect a large probability <span class="math inline">\(\phi(\mathbf{b^\mathsf{T} x_i})\)</span> to agree with
<span class="math inline">\(y_i = +1\)</span>.</p>
<p>In turn, if we have an incorrect prediction, we would expect a small probability
<span class="math inline">\(\phi(\mathbf{b^\mathsf{T} x_i})\)</span> to mismatch <span class="math inline">\(y_i = +1\)</span>. Likewise, we would
also expect a large probability <span class="math inline">\(\phi(\mathbf{b^\mathsf{T} x_i})\)</span> to disagree
with an observed <span class="math inline">\(y_i = -1\)</span>.</p>
<ul>
<li><p>With agreements, <span class="math inline">\(e^{-y_i \mathbf{b^\mathsf{T} x_i} }\)</span> will be small, and
will consequently give you a small error.</p></li>
<li><p>With disagreements, <span class="math inline">\(e^{-y_i \mathbf{b^\mathsf{T} x_i} }\)</span> will be large, and
will consequently give you a large error.</p></li>
</ul>
<p>We will also refer to our <span class="math inline">\(E_{in}(\mathbf{b})\)</span> quantity as the
<strong>mean cross-entropy error</strong>. Technically it isn’t a “cross-entropy” measure in
the classification sense, however we will ignore that technicality for now.</p>
<p>How do we find <span class="math inline">\(\mathbf{b}\)</span> that minimizes the criterion:</p>
<p><span class="math display" id="eq:702-30">\[
\min_{\mathbf{b}} \left\{ \underbrace{ \frac{1}{n} \sum_{i=1}^{n} \underbrace{ \ln\left( 1 + e^{-y_i \mathbf{b^\mathsf{T} x_i} } \right) }_{\text{pointwise error}} }_{E_{in}(\mathbf{b})} \right\}
\tag{21.30}
\]</span></p>
<p>This expression does not have a closed analytical solution. So in order to
find the minimum, we need to apply iterative optimization methods. For example,
gradient descent. Consequently, calculating the gradient of the above
expression, we find:</p>
<p><span class="math display" id="eq:702-31">\[
\nabla E_{in}(\mathbf{b}) = - \frac{1}{n} \sum_{i=1}^{n} \left( \frac{1}{1 + e^{-y_i \mathbf{b^\mathsf{T} x_i} } }  \right) y_i \mathbf{x_i}
\tag{21.31}
\]</span></p>
<p>and we would use <a href="gradient.html#gradient">gradient descent</a> to compute the minimum.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="classif.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="discrim.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["allmodelsarewrong.pdf", "allmodelsarewrong.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
