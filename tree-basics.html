<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>34 Building Binary Trees | All Models Are Wrong: Concepts of Statistical Learning</title>
  <meta name="description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  <meta name="generator" content="bookdown 0.16 and GitBook 2.6.7" />

  <meta property="og:title" content="34 Building Binary Trees | All Models Are Wrong: Concepts of Statistical Learning" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://allmodelsarewrong.github.io" />
  
  <meta property="og:description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  <meta name="github-repo" content="allmodelsarewrong/allmodelsarewrong.github.io" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="34 Building Binary Trees | All Models Are Wrong: Concepts of Statistical Learning" />
  
  <meta name="twitter:description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  

<meta name="author" content="Gaston Sanchez Ethan Marzban" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="tree-splits.html"/>
<link rel="next" href="bagging.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><b>All Models Are Wrong (CSL)</b><br><small>G. Sanchez & E. Marzban</small></a></li>

<li class="divider"></li>
<li class="part"><span><b>I Welcome</b></span></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="about.html"><a href="about.html"><i class="fa fa-check"></i><b>1</b> About this book</a><ul>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#prerequisites"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="part"><span><b>II Intro</b></span></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#basic-notation"><i class="fa fa-check"></i><b>2.1</b> Basic Notation</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="duality.html"><a href="duality.html"><i class="fa fa-check"></i><b>3</b> Geometric Duality</a><ul>
<li class="chapter" data-level="3.1" data-path="duality.html"><a href="duality.html#rows-space"><i class="fa fa-check"></i><b>3.1</b> Rows Space</a></li>
<li class="chapter" data-level="3.2" data-path="duality.html"><a href="duality.html#columns-space"><i class="fa fa-check"></i><b>3.2</b> Columns Space</a></li>
<li class="chapter" data-level="3.3" data-path="duality.html"><a href="duality.html#cloud-of-individuals"><i class="fa fa-check"></i><b>3.3</b> Cloud of Individuals</a><ul>
<li class="chapter" data-level="3.3.1" data-path="duality.html"><a href="duality.html#average-individual"><i class="fa fa-check"></i><b>3.3.1</b> Average Individual</a></li>
<li class="chapter" data-level="3.3.2" data-path="duality.html"><a href="duality.html#centered-data"><i class="fa fa-check"></i><b>3.3.2</b> Centered Data</a></li>
<li class="chapter" data-level="3.3.3" data-path="duality.html"><a href="duality.html#distance-between-individuals"><i class="fa fa-check"></i><b>3.3.3</b> Distance between individuals</a></li>
<li class="chapter" data-level="3.3.4" data-path="duality.html"><a href="duality.html#distance-to-the-centroid"><i class="fa fa-check"></i><b>3.3.4</b> Distance to the centroid</a></li>
<li class="chapter" data-level="3.3.5" data-path="duality.html"><a href="duality.html#measures-of-dispersion"><i class="fa fa-check"></i><b>3.3.5</b> Measures of Dispersion</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="duality.html"><a href="duality.html#cloud-of-variables"><i class="fa fa-check"></i><b>3.4</b> Cloud of Variables</a><ul>
<li class="chapter" data-level="3.4.1" data-path="duality.html"><a href="duality.html#mean-of-a-variable"><i class="fa fa-check"></i><b>3.4.1</b> Mean of a Variable</a></li>
<li class="chapter" data-level="3.4.2" data-path="duality.html"><a href="duality.html#variance-of-a-variable"><i class="fa fa-check"></i><b>3.4.2</b> Variance of a Variable</a></li>
<li class="chapter" data-level="3.4.3" data-path="duality.html"><a href="duality.html#variance-with-vector-notation"><i class="fa fa-check"></i><b>3.4.3</b> Variance with Vector Notation</a></li>
<li class="chapter" data-level="3.4.4" data-path="duality.html"><a href="duality.html#standard-deviation-as-a-norm"><i class="fa fa-check"></i><b>3.4.4</b> Standard Deviation as a Norm</a></li>
<li class="chapter" data-level="3.4.5" data-path="duality.html"><a href="duality.html#covariance"><i class="fa fa-check"></i><b>3.4.5</b> Covariance</a></li>
<li class="chapter" data-level="3.4.6" data-path="duality.html"><a href="duality.html#correlation"><i class="fa fa-check"></i><b>3.4.6</b> Correlation</a></li>
<li class="chapter" data-level="3.4.7" data-path="duality.html"><a href="duality.html#geometry-of-correlation"><i class="fa fa-check"></i><b>3.4.7</b> Geometry of Correlation</a></li>
<li class="chapter" data-level="3.4.8" data-path="duality.html"><a href="duality.html#orthogonal-projections"><i class="fa fa-check"></i><b>3.4.8</b> Orthogonal Projections</a></li>
<li class="chapter" data-level="3.4.9" data-path="duality.html"><a href="duality.html#the-mean-as-an-orthogonal-projection"><i class="fa fa-check"></i><b>3.4.9</b> The mean as an orthogonal projection</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Unsupervised I: PCA</b></span></li>
<li class="chapter" data-level="4" data-path="pca.html"><a href="pca.html"><i class="fa fa-check"></i><b>4</b> Principal Components Analysis</a><ul>
<li class="chapter" data-level="4.1" data-path="pca.html"><a href="pca.html#low-dimensional-representations"><i class="fa fa-check"></i><b>4.1</b> Low-dimensional Representations</a></li>
<li class="chapter" data-level="4.2" data-path="pca.html"><a href="pca.html#projections"><i class="fa fa-check"></i><b>4.2</b> Projections</a><ul>
<li class="chapter" data-level="4.2.1" data-path="pca.html"><a href="pca.html#vector-and-scalar-projections"><i class="fa fa-check"></i><b>4.2.1</b> Vector and Scalar Projections</a></li>
<li class="chapter" data-level="4.2.2" data-path="pca.html"><a href="pca.html#projected-inertia"><i class="fa fa-check"></i><b>4.2.2</b> Projected Inertia</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="pca.html"><a href="pca.html#maximization-problem"><i class="fa fa-check"></i><b>4.3</b> Maximization Problem</a><ul>
<li class="chapter" data-level="4.3.1" data-path="pca.html"><a href="pca.html#eigenvectors-of-mathbfs"><i class="fa fa-check"></i><b>4.3.1</b> Eigenvectors of <span class="math inline">\(\mathbf{S}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="pca.html"><a href="pca.html#another-perspective-of-pca"><i class="fa fa-check"></i><b>4.4</b> Another Perspective of PCA</a><ul>
<li class="chapter" data-level="4.4.1" data-path="pca.html"><a href="pca.html#finding-principal-components"><i class="fa fa-check"></i><b>4.4.1</b> Finding Principal Components</a></li>
<li class="chapter" data-level="4.4.2" data-path="pca.html"><a href="pca.html#finding-the-first-pc"><i class="fa fa-check"></i><b>4.4.2</b> Finding the first PC</a></li>
<li class="chapter" data-level="4.4.3" data-path="pca.html"><a href="pca.html#finding-the-second-pc"><i class="fa fa-check"></i><b>4.4.3</b> Finding the second PC</a></li>
<li class="chapter" data-level="4.4.4" data-path="pca.html"><a href="pca.html#finding-all-pcs"><i class="fa fa-check"></i><b>4.4.4</b> Finding all PCs</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="pca.html"><a href="pca.html#data-decomposition-model"><i class="fa fa-check"></i><b>4.5</b> Data Decomposition Model</a><ul>
<li class="chapter" data-level="4.5.1" data-path="pca.html"><a href="pca.html#alternative-approaches"><i class="fa fa-check"></i><b>4.5.1</b> Alternative Approaches</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Linear Regression</b></span></li>
<li class="chapter" data-level="5" data-path="ols.html"><a href="ols.html"><i class="fa fa-check"></i><b>5</b> Linear Regression</a><ul>
<li class="chapter" data-level="5.1" data-path="ols.html"><a href="ols.html#motivation"><i class="fa fa-check"></i><b>5.1</b> Motivation</a></li>
<li class="chapter" data-level="5.2" data-path="ols.html"><a href="ols.html#the-ideaintuition-of-regression"><i class="fa fa-check"></i><b>5.2</b> The Idea/Intuition of Regression</a></li>
<li class="chapter" data-level="5.3" data-path="ols.html"><a href="ols.html#the-linear-regression-model"><i class="fa fa-check"></i><b>5.3</b> The Linear Regression Model</a></li>
<li class="chapter" data-level="5.4" data-path="ols.html"><a href="ols.html#the-error-measure"><i class="fa fa-check"></i><b>5.4</b> The Error Measure</a></li>
<li class="chapter" data-level="5.5" data-path="ols.html"><a href="ols.html#the-least-squares-algorithm"><i class="fa fa-check"></i><b>5.5</b> The Least Squares Algorithm</a></li>
<li class="chapter" data-level="5.6" data-path="ols.html"><a href="ols.html#geometries-of-ols"><i class="fa fa-check"></i><b>5.6</b> Geometries of OLS</a><ul>
<li class="chapter" data-level="5.6.1" data-path="ols.html"><a href="ols.html#rows-perspective"><i class="fa fa-check"></i><b>5.6.1</b> Rows Perspective</a></li>
<li class="chapter" data-level="5.6.2" data-path="ols.html"><a href="ols.html#columns-perspective"><i class="fa fa-check"></i><b>5.6.2</b> Columns Perspective</a></li>
<li class="chapter" data-level="5.6.3" data-path="ols.html"><a href="ols.html#parameters-perspective"><i class="fa fa-check"></i><b>5.6.3</b> Parameters Perspective</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="gradient.html"><a href="gradient.html"><i class="fa fa-check"></i><b>6</b> Gradient Descent</a><ul>
<li class="chapter" data-level="6.1" data-path="gradient.html"><a href="gradient.html#error-surface"><i class="fa fa-check"></i><b>6.1</b> Error Surface</a></li>
<li class="chapter" data-level="6.2" data-path="gradient.html"><a href="gradient.html#idea-of-gradient-descent"><i class="fa fa-check"></i><b>6.2</b> Idea of Gradient Descent</a></li>
<li class="chapter" data-level="6.3" data-path="gradient.html"><a href="gradient.html#moving-down-an-error-surface"><i class="fa fa-check"></i><b>6.3</b> Moving Down an Error Surface</a><ul>
<li class="chapter" data-level="6.3.1" data-path="gradient.html"><a href="gradient.html#the-direction-of-mathbfv"><i class="fa fa-check"></i><b>6.3.1</b> The direction of <span class="math inline">\(\mathbf{v}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="gradient.html"><a href="gradient.html#gradient-descent-and-our-model"><i class="fa fa-check"></i><b>6.4</b> Gradient Descent and our Model</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="olsml.html"><a href="olsml.html"><i class="fa fa-check"></i><b>7</b> Regression via Maximum Likelihood</a><ul>
<li class="chapter" data-level="7.1" data-path="olsml.html"><a href="olsml.html#linear-regression-reminder"><i class="fa fa-check"></i><b>7.1</b> Linear Regression Reminder</a><ul>
<li class="chapter" data-level="7.1.1" data-path="olsml.html"><a href="olsml.html#maximum-likelihood"><i class="fa fa-check"></i><b>7.1.1</b> Maximum Likelihood</a></li>
<li class="chapter" data-level="7.1.2" data-path="olsml.html"><a href="olsml.html#ml-estimator-of-sigma2"><i class="fa fa-check"></i><b>7.1.2</b> ML Estimator of <span class="math inline">\(\sigma^2\)</span></a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>V Learning Concepts</b></span></li>
<li class="chapter" data-level="8" data-path="learning.html"><a href="learning.html"><i class="fa fa-check"></i><b>8</b> Theoretical Framework</a><ul>
<li class="chapter" data-level="8.1" data-path="learning.html"><a href="learning.html#mental-map"><i class="fa fa-check"></i><b>8.1</b> Mental Map</a></li>
<li class="chapter" data-level="8.2" data-path="learning.html"><a href="learning.html#kinds-of-predictions"><i class="fa fa-check"></i><b>8.2</b> Kinds of Predictions</a><ul>
<li class="chapter" data-level="8.2.1" data-path="learning.html"><a href="learning.html#two-types-of-data"><i class="fa fa-check"></i><b>8.2.1</b> Two Types of Data</a></li>
<li class="chapter" data-level="8.2.2" data-path="learning.html"><a href="learning.html#two-types-of-predictions"><i class="fa fa-check"></i><b>8.2.2</b> Two Types of Predictions</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="learning.html"><a href="learning.html#two-types-of-errors"><i class="fa fa-check"></i><b>8.3</b> Two Types of Errors</a><ul>
<li class="chapter" data-level="8.3.1" data-path="learning.html"><a href="learning.html#individual-errors"><i class="fa fa-check"></i><b>8.3.1</b> Individual Errors</a></li>
<li class="chapter" data-level="8.3.2" data-path="learning.html"><a href="learning.html#overall-errors"><i class="fa fa-check"></i><b>8.3.2</b> Overall Errors</a></li>
<li class="chapter" data-level="8.3.3" data-path="learning.html"><a href="learning.html#auxiliary-technicality"><i class="fa fa-check"></i><b>8.3.3</b> Auxiliary Technicality</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="learning.html"><a href="learning.html#noisy-targets"><i class="fa fa-check"></i><b>8.4</b> Noisy Targets</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="mse.html"><a href="mse.html"><i class="fa fa-check"></i><b>9</b> MSE of Estimator</a><ul>
<li class="chapter" data-level="9.1" data-path="mse.html"><a href="mse.html#mse-of-an-estimator"><i class="fa fa-check"></i><b>9.1</b> MSE of an Estimator</a><ul>
<li class="chapter" data-level="9.1.1" data-path="mse.html"><a href="mse.html#prototypical-cases-of-bias-and-variance"><i class="fa fa-check"></i><b>9.1.1</b> Prototypical Cases of Bias and Variance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="biasvar.html"><a href="biasvar.html"><i class="fa fa-check"></i><b>10</b> Bias-Variance Tradeoff</a><ul>
<li class="chapter" data-level="10.1" data-path="biasvar.html"><a href="biasvar.html#introduction"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="biasvar.html"><a href="biasvar.html#motivation-example"><i class="fa fa-check"></i><b>10.2</b> Motivation Example</a><ul>
<li class="chapter" data-level="10.2.1" data-path="biasvar.html"><a href="biasvar.html#two-hypotheses"><i class="fa fa-check"></i><b>10.2.1</b> Two Hypotheses</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="biasvar.html"><a href="biasvar.html#learning-from-two-points"><i class="fa fa-check"></i><b>10.3</b> Learning from two points</a></li>
<li class="chapter" data-level="10.4" data-path="biasvar.html"><a href="biasvar.html#bias-variance-derivation"><i class="fa fa-check"></i><b>10.4</b> Bias-Variance Derivation</a><ul>
<li class="chapter" data-level="10.4.1" data-path="biasvar.html"><a href="biasvar.html#out-of-sample-predictions"><i class="fa fa-check"></i><b>10.4.1</b> Out-of-Sample Predictions</a></li>
<li class="chapter" data-level="10.4.2" data-path="biasvar.html"><a href="biasvar.html#noisy-target"><i class="fa fa-check"></i><b>10.4.2</b> Noisy Target</a></li>
<li class="chapter" data-level="10.4.3" data-path="biasvar.html"><a href="biasvar.html#types-of-theoretical-mses"><i class="fa fa-check"></i><b>10.4.3</b> Types of Theoretical MSEs</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="biasvar.html"><a href="biasvar.html#the-tradeoff"><i class="fa fa-check"></i><b>10.5</b> The Tradeoff</a><ul>
<li class="chapter" data-level="10.5.1" data-path="biasvar.html"><a href="biasvar.html#bias-variance-tradeoff-picture"><i class="fa fa-check"></i><b>10.5.1</b> Bias-Variance Tradeoff Picture</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="overfit.html"><a href="overfit.html"><i class="fa fa-check"></i><b>11</b> Overfitting</a></li>
<li class="chapter" data-level="12" data-path="phases.html"><a href="phases.html"><i class="fa fa-check"></i><b>12</b> Learning Phases</a><ul>
<li class="chapter" data-level="12.1" data-path="phases.html"><a href="phases.html#introduction-1"><i class="fa fa-check"></i><b>12.1</b> Introduction</a></li>
<li class="chapter" data-level="12.2" data-path="phases.html"><a href="phases.html#model-assessment"><i class="fa fa-check"></i><b>12.2</b> Model Assessment</a><ul>
<li class="chapter" data-level="12.2.1" data-path="phases.html"><a href="phases.html#holdout-test-set"><i class="fa fa-check"></i><b>12.2.1</b> Holdout Test Set</a></li>
<li class="chapter" data-level="12.2.2" data-path="phases.html"><a href="phases.html#why-does-a-test-set-work"><i class="fa fa-check"></i><b>12.2.2</b> Why does a test set work?</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="phases.html"><a href="phases.html#model-selection"><i class="fa fa-check"></i><b>12.3</b> Model Selection</a><ul>
<li class="chapter" data-level="12.3.1" data-path="phases.html"><a href="phases.html#three-way-holdout-method"><i class="fa fa-check"></i><b>12.3.1</b> Three-way Holdout Method</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="phases.html"><a href="phases.html#model-training"><i class="fa fa-check"></i><b>12.4</b> Model Training</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="resampling.html"><a href="resampling.html"><i class="fa fa-check"></i><b>13</b> Resampling Approaches</a><ul>
<li class="chapter" data-level="13.1" data-path="resampling.html"><a href="resampling.html#general-sampling-blueprint"><i class="fa fa-check"></i><b>13.1</b> General Sampling Blueprint</a></li>
<li class="chapter" data-level="13.2" data-path="resampling.html"><a href="resampling.html#monte-carlo-cross-validation"><i class="fa fa-check"></i><b>13.2</b> Monte Carlo Cross Validation</a></li>
<li class="chapter" data-level="13.3" data-path="resampling.html"><a href="resampling.html#bootstrap-method"><i class="fa fa-check"></i><b>13.3</b> Bootstrap Method</a></li>
<li class="chapter" data-level="13.4" data-path="resampling.html"><a href="resampling.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>13.4</b> <span class="math inline">\(K\)</span>-Fold Cross-Validation</a><ul>
<li class="chapter" data-level="13.4.1" data-path="resampling.html"><a href="resampling.html#leave-one-out-cross-validation-loocv"><i class="fa fa-check"></i><b>13.4.1</b> Leave-One-Out Cross Validation (LOOCV)</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>VI Regularization</b></span></li>
<li class="chapter" data-level="14" data-path="regular.html"><a href="regular.html"><i class="fa fa-check"></i><b>14</b> Regularization Techniques</a><ul>
<li class="chapter" data-level="14.1" data-path="regular.html"><a href="regular.html#multicollinearity-issues"><i class="fa fa-check"></i><b>14.1</b> Multicollinearity Issues</a><ul>
<li class="chapter" data-level="14.1.1" data-path="regular.html"><a href="regular.html#toy-example"><i class="fa fa-check"></i><b>14.1.1</b> Toy Example</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="regular.html"><a href="regular.html#irregular-coefficients"><i class="fa fa-check"></i><b>14.2</b> Irregular Coefficients</a></li>
<li class="chapter" data-level="14.3" data-path="regular.html"><a href="regular.html#connection-to-regularization"><i class="fa fa-check"></i><b>14.3</b> Connection to Regularization</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="pcr.html"><a href="pcr.html"><i class="fa fa-check"></i><b>15</b> Principal Components Regression</a><ul>
<li class="chapter" data-level="15.1" data-path="pcr.html"><a href="pcr.html#motivation-example-1"><i class="fa fa-check"></i><b>15.1</b> Motivation Example</a></li>
<li class="chapter" data-level="15.2" data-path="pcr.html"><a href="pcr.html#the-pcr-model"><i class="fa fa-check"></i><b>15.2</b> The PCR Model</a></li>
<li class="chapter" data-level="15.3" data-path="pcr.html"><a href="pcr.html#how-does-pcr-work"><i class="fa fa-check"></i><b>15.3</b> How does PCR work?</a><ul>
<li class="chapter" data-level="15.3.1" data-path="pcr.html"><a href="pcr.html#transition-formula"><i class="fa fa-check"></i><b>15.3.1</b> Transition Formula</a></li>
<li class="chapter" data-level="15.3.2" data-path="pcr.html"><a href="pcr.html#size-of-coefficients"><i class="fa fa-check"></i><b>15.3.2</b> Size of Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="pcr.html"><a href="pcr.html#selecting-number-of-pcs"><i class="fa fa-check"></i><b>15.4</b> Selecting Number of PCs</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="pls.html"><a href="pls.html"><i class="fa fa-check"></i><b>16</b> Partial Least Squares Regression</a><ul>
<li class="chapter" data-level="16.1" data-path="pls.html"><a href="pls.html#motivation-example-2"><i class="fa fa-check"></i><b>16.1</b> Motivation Example</a></li>
<li class="chapter" data-level="16.2" data-path="pls.html"><a href="pls.html#the-plsr-model"><i class="fa fa-check"></i><b>16.2</b> The PLSR Model</a></li>
<li class="chapter" data-level="16.3" data-path="pls.html"><a href="pls.html#how-does-plsr-work"><i class="fa fa-check"></i><b>16.3</b> How does PLSR work?</a></li>
<li class="chapter" data-level="16.4" data-path="pls.html"><a href="pls.html#plsr-algorithm"><i class="fa fa-check"></i><b>16.4</b> PLSR Algorithm</a><ul>
<li class="chapter" data-level="16.4.1" data-path="pls.html"><a href="pls.html#pls-solution-with-original-variables"><i class="fa fa-check"></i><b>16.4.1</b> PLS Solution with original variables</a></li>
<li class="chapter" data-level="16.4.2" data-path="pls.html"><a href="pls.html#size-of-coefficients-1"><i class="fa fa-check"></i><b>16.4.2</b> Size of Coefficients</a></li>
<li class="chapter" data-level="16.4.3" data-path="pls.html"><a href="pls.html#some-properties"><i class="fa fa-check"></i><b>16.4.3</b> Some Properties</a></li>
</ul></li>
<li class="chapter" data-level="16.5" data-path="pls.html"><a href="pls.html#selecting-number-of-pls-components"><i class="fa fa-check"></i><b>16.5</b> Selecting Number of PLS Components</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="ridge.html"><a href="ridge.html"><i class="fa fa-check"></i><b>17</b> Ridge Regression</a><ul>
<li class="chapter" data-level="17.1" data-path="ridge.html"><a href="ridge.html#a-new-minimization-problem"><i class="fa fa-check"></i><b>17.1</b> A New Minimization Problem</a><ul>
<li class="chapter" data-level="17.1.1" data-path="ridge.html"><a href="ridge.html#constraining-regression-coefficients"><i class="fa fa-check"></i><b>17.1.1</b> Constraining Regression Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="ridge.html"><a href="ridge.html#a-new-minimization-solution"><i class="fa fa-check"></i><b>17.2</b> A New Minimization Solution</a></li>
<li class="chapter" data-level="17.3" data-path="ridge.html"><a href="ridge.html#what-does-rr-accomplish"><i class="fa fa-check"></i><b>17.3</b> What does RR accomplish?</a><ul>
<li class="chapter" data-level="17.3.1" data-path="ridge.html"><a href="ridge.html#how-to-find-lambda"><i class="fa fa-check"></i><b>17.3.1</b> How to find <span class="math inline">\(\lambda\)</span>?</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>VII Extending Linear Regression</b></span></li>
<li class="chapter" data-level="18" data-path="linear-extensions.html"><a href="linear-extensions.html"><i class="fa fa-check"></i><b>18</b> Beyond Linear Regression</a><ul>
<li class="chapter" data-level="18.1" data-path="linear-extensions.html"><a href="linear-extensions.html#introduction-2"><i class="fa fa-check"></i><b>18.1</b> Introduction</a></li>
<li class="chapter" data-level="18.2" data-path="linear-extensions.html"><a href="linear-extensions.html#expanding-the-regression-horizon"><i class="fa fa-check"></i><b>18.2</b> Expanding the Regression Horizon</a><ul>
<li class="chapter" data-level="18.2.1" data-path="linear-extensions.html"><a href="linear-extensions.html#linearity"><i class="fa fa-check"></i><b>18.2.1</b> Linearity</a></li>
<li class="chapter" data-level="18.2.2" data-path="linear-extensions.html"><a href="linear-extensions.html#parametric-and-nonparametric"><i class="fa fa-check"></i><b>18.2.2</b> Parametric and Nonparametric</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="linear-extensions.html"><a href="linear-extensions.html#transforming-features"><i class="fa fa-check"></i><b>18.3</b> Transforming Features</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="nonparametric.html"><a href="nonparametric.html"><i class="fa fa-check"></i><b>19</b> Nonparametric Regression</a><ul>
<li class="chapter" data-level="19.1" data-path="nonparametric.html"><a href="nonparametric.html#conditional-averages"><i class="fa fa-check"></i><b>19.1</b> Conditional Averages</a></li>
<li class="chapter" data-level="19.2" data-path="nonparametric.html"><a href="nonparametric.html#looking-at-the-neighbors"><i class="fa fa-check"></i><b>19.2</b> Looking at the Neighbors</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="knn.html"><a href="knn.html"><i class="fa fa-check"></i><b>20</b> Nearest Neighbor Estimates</a><ul>
<li class="chapter" data-level="20.1" data-path="knn.html"><a href="knn.html#introduction-3"><i class="fa fa-check"></i><b>20.1</b> Introduction</a></li>
<li class="chapter" data-level="20.2" data-path="knn.html"><a href="knn.html#k-nearest-neighbors-knn"><i class="fa fa-check"></i><b>20.2</b> <span class="math inline">\(k\)</span> Nearest Neighbors (KNN)</a><ul>
<li class="chapter" data-level="20.2.1" data-path="knn.html"><a href="knn.html#distance-measures"><i class="fa fa-check"></i><b>20.2.1</b> Distance Measures</a></li>
<li class="chapter" data-level="20.2.2" data-path="knn.html"><a href="knn.html#knn-estimator"><i class="fa fa-check"></i><b>20.2.2</b> KNN Estimator</a></li>
<li class="chapter" data-level="20.2.3" data-path="knn.html"><a href="knn.html#how-to-find-k"><i class="fa fa-check"></i><b>20.2.3</b> How to find <span class="math inline">\(k\)</span>?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="21" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html"><i class="fa fa-check"></i><b>21</b> Kernel Smoothers</a><ul>
<li class="chapter" data-level="21.1" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#introduction-4"><i class="fa fa-check"></i><b>21.1</b> Introduction</a></li>
<li class="chapter" data-level="21.2" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#kernel-smoothers-1"><i class="fa fa-check"></i><b>21.2</b> Kernel Smoothers</a><ul>
<li class="chapter" data-level="21.2.1" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#kernel-functions"><i class="fa fa-check"></i><b>21.2.1</b> Kernel Functions</a></li>
<li class="chapter" data-level="21.2.2" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#weights-from-kernels"><i class="fa fa-check"></i><b>21.2.2</b> Weights from Kernels</a></li>
<li class="chapter" data-level="21.2.3" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#kernel-estimator"><i class="fa fa-check"></i><b>21.2.3</b> Kernel Estimator</a></li>
</ul></li>
<li class="chapter" data-level="21.3" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#local-polynomial-estimators"><i class="fa fa-check"></i><b>21.3</b> Local Polynomial Estimators</a></li>
</ul></li>
<li class="part"><span><b>VIII Classification</b></span></li>
<li class="chapter" data-level="22" data-path="classif.html"><a href="classif.html"><i class="fa fa-check"></i><b>22</b> Classification</a><ul>
<li class="chapter" data-level="22.1" data-path="classif.html"><a href="classif.html#introduction-5"><i class="fa fa-check"></i><b>22.1</b> Introduction</a><ul>
<li class="chapter" data-level="22.1.1" data-path="classif.html"><a href="classif.html#credit-score-example"><i class="fa fa-check"></i><b>22.1.1</b> Credit Score Example</a></li>
<li class="chapter" data-level="22.1.2" data-path="classif.html"><a href="classif.html#toy-example-1"><i class="fa fa-check"></i><b>22.1.2</b> Toy Example</a></li>
<li class="chapter" data-level="22.1.3" data-path="classif.html"><a href="classif.html#two-class-problem"><i class="fa fa-check"></i><b>22.1.3</b> Two-class Problem</a></li>
<li class="chapter" data-level="22.1.4" data-path="classif.html"><a href="classif.html#bayes-rule-reminder"><i class="fa fa-check"></i><b>22.1.4</b> Bayes’ Rule Reminder</a></li>
</ul></li>
<li class="chapter" data-level="22.2" data-path="classif.html"><a href="classif.html#bayes-classifier"><i class="fa fa-check"></i><b>22.2</b> Bayes Classifier</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="logistic.html"><a href="logistic.html"><i class="fa fa-check"></i><b>23</b> Logistic Regression</a><ul>
<li class="chapter" data-level="23.1" data-path="logistic.html"><a href="logistic.html#motivation-1"><i class="fa fa-check"></i><b>23.1</b> Motivation</a><ul>
<li class="chapter" data-level="23.1.1" data-path="logistic.html"><a href="logistic.html#first-approach-fitting-a-line"><i class="fa fa-check"></i><b>23.1.1</b> First Approach: Fitting a Line</a></li>
<li class="chapter" data-level="23.1.2" data-path="logistic.html"><a href="logistic.html#second-approach-harsh-thresholding"><i class="fa fa-check"></i><b>23.1.2</b> Second Approach: Harsh Thresholding</a></li>
<li class="chapter" data-level="23.1.3" data-path="logistic.html"><a href="logistic.html#third-approach-conditional-means"><i class="fa fa-check"></i><b>23.1.3</b> Third Approach: Conditional Means</a></li>
</ul></li>
<li class="chapter" data-level="23.2" data-path="logistic.html"><a href="logistic.html#logistic-regression-model"><i class="fa fa-check"></i><b>23.2</b> Logistic Regression Model</a><ul>
<li class="chapter" data-level="23.2.1" data-path="logistic.html"><a href="logistic.html#the-criterion-being-optimized"><i class="fa fa-check"></i><b>23.2.1</b> The Criterion Being Optimized</a></li>
<li class="chapter" data-level="23.2.2" data-path="logistic.html"><a href="logistic.html#another-way-to-solve-logistic-regression"><i class="fa fa-check"></i><b>23.2.2</b> Another Way to Solve Logistic Regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="24" data-path="discrim.html"><a href="discrim.html"><i class="fa fa-check"></i><b>24</b> Preamble for Discriminant Analysis</a><ul>
<li class="chapter" data-level="24.1" data-path="discrim.html"><a href="discrim.html#motivation-2"><i class="fa fa-check"></i><b>24.1</b> Motivation</a><ul>
<li class="chapter" data-level="24.1.1" data-path="discrim.html"><a href="discrim.html#distinguishing-species"><i class="fa fa-check"></i><b>24.1.1</b> Distinguishing Species</a></li>
<li class="chapter" data-level="24.1.2" data-path="discrim.html"><a href="discrim.html#sum-of-squares-decomposition"><i class="fa fa-check"></i><b>24.1.2</b> Sum of Squares Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="24.2" data-path="discrim.html"><a href="discrim.html#derived-ratios-from-sum-of-squares"><i class="fa fa-check"></i><b>24.2</b> Derived Ratios from Sum-of-Squares</a><ul>
<li class="chapter" data-level="24.2.1" data-path="discrim.html"><a href="discrim.html#correlation-ratio"><i class="fa fa-check"></i><b>24.2.1</b> Correlation Ratio</a></li>
<li class="chapter" data-level="24.2.2" data-path="discrim.html"><a href="discrim.html#f-ratio"><i class="fa fa-check"></i><b>24.2.2</b> F-Ratio</a></li>
</ul></li>
<li class="chapter" data-level="24.3" data-path="discrim.html"><a href="discrim.html#geometric-perspective"><i class="fa fa-check"></i><b>24.3</b> Geometric Perspective</a><ul>
<li class="chapter" data-level="24.3.1" data-path="discrim.html"><a href="discrim.html#clouds-from-class-structure"><i class="fa fa-check"></i><b>24.3.1</b> Clouds from Class Structure</a></li>
<li class="chapter" data-level="24.3.2" data-path="discrim.html"><a href="discrim.html#dispersion-decomposition"><i class="fa fa-check"></i><b>24.3.2</b> Dispersion Decomposition</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="25" data-path="cda.html"><a href="cda.html"><i class="fa fa-check"></i><b>25</b> Canonical Discriminant Analysis</a><ul>
<li class="chapter" data-level="25.1" data-path="cda.html"><a href="cda.html#cda-semi-supervised-aspect"><i class="fa fa-check"></i><b>25.1</b> CDA: Semi-Supervised Aspect</a></li>
<li class="chapter" data-level="25.2" data-path="cda.html"><a href="cda.html#looking-for-a-discriminant-axis"><i class="fa fa-check"></i><b>25.2</b> Looking for a discriminant axis</a></li>
<li class="chapter" data-level="25.3" data-path="cda.html"><a href="cda.html#looking-for-a-compromise-criterion"><i class="fa fa-check"></i><b>25.3</b> Looking for a Compromise Criterion</a><ul>
<li class="chapter" data-level="25.3.1" data-path="cda.html"><a href="cda.html#correlation-ratio-criterion"><i class="fa fa-check"></i><b>25.3.1</b> Correlation Ratio Criterion</a></li>
<li class="chapter" data-level="25.3.2" data-path="cda.html"><a href="cda.html#f-ratio-criterion"><i class="fa fa-check"></i><b>25.3.2</b> F-ratio Criterion</a></li>
<li class="chapter" data-level="25.3.3" data-path="cda.html"><a href="cda.html#a-special-pca"><i class="fa fa-check"></i><b>25.3.3</b> A Special PCA</a></li>
</ul></li>
<li class="chapter" data-level="25.4" data-path="cda.html"><a href="cda.html#cda-supervised-aspect"><i class="fa fa-check"></i><b>25.4</b> CDA: Supervised Aspect</a><ul>
<li class="chapter" data-level="25.4.1" data-path="cda.html"><a href="cda.html#distance-behind-cda"><i class="fa fa-check"></i><b>25.4.1</b> Distance behind CDA</a></li>
<li class="chapter" data-level="25.4.2" data-path="cda.html"><a href="cda.html#predictive-idea"><i class="fa fa-check"></i><b>25.4.2</b> Predictive Idea</a></li>
<li class="chapter" data-level="25.4.3" data-path="cda.html"><a href="cda.html#cda-classifier"><i class="fa fa-check"></i><b>25.4.3</b> CDA Classifier</a></li>
<li class="chapter" data-level="25.4.4" data-path="cda.html"><a href="cda.html#limitations-of-cda-classifier"><i class="fa fa-check"></i><b>25.4.4</b> Limitations of CDA classifier</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="26" data-path="discanalysis.html"><a href="discanalysis.html"><i class="fa fa-check"></i><b>26</b> Discriminant Analysis</a><ul>
<li class="chapter" data-level="26.1" data-path="discanalysis.html"><a href="discanalysis.html#probabilistic-da"><i class="fa fa-check"></i><b>26.1</b> Probabilistic DA</a><ul>
<li class="chapter" data-level="26.1.1" data-path="discanalysis.html"><a href="discanalysis.html#normal-distributions"><i class="fa fa-check"></i><b>26.1.1</b> Normal Distributions</a></li>
<li class="chapter" data-level="26.1.2" data-path="discanalysis.html"><a href="discanalysis.html#estimating-parameters-of-normal-distributions"><i class="fa fa-check"></i><b>26.1.2</b> Estimating Parameters of Normal Distributions</a></li>
</ul></li>
<li class="chapter" data-level="26.2" data-path="discanalysis.html"><a href="discanalysis.html#discriminant-functions"><i class="fa fa-check"></i><b>26.2</b> Discriminant Functions</a></li>
<li class="chapter" data-level="26.3" data-path="discanalysis.html"><a href="discanalysis.html#quadratic-discriminant-analysis-qda"><i class="fa fa-check"></i><b>26.3</b> Quadratic Discriminant Analysis (QDA)</a></li>
<li class="chapter" data-level="26.4" data-path="discanalysis.html"><a href="discanalysis.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>26.4</b> Linear Discriminant Analysis</a><ul>
<li class="chapter" data-level="26.4.1" data-path="discanalysis.html"><a href="discanalysis.html#canonical-discriminant-analysis"><i class="fa fa-check"></i><b>26.4.1</b> Canonical Discriminant Analysis</a></li>
<li class="chapter" data-level="26.4.2" data-path="discanalysis.html"><a href="discanalysis.html#naive-bayes"><i class="fa fa-check"></i><b>26.4.2</b> Naive Bayes</a></li>
<li class="chapter" data-level="26.4.3" data-path="discanalysis.html"><a href="discanalysis.html#fifth-case"><i class="fa fa-check"></i><b>26.4.3</b> Fifth Case</a></li>
</ul></li>
<li class="chapter" data-level="26.5" data-path="discanalysis.html"><a href="discanalysis.html#comparing-the-cases"><i class="fa fa-check"></i><b>26.5</b> Comparing the Cases</a></li>
</ul></li>
<li class="chapter" data-level="27" data-path="classperformance.html"><a href="classperformance.html"><i class="fa fa-check"></i><b>27</b> Performance of Classifiers</a><ul>
<li class="chapter" data-level="27.1" data-path="classperformance.html"><a href="classperformance.html#classification-error-measures"><i class="fa fa-check"></i><b>27.1</b> Classification Error Measures</a><ul>
<li class="chapter" data-level="27.1.1" data-path="classperformance.html"><a href="classperformance.html#errors-for-binary-response"><i class="fa fa-check"></i><b>27.1.1</b> Errors for Binary Response</a></li>
<li class="chapter" data-level="27.1.2" data-path="classperformance.html"><a href="classperformance.html#error-for-categorical-response"><i class="fa fa-check"></i><b>27.1.2</b> Error for Categorical Response</a></li>
</ul></li>
<li class="chapter" data-level="27.2" data-path="classperformance.html"><a href="classperformance.html#confusion-matrices"><i class="fa fa-check"></i><b>27.2</b> Confusion Matrices</a></li>
<li class="chapter" data-level="27.3" data-path="classperformance.html"><a href="classperformance.html#binary-response-example"><i class="fa fa-check"></i><b>27.3</b> Binary Response Example</a><ul>
<li class="chapter" data-level="27.3.1" data-path="classperformance.html"><a href="classperformance.html#application-for-checking-account"><i class="fa fa-check"></i><b>27.3.1</b> Application for Checking Account</a></li>
<li class="chapter" data-level="27.3.2" data-path="classperformance.html"><a href="classperformance.html#application-for-loan"><i class="fa fa-check"></i><b>27.3.2</b> Application for Loan</a></li>
</ul></li>
<li class="chapter" data-level="27.4" data-path="classperformance.html"><a href="classperformance.html#decision-rules-and-errors"><i class="fa fa-check"></i><b>27.4</b> Decision Rules and Errors</a></li>
<li class="chapter" data-level="27.5" data-path="classperformance.html"><a href="classperformance.html#roc-curves"><i class="fa fa-check"></i><b>27.5</b> ROC Curves</a><ul>
<li class="chapter" data-level="27.5.1" data-path="classperformance.html"><a href="classperformance.html#graphing-roc-curves"><i class="fa fa-check"></i><b>27.5.1</b> Graphing ROC curves</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IX Unsupervised II: Clustering</b></span></li>
<li class="chapter" data-level="28" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>28</b> Clustering</a><ul>
<li class="chapter" data-level="28.1" data-path="clustering.html"><a href="clustering.html#about-clustering"><i class="fa fa-check"></i><b>28.1</b> About Clustering</a><ul>
<li class="chapter" data-level="28.1.1" data-path="clustering.html"><a href="clustering.html#types-of-clustering"><i class="fa fa-check"></i><b>28.1.1</b> Types of Clustering</a></li>
<li class="chapter" data-level="28.1.2" data-path="clustering.html"><a href="clustering.html#hard-clustering"><i class="fa fa-check"></i><b>28.1.2</b> Hard Clustering</a></li>
</ul></li>
<li class="chapter" data-level="28.2" data-path="clustering.html"><a href="clustering.html#dispersion-measures"><i class="fa fa-check"></i><b>28.2</b> Dispersion Measures</a></li>
<li class="chapter" data-level="28.3" data-path="clustering.html"><a href="clustering.html#complexity-in-clustering"><i class="fa fa-check"></i><b>28.3</b> Complexity in Clustering</a></li>
</ul></li>
<li class="chapter" data-level="29" data-path="kmeans.html"><a href="kmeans.html"><i class="fa fa-check"></i><b>29</b> K-Means</a><ul>
<li class="chapter" data-level="29.1" data-path="kmeans.html"><a href="kmeans.html#toy-example-2"><i class="fa fa-check"></i><b>29.1</b> Toy Example</a></li>
<li class="chapter" data-level="29.2" data-path="kmeans.html"><a href="kmeans.html#what-does-k-means-do"><i class="fa fa-check"></i><b>29.2</b> What does K-means do?</a></li>
<li class="chapter" data-level="29.3" data-path="kmeans.html"><a href="kmeans.html#k-means-algorithms"><i class="fa fa-check"></i><b>29.3</b> K-Means Algorithms</a><ul>
<li class="chapter" data-level="29.3.1" data-path="kmeans.html"><a href="kmeans.html#classic-version"><i class="fa fa-check"></i><b>29.3.1</b> Classic Version</a></li>
<li class="chapter" data-level="29.3.2" data-path="kmeans.html"><a href="kmeans.html#moving-centers-algorithm"><i class="fa fa-check"></i><b>29.3.2</b> Moving Centers Algorithm</a></li>
<li class="chapter" data-level="29.3.3" data-path="kmeans.html"><a href="kmeans.html#dynamic-clouds"><i class="fa fa-check"></i><b>29.3.3</b> Dynamic Clouds</a></li>
<li class="chapter" data-level="29.3.4" data-path="kmeans.html"><a href="kmeans.html#choosing-k"><i class="fa fa-check"></i><b>29.3.4</b> Choosing <span class="math inline">\(K\)</span></a></li>
<li class="chapter" data-level="29.3.5" data-path="kmeans.html"><a href="kmeans.html#comments"><i class="fa fa-check"></i><b>29.3.5</b> Comments</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="30" data-path="hclus.html"><a href="hclus.html"><i class="fa fa-check"></i><b>30</b> Hierarchical Clustering</a><ul>
<li class="chapter" data-level="30.1" data-path="hclus.html"><a href="hclus.html#agglomerative-methods"><i class="fa fa-check"></i><b>30.1</b> Agglomerative Methods</a></li>
<li class="chapter" data-level="30.2" data-path="hclus.html"><a href="hclus.html#example-single-linkage"><i class="fa fa-check"></i><b>30.2</b> Example: Single Linkage</a><ul>
<li class="chapter" data-level="30.2.1" data-path="hclus.html"><a href="hclus.html#dendrogram"><i class="fa fa-check"></i><b>30.2.1</b> Dendrogram</a></li>
</ul></li>
<li class="chapter" data-level="30.3" data-path="hclus.html"><a href="hclus.html#example-complete-linkage"><i class="fa fa-check"></i><b>30.3</b> Example: Complete Linkage</a><ul>
<li class="chapter" data-level="30.3.1" data-path="hclus.html"><a href="hclus.html#cutting-dendograms"><i class="fa fa-check"></i><b>30.3.1</b> Cutting Dendograms</a></li>
<li class="chapter" data-level="30.3.2" data-path="hclus.html"><a href="hclus.html#pros-and-coons"><i class="fa fa-check"></i><b>30.3.2</b> Pros and COons</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>X Tree-based Methods</b></span></li>
<li class="chapter" data-level="31" data-path="trees.html"><a href="trees.html"><i class="fa fa-check"></i><b>31</b> Intro to Decision Trees</a><ul>
<li class="chapter" data-level="31.1" data-path="trees.html"><a href="trees.html#introduction-6"><i class="fa fa-check"></i><b>31.1</b> Introduction</a></li>
<li class="chapter" data-level="31.2" data-path="trees.html"><a href="trees.html#some-terminology"><i class="fa fa-check"></i><b>31.2</b> Some Terminology</a><ul>
<li class="chapter" data-level="31.2.1" data-path="trees.html"><a href="trees.html#binary-trees"><i class="fa fa-check"></i><b>31.2.1</b> Binary Trees</a></li>
</ul></li>
<li class="chapter" data-level="31.3" data-path="trees.html"><a href="trees.html#space-partitions"><i class="fa fa-check"></i><b>31.3</b> Space Partitions</a><ul>
<li class="chapter" data-level="31.3.1" data-path="trees.html"><a href="trees.html#the-process-of-building-a-tree"><i class="fa fa-check"></i><b>31.3.1</b> The Process of Building a Tree</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="32" data-path="tree-impurities.html"><a href="tree-impurities.html"><i class="fa fa-check"></i><b>32</b> Binary Splits and Impurity</a><ul>
<li class="chapter" data-level="32.1" data-path="tree-impurities.html"><a href="tree-impurities.html#binary-partitions"><i class="fa fa-check"></i><b>32.1</b> Binary Partitions</a><ul>
<li class="chapter" data-level="32.1.1" data-path="tree-impurities.html"><a href="tree-impurities.html#splits-of-binary-variables"><i class="fa fa-check"></i><b>32.1.1</b> Splits of Binary variables</a></li>
<li class="chapter" data-level="32.1.2" data-path="tree-impurities.html"><a href="tree-impurities.html#splits-of-nominal-variables"><i class="fa fa-check"></i><b>32.1.2</b> Splits of Nominal Variables</a></li>
<li class="chapter" data-level="32.1.3" data-path="tree-impurities.html"><a href="tree-impurities.html#splits-of-ordinal-variables"><i class="fa fa-check"></i><b>32.1.3</b> Splits of Ordinal Variables</a></li>
<li class="chapter" data-level="32.1.4" data-path="tree-impurities.html"><a href="tree-impurities.html#splits-continuous-variables"><i class="fa fa-check"></i><b>32.1.4</b> Splits Continuous variables</a></li>
</ul></li>
<li class="chapter" data-level="32.2" data-path="tree-impurities.html"><a href="tree-impurities.html#measures-of-impurity"><i class="fa fa-check"></i><b>32.2</b> Measures of Impurity</a><ul>
<li class="chapter" data-level="32.2.1" data-path="tree-impurities.html"><a href="tree-impurities.html#entropy"><i class="fa fa-check"></i><b>32.2.1</b> Entropy</a></li>
<li class="chapter" data-level="32.2.2" data-path="tree-impurities.html"><a href="tree-impurities.html#the-math-behind-entropy"><i class="fa fa-check"></i><b>32.2.2</b> The Math Behind Entropy</a></li>
<li class="chapter" data-level="32.2.3" data-path="tree-impurities.html"><a href="tree-impurities.html#gini-impurity"><i class="fa fa-check"></i><b>32.2.3</b> Gini Impurity</a></li>
<li class="chapter" data-level="32.2.4" data-path="tree-impurities.html"><a href="tree-impurities.html#variance-based-impurity"><i class="fa fa-check"></i><b>32.2.4</b> Variance-based Impurity</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="33" data-path="tree-splits.html"><a href="tree-splits.html"><i class="fa fa-check"></i><b>33</b> Splitting Nodes</a><ul>
<li class="chapter" data-level="33.1" data-path="tree-splits.html"><a href="tree-splits.html#entropy-based-splits"><i class="fa fa-check"></i><b>33.1</b> Entropy-based Splits</a></li>
<li class="chapter" data-level="33.2" data-path="tree-splits.html"><a href="tree-splits.html#gini-index-based-splits"><i class="fa fa-check"></i><b>33.2</b> Gini-index based Splits</a></li>
<li class="chapter" data-level="33.3" data-path="tree-splits.html"><a href="tree-splits.html#looking-for-the-best-split"><i class="fa fa-check"></i><b>33.3</b> Looking for the best split</a></li>
</ul></li>
<li class="chapter" data-level="34" data-path="tree-basics.html"><a href="tree-basics.html"><i class="fa fa-check"></i><b>34</b> Building Binary Trees</a><ul>
<li class="chapter" data-level="34.1" data-path="tree-basics.html"><a href="tree-basics.html#node-splitting-stopping-criteria"><i class="fa fa-check"></i><b>34.1</b> Node-Splitting Stopping Criteria</a></li>
<li class="chapter" data-level="34.2" data-path="tree-basics.html"><a href="tree-basics.html#issues-with-trees"><i class="fa fa-check"></i><b>34.2</b> Issues with Trees</a><ul>
<li class="chapter" data-level="34.2.1" data-path="tree-basics.html"><a href="tree-basics.html#bias-variance-of-trees"><i class="fa fa-check"></i><b>34.2.1</b> Bias-Variance of Trees</a></li>
</ul></li>
<li class="chapter" data-level="34.3" data-path="tree-basics.html"><a href="tree-basics.html#pruning-a-tree"><i class="fa fa-check"></i><b>34.3</b> Pruning a Tree</a></li>
<li class="chapter" data-level="34.4" data-path="tree-basics.html"><a href="tree-basics.html#pros-and-cons-of-trees"><i class="fa fa-check"></i><b>34.4</b> Pros and Cons of Trees</a><ul>
<li class="chapter" data-level="34.4.1" data-path="tree-basics.html"><a href="tree-basics.html#advantages-of-trees"><i class="fa fa-check"></i><b>34.4.1</b> Advantages of Trees</a></li>
<li class="chapter" data-level="34.4.2" data-path="tree-basics.html"><a href="tree-basics.html#disadvantages-of-trees"><i class="fa fa-check"></i><b>34.4.2</b> Disadvantages of Trees</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="35" data-path="bagging.html"><a href="bagging.html"><i class="fa fa-check"></i><b>35</b> Bagging</a><ul>
<li class="chapter" data-level="35.1" data-path="bagging.html"><a href="bagging.html#introduction-7"><i class="fa fa-check"></i><b>35.1</b> Introduction</a><ul>
<li class="chapter" data-level="35.1.1" data-path="bagging.html"><a href="bagging.html#idea-of-bagging"><i class="fa fa-check"></i><b>35.1.1</b> Idea of Bagging</a></li>
</ul></li>
<li class="chapter" data-level="35.2" data-path="bagging.html"><a href="bagging.html#why-bother-bagging"><i class="fa fa-check"></i><b>35.2</b> Why Bother Bagging?</a></li>
</ul></li>
<li class="chapter" data-level="36" data-path="forest.html"><a href="forest.html"><i class="fa fa-check"></i><b>36</b> Random Forests</a><ul>
<li class="chapter" data-level="36.1" data-path="forest.html"><a href="forest.html#introduction-8"><i class="fa fa-check"></i><b>36.1</b> Introduction</a></li>
<li class="chapter" data-level="36.2" data-path="forest.html"><a href="forest.html#algorithm-1"><i class="fa fa-check"></i><b>36.2</b> Algorithm</a><ul>
<li class="chapter" data-level="36.2.1" data-path="forest.html"><a href="forest.html#two-sources-of-randomness"><i class="fa fa-check"></i><b>36.2.1</b> Two Sources of Randomness</a></li>
<li class="chapter" data-level="36.2.2" data-path="forest.html"><a href="forest.html#regressions-and-classification-forests"><i class="fa fa-check"></i><b>36.2.2</b> Regressions and Classification Forests</a></li>
<li class="chapter" data-level="36.2.3" data-path="forest.html"><a href="forest.html#key-advantage-of-random-forests"><i class="fa fa-check"></i><b>36.2.3</b> Key Advantage of Random Forests</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Made with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">All Models Are Wrong: Concepts of Statistical Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="tree-basics" class="section level1">
<h1><span class="header-section-number">34</span> Building Binary Trees</h1>
<p>As previously mentioned, the building process of a binary tree requires three
main components:</p>
<ol style="list-style-type: decimal">
<li><p>Establish the set of all possible binary splits.</p></li>
<li><p>Define a criterion to find the “best” node split, which requires some
criterion to be optimized based on measures of impurity.</p></li>
<li><p>Determine a rule to declare a given node as either internal node, or leaf node.</p></li>
</ol>
<p>So far, we have described the first two aspects. But we still need to talk
about how to stop the tree-growing process.</p>
<div id="node-splitting-stopping-criteria" class="section level2">
<h2><span class="header-section-number">34.1</span> Node-Splitting Stopping Criteria</h2>
<p>To grow a tree, we need to recursively apply the steps described in the previous
chapter, to all child nodes. But at some point we need to decide whether to stop
splitting a given node.</p>
<p>In no particular order of importance, here are a few criteria for stopping
splitting a node. We stop splitting a node when/if:</p>
<ul>
<li>The node is pure (i.e. a node contains objects of only one class).</li>
</ul>
<p><img src="images/trees/stop-split-pure.svg" width="55%" style="display: block; margin: auto;" /></p>
<ul>
<li>We run out of features (that is, there are no more features to split on).</li>
</ul>
<p><img src="images/trees/stop-split-exhaust.svg" width="55%" style="display: block; margin: auto;" /></p>
<ul>
<li>A pre-specified minimum number of objects in a node is reached. For example,
stop splitting when a node contains at most 10 elements.
<ul>
<li>In <code>rpart()</code>, this can be specified using the <code>minsplit</code> parameter.</li>
</ul></li>
</ul>
<p><img src="images/trees/stop-split-minsplit.svg" width="55%" style="display: block; margin: auto;" /></p>
<ul>
<li>Size of resulting nodes would be “too small” (e.g. 10, 15, 20, etc.). This
is closely related to the previous rule, but a bit different. For example,
we can have a node with enough objects greater than the minimum required.
However, all resulting binary splits are deemed too small.
<ul>
<li>In <code>rpart()</code>, this can be specified using the  parameter
<code>minbucket</code>.</li>
</ul></li>
</ul>
<p><img src="images/trees/stop-split-minbucket.svg" width="55%" style="display: block; margin: auto;" /></p>
<ul>
<li>A certain depth level is reached. This is another condition in which a
pre-determined level of depth is chosen, in order to prevent “large” overfitting
trees.
<ul>
<li>In <code>rpart()</code>, this can be specified using the <code>maxdepth</code> parameter.</li>
</ul></li>
</ul>
<p><img src="images/trees/stop-split-level.svg" width="65%" style="display: block; margin: auto;" /></p>
</div>
<div id="issues-with-trees" class="section level2">
<h2><span class="header-section-number">34.2</span> Issues with Trees</h2>
<p>How good are trees? Do they predict well? Yes and No. A tree can achieve
high prediction performance … of the training data. The larger the tree,
the better its in-sample performance. In some cases, a (very large) tree can
even accomplish zero (or near-zero) error rates. But as you can imagine, this
optential for having good in-sample predictions comes with a price: large
test error. Why? Because the tree structure is highly dependent on early
splits. Put it otherwise, trees are complex models.</p>
<p>To illustrate the complexity of trees, let’s use the Heart data set from
<em>An Introduction to Statistical Learning</em> (ISL, by James te al). This data
contains 14 variables observed on 303 individuals. The response variable is
AHD. We have randomly split the data into training (90%) and testing test (10%),
using the training set to fit the response. For illustration purposes, we
repeat this operation four times, obtaining four different trees, displayed in
the following figure.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-375"></span>
<img src="images/trees/heart-trees.svg" alt="Different Tress" width="90%" />
<p class="caption">
Figure 34.1: Different Tress
</p>
</div>
<p>Notice how different the trees are. This is why we say that trees tend to
have high complexity (i.e. high flexibility). Recall that the way in which
splits are chosen is by looking for the best split at a given step, without
paying attention to what subsequent splits may look like. It can be the case
that a less good split leads to a better tree later on. But this is not how
the algorithm proceeds.</p>
<p>In other words, trees can become very complex “too soon”, because splits
are obtained with a “short-term view”, (best split <em>now</em> with instant
gratification), instead of a "long-term view (less good split now with delayed
gratification).</p>
<p>Up to now, we are using the notion of complexity in a vague manner. But it is
important to think about this for a minute. How do we define complexity in
decision trees? Well, there is no unique answer to this question. Perhaps the
most natural way to think about a tree’s complexity is by looking at its
depth (how tall, or small, a tree is). The taller the tree (that is, the
deeper it is), the more complexity.</p>
<p>Knowing that larger trees (large depth level) tend to result in poor out-of-sample
prediction, it feels intuitive that we should look for smaller, less complex,
trees. We may be willing to sacrifice some of the in-sample error in exchange of
increasing the out-of-sample performance. So far, so good.
But things are not that really simple. To see this, we need to talk about the
bias-variance tradeoff of decision trees.</p>
<div id="bias-variance-of-trees" class="section level3">
<h3><span class="header-section-number">34.2.1</span> Bias-Variance of Trees</h3>
<p>In general, decision trees have a bias and a variance problem. With small depth
levels, they have a bias problem. With large depth levels, they have a variance
problem.</p>
<p>The following diagram shows typical error curves of decision trees:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-376"></span>
<img src="images/trees/error-curve-trees0.svg" alt="Typical error curves of decision trees" width="70%" />
<p class="caption">
Figure 34.2: Typical error curves of decision trees
</p>
</div>
<p>Small trees (i.e. small depth) tend to have a bias problem (underfitting).
Conversely, large trees (i.e. large depth) tend to have a variance problem
(overfitting).</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-377"></span>
<img src="images/trees/error-curve-trees1.svg" alt="Bias-Variance problem of trees" width="70%" />
<p class="caption">
Figure 34.3: Bias-Variance problem of trees
</p>
</div>
<p>Because the depth of trees is measured in integer steps (not in a continuous
scale), it is very difficult to find the “sweet spot” for the right depth level.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-378"></span>
<img src="images/trees/error-curve-trees2.svg" alt="Bias-Variance problem of trees" width="70%" />
<p class="caption">
Figure 34.4: Bias-Variance problem of trees
</p>
</div>
<p>Compared to penalized criteria like those used in penalized methods (e.g. ridge
regression and lasso), using tree depth to measure complexity, does not provide
the same “fine granularity” of a penalty tuning parameter. Why? Because the
depth level of a tree is an integer number. You either have a tree of depth 2
or 3 or 4, but you cannot have a tree of depth level 2.78 or 3.125. Therefore,
finding the sweet spot for level of depth that minimizes the
test-error does not allow us to have a fine grain sensitive analysis of what the
best tradeoff is between bias and variance. And this is, to a large extent, the
downfall of decision trees. This is why in practice they don’t tend to work
as expected (in terms of generalization power).</p>
<p>The main strategy to overcome this limitation is not by changing the depth of
the tree, but by focusing on building large trees, and tackling the high
variance issue with resampling techniques, and aggregating several versions of
individual trees. These are the topics of the next chapters.</p>
</div>
</div>
<div id="pruning-a-tree" class="section level2">
<h2><span class="header-section-number">34.3</span> Pruning a Tree</h2>
<p>Now that we have described all the main aspects to grow a tree, as well as
some of the main drawbacks that a tree suffers from, we can present the formal
algorithm of CART-trees, based on cost-complexity pruning.</p>
<p>The building process of CART-style binary trees is to grow a “large” tree and
then prune off branches (from the bottom up) until we get an “okay size” tree.
Finding the pruned tree of “okay size” implies finding a subtree of the large
tree that is “optimal” in some predictive-power sense. This, in turn, requires
to handle a tree’s complexity in a different manner: not in terms of a tree’s
depth, but in terms of its number of terminal nodes. Here are the main stages:</p>
<ol style="list-style-type: decimal">
<li><p>Grow a large tree, denoted by <span class="math inline">\(T_{\max}\)</span>, typically obtained by getting
nodes of a pre-determined minimum size.</p></li>
<li><p>Calculate <span class="math inline">\(err(\tau)\)</span>, the error at each terminal node <span class="math inline">\(\tau \in T_{\max}\)</span>.
Aggregating these errors <span class="math inline">\(err(\tau)\)</span> gives us an overall measure of error
for the tree: <span class="math inline">\(E(T_{\max})\)</span></p></li>
<li><p>Prune <span class="math inline">\(T_{\max}\)</span>, from the bottom up, by getting rid of terminal nodes
that don’t provide “enough” information gain (i.e. useless leaf nodes). This
is done by including a tuning parameter that penalizes the “complexity” of
the subtrees (complexity in terms of number of terminal nodes).</p></li>
</ol>
<p>To introduce the cost-complexity criterion, we need to first introduce
some terminology and notation. We denote <span class="math inline">\(err(\tau)\)</span>, the error measure of
terminal node <span class="math inline">\(\tau\)</span>. The type of error function will depend on the type of
response:</p>
<ul>
<li><p><span class="math inline">\(err()\)</span> will be a misclassification rate when dealing with a categorical
response.</p></li>
<li><p><span class="math inline">\(err()\)</span> will be a regression error, e.g. squared error, when dealing with a
quantitative response.</p></li>
</ul>
<p>In order to use a penalized criterion, we define the penalized error in terminal
<span class="math inline">\(\tau\)</span> with <span class="math inline">\(\alpha\)</span> complexity parameter as:</p>
<p><span class="math display">\[
err_{\alpha} (\tau) = err(\tau) + \alpha
\]</span></p>
<p>where <span class="math inline">\(\alpha\)</span> is defined on the interval <span class="math inline">\([0, \infty)\)</span>].</p>
<p>The total error of a (sub)tree <span class="math inline">\(T\)</span>, also referred to as the cost of a tree,
is then calculated as the sum of penalized errors of all terminal nodes:</p>
<p><span class="math display">\[
E_{\alpha} (T) = \sum_{\ell}^{L} err_{\alpha}(\tau_{\ell}) = E(T) + \alpha |\tilde{T}|
\]</span></p>
<p>where <span class="math inline">\(L = |\tilde{T}|\)</span> is the number of terminal nodes in subtree <span class="math inline">\(T\)</span> of
<span class="math inline">\(T_{\max}\)</span>.</p>
<p>The penalty parameter <span class="math inline">\(\alpha\)</span> determines the tree size. When <span class="math inline">\(\alpha\)</span> is very
small, the penalty will be small, and so the size of the subtree <span class="math inline">\(T(\alpha)\)</span>
will be large. Setting <span class="math inline">\(\alpha = 0\)</span>, and the obtained tree will be <span class="math inline">\(T_{\max}\)</span>,
the largest possible tree. As we increase <span class="math inline">\(\alpha\)</span>, the minimizing subtrees
<span class="math inline">\(T(\alpha)\)</span> will have fewer and fewer terminal nodes. Taking things to the
extreme, with a very large <span class="math inline">\(\alpha\)</span>, all nodes will be pruned, resulting in
<span class="math inline">\(T_{\max}\)</span> being the root node.</p>
<p>The idea is to choose several values of <span class="math inline">\(\alpha\)</span>, in an increasing order,
and then look for the subtree <span class="math inline">\(T_m = T(\alpha_m)\)</span> that minimizes the penalized
error.</p>
<p>It can be shown that the sequence of penalty values</p>
<p><span class="math display">\[
0 = \alpha_0 &lt; \alpha_1 &lt; \alpha_2 &lt; \dots &lt; \alpha_M
\]</span></p>
<p>corresponds to a finte sequence of nested subtrees of <span class="math inline">\(T_{\max}\)</span></p>
<p><span class="math display">\[
T_{\max} = T_0 \supseteq T_1 \supseteq T_2 \supseteq \dots \supseteq T_M
\]</span></p>
<p>Because the complexity (i.e. penalty) parameter is a tuning parameter, we
typically use cross-validation to find the subtree <span class="math inline">\(T&#39; \subseteq T\)</span> that results
in the smallest cost.</p>
<p>In the R package <code>"rpart"</code>, the function <code>rpart()</code> allows you to specify the
argument <code>cp</code> which controls the complexity parameter.</p>
</div>
<div id="pros-and-cons-of-trees" class="section level2">
<h2><span class="header-section-number">34.4</span> Pros and Cons of Trees</h2>
<p>Let’s review some of the advantages and disadvantages of trees.</p>
<div id="advantages-of-trees" class="section level3">
<h3><span class="header-section-number">34.4.1</span> Advantages of Trees</h3>
<ul>
<li><p>They work for both types of responses: 1) categorical and 2) quantitative.
When dealing with a categorical response we talk about classification tree;
when dealing with a quantitative response we talk about regression tree.</p></li>
<li><p>They work with <strong>any</strong> type of predictors: binary, nominal, ordinal,
continuous, even with missing values.</p></li>
<li><p>They can detect interactions among the predictors.</p></li>
<li><p>Another advantage is that there’s almost no need to preprocess data for a
decision tree. This is very rare among statistical methods.</p></li>
<li><p>Likewise, decision trees make no stochastic assumptions: e.g. no normality
assumptions, or other specific distributions about the data.</p></li>
<li><p>In general, as long as the resulting trees are not too small, they tend to
have low bias.</p></li>
<li><p>The graphical display of a tree tends to be easy to read and interpret (at
least the main output; there are some summary statistics that might require
further background knowledge).</p></li>
</ul>
</div>
<div id="disadvantages-of-trees" class="section level3">
<h3><span class="header-section-number">34.4.2</span> Disadvantages of Trees</h3>
<ul>
<li><p>Trees are highly dependent on the training set. That is, given a different
training set, there is a high chance that you will obtain a different tree.
Which means they are prone to overfitting.</p></li>
<li><p>Because of this dependency, they suffer from high variance (this is perhaps
the main “Achilles’ heel” of decision trees).</p></li>
<li><p>Training a decision tree requires a reasonably large number of observations:
as a rule of thumb there should be at least 30-50 observations per node.</p></li>
<li><p>Recall that decision trees produce splits which involve rectangular regions
in the feature space which may not necessarily match the distribution of the
observations.</p></li>
<li><p>Finding the optimal sweetspot in bias-variance tradeoff is difficult.</p></li>
</ul>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="tree-splits.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="bagging.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["allmodelsarewrong.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
