<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3 Geometric Duality | All Models Are Wrong: Concepts of Statistical Learning</title>
  <meta name="description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  <meta name="generator" content="bookdown 0.16 and GitBook 2.6.7" />

  <meta property="og:title" content="3 Geometric Duality | All Models Are Wrong: Concepts of Statistical Learning" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://allmodelsarewrong.github.io" />
  
  <meta property="og:description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  <meta name="github-repo" content="allmodelsarewrong/allmodelsarewrong.github.io" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3 Geometric Duality | All Models Are Wrong: Concepts of Statistical Learning" />
  
  <meta name="twitter:description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  

<meta name="author" content="Gaston Sanchez Ethan Marzban" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="intro.html"/>
<link rel="next" href="pca.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><b>All Models Are Wrong (CSL)</b><br><small>by G. Sanchez & E. Marzban</small></a></li>

<li class="divider"></li>
<li class="part"><span><b>I Welcome</b></span></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="about.html"><a href="about.html"><i class="fa fa-check"></i><b>1</b> About this book</a><ul>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#prerequisites"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="part"><span><b>II Intro</b></span></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#basic-notation"><i class="fa fa-check"></i><b>2.1</b> Basic Notation</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="duality.html"><a href="duality.html"><i class="fa fa-check"></i><b>3</b> Geometric Duality</a><ul>
<li class="chapter" data-level="3.1" data-path="duality.html"><a href="duality.html#rows-space"><i class="fa fa-check"></i><b>3.1</b> Rows Space</a></li>
<li class="chapter" data-level="3.2" data-path="duality.html"><a href="duality.html#columns-space"><i class="fa fa-check"></i><b>3.2</b> Columns Space</a></li>
<li class="chapter" data-level="3.3" data-path="duality.html"><a href="duality.html#cloud-of-individuals"><i class="fa fa-check"></i><b>3.3</b> Cloud of Individuals</a><ul>
<li class="chapter" data-level="3.3.1" data-path="duality.html"><a href="duality.html#average-individual"><i class="fa fa-check"></i><b>3.3.1</b> Average Individual</a></li>
<li class="chapter" data-level="3.3.2" data-path="duality.html"><a href="duality.html#centered-data"><i class="fa fa-check"></i><b>3.3.2</b> Centered Data</a></li>
<li class="chapter" data-level="3.3.3" data-path="duality.html"><a href="duality.html#distance-between-individuals"><i class="fa fa-check"></i><b>3.3.3</b> Distance between individuals</a></li>
<li class="chapter" data-level="3.3.4" data-path="duality.html"><a href="duality.html#distance-to-the-centroid"><i class="fa fa-check"></i><b>3.3.4</b> Distance to the centroid</a></li>
<li class="chapter" data-level="3.3.5" data-path="duality.html"><a href="duality.html#measures-of-dispersion"><i class="fa fa-check"></i><b>3.3.5</b> Measures of Dispersion</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="duality.html"><a href="duality.html#cloud-of-variables"><i class="fa fa-check"></i><b>3.4</b> Cloud of Variables</a><ul>
<li class="chapter" data-level="3.4.1" data-path="duality.html"><a href="duality.html#mean-of-a-variable"><i class="fa fa-check"></i><b>3.4.1</b> Mean of a Variable</a></li>
<li class="chapter" data-level="3.4.2" data-path="duality.html"><a href="duality.html#variance-of-a-variable"><i class="fa fa-check"></i><b>3.4.2</b> Variance of a Variable</a></li>
<li class="chapter" data-level="3.4.3" data-path="duality.html"><a href="duality.html#variance-with-vector-notation"><i class="fa fa-check"></i><b>3.4.3</b> Variance with Vector Notation</a></li>
<li class="chapter" data-level="3.4.4" data-path="duality.html"><a href="duality.html#standard-deviation-as-a-norm"><i class="fa fa-check"></i><b>3.4.4</b> Standard Deviation as a Norm</a></li>
<li class="chapter" data-level="3.4.5" data-path="duality.html"><a href="duality.html#covariance"><i class="fa fa-check"></i><b>3.4.5</b> Covariance</a></li>
<li class="chapter" data-level="3.4.6" data-path="duality.html"><a href="duality.html#correlation"><i class="fa fa-check"></i><b>3.4.6</b> Correlation</a></li>
<li class="chapter" data-level="3.4.7" data-path="duality.html"><a href="duality.html#geometry-of-correlation"><i class="fa fa-check"></i><b>3.4.7</b> Geometry of Correlation</a></li>
<li class="chapter" data-level="3.4.8" data-path="duality.html"><a href="duality.html#orthogonal-projections"><i class="fa fa-check"></i><b>3.4.8</b> Orthogonal Projections</a></li>
<li class="chapter" data-level="3.4.9" data-path="duality.html"><a href="duality.html#the-mean-as-an-orthogonal-projection"><i class="fa fa-check"></i><b>3.4.9</b> The mean as an orthogonal projection</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Unsupervised I: PCA</b></span></li>
<li class="chapter" data-level="4" data-path="pca.html"><a href="pca.html"><i class="fa fa-check"></i><b>4</b> Principal Components Analysis</a><ul>
<li class="chapter" data-level="4.1" data-path="pca.html"><a href="pca.html#low-dimensional-representations"><i class="fa fa-check"></i><b>4.1</b> Low-dimensional Representations</a></li>
<li class="chapter" data-level="4.2" data-path="pca.html"><a href="pca.html#projections"><i class="fa fa-check"></i><b>4.2</b> Projections</a><ul>
<li class="chapter" data-level="4.2.1" data-path="pca.html"><a href="pca.html#vector-and-scalar-projections"><i class="fa fa-check"></i><b>4.2.1</b> Vector and Scalar Projections</a></li>
<li class="chapter" data-level="4.2.2" data-path="pca.html"><a href="pca.html#projected-inertia"><i class="fa fa-check"></i><b>4.2.2</b> Projected Inertia</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="pca.html"><a href="pca.html#maximization-problem"><i class="fa fa-check"></i><b>4.3</b> Maximization Problem</a><ul>
<li class="chapter" data-level="4.3.1" data-path="pca.html"><a href="pca.html#eigenvectors-of-mathbfs"><i class="fa fa-check"></i><b>4.3.1</b> Eigenvectors of <span class="math inline">\(\mathbf{S}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="pca.html"><a href="pca.html#another-perspective-of-pca"><i class="fa fa-check"></i><b>4.4</b> Another Perspective of PCA</a><ul>
<li class="chapter" data-level="4.4.1" data-path="pca.html"><a href="pca.html#finding-principal-components"><i class="fa fa-check"></i><b>4.4.1</b> Finding Principal Components</a></li>
<li class="chapter" data-level="4.4.2" data-path="pca.html"><a href="pca.html#finding-the-first-pc"><i class="fa fa-check"></i><b>4.4.2</b> Finding the first PC</a></li>
<li class="chapter" data-level="4.4.3" data-path="pca.html"><a href="pca.html#finding-the-second-pc"><i class="fa fa-check"></i><b>4.4.3</b> Finding the second PC</a></li>
<li class="chapter" data-level="4.4.4" data-path="pca.html"><a href="pca.html#finding-all-pcs"><i class="fa fa-check"></i><b>4.4.4</b> Finding all PCs</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="pca.html"><a href="pca.html#data-decomposition-model"><i class="fa fa-check"></i><b>4.5</b> Data Decomposition Model</a><ul>
<li class="chapter" data-level="4.5.1" data-path="pca.html"><a href="pca.html#alternative-approaches"><i class="fa fa-check"></i><b>4.5.1</b> Alternative Approaches</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Linear Regression</b></span></li>
<li class="chapter" data-level="5" data-path="ols.html"><a href="ols.html"><i class="fa fa-check"></i><b>5</b> Linear Regression</a><ul>
<li class="chapter" data-level="5.1" data-path="ols.html"><a href="ols.html#motivation"><i class="fa fa-check"></i><b>5.1</b> Motivation</a></li>
<li class="chapter" data-level="5.2" data-path="ols.html"><a href="ols.html#the-ideaintuition-of-regression"><i class="fa fa-check"></i><b>5.2</b> The Idea/Intuition of Regression</a></li>
<li class="chapter" data-level="5.3" data-path="ols.html"><a href="ols.html#the-linear-regression-model"><i class="fa fa-check"></i><b>5.3</b> The Linear Regression Model</a></li>
<li class="chapter" data-level="5.4" data-path="ols.html"><a href="ols.html#the-error-measure"><i class="fa fa-check"></i><b>5.4</b> The Error Measure</a></li>
<li class="chapter" data-level="5.5" data-path="ols.html"><a href="ols.html#the-least-squares-algorithm"><i class="fa fa-check"></i><b>5.5</b> The Least Squares Algorithm</a></li>
<li class="chapter" data-level="5.6" data-path="ols.html"><a href="ols.html#geometries-of-ols"><i class="fa fa-check"></i><b>5.6</b> Geometries of OLS</a><ul>
<li class="chapter" data-level="5.6.1" data-path="ols.html"><a href="ols.html#rows-perspective"><i class="fa fa-check"></i><b>5.6.1</b> Rows Perspective</a></li>
<li class="chapter" data-level="5.6.2" data-path="ols.html"><a href="ols.html#columns-perspective"><i class="fa fa-check"></i><b>5.6.2</b> Columns Perspective</a></li>
<li class="chapter" data-level="5.6.3" data-path="ols.html"><a href="ols.html#parameters-perspective"><i class="fa fa-check"></i><b>5.6.3</b> Parameters Perspective</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="gradient.html"><a href="gradient.html"><i class="fa fa-check"></i><b>6</b> Gradient Descent</a><ul>
<li class="chapter" data-level="6.1" data-path="gradient.html"><a href="gradient.html#error-surface"><i class="fa fa-check"></i><b>6.1</b> Error Surface</a></li>
<li class="chapter" data-level="6.2" data-path="gradient.html"><a href="gradient.html#idea-of-gradient-descent"><i class="fa fa-check"></i><b>6.2</b> Idea of Gradient Descent</a></li>
<li class="chapter" data-level="6.3" data-path="gradient.html"><a href="gradient.html#moving-down-an-error-surface"><i class="fa fa-check"></i><b>6.3</b> Moving Down an Error Surface</a><ul>
<li class="chapter" data-level="6.3.1" data-path="gradient.html"><a href="gradient.html#the-direction-of-mathbfv"><i class="fa fa-check"></i><b>6.3.1</b> The direction of <span class="math inline">\(\mathbf{v}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="gradient.html"><a href="gradient.html#gradient-descent-and-our-model"><i class="fa fa-check"></i><b>6.4</b> Gradient Descent and our Model</a></li>
</ul></li>
<li class="part"><span><b>V Learning Concepts</b></span></li>
<li class="chapter" data-level="7" data-path="learning.html"><a href="learning.html"><i class="fa fa-check"></i><b>7</b> Theoretical Framework</a><ul>
<li class="chapter" data-level="7.1" data-path="learning.html"><a href="learning.html#mental-map"><i class="fa fa-check"></i><b>7.1</b> Mental Map</a></li>
<li class="chapter" data-level="7.2" data-path="learning.html"><a href="learning.html#kinds-of-predictions"><i class="fa fa-check"></i><b>7.2</b> Kinds of Predictions</a><ul>
<li class="chapter" data-level="7.2.1" data-path="learning.html"><a href="learning.html#two-types-of-data"><i class="fa fa-check"></i><b>7.2.1</b> Two Types of Data</a></li>
<li class="chapter" data-level="7.2.2" data-path="learning.html"><a href="learning.html#two-types-of-predictions"><i class="fa fa-check"></i><b>7.2.2</b> Two Types of Predictions</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="learning.html"><a href="learning.html#two-types-of-errors"><i class="fa fa-check"></i><b>7.3</b> Two Types of Errors</a><ul>
<li class="chapter" data-level="7.3.1" data-path="learning.html"><a href="learning.html#individual-errors"><i class="fa fa-check"></i><b>7.3.1</b> Individual Errors</a></li>
<li class="chapter" data-level="7.3.2" data-path="learning.html"><a href="learning.html#overall-errors"><i class="fa fa-check"></i><b>7.3.2</b> Overall Errors</a></li>
<li class="chapter" data-level="7.3.3" data-path="learning.html"><a href="learning.html#auxiliary-technicality"><i class="fa fa-check"></i><b>7.3.3</b> Auxiliary Technicality</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="learning.html"><a href="learning.html#noisy-targets"><i class="fa fa-check"></i><b>7.4</b> Noisy Targets</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="mse.html"><a href="mse.html"><i class="fa fa-check"></i><b>8</b> MSE of Estimator</a><ul>
<li class="chapter" data-level="8.1" data-path="mse.html"><a href="mse.html#mse-of-an-estimator"><i class="fa fa-check"></i><b>8.1</b> MSE of an Estimator</a><ul>
<li class="chapter" data-level="8.1.1" data-path="mse.html"><a href="mse.html#prototypical-cases-of-bias-and-variance"><i class="fa fa-check"></i><b>8.1.1</b> Prototypical Cases of Bias and Variance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="biasvar.html"><a href="biasvar.html"><i class="fa fa-check"></i><b>9</b> Bias-Variance Tradeoff</a><ul>
<li class="chapter" data-level="9.1" data-path="biasvar.html"><a href="biasvar.html#introduction"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="biasvar.html"><a href="biasvar.html#motivation-example"><i class="fa fa-check"></i><b>9.2</b> Motivation Example</a><ul>
<li class="chapter" data-level="9.2.1" data-path="biasvar.html"><a href="biasvar.html#two-hypotheses"><i class="fa fa-check"></i><b>9.2.1</b> Two Hypotheses</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="biasvar.html"><a href="biasvar.html#learning-from-two-points"><i class="fa fa-check"></i><b>9.3</b> Learning from two points</a></li>
<li class="chapter" data-level="9.4" data-path="biasvar.html"><a href="biasvar.html#bias-variance-derivation"><i class="fa fa-check"></i><b>9.4</b> Bias-Variance Derivation</a><ul>
<li class="chapter" data-level="9.4.1" data-path="biasvar.html"><a href="biasvar.html#out-of-sample-predictions"><i class="fa fa-check"></i><b>9.4.1</b> Out-of-Sample Predictions</a></li>
<li class="chapter" data-level="9.4.2" data-path="biasvar.html"><a href="biasvar.html#noisy-target"><i class="fa fa-check"></i><b>9.4.2</b> Noisy Target</a></li>
<li class="chapter" data-level="9.4.3" data-path="biasvar.html"><a href="biasvar.html#types-of-theoretical-mses"><i class="fa fa-check"></i><b>9.4.3</b> Types of Theoretical MSEs</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="biasvar.html"><a href="biasvar.html#the-tradeoff"><i class="fa fa-check"></i><b>9.5</b> The Tradeoff</a><ul>
<li class="chapter" data-level="9.5.1" data-path="biasvar.html"><a href="biasvar.html#bias-variance-tradeoff-picture"><i class="fa fa-check"></i><b>9.5.1</b> Bias-Variance Tradeoff Picture</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="phases.html"><a href="phases.html"><i class="fa fa-check"></i><b>10</b> Learning Phases</a><ul>
<li class="chapter" data-level="10.1" data-path="phases.html"><a href="phases.html#introduction-1"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="phases.html"><a href="phases.html#model-assessment"><i class="fa fa-check"></i><b>10.2</b> Model Assessment</a><ul>
<li class="chapter" data-level="10.2.1" data-path="phases.html"><a href="phases.html#holdout-test-set"><i class="fa fa-check"></i><b>10.2.1</b> Holdout Test Set</a></li>
<li class="chapter" data-level="10.2.2" data-path="phases.html"><a href="phases.html#why-does-a-test-set-work"><i class="fa fa-check"></i><b>10.2.2</b> Why does a test set work?</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="phases.html"><a href="phases.html#model-selection"><i class="fa fa-check"></i><b>10.3</b> Model Selection</a><ul>
<li class="chapter" data-level="10.3.1" data-path="phases.html"><a href="phases.html#three-way-holdout-method"><i class="fa fa-check"></i><b>10.3.1</b> Three-way Holdout Method</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="phases.html"><a href="phases.html#model-training"><i class="fa fa-check"></i><b>10.4</b> Model Training</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="resampling.html"><a href="resampling.html"><i class="fa fa-check"></i><b>11</b> Resample Approaches</a><ul>
<li class="chapter" data-level="11.1" data-path="resampling.html"><a href="resampling.html#general-sampling-blueprint"><i class="fa fa-check"></i><b>11.1</b> General Sampling Blueprint</a></li>
<li class="chapter" data-level="11.2" data-path="resampling.html"><a href="resampling.html#monte-carlo-cross-validation"><i class="fa fa-check"></i><b>11.2</b> Monte Carlo Cross Validation</a></li>
<li class="chapter" data-level="11.3" data-path="resampling.html"><a href="resampling.html#bootstrap-method"><i class="fa fa-check"></i><b>11.3</b> Bootstrap Method</a></li>
<li class="chapter" data-level="11.4" data-path="resampling.html"><a href="resampling.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>11.4</b> <span class="math inline">\(K\)</span>-Fold Cross-Validation</a><ul>
<li class="chapter" data-level="11.4.1" data-path="resampling.html"><a href="resampling.html#leave-one-out-cross-validation-loocv"><i class="fa fa-check"></i><b>11.4.1</b> Leave-One-Out Cross Validation (LOOCV)</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>VI Regularization</b></span></li>
<li class="chapter" data-level="12" data-path="regular.html"><a href="regular.html"><i class="fa fa-check"></i><b>12</b> Regularization Techniques</a><ul>
<li class="chapter" data-level="12.1" data-path="regular.html"><a href="regular.html#multicollinearity-issues"><i class="fa fa-check"></i><b>12.1</b> Multicollinearity Issues</a><ul>
<li class="chapter" data-level="12.1.1" data-path="regular.html"><a href="regular.html#toy-example"><i class="fa fa-check"></i><b>12.1.1</b> Toy Example</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="regular.html"><a href="regular.html#irregular-coefficients"><i class="fa fa-check"></i><b>12.2</b> Irregular Coefficients</a></li>
<li class="chapter" data-level="12.3" data-path="regular.html"><a href="regular.html#connection-to-regularization"><i class="fa fa-check"></i><b>12.3</b> Connection to Regularization</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="pcr.html"><a href="pcr.html"><i class="fa fa-check"></i><b>13</b> Principal Components Regression</a><ul>
<li class="chapter" data-level="13.1" data-path="pcr.html"><a href="pcr.html#motivation-example-1"><i class="fa fa-check"></i><b>13.1</b> Motivation Example</a></li>
<li class="chapter" data-level="13.2" data-path="pcr.html"><a href="pcr.html#the-pcr-model"><i class="fa fa-check"></i><b>13.2</b> The PCR Model</a></li>
<li class="chapter" data-level="13.3" data-path="pcr.html"><a href="pcr.html#how-does-pcr-work"><i class="fa fa-check"></i><b>13.3</b> How does PCR work?</a><ul>
<li class="chapter" data-level="13.3.1" data-path="pcr.html"><a href="pcr.html#transition-formula"><i class="fa fa-check"></i><b>13.3.1</b> Transition Formula</a></li>
<li class="chapter" data-level="13.3.2" data-path="pcr.html"><a href="pcr.html#size-of-coefficients"><i class="fa fa-check"></i><b>13.3.2</b> Size of Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="pcr.html"><a href="pcr.html#selecting-number-of-pcs"><i class="fa fa-check"></i><b>13.4</b> Selecting Number of PCs</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="pls.html"><a href="pls.html"><i class="fa fa-check"></i><b>14</b> Partial Least Squares Regression</a><ul>
<li class="chapter" data-level="14.1" data-path="pls.html"><a href="pls.html#motivation-example-2"><i class="fa fa-check"></i><b>14.1</b> Motivation Example</a></li>
<li class="chapter" data-level="14.2" data-path="pls.html"><a href="pls.html#the-plsr-model"><i class="fa fa-check"></i><b>14.2</b> The PLSR Model</a></li>
<li class="chapter" data-level="14.3" data-path="pls.html"><a href="pls.html#how-does-plsr-work"><i class="fa fa-check"></i><b>14.3</b> How does PLSR work?</a></li>
<li class="chapter" data-level="14.4" data-path="pls.html"><a href="pls.html#plsr-algorithm"><i class="fa fa-check"></i><b>14.4</b> PLSR Algorithm</a><ul>
<li class="chapter" data-level="14.4.1" data-path="pls.html"><a href="pls.html#pls-solution-with-original-variables"><i class="fa fa-check"></i><b>14.4.1</b> PLS Solution with original variables</a></li>
<li class="chapter" data-level="14.4.2" data-path="pls.html"><a href="pls.html#size-of-coefficients-1"><i class="fa fa-check"></i><b>14.4.2</b> Size of Coefficients</a></li>
<li class="chapter" data-level="14.4.3" data-path="pls.html"><a href="pls.html#some-properties"><i class="fa fa-check"></i><b>14.4.3</b> Some Properties</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="pls.html"><a href="pls.html#selecting-number-of-pls-components"><i class="fa fa-check"></i><b>14.5</b> Selecting Number of PLS Components</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="ridge.html"><a href="ridge.html"><i class="fa fa-check"></i><b>15</b> Ridge Regression</a><ul>
<li class="chapter" data-level="15.1" data-path="ridge.html"><a href="ridge.html#a-new-minimization-problem"><i class="fa fa-check"></i><b>15.1</b> A New Minimization Problem</a><ul>
<li class="chapter" data-level="15.1.1" data-path="ridge.html"><a href="ridge.html#constraining-regression-coefficients"><i class="fa fa-check"></i><b>15.1.1</b> Constraining Regression Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="ridge.html"><a href="ridge.html#a-new-minimization-solution"><i class="fa fa-check"></i><b>15.2</b> A New Minimization Solution</a></li>
<li class="chapter" data-level="15.3" data-path="ridge.html"><a href="ridge.html#what-does-rr-accomplish"><i class="fa fa-check"></i><b>15.3</b> What does RR accomplish?</a><ul>
<li class="chapter" data-level="15.3.1" data-path="ridge.html"><a href="ridge.html#how-to-find-lambda"><i class="fa fa-check"></i><b>15.3.1</b> How to find <span class="math inline">\(\lambda\)</span>?</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>VII Extending Linear Regression</b></span></li>
<li class="chapter" data-level="16" data-path="linear-extensions.html"><a href="linear-extensions.html"><i class="fa fa-check"></i><b>16</b> Beyond Linear Regression</a><ul>
<li class="chapter" data-level="16.1" data-path="linear-extensions.html"><a href="linear-extensions.html#introduction-2"><i class="fa fa-check"></i><b>16.1</b> Introduction</a></li>
<li class="chapter" data-level="16.2" data-path="linear-extensions.html"><a href="linear-extensions.html#expanding-the-regression-horizon"><i class="fa fa-check"></i><b>16.2</b> Expanding the Regression Horizon</a><ul>
<li class="chapter" data-level="16.2.1" data-path="linear-extensions.html"><a href="linear-extensions.html#linearity"><i class="fa fa-check"></i><b>16.2.1</b> Linearity</a></li>
<li class="chapter" data-level="16.2.2" data-path="linear-extensions.html"><a href="linear-extensions.html#parametric-and-nonparametric"><i class="fa fa-check"></i><b>16.2.2</b> Parametric and Nonparametric</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="linear-extensions.html"><a href="linear-extensions.html#transforming-features"><i class="fa fa-check"></i><b>16.3</b> Transforming Features</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="nonparametric.html"><a href="nonparametric.html"><i class="fa fa-check"></i><b>17</b> Nonparametric Regression</a><ul>
<li class="chapter" data-level="17.1" data-path="nonparametric.html"><a href="nonparametric.html#conditional-averages"><i class="fa fa-check"></i><b>17.1</b> Conditional Averages</a></li>
<li class="chapter" data-level="17.2" data-path="nonparametric.html"><a href="nonparametric.html#looking-at-the-neighbors"><i class="fa fa-check"></i><b>17.2</b> Looking at the Neighbors</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="knn.html"><a href="knn.html"><i class="fa fa-check"></i><b>18</b> Nearest Neighbor Estimates</a><ul>
<li class="chapter" data-level="18.1" data-path="knn.html"><a href="knn.html#introduction-3"><i class="fa fa-check"></i><b>18.1</b> Introduction</a></li>
<li class="chapter" data-level="18.2" data-path="knn.html"><a href="knn.html#k-nearest-neighbors-knn"><i class="fa fa-check"></i><b>18.2</b> <span class="math inline">\(k\)</span> Nearest Neighbors (KNN)</a><ul>
<li class="chapter" data-level="18.2.1" data-path="knn.html"><a href="knn.html#distance-measures"><i class="fa fa-check"></i><b>18.2.1</b> Distance Measures</a></li>
<li class="chapter" data-level="18.2.2" data-path="knn.html"><a href="knn.html#knn-estimator"><i class="fa fa-check"></i><b>18.2.2</b> KNN Estimator</a></li>
<li class="chapter" data-level="18.2.3" data-path="knn.html"><a href="knn.html#how-to-find-k"><i class="fa fa-check"></i><b>18.2.3</b> How to find <span class="math inline">\(k\)</span>?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html"><i class="fa fa-check"></i><b>19</b> Kernel Smoothers</a><ul>
<li class="chapter" data-level="19.1" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#introduction-4"><i class="fa fa-check"></i><b>19.1</b> Introduction</a></li>
<li class="chapter" data-level="19.2" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#kernel-smoothers-1"><i class="fa fa-check"></i><b>19.2</b> Kernel Smoothers</a><ul>
<li class="chapter" data-level="19.2.1" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#kernel-functions"><i class="fa fa-check"></i><b>19.2.1</b> Kernel Functions</a></li>
<li class="chapter" data-level="19.2.2" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#weights-from-kernels"><i class="fa fa-check"></i><b>19.2.2</b> Weights from Kernels</a></li>
<li class="chapter" data-level="19.2.3" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#kernel-estimator"><i class="fa fa-check"></i><b>19.2.3</b> Kernel Estimator</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#local-polynomial-estimators"><i class="fa fa-check"></i><b>19.3</b> Local Polynomial Estimators</a></li>
</ul></li>
<li class="part"><span><b>VIII Classification</b></span></li>
<li class="chapter" data-level="20" data-path="classif.html"><a href="classif.html"><i class="fa fa-check"></i><b>20</b> Classification</a><ul>
<li class="chapter" data-level="20.1" data-path="classif.html"><a href="classif.html#introduction-5"><i class="fa fa-check"></i><b>20.1</b> Introduction</a><ul>
<li class="chapter" data-level="20.1.1" data-path="classif.html"><a href="classif.html#credit-score-example"><i class="fa fa-check"></i><b>20.1.1</b> Credit Score Example</a></li>
<li class="chapter" data-level="20.1.2" data-path="classif.html"><a href="classif.html#toy-example-1"><i class="fa fa-check"></i><b>20.1.2</b> Toy Example</a></li>
<li class="chapter" data-level="20.1.3" data-path="classif.html"><a href="classif.html#two-class-problem"><i class="fa fa-check"></i><b>20.1.3</b> Two-class Problem</a></li>
<li class="chapter" data-level="20.1.4" data-path="classif.html"><a href="classif.html#bayes-rule-reminder"><i class="fa fa-check"></i><b>20.1.4</b> Bayes’ Rule Reminder</a></li>
</ul></li>
<li class="chapter" data-level="20.2" data-path="classif.html"><a href="classif.html#bayes-classifier"><i class="fa fa-check"></i><b>20.2</b> Bayes Classifier</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="logistic.html"><a href="logistic.html"><i class="fa fa-check"></i><b>21</b> Logistic Regression</a><ul>
<li class="chapter" data-level="21.1" data-path="logistic.html"><a href="logistic.html#motivation-1"><i class="fa fa-check"></i><b>21.1</b> Motivation</a><ul>
<li class="chapter" data-level="21.1.1" data-path="logistic.html"><a href="logistic.html#first-approach-fitting-a-line"><i class="fa fa-check"></i><b>21.1.1</b> First Approach: Fitting a Line</a></li>
<li class="chapter" data-level="21.1.2" data-path="logistic.html"><a href="logistic.html#second-approach-harsh-thresholding"><i class="fa fa-check"></i><b>21.1.2</b> Second Approach: Harsh Thresholding</a></li>
<li class="chapter" data-level="21.1.3" data-path="logistic.html"><a href="logistic.html#third-approach-conditional-means"><i class="fa fa-check"></i><b>21.1.3</b> Third Approach: Conditional Means</a></li>
</ul></li>
<li class="chapter" data-level="21.2" data-path="logistic.html"><a href="logistic.html#logistic-regression-model"><i class="fa fa-check"></i><b>21.2</b> Logistic Regression Model</a><ul>
<li class="chapter" data-level="21.2.1" data-path="logistic.html"><a href="logistic.html#the-criterion-being-optimized"><i class="fa fa-check"></i><b>21.2.1</b> The Criterion Being Optimized</a></li>
<li class="chapter" data-level="21.2.2" data-path="logistic.html"><a href="logistic.html#another-way-to-solve-logistic-regression"><i class="fa fa-check"></i><b>21.2.2</b> Another Way to Solve Logistic Regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="22" data-path="discrim.html"><a href="discrim.html"><i class="fa fa-check"></i><b>22</b> Preamble for Discriminant Analysis</a><ul>
<li class="chapter" data-level="22.1" data-path="discrim.html"><a href="discrim.html#motivation-2"><i class="fa fa-check"></i><b>22.1</b> Motivation</a><ul>
<li class="chapter" data-level="22.1.1" data-path="discrim.html"><a href="discrim.html#distinguishing-species"><i class="fa fa-check"></i><b>22.1.1</b> Distinguishing Species</a></li>
<li class="chapter" data-level="22.1.2" data-path="discrim.html"><a href="discrim.html#sum-of-squares-decomposition"><i class="fa fa-check"></i><b>22.1.2</b> Sum of Squares Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="22.2" data-path="discrim.html"><a href="discrim.html#derived-ratios-from-sum-of-squares"><i class="fa fa-check"></i><b>22.2</b> Derived Ratios from Sum-of-Squares</a><ul>
<li class="chapter" data-level="22.2.1" data-path="discrim.html"><a href="discrim.html#correlation-ratio"><i class="fa fa-check"></i><b>22.2.1</b> Correlation Ratio</a></li>
<li class="chapter" data-level="22.2.2" data-path="discrim.html"><a href="discrim.html#f-ratio"><i class="fa fa-check"></i><b>22.2.2</b> F-Ratio</a></li>
</ul></li>
<li class="chapter" data-level="22.3" data-path="discrim.html"><a href="discrim.html#geometric-perspective"><i class="fa fa-check"></i><b>22.3</b> Geometric Perspective</a><ul>
<li class="chapter" data-level="22.3.1" data-path="discrim.html"><a href="discrim.html#clouds-from-class-structure"><i class="fa fa-check"></i><b>22.3.1</b> Clouds from Class Structure</a></li>
<li class="chapter" data-level="22.3.2" data-path="discrim.html"><a href="discrim.html#dispersion-decomposition"><i class="fa fa-check"></i><b>22.3.2</b> Dispersion Decomposition</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="23" data-path="cda.html"><a href="cda.html"><i class="fa fa-check"></i><b>23</b> Canonical Discriminant Analysis</a><ul>
<li class="chapter" data-level="23.1" data-path="cda.html"><a href="cda.html#cda-semi-supervised-aspect"><i class="fa fa-check"></i><b>23.1</b> CDA: Semi-Supervised Aspect</a></li>
<li class="chapter" data-level="23.2" data-path="cda.html"><a href="cda.html#looking-for-a-discriminant-axis"><i class="fa fa-check"></i><b>23.2</b> Looking for a discriminant axis</a></li>
<li class="chapter" data-level="23.3" data-path="cda.html"><a href="cda.html#looking-for-a-compromise-criterion"><i class="fa fa-check"></i><b>23.3</b> Looking for a Compromise Criterion</a><ul>
<li class="chapter" data-level="23.3.1" data-path="cda.html"><a href="cda.html#correlation-ratio-criterion"><i class="fa fa-check"></i><b>23.3.1</b> Correlation Ratio Criterion</a></li>
<li class="chapter" data-level="23.3.2" data-path="cda.html"><a href="cda.html#f-ratio-criterion"><i class="fa fa-check"></i><b>23.3.2</b> F-ratio Criterion</a></li>
<li class="chapter" data-level="23.3.3" data-path="cda.html"><a href="cda.html#a-special-pca"><i class="fa fa-check"></i><b>23.3.3</b> A Special PCA</a></li>
</ul></li>
<li class="chapter" data-level="23.4" data-path="cda.html"><a href="cda.html#cda-supervised-aspect"><i class="fa fa-check"></i><b>23.4</b> CDA: Supervised Aspect</a><ul>
<li class="chapter" data-level="23.4.1" data-path="cda.html"><a href="cda.html#distance-behind-cda"><i class="fa fa-check"></i><b>23.4.1</b> Distance behind CDA</a></li>
<li class="chapter" data-level="23.4.2" data-path="cda.html"><a href="cda.html#predictive-idea"><i class="fa fa-check"></i><b>23.4.2</b> Predictive Idea</a></li>
<li class="chapter" data-level="23.4.3" data-path="cda.html"><a href="cda.html#cda-classifier"><i class="fa fa-check"></i><b>23.4.3</b> CDA Classifier</a></li>
<li class="chapter" data-level="23.4.4" data-path="cda.html"><a href="cda.html#limitations-of-cda-classifier"><i class="fa fa-check"></i><b>23.4.4</b> Limitations of CDA classifier</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="24" data-path="discanalysis.html"><a href="discanalysis.html"><i class="fa fa-check"></i><b>24</b> Discriminant Analysis</a><ul>
<li class="chapter" data-level="24.1" data-path="discanalysis.html"><a href="discanalysis.html#probabilistic-da"><i class="fa fa-check"></i><b>24.1</b> Probabilistic DA</a><ul>
<li class="chapter" data-level="24.1.1" data-path="discanalysis.html"><a href="discanalysis.html#normal-distributions"><i class="fa fa-check"></i><b>24.1.1</b> Normal Distributions</a></li>
<li class="chapter" data-level="24.1.2" data-path="discanalysis.html"><a href="discanalysis.html#estimating-parameters-of-normal-distributions"><i class="fa fa-check"></i><b>24.1.2</b> Estimating Parameters of Normal Distributions</a></li>
</ul></li>
<li class="chapter" data-level="24.2" data-path="discanalysis.html"><a href="discanalysis.html#discriminant-functions"><i class="fa fa-check"></i><b>24.2</b> Discriminant Functions</a></li>
<li class="chapter" data-level="24.3" data-path="discanalysis.html"><a href="discanalysis.html#quadratic-discriminant-analysis-qda"><i class="fa fa-check"></i><b>24.3</b> Quadratic Discriminant Analysis (QDA)</a></li>
<li class="chapter" data-level="24.4" data-path="discanalysis.html"><a href="discanalysis.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>24.4</b> Linear Discriminant Analysis</a><ul>
<li class="chapter" data-level="24.4.1" data-path="discanalysis.html"><a href="discanalysis.html#canonical-discriminant-analysis"><i class="fa fa-check"></i><b>24.4.1</b> Canonical Discriminant Analysis</a></li>
<li class="chapter" data-level="24.4.2" data-path="discanalysis.html"><a href="discanalysis.html#naive-bayes"><i class="fa fa-check"></i><b>24.4.2</b> Naive Bayes</a></li>
<li class="chapter" data-level="24.4.3" data-path="discanalysis.html"><a href="discanalysis.html#fifth-case"><i class="fa fa-check"></i><b>24.4.3</b> Fifth Case</a></li>
</ul></li>
<li class="chapter" data-level="24.5" data-path="discanalysis.html"><a href="discanalysis.html#comparing-the-cases"><i class="fa fa-check"></i><b>24.5</b> Comparing the Cases</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="classperformance.html"><a href="classperformance.html"><i class="fa fa-check"></i><b>25</b> Performance of Classifiers</a><ul>
<li class="chapter" data-level="25.1" data-path="classperformance.html"><a href="classperformance.html#classification-error-measures"><i class="fa fa-check"></i><b>25.1</b> Classification Error Measures</a><ul>
<li class="chapter" data-level="25.1.1" data-path="classperformance.html"><a href="classperformance.html#errors-for-binary-response"><i class="fa fa-check"></i><b>25.1.1</b> Errors for Binary Response</a></li>
<li class="chapter" data-level="25.1.2" data-path="classperformance.html"><a href="classperformance.html#error-for-categorical-response"><i class="fa fa-check"></i><b>25.1.2</b> Error for Categorical Response</a></li>
</ul></li>
<li class="chapter" data-level="25.2" data-path="classperformance.html"><a href="classperformance.html#confusion-matrices"><i class="fa fa-check"></i><b>25.2</b> Confusion Matrices</a></li>
<li class="chapter" data-level="25.3" data-path="classperformance.html"><a href="classperformance.html#binary-response-example"><i class="fa fa-check"></i><b>25.3</b> Binary Response Example</a><ul>
<li class="chapter" data-level="25.3.1" data-path="classperformance.html"><a href="classperformance.html#application-for-checking-account"><i class="fa fa-check"></i><b>25.3.1</b> Application for Checking Account</a></li>
<li class="chapter" data-level="25.3.2" data-path="classperformance.html"><a href="classperformance.html#application-for-loan"><i class="fa fa-check"></i><b>25.3.2</b> Application for Loan</a></li>
</ul></li>
<li class="chapter" data-level="25.4" data-path="classperformance.html"><a href="classperformance.html#decision-rules-and-errors"><i class="fa fa-check"></i><b>25.4</b> Decision Rules and Errors</a></li>
<li class="chapter" data-level="25.5" data-path="classperformance.html"><a href="classperformance.html#roc-curves"><i class="fa fa-check"></i><b>25.5</b> ROC Curves</a><ul>
<li class="chapter" data-level="25.5.1" data-path="classperformance.html"><a href="classperformance.html#graphing-roc-curves"><i class="fa fa-check"></i><b>25.5.1</b> Graphing ROC curves</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IX Unsupervised II: Clustering</b></span></li>
<li class="chapter" data-level="26" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>26</b> Clustering</a><ul>
<li class="chapter" data-level="26.1" data-path="clustering.html"><a href="clustering.html#about-clustering"><i class="fa fa-check"></i><b>26.1</b> About Clustering</a><ul>
<li class="chapter" data-level="26.1.1" data-path="clustering.html"><a href="clustering.html#types-of-clustering"><i class="fa fa-check"></i><b>26.1.1</b> Types of Clustering</a></li>
<li class="chapter" data-level="26.1.2" data-path="clustering.html"><a href="clustering.html#hard-clustering"><i class="fa fa-check"></i><b>26.1.2</b> Hard Clustering</a></li>
</ul></li>
<li class="chapter" data-level="26.2" data-path="clustering.html"><a href="clustering.html#dispersion-measures"><i class="fa fa-check"></i><b>26.2</b> Dispersion Measures</a></li>
<li class="chapter" data-level="26.3" data-path="clustering.html"><a href="clustering.html#complexity-in-clustering"><i class="fa fa-check"></i><b>26.3</b> Complexity in Clustering</a></li>
</ul></li>
<li class="chapter" data-level="27" data-path="kmeans.html"><a href="kmeans.html"><i class="fa fa-check"></i><b>27</b> K-Means</a><ul>
<li class="chapter" data-level="27.1" data-path="kmeans.html"><a href="kmeans.html#toy-example-2"><i class="fa fa-check"></i><b>27.1</b> Toy Example</a></li>
<li class="chapter" data-level="27.2" data-path="kmeans.html"><a href="kmeans.html#what-does-k-means-do"><i class="fa fa-check"></i><b>27.2</b> What does K-means do?</a></li>
<li class="chapter" data-level="27.3" data-path="kmeans.html"><a href="kmeans.html#k-means-algorithms"><i class="fa fa-check"></i><b>27.3</b> K-Means Algorithms</a><ul>
<li class="chapter" data-level="27.3.1" data-path="kmeans.html"><a href="kmeans.html#classic-version"><i class="fa fa-check"></i><b>27.3.1</b> Classic Version</a></li>
<li class="chapter" data-level="27.3.2" data-path="kmeans.html"><a href="kmeans.html#moving-centers-algorithm"><i class="fa fa-check"></i><b>27.3.2</b> Moving Centers Algorithm</a></li>
<li class="chapter" data-level="27.3.3" data-path="kmeans.html"><a href="kmeans.html#dynamic-clouds"><i class="fa fa-check"></i><b>27.3.3</b> Dynamic Clouds</a></li>
<li class="chapter" data-level="27.3.4" data-path="kmeans.html"><a href="kmeans.html#choosing-k"><i class="fa fa-check"></i><b>27.3.4</b> Choosing <span class="math inline">\(K\)</span></a></li>
<li class="chapter" data-level="27.3.5" data-path="kmeans.html"><a href="kmeans.html#comments"><i class="fa fa-check"></i><b>27.3.5</b> Comments</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="28" data-path="hclus.html"><a href="hclus.html"><i class="fa fa-check"></i><b>28</b> Hierarchical Clustering</a><ul>
<li class="chapter" data-level="28.1" data-path="hclus.html"><a href="hclus.html#agglomerative-methods"><i class="fa fa-check"></i><b>28.1</b> Agglomerative Methods</a></li>
<li class="chapter" data-level="28.2" data-path="hclus.html"><a href="hclus.html#example-single-linkage"><i class="fa fa-check"></i><b>28.2</b> Example: Single Linkage</a><ul>
<li class="chapter" data-level="28.2.1" data-path="hclus.html"><a href="hclus.html#dendrogram"><i class="fa fa-check"></i><b>28.2.1</b> Dendrogram</a></li>
</ul></li>
<li class="chapter" data-level="28.3" data-path="hclus.html"><a href="hclus.html#example-complete-linkage"><i class="fa fa-check"></i><b>28.3</b> Example: Complete Linkage</a><ul>
<li class="chapter" data-level="28.3.1" data-path="hclus.html"><a href="hclus.html#cutting-dendograms"><i class="fa fa-check"></i><b>28.3.1</b> Cutting Dendograms</a></li>
<li class="chapter" data-level="28.3.2" data-path="hclus.html"><a href="hclus.html#pros-and-coons"><i class="fa fa-check"></i><b>28.3.2</b> Pros and COons</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>X Tree-based Methods</b></span></li>
<li class="chapter" data-level="29" data-path="trees.html"><a href="trees.html"><i class="fa fa-check"></i><b>29</b> Intro to Decision Trees</a><ul>
<li class="chapter" data-level="29.1" data-path="trees.html"><a href="trees.html#introduction-6"><i class="fa fa-check"></i><b>29.1</b> Introduction</a></li>
<li class="chapter" data-level="29.2" data-path="trees.html"><a href="trees.html#some-terminology"><i class="fa fa-check"></i><b>29.2</b> Some Terminology</a></li>
<li class="chapter" data-level="29.3" data-path="trees.html"><a href="trees.html#binary-trees"><i class="fa fa-check"></i><b>29.3</b> Binary Trees</a></li>
<li class="chapter" data-level="29.4" data-path="trees.html"><a href="trees.html#space-partitions"><i class="fa fa-check"></i><b>29.4</b> Space Partitions</a><ul>
<li class="chapter" data-level="29.4.1" data-path="trees.html"><a href="trees.html#the-process-of-building-a-tree"><i class="fa fa-check"></i><b>29.4.1</b> The Process of Building a Tree</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="30" data-path="tree-basics.html"><a href="tree-basics.html"><i class="fa fa-check"></i><b>30</b> Building Trees</a><ul>
<li class="chapter" data-level="30.1" data-path="tree-basics.html"><a href="tree-basics.html#binary-partitions"><i class="fa fa-check"></i><b>30.1</b> Binary Partitions</a><ul>
<li class="chapter" data-level="30.1.1" data-path="tree-basics.html"><a href="tree-basics.html#splits-of-binary-variables"><i class="fa fa-check"></i><b>30.1.1</b> Splits of Binary variables</a></li>
<li class="chapter" data-level="30.1.2" data-path="tree-basics.html"><a href="tree-basics.html#splits-of-nominal-variables"><i class="fa fa-check"></i><b>30.1.2</b> Splits of Nominal Variables</a></li>
<li class="chapter" data-level="30.1.3" data-path="tree-basics.html"><a href="tree-basics.html#splits-of-ordinal-variables"><i class="fa fa-check"></i><b>30.1.3</b> Splits of Ordinal Variables</a></li>
<li class="chapter" data-level="30.1.4" data-path="tree-basics.html"><a href="tree-basics.html#splits-continuous-variables"><i class="fa fa-check"></i><b>30.1.4</b> Splits Continuous variables</a></li>
</ul></li>
<li class="chapter" data-level="30.2" data-path="tree-basics.html"><a href="tree-basics.html#measures-of-heterogeneity"><i class="fa fa-check"></i><b>30.2</b> Measures of Heterogeneity</a><ul>
<li class="chapter" data-level="30.2.1" data-path="tree-basics.html"><a href="tree-basics.html#entropy"><i class="fa fa-check"></i><b>30.2.1</b> Entropy</a></li>
<li class="chapter" data-level="30.2.2" data-path="tree-basics.html"><a href="tree-basics.html#the-math-behind-entropy"><i class="fa fa-check"></i><b>30.2.2</b> The Math Behind Entropy</a></li>
<li class="chapter" data-level="30.2.3" data-path="tree-basics.html"><a href="tree-basics.html#gini-impurity"><i class="fa fa-check"></i><b>30.2.3</b> Gini Impurity</a></li>
<li class="chapter" data-level="30.2.4" data-path="tree-basics.html"><a href="tree-basics.html#toy-example-3"><i class="fa fa-check"></i><b>30.2.4</b> Toy Example</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Made with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">All Models Are Wrong: Concepts of Statistical Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="duality" class="section level1">
<h1><span class="header-section-number">3</span> Geometric Duality</h1>
<p>Before discussing unsupervised as well as supervised learning methods,
we prefer to give you a prelude by
talking and thinking about data in a geometric sense. This chapter will set
the stage for most of the topics covered in later chapters.</p>
<p>Let’s suppose we have some data in the form of a data matrix. For convenience
purposes, let’s also suppose that all variables are measured in a real-value
scale.
Obviously not all data is expressed or even encoded numerically. You may have
categorical or symbolic data. But for this illustration, let’s assume that any
categorical and symbolic data has already been transformed into a numeric scale
(e.g. dummy indicators, optimal scaling).</p>
<p>It’s very enlightening to think of a data matrix as viewed from the glass of
Geometry. The key idea is to think of the data in a matrix as elements living
in a multidimensional space. Actually, we can regard a data matrix from two
apparently different perspectives that, in reality, are intimately connected:
the <strong>rows perspective</strong> and the <strong>columns perspective</strong>. In order to explain
these perspectives, let’s use the following diagram of a data matrix <span class="math inline">\(\mathbf{X}\)</span>
with <span class="math inline">\(n\)</span> rows and <span class="math inline">\(p\)</span> columns, with <span class="math inline">\(x_{ij}\)</span> representing the element in the
<span class="math inline">\(i\)</span>-th row and <span class="math inline">\(j\)</span>-th column.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-8"></span>
<img src="images/duality/data-perspectives.svg" alt="Duality of a data matrix" width="70%" />
<p class="caption">
Figure 3.1: Duality of a data matrix
</p>
</div>
<p>When we look at a data matrix from the <em>columns</em> perpective what we are doing is
focusing on the <span class="math inline">\(p\)</span> variables. In a similar way, when looking at a data matrix
from its <em>rows</em> perspective, we are focusing on the <span class="math inline">\(n\)</span> individuals.
Like a coin, though, this matrix has two sides: a rows side, and a columns side.
That is, we could look at the data from the rows point of view, or the columns
point of view. These two views are (of course) not completely independent.
This double perspective or <strong>duality</strong> for short, is like the two sides of the
same coin.</p>
<div id="rows-space" class="section level2">
<h2><span class="header-section-number">3.1</span> Rows Space</h2>
<p>We know that human vision is limited to three-dimensions, but pretend that you
had superpowers that let you visualize a space with any number of dimensions.</p>
<p>Because each row of the data matrix has <span class="math inline">\(p\)</span> elements, we can regard individuals
as objects that live in a <span class="math inline">\(p\)</span>-dimensional space. For visualization purposes,
think of each variable as playing the role of a dimension associated to a given
axis in this space; likewise, consider each of the <span class="math inline">\(n\)</span> individuals as being
depicted as a point (or particle) in such space, like in the following diagram:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-9"></span>
<img src="images/duality/rows-space.svg" alt="Rows space" width="85%" />
<p class="caption">
Figure 3.2: Rows space
</p>
</div>
<p>In the figure above, even though we are showing only three axes, you should
pretend that you are visualizing a <span class="math inline">\(p\)</span>-dimensional space (imaging that there
are <span class="math inline">\(p\)</span> axes).
Each point in this space corresponds to a single individual, and they all form
what we can call a <em>cloud of points</em>.</p>
</div>
<div id="columns-space" class="section level2">
<h2><span class="header-section-number">3.2</span> Columns Space</h2>
<p>We can do the same visual exercise with the columns of a data matrix. Since each
variable has <span class="math inline">\(n\)</span> elements, we can regard the set of <span class="math inline">\(p\)</span> variables as objects
that live in an <span class="math inline">\(n\)</span>-dimensional space. However, instead of representing each
variable with a dot, it’s better to graphically represent them with an arrow
(or vector). Why? Because of two reasons: one is to distinguish them from the
individuals (dots). But more important, because the esential thing with a variable
is not really its magnitude (and therefore its position) but its direction.
Often, as part of the preprocessing steps we apply transformations on variables
that change their scales (e.g. shrinking them, or stretching them) without
modifying their directions.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-10"></span>
<img src="images/duality/columns-space.svg" alt="Columns space" width="85%" />
<p class="caption">
Figure 3.3: Columns space
</p>
</div>
<p>Analogously to the rows space and its cloud of individuals, you should also
pretend that the image above is displaying an <span class="math inline">\(n\)</span>-dimensional space with
a bunch of blue arrows pointing in various directions.</p>
<div id="whats-next" class="section level4 unnumbered">
<h4>What’s next?</h4>
<p>Now that we know how to think of data from a geometric perspective, the next
step is to discuss a handful of common operations that can be performed with
points and vectors that live in some geometric space.</p>
<hr />
</div>
</div>
<div id="cloud-of-individuals" class="section level2">
<h2><span class="header-section-number">3.3</span> Cloud of Individuals</h2>
<p>In the previous chapter we introduce the powerful idea of looking at the
rows and columns of a data matrix from the lens of geometry.
We are assuming in general that the rows have to do with <span class="math inline">\(n\)</span> individuals that
lie in a <span class="math inline">\(p\)</span>-dimensional space.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-11"></span>
<img src="images/duality/cloud-obs-centering0.svg" alt="Cloud of points" width="55%" />
<p class="caption">
Figure 3.4: Cloud of points
</p>
</div>
<p>Let’s start describing a set of common operations that we can apply on the
individuals (living in a <span class="math inline">\(p\)</span>-dimensional space).</p>
<div id="average-individual" class="section level3">
<h3><span class="header-section-number">3.3.1</span> Average Individual</h3>
<p>We can ask about the typical or average individual.</p>
<p>If you only have one variable, then all the individual points lie in a
one-dimensional space, which is basically a line:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-12"></span>
<img src="images/duality/mean-number-line.svg" alt="Points in one dimension" width="30%" />
<p class="caption">
Figure 3.5: Points in one dimension
</p>
</div>
<p>In this case, the average individual is simply the average of the values, which
geometrically corresponds to the balancing point:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-13"></span>
<img src="images/duality/mean-balancing.svg" alt="Average individual" width="70%" />
<p class="caption">
Figure 3.6: Average individual
</p>
</div>
<p>Algebraically we have: individuals <span class="math inline">\(x_1, x_2, \dots, x_n\)</span>, and the average is:</p>
<p><span class="math display">\[
\bar{x} = \frac{x_1 + \dots + x_n}{n} = \frac{1}{n} \sum_{i=1}^{n} x_i
\]</span></p>
<p>In vector notation, the average can be calculated with an inner product between
<span class="math inline">\(\mathbf{x} = (x_1, x_2, \dots, x_n)\)</span>, and a constant vector of <span class="math inline">\(n\)</span>-ones <span class="math inline">\(\mathbf{1}\)</span>:</p>
<p><span class="math display">\[
\bar{x} = \mathbf{x^\mathsf{T}1}
\]</span></p>
<p>What about the multivariate case? It turns out that we can also ask about the
average individual of a cloud of points, like in the following figure:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-14"></span>
<img src="images/duality/cloud-obs-centering1.svg" alt="Cloud of points with centroid (i.e. average individual)" width="55%" />
<p class="caption">
Figure 3.7: Cloud of points with centroid (i.e. average individual)
</p>
</div>
<p>The average individual, in a <span class="math inline">\(p\)</span>-dimensional space is the point <span class="math inline">\(\mathbf{\vec{g}}\)</span>
containing as coordiantes the averages of all the variables:</p>
<p><span class="math display">\[
\mathbf{\vec{g}} = (\bar{x}_1, \bar{x}_2, \dots, \bar{x}_j)
\]</span></p>
<p>where <span class="math inline">\(\bar{x}_j\)</span> is the average of the <span class="math inline">\(j\)</span>-th variable.</p>
<p>This average individual <span class="math inline">\(\mathbf{\vec{g}}\)</span> is also known as the <strong>centroid</strong>,
<em>barycenter</em>, or <em>center of gravity</em> of the cloud of points.</p>
</div>
<div id="centered-data" class="section level3">
<h3><span class="header-section-number">3.3.2</span> Centered Data</h3>
<p>Often, it is convenient to transform the data in such a way that the centroid of
a data set becomes the origin of the cloud of points. Geometrically, this type
of transformation involves a shif of the axes in the <span class="math inline">\(p\)</span>-dimensional space.
Algebraically, this transformation corresponds to expresing the values of
each variable in terms of deviations from their means.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-15"></span>
<img src="images/duality/cloud-obs-centering2.svg" alt="Cloud of points of mean-centered data" width="55%" />
<p class="caption">
Figure 3.8: Cloud of points of mean-centered data
</p>
</div>
</div>
<div id="distance-between-individuals" class="section level3">
<h3><span class="header-section-number">3.3.3</span> Distance between individuals</h3>
<p>Another common operation that we may be interested in is the distance between
two individuals. Obviously the notion of distance is not unique, since you can
choose different types of distance measures. Perhaps the most comon type of
distance is the (squared) Euclidean distance. Unless otherwise mentioned, this
will be the default distance used in this book.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-16"></span>
<img src="images/duality/cloud-obs-distance.svg" alt="Distance between two individuals" width="55%" />
<p class="caption">
Figure 3.9: Distance between two individuals
</p>
</div>
<p>If you have one variable <span class="math inline">\(X\)</span>, then the squared distance <span class="math inline">\(d^2(i,\ell)\)</span> between
two individuals <span class="math inline">\(x_i\)</span> and <span class="math inline">\(x_\ell\)</span> is:</p>
<p><span class="math display">\[
d^2(i,\ell) = (x_i - x_\ell)^2
\]</span></p>
<p>In general, with <span class="math inline">\(p\)</span> variables, the squared distance between the <span class="math inline">\(i\)</span>-th individual
and the <span class="math inline">\(\ell\)</span>-th individual is:</p>
<p><span class="math display">\[\begin{align*}
d^2(i,\ell) &amp;= (x_{i1} - x_{\ell 1})^2 + (x_{i2} - x_{\ell 2})^2 + \dots + (x_{ip} - x_{\ell p})^2 \\
&amp;= (\mathbf{\vec{x}_i} - \mathbf{\vec{x}_\ell})^\mathsf{T} (\mathbf{\vec{x}_i} - \mathbf{\vec{x}_\ell})
\end{align*}\]</span></p>
</div>
<div id="distance-to-the-centroid" class="section level3">
<h3><span class="header-section-number">3.3.4</span> Distance to the centroid</h3>
<p>A special case is the distance between any individual <span class="math inline">\(i\)</span> and the average individual:</p>
<p><span class="math display">\[\begin{align*}
d^2(i,g) &amp;= (x_{i1} - \bar{x}_1)^2 + (x_{i2} - \bar{x}_2)^2 + \dots + (x_{ip} - \bar{x}_p)^2 \\
&amp;= (\mathbf{\vec{x}_i} - \mathbf{\vec{g}})^\mathsf{T} (\mathbf{\vec{x}_i} - \mathbf{\vec{g}})
\end{align*}\]</span></p>
</div>
<div id="measures-of-dispersion" class="section level3">
<h3><span class="header-section-number">3.3.5</span> Measures of Dispersion</h3>
<p>What else can we calculate with the individuals? Think about it. So far we’ve
seen how to calculate the average individual, as well as distances between
individuals. The average individual or centroid plays the role of a measure
of center. And everytime you get a measure of center, it makes sense to get a
measure of spread.</p>
<div id="overall-dispersion" class="section level4 unnumbered">
<h4>Overall Dispersion</h4>
<p>One way to compute a measure of scatter among individuals is to consider all the
squared distances between pairs of individuals. For instance, say you have three
individuals <span class="math inline">\(a\)</span>, <span class="math inline">\(b\)</span>, and <span class="math inline">\(c\)</span>. We can calculate all pairwise distances and add
them up:</p>
<p><span class="math display">\[
d^2(a,b) + d^2(b,a) + d^2(a,c) + d^2(c,a) + d^2(b,c) + d^2(c,b)
\]</span></p>
<p>In general, when you have <span class="math inline">\(n\)</span> individuals, you can obtain up to <span class="math inline">\(n^2\)</span> squared
distances. We will give the generic name of <strong>Overall Dispersion</strong> to the sum
of all squared pairwise distances:</p>
<p><span class="math display">\[
\text{overall dispersion} = \sum_{i=1}^{n} \sum_{\ell=1}^{n} d^2(i,\ell)
\]</span></p>
</div>
<div id="inertia" class="section level4 unnumbered">
<h4>Inertia</h4>
<p>Another measure of scatter among individuals can be computed by adding the
distances between all individuals and the centroid.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-17"></span>
<img src="images/duality/cloud-inertia.svg" alt="Inertia" width="55%" />
<p class="caption">
Figure 3.10: Inertia
</p>
</div>
<p>The sum of squared distances from each point to the centroid then becomes</p>
<p><span class="math display">\[
\frac{1}{n} \sum_{i=1}^{n} d^2(i,g) = \frac{1}{n} \sum_{i=1}^{n} (\mathbf{\vec{x}_i} - \mathbf{\vec{g}})^\mathsf{T} (\mathbf{\vec{x}_i} - \mathbf{\vec{g}})
\]</span></p>
<p>We will name this measure <strong>Inertia</strong>, borrowing this term from the concept of
inertia used in mechanics (in physics).</p>
<p><span class="math display">\[
\text{Inertia} = \frac{1}{n} \sum_{i=1}^{n} d^2(i,g)
\]</span></p>
<p>What is the motivation behind this measure? Consider the <span class="math inline">\(p = 1\)</span> case; i.e. 
when <span class="math inline">\(\mathbf{X}\)</span> is simply a column vector</p>
<p><span class="math display">\[
\mathbf{X} = \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \\ \end{pmatrix}
\]</span></p>
<p>The centroid will simply be the mean of these points: i.e. 
<span class="math inline">\(\bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i\)</span>.</p>
<p>The sum of squared-distances from each point to the centroid then becomes:</p>
<p><span class="math display">\[
(x_1 - \bar{x})^2 + (x_2 - \bar{x})^2 + \dots + (x_n - \bar{x})^2 = \sum_{i=1}^{n} (x_i - \bar{x})^2
\]</span></p>
<p>Does the above formula look familiar? What if we take the average of the squared
distances:</p>
<p><span class="math display">\[
\frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})^2 = \frac{(x_1 - \bar{x})^2 + (x_2 - \bar{x})^2 + \dots + (x_n - \bar{x})^2}{n}
\]</span></p>
<p>Same question: Do you recognize this formula? You better do…
This is nothing else than the
formula of the variance of <span class="math inline">\(X\)</span>. And yes, we are dividing by <span class="math inline">\(n\)</span> (not by <span class="math inline">\(n-1\)</span>).
Hence, you can think of inertia as a multidimensional extension of variance,
which gives the typical squared distance around the centroid.</p>
</div>
<div id="overall-dispersion-and-inertia" class="section level4 unnumbered">
<h4>Overall Dispersion and Inertia</h4>
<p>Interestingly, the <em>overall dispersion</em> and the <em>inertia</em> are connected through
the following relation:</p>
<p><span class="math display">\[\begin{align*}
\text{overall dispersion} &amp;= \sum_{i=1}^{n} \sum_{\ell=1}^{n} d^2(i,\ell) \\
&amp;= 2n \sum_{i=1}^{n} d^2(i,g) \\
&amp;= (2n^2) \text{Inertia}
\end{align*}\]</span></p>
<p>The proof of this relation is left as a homework exercise.</p>
<hr />
</div>
</div>
</div>
<div id="cloud-of-variables" class="section level2">
<h2><span class="header-section-number">3.4</span> Cloud of Variables</h2>
<p>The starting point when analyzing variables involves computing various summary
measures—such as means, and variances—to get an idea of the common or central
values, and the amount of variability of each variable. In this chapter we will
review how concepts like the mean of a variable, the variance, covariance,
and correlation, can be interpreted in a geometric sense, as well as their
expressions in terms of vector-matrix operations.</p>
<div id="mean-of-a-variable" class="section level3">
<h3><span class="header-section-number">3.4.1</span> Mean of a Variable</h3>
<p>To measure variation, we usually begin by calculating a “typical” value.
The idea is to summarize the values of a variable with one or two representative
values. You will find this notion under several terms like measures of center,
location, central tendency, or centrality.</p>
<p>The prototypical summary value of center is the <strong>mean</strong>, sometimes referred to
as average. The mean of an <span class="math inline">\(n-\)</span>element variable <span class="math inline">\(X = (x_1, x_2, \dots, x_n)\)</span>,
represented by <span class="math inline">\(\bar{x}\)</span>, is obtained by adding all the <span class="math inline">\(x_i\)</span> values and then
dividing by their total number <span class="math inline">\(n\)</span>:</p>
<p><span class="math display">\[
\bar{x} = \frac{x_1 + x_2 + \dots + x_n}{n}
\]</span></p>
<p>Using summation notation we can express <span class="math inline">\(\bar{x}\)</span> in a very compact way as:</p>
<p><span class="math display">\[
\bar{x} = \frac{1}{n} \sum_{i = 1}^{n} x_i
\]</span></p>
<p>If you associate a constant weight of <span class="math inline">\(1/n\)</span> to each observation <span class="math inline">\(x_i\)</span>, you can look at the formula of the mean as a weighted sum:</p>
<p><span class="math display">\[
\bar{x} = \frac{1}{n} x_1 + \frac{1}{n} x_2 + \dots + \frac{1}{n} x_n
\]</span></p>
<p>This is a slightly different way of looking at the mean that will allow you to
generalize the concept of an “average” as a <em>weighted aggregation of information</em>.
For example, if we denote the weight of the <span class="math inline">\(i\)</span>-th individual as <span class="math inline">\(w_i\)</span>, then the
average can be expressed as:</p>
<p><span class="math display">\[
\bar{x} = w_1 x_1 + w_2 x_2 + \dots + w_n x_n = \sum_{i=1}^{n} w_i x_i = \mathbf{w^\mathsf{T} x}
\]</span></p>
</div>
<div id="variance-of-a-variable" class="section level3">
<h3><span class="header-section-number">3.4.2</span> Variance of a Variable</h3>
<p>A measure of center such as the mean is not enoguh to summarize the information
of a variable. We also need a measure of the amount of variability.
Synonym terms are variation, spread, scatter, and dispersion.</p>
<p>Because of its relevance and importance for statistical learning methods, we
will focus on one particular measure of spread: the <strong>variance</strong>
(and its square root the standard deviation).</p>
<p>Simply put, the variance is a measure of spread around the mean. The main idea
behind the calculation of the variance is to quantify the typical concentration
of values around the mean. The way this is done is by averaging the squared
deviations from the mean.</p>
<p><span class="math display">\[
var(X) = \frac{(x_1 - \bar{x})^2 + \dots + (x_n - \bar{x})^2}{n} = \frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})^2
\]</span></p>
<p>Let’s disect the terms and operations involved in the formula of the variance.</p>
<ul>
<li><p>the main terms are the <em>deviations from the mean</em> <span class="math inline">\((x_i - \bar{x})\)</span>, that is, the difference between each observation <span class="math inline">\(x_i\)</span> and the mean <span class="math inline">\(\bar{x}\)</span>.</p></li>
<li><p>conceptually speaking, we want to know what is the average size of the deviations around the mean.</p></li>
<li><p>simply averaging the deviations won’t work because their sum is zero (i.e. the sum of deviations around the mean will cancel out because the mean is the balancing point).</p></li>
<li><p>this is why we square each deviation: <span class="math inline">\((x_i - \bar{x})^2\)</span>, which literally means getting the squared distance from <span class="math inline">\(x_i\)</span> to <span class="math inline">\(\bar{x}\)</span>.</p></li>
<li><p>having squared all the deviations, then we average them to get the variance.</p></li>
</ul>
<p>Because the variance has squared units, we need to take the square root to
“recover” the original units in which <span class="math inline">\(X\)</span> is expressed.
This gives us the <strong>standard deviation</strong></p>
<p><span class="math display">\[
sd(X) = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})^2}
\]</span></p>
<p>In this sense, you can say that the standard deviation is roughly the average
distance that the data points vary from the mean.</p>
<div id="sample-variance" class="section level4 unnumbered">
<h4>Sample Variance</h4>
<p>In practice, you will often find two versions of the formula for the variance:
one in which the sum of squared deviations is divided by <span class="math inline">\(n\)</span>, and another one
in which the division is done by <span class="math inline">\(n-1\)</span>. Each version is associated to the
statistical inference view of variance in terms of whether the data comes from
the <em>population</em> or from a <em>sample</em> of the population.</p>
<p>The <em>population variance</em> is obtained dividing by <span class="math inline">\(n\)</span>:</p>
<p><span class="math display">\[
\textsf{population variance:} \quad \frac{1}{(n)} \sum_{i=1}^{n} (x_i - \bar{x})^2
\]</span></p>
<p>The <em>sample variance</em> is obtained dividing by <span class="math inline">\(n - 1\)</span> instead of dividing by <span class="math inline">\(n\)</span>.
The reason for doing this is to get an unbiased estimor of the population variance:</p>
<p><span class="math display">\[
\textsf{sample variance:} \quad \frac{1}{(n-1)} \sum_{i=1}^{n} (x_i - \bar{x})^2
\]</span></p>
<p>It is important to note that most statistical software compute the variance with
the unbiased version.
If you implement your own functions and are planning to compare them against
other software, then it is crucial to known what other programmers are using for
computing the variance. Otherwise, your results might be a bit different from
the ones with other people’s code.</p>
<p>In this book, unless indicated otherwise, we will use the factor <span class="math inline">\(\frac{1}{n}\)</span>
when introducing concepts of variance, and related measures. If needed, we will
let you know when a formula needs to use the factor <span class="math inline">\(\frac{1}{n-1}\)</span>.</p>
</div>
</div>
<div id="variance-with-vector-notation" class="section level3">
<h3><span class="header-section-number">3.4.3</span> Variance with Vector Notation</h3>
<p>In a similar way to expressing the mean with vector notation, you can also
formulate the variance in terms of vector-matrix notation. First, notice that
the formula of the variance consists of the addition of squared terms.
Second, recall that a sum of numbers can be expressed with an inner product by
using the unit vector (or summation operator). If we denote a vector of ones of
size <span class="math inline">\(n\)</span> as <span class="math inline">\(\mathbf{1}_{n}\)</span>, then the variance of a vector <span class="math inline">\(\mathbf{x}\)</span> can be
obtained with the following inner product:</p>
<p><span class="math display">\[
var(\mathbf{x}) = \frac{1}{n} (\mathbf{x} - \mathbf{\bar{x}})^\mathsf{T} (\mathbf{x} - \mathbf{\bar{x}})
\]</span></p>
<p>where <span class="math inline">\(\mathbf{\bar{x}}\)</span> is an <span class="math inline">\(n\)</span>-element vector of mean values <span class="math inline">\(\bar{x}\)</span>.</p>
<p>Assuming that <span class="math inline">\(\mathbf{x}\)</span> is already mean-centered, then the variance is
proportional to the squared norm of <span class="math inline">\(\mathbf{x}\)</span></p>
<p><span class="math display">\[
var(\mathbf{x}) = \frac{1}{n} \hspace{1mm} \mathbf{x}^{\mathsf{T}} \mathbf{x} = \frac{1}{n} \| \mathbf{x} \|^2
\]</span></p>
<p>This means that we can formulate the variance with the general notion of an
inner product:</p>
<p><span class="math display">\[
var(\mathbf{x}) = \frac{1}{n} \langle \mathbf{x}, \mathbf{x} \rangle
\]</span></p>
</div>
<div id="standard-deviation-as-a-norm" class="section level3">
<h3><span class="header-section-number">3.4.4</span> Standard Deviation as a Norm</h3>
<p>If we use a metric matrix <span class="math inline">\(\mathbf{D} = diag(1/n)\)</span> then we have that the variance
is given by a special type of inner product:</p>
<p><span class="math display">\[
var(\mathbf{x}) = \langle \mathbf{x}, \mathbf{x} \rangle_{D} = \mathbf{x}^{\mathsf{T}} \mathbf{D x}
\]</span></p>
<p>From this point of view, we can say that the variance of <span class="math inline">\(\mathbf{x}\)</span> is
equivalent to its squared norm when the vector space is endowed with a metric
<span class="math inline">\(\mathbf{D}\)</span>. Consequently, the standard deviation is simply the length of
<span class="math inline">\(\mathbf{x}\)</span> in this particular geometric space.</p>
<p><span class="math display">\[
sd(\mathbf{x}) = \| \mathbf{x} \|_{D}
\]</span></p>
<p>When looking at the standard deviation from this perspective, you can actually
say that the amount of spread of a vector <span class="math inline">\(\mathbf{x}\)</span> is actually its length
(under the metric <span class="math inline">\(\mathbf{D}\)</span>).</p>
</div>
<div id="covariance" class="section level3">
<h3><span class="header-section-number">3.4.5</span> Covariance</h3>
<p>The covariance generalizes the concept of variance for two variables. Recall that the formula for the covariance between <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> is:</p>
<p><span class="math display">\[
cov(\mathbf{x, y}) = \frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x}) (y_i - \bar{y})
\]</span></p>
<p>where <span class="math inline">\(\bar{x}\)</span> is the mean value of <span class="math inline">\(\mathbf{x}\)</span> obtained as:</p>
<p><span class="math display">\[
\bar{x} = \frac{1}{n} (x_1 + x_2 + \dots + x_n) = \frac{1}{n} \sum_{i = 1}^{n} x_i
\]</span></p>
<p>and <span class="math inline">\(\bar{y}\)</span> is the mean value of <span class="math inline">\(\mathbf{y}\)</span>:</p>
<p><span class="math display">\[
\bar{y} = \frac{1}{n} (y_1 + y_2 + \dots + y_n) = \frac{1}{n} \sum_{i = 1}^{n} y_i
\]</span></p>
<p>Basically, the covariance is a statistical summary that is used to assess the
<strong>linear association between pairs of variables</strong>.</p>
<p>Assuming that the variables are mean-centered, we can get a more compact
expression of the covariance in vector notation:</p>
<p><span class="math display">\[
cov(\mathbf{x, y}) = \frac{1}{n} (\mathbf{x^{\mathsf{T}} y})
\]</span></p>
<p>Properties of covariance:</p>
<ul>
<li>the covariance is a symmetric index: <span class="math inline">\(cov(X,Y) = cov(Y,X)\)</span></li>
<li>the covariance can take any real value (negative, null, positive)</li>
<li>the covariance is linked to variances under the name of the Cauchy-Schwarz inequality:
<span class="math display">\[cov(X,Y)^2 \leq var(X) var(Y) \]</span></li>
</ul>
</div>
<div id="correlation" class="section level3">
<h3><span class="header-section-number">3.4.6</span> Correlation</h3>
<p>Although the covariance indicates the direction—positive or negative—of a possible linear relation, it does not tell us how big or small the relation might be. To have a more interpretable index, we must transform the convariance into a unit-free measure. To do this we must consider the standard deviations of the variables so we can normalize the covariance. The result of this normalization is the coefficient of linear correlation defined as:</p>
<p><span class="math display">\[
cor(X, Y) = \frac{cov(X, Y)}{\sqrt{var(X)} \sqrt{var(Y)}}
\]</span></p>
<p>Representing <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> as vectors <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span>, we can express the correlation as:</p>
<p><span class="math display">\[
cor(\mathbf{x}, \mathbf{y}) = \frac{cov(\mathbf{x}, \mathbf{y})}{\sqrt{var(\mathbf{x})} \sqrt{var(\mathbf{y})}}
\]</span></p>
<p>Assuming that <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> are mean-centered, we can express the correlation as:</p>
<p><span class="math display">\[
cor(\mathbf{x, y}) = \frac{\mathbf{x^{\mathsf{T}} y}}{\|\mathbf{x}\| \hspace{1mm} \|\mathbf{y}\|}
\]</span></p>
<p>As it turns out, the norm of a mean-centered variable <span class="math inline">\(\mathbf{x}\)</span> is
proportional to the square root of its variance (or standard deviation):</p>
<p><span class="math display">\[
\| \mathbf{x} \| = \sqrt{\mathbf{x^{\mathsf{T}} x}} 
 = \sqrt{n} \sqrt{var(\mathbf{x})}
\]</span></p>
<p>Consequently, we can also express the correlation with inner products as:</p>
<p><span class="math display">\[
cor(\mathbf{x, y}) = \frac{\mathbf{x^{\mathsf{T}} y}}{\sqrt{(\mathbf{x^{\mathsf{T}} x})} \sqrt{(\mathbf{y^{\mathsf{T}} y})}}
\]</span></p>
<p>or equivalently:</p>
<p><span class="math display">\[
cor(\mathbf{x, y}) = \frac{\mathbf{x^{\mathsf{T}} y}}{\| \mathbf{x} \| \hspace{1mm} \| \mathbf{y} \|}
\]</span></p>
<p>In the case that both <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> are standardized (mean zero
and unit variance), that is:</p>
<p><span class="math display">\[
\mathbf{x} = \begin{bmatrix}
\frac{x_1 - \bar{x}}{\sigma_{x}} \\
\frac{x_2 - \bar{x}}{\sigma_{x}} \\
\vdots \\
\frac{x_n - \bar{x}}{\sigma_{x}}
\end{bmatrix},
\hspace{5mm}
\mathbf{y} = \begin{bmatrix}
\frac{y_1 - \bar{y}}{\sigma_{y}} \\
\frac{y_2 - \bar{y}}{\sigma_{y}} \\
\vdots \\
\frac{y_n - \bar{y}}{\sigma_{y}}
\end{bmatrix}
\]</span></p>
<p>the correlation is simply the inner product:</p>
<p><span class="math display">\[
cor(\mathbf{x, y}) = \mathbf{x^{\mathsf{T}} y} \hspace{5mm} \textsf{(standardized variables)}
\]</span></p>
</div>
<div id="geometry-of-correlation" class="section level3">
<h3><span class="header-section-number">3.4.7</span> Geometry of Correlation</h3>
<p>Let’s look at two variables (i.e. vectors) from a geometric perspective.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-18"></span>
<img src="allmodelsarewrong_files/figure-html/unnamed-chunk-18-1.png" alt="Two vectors in a 2-dimensional space" width="70%" />
<p class="caption">
Figure 3.11: Two vectors in a 2-dimensional space
</p>
</div>
<p>The inner product ot two mean-centered vectors <span class="math inline">\(\langle \mathbf{x}, \mathbf{y} \rangle\)</span> is obtained with the following equation:</p>
<p><span class="math display">\[
\mathbf{x^{\mathsf{T}} y} = \|\mathbf{x}\| \hspace{1mm} \|\mathbf{y}\| \hspace{1mm} cos(\theta_{x,y})
\]</span></p>
<p>where <span class="math inline">\(cos(\theta_{x,y})\)</span> is the angle between <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span>. Rearranging the terms in the previous equation we get that:</p>
<p><span class="math display">\[
cos(\theta_{x,y}) = \frac{\mathbf{x^\mathsf{T} y}}{\|\mathbf{x}\| \hspace{1mm} \|\mathbf{y}\|} = cor(\mathbf{x, y}) 
\]</span></p>
<p>which means that the correlation between mean-centered vectors <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> turns out to be the cosine of the angle between <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span>.</p>
</div>
<div id="orthogonal-projections" class="section level3">
<h3><span class="header-section-number">3.4.8</span> Orthogonal Projections</h3>
<p>Last but not least, we finish this chapter with a discussion of projections.
To be more specific, the statistical interpretation of orthogonal projections.</p>
<p>Let’s motivate this discussion with the following question: Consider two
variables <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span>. Can we approximate one of the variables
in terms of the other? This is
an asymmetric type of association since we seek to say something about the
variability of one variable, say <span class="math inline">\(\mathbf{y}\)</span>, in terms of the variability of
<span class="math inline">\(\mathbf{x}\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-19"></span>
<img src="images/duality/vector-projection0.svg" alt="Two vectors in n-dimensional space" width="60%" />
<p class="caption">
Figure 3.12: Two vectors in n-dimensional space
</p>
</div>
<p>We can think of several ways to approximate <span class="math inline">\(\mathbf{y}\)</span> in terms of
<span class="math inline">\(\mathbf{x}\)</span>. The approximation of <span class="math inline">\(\mathbf{y}\)</span>, denoted by <span class="math inline">\(\mathbf{\hat{y}}\)</span>,
means finding a scalar <span class="math inline">\(b\)</span> such that:</p>
<p><span class="math display">\[
\mathbf{\hat{y}} = b \mathbf{x}
\]</span></p>
<p>The common approach to get <span class="math inline">\(\mathbf{\hat{y}}\)</span> in some optimal way is by
minimizing the square difference between <span class="math inline">\(\mathbf{y}\)</span> and <span class="math inline">\(\mathbf{\hat{y}}\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-20"></span>
<img src="images/duality/vector-projection1.svg" alt="Orthogonal projection of y onto x" width="60%" />
<p class="caption">
Figure 3.13: Orthogonal projection of y onto x
</p>
</div>
<p>The answer to this question comes in the form of a projection. More precisely,
we orthogonally project <span class="math inline">\(\mathbf{y}\)</span> onto <span class="math inline">\(\mathbf{x}\)</span>:</p>
<p><span class="math display">\[
\mathbf{\hat{y}} = \mathbf{x} \left( \frac{\mathbf{y^\mathsf{T} x}}{\mathbf{x^\mathsf{T} x}} \right)
\]</span></p>
<p>or equivalently:</p>
<p><span class="math display">\[
\mathbf{\hat{y}} = \mathbf{x} \left( \frac{\mathbf{y^\mathsf{T} x}}{\| \mathbf{x} \|^2} \right)
\]</span></p>
<p>For convenience purposes, we can rewrite the above equation in a slightly different
format:</p>
<p><span class="math display">\[
\mathbf{\hat{y}} = \mathbf{x} (\mathbf{x^\mathsf{T}x})^{-1} \mathbf{x^\mathsf{T}y}
\]</span></p>
<p>If you are familiar with linear regression, you should be able to recognize
this equation. We’ll come back to this when we get to the chapter about
<a href="ols.html#ols">Linear regression</a>.</p>
</div>
<div id="the-mean-as-an-orthogonal-projection" class="section level3">
<h3><span class="header-section-number">3.4.9</span> The mean as an orthogonal projection</h3>
<p>Let’s go back to the concept of mean of a variable. As we previously mention,
a variable <span class="math inline">\(X = (x_1, \dots, x_n)\)</span>, can be thought of a vector <span class="math inline">\(\mathbf{x}\)</span>
in an <span class="math inline">\(n\)</span>-dimensional space. Furthermore, let’s also consider the constant
vector <span class="math inline">\(\mathbf{1}\)</span> of size <span class="math inline">\(n\)</span>. Here’s a conceptual diagram for this situation:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-21"></span>
<img src="images/duality/mean-projection0.svg" alt="Two vectors in n-dimensional space" width="60%" />
<p class="caption">
Figure 3.14: Two vectors in n-dimensional space
</p>
</div>
<p>Out of curiosity, what happens when we ask about the orthogonal projection of
<span class="math inline">\(\mathbf{x}\)</span> onto <span class="math inline">\(\mathbf{1}\)</span>? Something like in the following picture:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-22"></span>
<img src="images/duality/mean-projection1.svg" alt="Orthogonal projection of vector x onto constant vector 1" width="60%" />
<p class="caption">
Figure 3.15: Orthogonal projection of vector x onto constant vector 1
</p>
</div>
<p>This projection is expressed in vector notation as:</p>
<p><span class="math display">\[
\mathbf{\hat{x}} = \mathbf{1} \left( \frac{\mathbf{x^\mathsf{T} 1}}{\mathbf{1^\mathsf{T} 1}} \right)
\]</span></p>
<p>or equivalently:</p>
<p><span class="math display">\[
\mathbf{\hat{x}} = \mathbf{1} \left( \frac{\mathbf{x^\mathsf{T} 1}}{\| \mathbf{1} \|^2} \right)
\]</span></p>
<p>Note that the term in parenthesis is just a scalar, so we can actually express
<span class="math inline">\(\mathbf{\hat{x}}\)</span> as <span class="math inline">\(b \mathbf{1}\)</span>. This means that a projection implies
multiplying <span class="math inline">\(\mathbf{1}\)</span> by some number <span class="math inline">\(b\)</span>, such that
<span class="math inline">\(\mathbf{\hat{x}} = b \mathbf{1}\)</span> is a stretched or shrinked version of
<span class="math inline">\(\mathbf{1}\)</span>. So, what is the scalar <span class="math inline">\(b\)</span>? It is simply the mean of <span class="math inline">\(\mathbf{x}\)</span>:</p>
<p><span class="math display">\[
\mathbf{\hat{x}} = \mathbf{1} \left( \frac{\mathbf{x^\mathsf{T} 1}}{\| \mathbf{1} \|^2} \right) = \bar{x} \mathbf{1}
\]</span></p>
<p>This is better appreciated in the following figure.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-23"></span>
<img src="images/duality/mean-projection2.svg" alt="Mean of x and its relation with the projection onto constant vector 1" width="60%" />
<p class="caption">
Figure 3.16: Mean of x and its relation with the projection onto constant vector 1
</p>
</div>
<p>What this tells us is that the mean of the variable <span class="math inline">\(X\)</span>, denoted by <span class="math inline">\(\bar{x}\)</span>,
has a very interesting geometric interpretation. As you can tell, <span class="math inline">\(\bar{x}\)</span> is
the scalar by which you would multiply <span class="math inline">\(\mathbf{1}\)</span> in order to obtain the
vector projection <span class="math inline">\(\mathbf{\hat{x}}\)</span>.</p>

</div>
</div>
</div>



            </section>

          </div>
        </div>
      </div>
<a href="intro.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="pca.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["allmodelsarewrong.pdf", "allmodelsarewrong.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
