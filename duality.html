<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3 Geometric Duality | All Models Are Wrong</title>
  <meta name="description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  <meta name="generator" content="bookdown 0.14 and GitBook 2.6.7" />

  <meta property="og:title" content="3 Geometric Duality | All Models Are Wrong" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://allmodelsarewrong.github.io" />
  
  <meta property="og:description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  <meta name="github-repo" content="allmodelsarewrong/allmodelsarewrong.github.io" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3 Geometric Duality | All Models Are Wrong" />
  
  <meta name="twitter:description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  

<meta name="author" content="by Gaston Sanchez, and Ethan Marzban" />


<meta name="date" content="2019-11-03" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="intro.html"/>
<link rel="next" href="pca.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><b>All Models Are Wrong (CSL)</b><br>by G. Sanchez & E. Marzban</a></li>

<li class="divider"></li>
<li class="part"><span><b>I Welcome</b></span></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="about.html"><a href="about.html"><i class="fa fa-check"></i><b>1</b> About this book</a><ul>
<li class="chapter" data-level="1.1" data-path="about.html"><a href="about.html#prerequisites"><i class="fa fa-check"></i><b>1.1</b> Prerequisites</a></li>
<li class="chapter" data-level="1.2" data-path="about.html"><a href="about.html#acknowledgements"><i class="fa fa-check"></i><b>1.2</b> Acknowledgements</a></li>
</ul></li>
<li class="part"><span><b>II Intro</b></span></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#basic-notation"><i class="fa fa-check"></i><b>2.1</b> Basic Notation</a></li>
<li class="chapter" data-level="2.2" data-path="intro.html"><a href="intro.html#about-statistical-learning"><i class="fa fa-check"></i><b>2.2</b> About Statistical Learning</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="duality.html"><a href="duality.html"><i class="fa fa-check"></i><b>3</b> Geometric Duality</a><ul>
<li class="chapter" data-level="3.1" data-path="duality.html"><a href="duality.html#rows-space"><i class="fa fa-check"></i><b>3.1</b> Rows Space</a></li>
<li class="chapter" data-level="3.2" data-path="duality.html"><a href="duality.html#columns-space"><i class="fa fa-check"></i><b>3.2</b> Columns Space</a></li>
<li class="chapter" data-level="3.3" data-path="duality.html"><a href="duality.html#cloud-of-individuals"><i class="fa fa-check"></i><b>3.3</b> Cloud of Individuals</a><ul>
<li class="chapter" data-level="3.3.1" data-path="duality.html"><a href="duality.html#average-individual"><i class="fa fa-check"></i><b>3.3.1</b> Average Individual</a></li>
<li class="chapter" data-level="3.3.2" data-path="duality.html"><a href="duality.html#centered-data"><i class="fa fa-check"></i><b>3.3.2</b> Centered Data</a></li>
<li class="chapter" data-level="3.3.3" data-path="duality.html"><a href="duality.html#distance-between-individuals"><i class="fa fa-check"></i><b>3.3.3</b> Distance between individuals</a></li>
<li class="chapter" data-level="3.3.4" data-path="duality.html"><a href="duality.html#distance-to-the-centroid"><i class="fa fa-check"></i><b>3.3.4</b> Distance to the centroid</a></li>
<li class="chapter" data-level="3.3.5" data-path="duality.html"><a href="duality.html#measures-of-dispersion"><i class="fa fa-check"></i><b>3.3.5</b> Measures of Dispersion</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="duality.html"><a href="duality.html#cloud-of-variables"><i class="fa fa-check"></i><b>3.4</b> Cloud of Variables</a><ul>
<li class="chapter" data-level="3.4.1" data-path="duality.html"><a href="duality.html#mean-of-a-variable"><i class="fa fa-check"></i><b>3.4.1</b> Mean of a Variable</a></li>
<li class="chapter" data-level="3.4.2" data-path="duality.html"><a href="duality.html#variance-of-a-variable"><i class="fa fa-check"></i><b>3.4.2</b> Variance of a Variable</a></li>
<li class="chapter" data-level="3.4.3" data-path="duality.html"><a href="duality.html#variance-with-vector-notation"><i class="fa fa-check"></i><b>3.4.3</b> Variance with Vector Notation</a></li>
<li class="chapter" data-level="3.4.4" data-path="duality.html"><a href="duality.html#standard-deviation-as-a-norm"><i class="fa fa-check"></i><b>3.4.4</b> Standard Deviation as a Norm</a></li>
<li class="chapter" data-level="3.4.5" data-path="duality.html"><a href="duality.html#covariance"><i class="fa fa-check"></i><b>3.4.5</b> Covariance</a></li>
<li class="chapter" data-level="3.4.6" data-path="duality.html"><a href="duality.html#correlation"><i class="fa fa-check"></i><b>3.4.6</b> Correlation</a></li>
<li class="chapter" data-level="3.4.7" data-path="duality.html"><a href="duality.html#geometry-of-correlation"><i class="fa fa-check"></i><b>3.4.7</b> Geometry of Correlation</a></li>
<li class="chapter" data-level="3.4.8" data-path="duality.html"><a href="duality.html#orthogonal-projections"><i class="fa fa-check"></i><b>3.4.8</b> Orthogonal Projections</a></li>
<li class="chapter" data-level="3.4.9" data-path="duality.html"><a href="duality.html#the-mean-as-an-orthogonal-projection"><i class="fa fa-check"></i><b>3.4.9</b> The mean as an orthogonal projection</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Unsupervised I: PCA</b></span></li>
<li class="chapter" data-level="4" data-path="pca.html"><a href="pca.html"><i class="fa fa-check"></i><b>4</b> Principal Components Analysis</a><ul>
<li class="chapter" data-level="4.1" data-path="pca.html"><a href="pca.html#low-dimensional-representations"><i class="fa fa-check"></i><b>4.1</b> Low-dimensional Representations</a></li>
<li class="chapter" data-level="4.2" data-path="pca.html"><a href="pca.html#pca-idea"><i class="fa fa-check"></i><b>4.2</b> PCA Idea</a><ul>
<li class="chapter" data-level="4.2.1" data-path="pca.html"><a href="pca.html#finding-principal-components"><i class="fa fa-check"></i><b>4.2.1</b> Finding Principal Components</a></li>
<li class="chapter" data-level="4.2.2" data-path="pca.html"><a href="pca.html#finding-the-first-pc"><i class="fa fa-check"></i><b>4.2.2</b> Finding the first PC</a></li>
<li class="chapter" data-level="4.2.3" data-path="pca.html"><a href="pca.html#finding-the-second-pc"><i class="fa fa-check"></i><b>4.2.3</b> Finding the second PC</a></li>
<li class="chapter" data-level="4.2.4" data-path="pca.html"><a href="pca.html#finding-all-pcs"><i class="fa fa-check"></i><b>4.2.4</b> Finding all PCs</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="pca.html"><a href="pca.html#pca-model"><i class="fa fa-check"></i><b>4.3</b> PCA Model</a></li>
<li class="chapter" data-level="4.4" data-path="pca.html"><a href="pca.html#another-perspective"><i class="fa fa-check"></i><b>4.4</b> Another Perspective</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Made with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">All Models Are Wrong</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="duality" class="section level1">
<h1><span class="header-section-number">3</span> Geometric Duality</h1>
<p>The way we like to introduce you to the statistical learning world, is by
talking and thinking about data in a geometric sense.</p>
<p>Let’s suppose we have some data in the form of a data matrix. For convenience
purposes, let’s also suppose that all variables are measured in a real-value scale.
Obviously not all data is expressed or even encoded numerically. You may have
categorical or symbolic data. But for this illustration, let’s assume that any
categorical and symbolic data has already been transformed into a numeric scale.</p>
<p>It’s very enlightening to think of a data matrix as viewed from the glass of
Geometry. The key idea is to think of the data in a matrix as elements living
in a multidimensional space. Actually, we can regard a data matrix from two
apparently different perspectives that, in reality, are intimately connected:
the <strong>rows perspective</strong> and the <strong>columns perspective</strong>. In order to explain
these perspectives, let me use the following diagram of a data matrix <span class="math inline">\(\mathbf{X}\)</span>
with <span class="math inline">\(n\)</span> rows and <span class="math inline">\(p\)</span> columns, with <span class="math inline">\(x_{ij}\)</span> representing the element in the <span class="math inline">\(i\)</span>-th
row and <span class="math inline">\(j\)</span>-th column.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-7"></span>
<img src="images/duality/data-perspectives.png" alt="Duality of a data matrix" width="70%" />
<p class="caption">
Figure 3.1: Duality of a data matrix
</p>
</div>
<p>When we look at a data matrix from the <em>columns</em> perpective what we are doing is focusing on the <span class="math inline">\(p\)</span> variables. In a similar way, when looking at a data matrix from its <em>rows</em> perspective, we are focusing on the <span class="math inline">\(n\)</span> individuals.
Like a coin, though, this matrix has two sides: a rows side, and a columns side.
That is, we could look at the data from the rows point of view, or the columns
point of view. These two views are (of course) not completely independent.
This double perspective or <strong>duality</strong> for short, is like the two sides of the
same coin.</p>
<div id="rows-space" class="section level2">
<h2><span class="header-section-number">3.1</span> Rows Space</h2>
<p>We know that human vision is limited to three-dimensions, but pretend that you had superpowers that let you visualize a space with any number of dimensions.</p>
<p>Because each row of the data matrix has <span class="math inline">\(p\)</span> elements, we can regard individuals as objects that live in a <span class="math inline">\(p\)</span>-dimensional space. For visualization purposes, think of each variable as playing the role of a dimension associated to a given axis in this space; likewise, consider each of the <span class="math inline">\(n\)</span> individuals as being depicted as a point (or particle) in such space, like in the following diagram:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-8"></span>
<img src="images/duality/rows-space.png" alt="Rows space" width="85%" />
<p class="caption">
Figure 3.2: Rows space
</p>
</div>
<p>In the figure above, even though I’m showing only three axes, you should pretend that you are visualizing a <span class="math inline">\(p\)</span>-dimensional space (imaging that there are <span class="math inline">\(p\)</span> axes). Each point in this space corresponds to a single individual, and they all form what you can call a <em>cloud of points</em>.</p>
</div>
<div id="columns-space" class="section level2">
<h2><span class="header-section-number">3.2</span> Columns Space</h2>
<p>We can do the same visual exercise with the columns of a data matrix. Since each variable has <span class="math inline">\(n\)</span> elements, we can regard the set of <span class="math inline">\(p\)</span> variables as objects that live in an <span class="math inline">\(n\)</span>-dimensional space. However, instead of representing each variable with a dot, it’s better to graphically represent them with an arrow (or vector). Why? Because of two reasons: one is to distinguish them from the individuals (dots). But more important, because the esential thing with a variable is not really its magnitude (and therefore its position) but its direction. Often, as part of the preprocessing steps we apply transformations on variables that change their scales (e.g. shrinking them, or stretching them) without modifying their directions.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-9"></span>
<img src="images/duality/columns-space.png" alt="Columns space" width="85%" />
<p class="caption">
Figure 3.3: Columns space
</p>
</div>
<p>Analogously to the rows space and its cloud of individuals, you should also
pretend that the image above is displaying an <span class="math inline">\(n\)</span>-dimensional space with
a bunch of blue arrows pointing in various directions.</p>
<div id="whats-next" class="section level4 unnumbered">
<h4>What’s next?</h4>
<p>Now that we know how to think of data from a geometric perspective, the next
step is to discuss a handful of common operations that can be performed with
points and vectors that live in some geometric space.</p>
<hr />
</div>
</div>
<div id="cloud-of-individuals" class="section level2">
<h2><span class="header-section-number">3.3</span> Cloud of Individuals</h2>
<p>In the previous chapter we introduce the powerful idea of looking at the
rows and columns of a data matrix from the lens of geometry.
We are assuming in general that the rows have to do with <span class="math inline">\(n\)</span> individuals that
lie in a <span class="math inline">\(p\)</span>-dimensional space.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-10"></span>
<img src="images/duality/cloud-obs-centering0.png" alt="Cloud of points" width="55%" />
<p class="caption">
Figure 3.4: Cloud of points
</p>
</div>
<p>Let’s start describing a set of common operations that we can apply on the individuals
(living in a <span class="math inline">\(p\)</span>-dimensional space).</p>
<div id="average-individual" class="section level3">
<h3><span class="header-section-number">3.3.1</span> Average Individual</h3>
<p>We can ask about the typical or average individual.</p>
<p>If you only have one variable, then all the individual points lie in a one-dimensional space, which is basically a line:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-11"></span>
<img src="images/duality/mean-number-line.png" alt="Points in one dimension" width="30%" />
<p class="caption">
Figure 3.5: Points in one dimension
</p>
</div>
<p>In this case, the average individual is simply the average of the values, which geometrically corresponds to the balancing point:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-12"></span>
<img src="images/duality/mean-balancing.png" alt="Average individual" width="70%" />
<p class="caption">
Figure 3.6: Average individual
</p>
</div>
<p>Algebraically we have: individuals <span class="math inline">\(x_1, x_2, \dots, x_n\)</span>, and the average is:</p>
<p><span class="math display">\[
\bar{x} = \frac{x_1 + \dots + x_n}{n} = \frac{1}{n} \sum_{i=1}^{n} x_i
\]</span></p>
<p>In vector notation, the average can be calculated with an inner product between <span class="math inline">\(\mathbf{x} = (x_1, x_2, \dots, x_n)\)</span>, and a constant vector of <span class="math inline">\(n\)</span>-ones <span class="math inline">\(\mathbf{1}\)</span>:</p>
<p><span class="math display">\[
\bar{x} = \mathbf{x^\mathsf{T}1}
\]</span></p>
<p>What about the multivariate case? It turns out that we can also ask about the average individual of a cloud of points, like in the following figure:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-13"></span>
<img src="images/duality/cloud-obs-centering1.png" alt="Cloud of points with centroid (i.e. average individual)" width="55%" />
<p class="caption">
Figure 3.7: Cloud of points with centroid (i.e. average individual)
</p>
</div>
<p>The average individual, in a <span class="math inline">\(p\)</span>-dimensional space is the point <span class="math inline">\(\mathbf{\vec{g}}\)</span>
containing as coordiantes the averages of all the variables:</p>
<p><span class="math display">\[
\mathbf{\vec{g}} = (\bar{x}_1, \bar{x}_2, \dots, \bar{x}_j)
\]</span></p>
<p>where <span class="math inline">\(\bar{x}_j\)</span> is the average of the <span class="math inline">\(j\)</span>-th variable.</p>
<p>This average individual <span class="math inline">\(\mathbf{\vec{g}}\)</span> is also known as the <strong>centroid</strong>, <em>barycenter</em>, or <em>center of gravity</em> of the cloud of points.</p>
</div>
<div id="centered-data" class="section level3">
<h3><span class="header-section-number">3.3.2</span> Centered Data</h3>
<p>Often, it is convenient to transform the data in such a way that the centroid of
a data set becomes the origin of the cloud of points. Geometrically, this type
of transformation involves a shif of the axes in the <span class="math inline">\(p\)</span>-dimensional space.
Algebraically, this transformation corresponds to expresing the values of
each variable in terms of deviations from their means.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-14"></span>
<img src="images/duality/cloud-obs-centering2.png" alt="Cloud of points of mean-centered data" width="55%" />
<p class="caption">
Figure 3.8: Cloud of points of mean-centered data
</p>
</div>
</div>
<div id="distance-between-individuals" class="section level3">
<h3><span class="header-section-number">3.3.3</span> Distance between individuals</h3>
<p>Another common operation that we may be interested in is the distance between two individuals. Obviously the notion of distance is not unique, since you can choose different types of distance measures. Perhaps the most comon type of distance is the (squared) Euclidean distance. Unless otherwise mentioned, this will be the default distance used in this book.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-15"></span>
<img src="images/duality/cloud-obs-distance.png" alt="Distance between two individuals" width="55%" />
<p class="caption">
Figure 3.9: Distance between two individuals
</p>
</div>
<p>If you have one variable <span class="math inline">\(X\)</span>, then the squared distance <span class="math inline">\(d^2(i,l)\)</span> between two individuals <span class="math inline">\(x_i\)</span> and <span class="math inline">\(x_l\)</span> is:</p>
<p><span class="math display">\[
d^2(i,l) = (x_i - x_l)^2
\]</span></p>
<p>In general, with <span class="math inline">\(p\)</span> variables, the squared distance between the <span class="math inline">\(i\)</span>-th individual
and the <span class="math inline">\(l\)</span>-th individual is:</p>
<p><span class="math display">\[\begin{align*}
d^2(i,l) &amp;= (x_{i1} - x_{l1})^2 + (x_{i2} - x_{l2})^2 + \dots + (x_{ip} - x_{lp})^2 \\
&amp;= (\mathbf{\vec{x}_i} - \mathbf{\vec{x}_l})^\mathsf{T} (\mathbf{\vec{x}_i} - \mathbf{\vec{x}_l})
\end{align*}\]</span></p>
</div>
<div id="distance-to-the-centroid" class="section level3">
<h3><span class="header-section-number">3.3.4</span> Distance to the centroid</h3>
<p>A special case is the distance between any individual <span class="math inline">\(i\)</span> and the average individual:</p>
<p><span class="math display">\[\begin{align*}
d^2(i,g) &amp;= (x_{i1} - \bar{x}_1)^2 + (x_{i2} - \bar{x}_2)^2 + \dots + (x_{ip} - \bar{x}_p)^2 \\
&amp;= (\mathbf{\vec{x}_i} - \mathbf{\vec{g}})^\mathsf{T} (\mathbf{\vec{x}_i} - \mathbf{\vec{g}})
\end{align*}\]</span></p>
</div>
<div id="measures-of-dispersion" class="section level3">
<h3><span class="header-section-number">3.3.5</span> Measures of Dispersion</h3>
<p>What else can we calculate with the individuals? Think about it. So far we’ve
seen how to calculate the average individual, as well as distances between
individuals. The average individual or centroid plays the role of a measure
of center. And everytime you get a measure of center, it makes sense to get a
measure of spread.</p>
<div id="overall-dispersion" class="section level4 unnumbered">
<h4>Overall Dispersion</h4>
<p>One way to compute a measure of scatter among individuals is to consider all the
squared distances between pairs of individuals. For instance, say you have three
individuals <span class="math inline">\(a\)</span>, <span class="math inline">\(b\)</span>, and <span class="math inline">\(c\)</span>. We can calculate all pairwise distances and add
them up:</p>
<p><span class="math display">\[
d^2(a,b) + d^2(b,a) + d^2(a,c) + d^2(c,a) + d^2(b,c) + d^2(c,b)
\]</span></p>
<p>In general, when you have <span class="math inline">\(n\)</span> individuals, you can obtain up to <span class="math inline">\(n^2\)</span> squared
distances. We will give the generic name of <strong>Overall Dispersion</strong> to the sum
of all squared pairwise distances:</p>
<p><span class="math display">\[
\text{overall dispersion} = \sum_{i=1}^{n} \sum_{l=1}^{n} d^2(i,l)
\]</span></p>
</div>
<div id="inertia" class="section level4 unnumbered">
<h4>Inertia</h4>
<p>Another measure of scatter among individuals can be computed by adding the
distances between all individuals and the centroid.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-16"></span>
<img src="images/duality/cloud-inertia.png" alt="Inertia" width="55%" />
<p class="caption">
Figure 3.10: Inertia
</p>
</div>
<p>The sum of squared distances from each point to the centroid then becomes</p>
<p><span class="math display">\[
\frac{1}{n} \sum_{i=1}^{n} d^2(i,g) = \frac{1}{n} \sum_{i=1}^{n} (\mathbf{\vec{x}_i} - \mathbf{\vec{g}})^\mathsf{T} (\mathbf{\vec{x}_i} - \mathbf{\vec{g}})
\]</span></p>
<p>We will name this measure <strong>Inertia</strong>, borrowing this term from the concept of
inertia used in mechanics (in physics).</p>
<p><span class="math display">\[
\text{Inertia} = \frac{1}{n} \sum_{i=1}^{n} d^2(i,g)
\]</span></p>
<p>What is the motivation behind this measure? Consider the <span class="math inline">\(p = 1\)</span> case; i.e. 
when <span class="math inline">\(\mathbf{X}\)</span> is simply a column vector</p>
<p><span class="math display">\[
\mathbf{X} = \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \\ \end{pmatrix}
\]</span></p>
<p>The centroid will simply be the mean of these points: i.e. 
<span class="math inline">\(\bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i\)</span>.</p>
<p>The sum of squared-distances from each point to the centroid then becomes:</p>
<p><span class="math display">\[
(x_1 - \bar{x})^2 + (x_2 - \bar{x})^2 + \dots + (x_n - \bar{x})^2 = \sum_{i=1}^{n} (x_i - \bar{x})^2
\]</span></p>
<p>Does the above formula look familiar? What if we take the average of the squared distances:</p>
<p><span class="math display">\[
\frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})^2 = \frac{(x_1 - \bar{x})^2 + (x_2 - \bar{x})^2 + \dots + (x_n - \bar{x})^2}{n}
\]</span></p>
<p>Same question: Do you recognize this formula? You better do…
This is nothing else than the
formula of the variance of <span class="math inline">\(X\)</span>. And yes, we are dividing by <span class="math inline">\(n\)</span> (not by <span class="math inline">\(n-1\)</span>).
Hence, you can think of inertia as a multidimensional extension of variance,
which gives the typical squared distance around the centroid.</p>
</div>
<div id="overall-dispersion-and-inertia" class="section level4 unnumbered">
<h4>Overall Dispersion and Inertia</h4>
<p>Interestingly, the <em>overall dispersion</em> and the <em>inertia</em> are connected through
the following relation:</p>
<p><span class="math display">\[\begin{align*}
\text{overall dispersion} &amp;= \sum_{i=1}^{n} \sum_{l=1}^{n} d^2(i,l) \\
&amp;= 2n \sum_{i=1}^{n} d^2(i,g) \\
&amp;= (2n^2) \text{Inertia}
\end{align*}\]</span></p>
<p>The proof of this relation is left as a homework exercise.</p>
<hr />
</div>
</div>
</div>
<div id="cloud-of-variables" class="section level2">
<h2><span class="header-section-number">3.4</span> Cloud of Variables</h2>
<p>The starting point when analyzing variables involves computing various summary
measures—such as means, and variances—to get an idea of the common or central
values, and the amount of variability of each variable. In this chapter we will
review how concepts like the mean of a variable, the variance, covariance,
and correlation, can be interpreted in a geometric sense, as well as their
expressions in terms of vector-matrix operations.</p>
<div id="mean-of-a-variable" class="section level3">
<h3><span class="header-section-number">3.4.1</span> Mean of a Variable</h3>
<p>To measure variation, we usually begin by calculating a “typical” value.
The idea is to summarize the values of a variable with one or two representative
values. You will find this notion under several terms like measures of center,
location, central tendency, or centrality.</p>
<p>The prototypical summary value of center is the <strong>mean</strong>, sometimes referred to
as average. The mean of an <span class="math inline">\(n-\)</span>element variable <span class="math inline">\(X = (x_1, x_2, \dots, x_n)\)</span>,
represented by <span class="math inline">\(\bar{x}\)</span>, is obtained by adding all the <span class="math inline">\(x_i\)</span> values and then
dividing by their total number <span class="math inline">\(n\)</span>:</p>
<p><span class="math display">\[
\bar{x} = \frac{x_1 + x_2 + \dots + x_n}{n}
\]</span></p>
<p>Using summation notation we can express <span class="math inline">\(\bar{x}\)</span> in a very compact way as:</p>
<p><span class="math display">\[
\bar{x} = \frac{1}{n} \sum_{i = 1}^{n} x_i
\]</span></p>
<p>If you associate a constant weight of <span class="math inline">\(1/n\)</span> to each observation <span class="math inline">\(x_i\)</span>, you can look at the formula of the mean as a weighted sum:</p>
<p><span class="math display">\[
\bar{x} = \frac{1}{n} x_1 + \frac{1}{n} x_2 + \dots + \frac{1}{n} x_n
\]</span></p>
<p>This is a slightly different way of looking at the mean that will allow you to
generalize the concept of an “average” as a <em>weighted aggregation of information</em>.
For example, if we denote the weight of the <span class="math inline">\(i\)</span>-th individual as <span class="math inline">\(w_i\)</span>, then the
average can be expressed as:</p>
<p><span class="math display">\[
\bar{x} = w_1 x_1 + w_2 x_2 + \dots + w_n x_n = \sum_{i=1}^{n} w_i x_i
\]</span></p>
</div>
<div id="variance-of-a-variable" class="section level3">
<h3><span class="header-section-number">3.4.2</span> Variance of a Variable</h3>
<p>A measure of center such as the mean is not enoguh to summarize the information
of a variable. We also need a measure of the amount of variability.
Synonym terms are variation, spread, scatter, and dispersion.</p>
<p>Because of its relevance and importance for statistical learning methods, we
will focus on one particular measure of spread: the <strong>variance</strong>
(and its square root the standard deviation).</p>
<p>Simply put, the variance is a measure of spread around the mean. The main idea
behind the calculation of the variance is to quantify the typical concentration
of values around the mean. The way this is done is by averaging the squared
deviations from the mean.</p>
<p><span class="math display">\[
var(X) = \frac{(x_1 - \bar{x})^2 + \dots + (x_n - \bar{x})^2}{n} = \frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})^2
\]</span></p>
<p>Let’s disect the terms and operations involved in the formula of the variance.</p>
<ul>
<li><p>the main terms are the <em>deviations from the mean</em> <span class="math inline">\((x_i - \bar{x})\)</span>, that is, the difference between each observation <span class="math inline">\(x_i\)</span> and the mean <span class="math inline">\(\bar{x}\)</span>.</p></li>
<li><p>conceptually speaking, we want to know what is the average size of the deviations around the mean.</p></li>
<li><p>simply averaging the deviations won’t work because their sum is zero (i.e. the sum of deviations around the mean will cancel out because the mean is the balancing point).</p></li>
<li><p>this is why we square each deviation: <span class="math inline">\((x_i - \bar{x})^2\)</span>, which literally means getting the squared distance from <span class="math inline">\(x_i\)</span> to <span class="math inline">\(\bar{x}\)</span>.</p></li>
<li><p>having squared all the deviations, then we average them to get the variance.</p></li>
</ul>
<p>Because the variance has squared units, we need to take the square root to
“recover” the original units in which <span class="math inline">\(X\)</span> is expressed.
This gives us the <strong>standard deviation</strong></p>
<p><span class="math display">\[
sd(X) = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})^2}
\]</span></p>
<p>In this sense, you can say that the standard deviation is roughly the average
distance that the data points vary from the mean.</p>
<div id="sample-variance" class="section level4 unnumbered">
<h4>Sample Variance</h4>
<p>In practice, you will often find two versions of the formula for the variance:
one in which the sum of squared deviations is divided by <span class="math inline">\(n\)</span>, and another one
in which the division is done by <span class="math inline">\(n-1\)</span>. Each version is associated to the
statistical inference view of variance in terms of whether the data comes from
the <em>population</em> or from a <em>sample</em> of the population.</p>
<p>The <em>population variance</em> is obtained dividing by <span class="math inline">\(n\)</span>:</p>
<p><span class="math display">\[
\textsf{population variance:} \quad \frac{1}{(n)} \sum_{i=1}^{n} (x_i - \bar{x})^2
\]</span></p>
<p>The <em>sample variance</em> is obtained dividing by <span class="math inline">\(n - 1\)</span> instead of dividing by <span class="math inline">\(n\)</span>.
The reason for doing this is to get an unbiased estimor of the population variance:</p>
<p><span class="math display">\[
\textsf{sample variance:} \quad \frac{1}{(n-1)} \sum_{i=1}^{n} (x_i - \bar{x})^2
\]</span></p>
<p>It is important to note that most statistical software compute the variance with
the unbiased version.
If you implement your own functions and are planning to compare them against
other software, then it is crucial to known what other programmers are using for
computing the variance. Otherwise, your results might be a bit different from
the ones with other people’s code.</p>
<p>In this book, unless indicated otherwise, we will use the factor <span class="math inline">\(\frac{1}{n}\)</span>
when introducing concepts of variance, and related measures. If needed, we will
let you know when a formula needs to use the factor <span class="math inline">\(\frac{1}{n-1}\)</span>.</p>
</div>
</div>
<div id="variance-with-vector-notation" class="section level3">
<h3><span class="header-section-number">3.4.3</span> Variance with Vector Notation</h3>
<p>In a similar way to expressing the mean with vector notation, you can also
formulate the variance in terms of vector-matrix notation. First, notice that
the formula of the variance consists of the addition of squared terms.
Second, recall that a sum of numbers can be expressed with an inner product by
using the unit vector (or summation operator). If we denote a vector of ones of
size <span class="math inline">\(n\)</span> as <span class="math inline">\(\mathbf{1}_{n}\)</span>, then the variance of a vector <span class="math inline">\(\mathbf{x}\)</span> can be
obtained with the following inner product:</p>
<p><span class="math display">\[
var(\mathbf{x}) = \frac{1}{n} (\mathbf{x} - \mathbf{\bar{x}})^\mathsf{T} (\mathbf{x} - \mathbf{\bar{x}})
\]</span></p>
<p>where <span class="math inline">\(\mathbf{\bar{x}}\)</span> is an <span class="math inline">\(n\)</span>-element vector of mean values <span class="math inline">\(\bar{x}\)</span>.</p>
<p>Assuming that <span class="math inline">\(\mathbf{x}\)</span> is already mean-centered, then the variance is
proportional to the squared norm of <span class="math inline">\(\mathbf{x}\)</span></p>
<p><span class="math display">\[
var(\mathbf{x}) = \frac{1}{n} \hspace{1mm} \mathbf{x}^{\mathsf{T}} \mathbf{x} = \frac{1}{n} \| \mathbf{x} \|^2
\]</span></p>
<p>This means that we can formulate the variance with the general notion of an
inner product:</p>
<p><span class="math display">\[
var(\mathbf{x}) = \frac{1}{n} \langle \mathbf{x}, \mathbf{x} \rangle
\]</span></p>
</div>
<div id="standard-deviation-as-a-norm" class="section level3">
<h3><span class="header-section-number">3.4.4</span> Standard Deviation as a Norm</h3>
<p>If we use a metric matrix <span class="math inline">\(\mathbf{D} = diag(1/n)\)</span> then we have that the variance
is given by a special type of inner product:</p>
<p><span class="math display">\[
var(\mathbf{x}) = \langle \mathbf{x}, \mathbf{x} \rangle_{D} = \mathbf{x}^{\mathsf{T}} \mathbf{D x}
\]</span></p>
<p>From this point of view, we can say that the variance of <span class="math inline">\(\mathbf{x}\)</span> is
equivalent to its squared norm when the vector space is endowed with a metric
<span class="math inline">\(\mathbf{D}\)</span>. Consequently, the standard deviation is simply the length of
<span class="math inline">\(\mathbf{x}\)</span> in this particular geometric space.</p>
<p><span class="math display">\[
sd(\mathbf{x}) = \| \mathbf{x} \|_{D}
\]</span></p>
<p>When looking at the standard deviation from this perspective, you can actually
say that the amount of spread of a vector <span class="math inline">\(\mathbf{x}\)</span> is actually its length
(under the metric <span class="math inline">\(\mathbf{D}\)</span>).</p>
</div>
<div id="covariance" class="section level3">
<h3><span class="header-section-number">3.4.5</span> Covariance</h3>
<p>The covariance generalizes the concept of variance for two variables. Recall that the formula for the covariance between <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> is:</p>
<p><span class="math display">\[
cov(\mathbf{x, y}) = \frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x}) (y_i - \bar{y})
\]</span></p>
<p>where <span class="math inline">\(\bar{x}\)</span> is the mean value of <span class="math inline">\(\mathbf{x}\)</span> obtained as:</p>
<p><span class="math display">\[
\bar{x} = \frac{1}{n} (x_1 + x_2 + \dots + x_n) = \frac{1}{n} \sum_{i = 1}^{n} x_i
\]</span></p>
<p>and <span class="math inline">\(\bar{y}\)</span> is the mean value of <span class="math inline">\(\mathbf{y}\)</span>:</p>
<p><span class="math display">\[
\bar{y} = \frac{1}{n} (y_1 + y_2 + \dots + y_n) = \frac{1}{n} \sum_{i = 1}^{n} y_i
\]</span></p>
<p>Basically, the covariance is a statistical summary that is used to assess the
<strong>linear association between pairs of variables</strong>.</p>
<p>Assuming that the variables are mean-centered, we can get a more compact
expression of the covariance in vector notation:</p>
<p><span class="math display">\[
cov(\mathbf{x, y}) = \frac{1}{n} (\mathbf{x^{\mathsf{T}} y})
\]</span></p>
<p>Properties of covariance:</p>
<ul>
<li>the covariance is a symmetric index: <span class="math inline">\(cov(X,Y) = cov(Y,X)\)</span></li>
<li>the covariance can take any real value (negative, null, positive)</li>
<li>the covariance is linked to variances under the name of the Cauchy-Schwarz inequality:
<span class="math display">\[cov(X,Y)^2 \leq var(X) var(Y) \]</span></li>
</ul>
</div>
<div id="correlation" class="section level3">
<h3><span class="header-section-number">3.4.6</span> Correlation</h3>
<p>Although the covariance indicates the direction—positive or negative—of a possible linear relation, it does not tell us how big or small the relation might be. To have a more interpretable index, we must transform the convariance into a unit-free measure. To do this we must consider the standard deviations of the variables so we can normalize the covariance. The result of this normalization is the coefficient of linear correlation defined as:</p>
<p><span class="math display">\[
cor(X, Y) = \frac{cov(X, Y)}{\sqrt{var(X)} \sqrt{var(Y)}}
\]</span></p>
<p>Representing <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> as vectors <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span>, we can express the correlation as:</p>
<p><span class="math display">\[
cor(\mathbf{x}, \mathbf{y}) = \frac{cov(\mathbf{x}, \mathbf{y})}{\sqrt{var(\mathbf{x})} \sqrt{var(\mathbf{y})}}
\]</span></p>
<p>Assuming that <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> are mean-centered, we can express the correlation as:</p>
<p><span class="math display">\[
cor(\mathbf{x, y}) = \frac{\mathbf{x^{\mathsf{T}} y}}{\|\mathbf{x}\| \|\mathbf{y}\|}
\]</span></p>
<p>As it turns out, the norm of a mean-centered variable <span class="math inline">\(\mathbf{x}\)</span> is
proportional to the square root of its variance (or standard deviation):</p>
<p><span class="math display">\[
\| \mathbf{x} \| = \sqrt{\mathbf{x^{\mathsf{T}} x}} 
 = \sqrt{n} \sqrt{var(\mathbf{x})}
\]</span></p>
<p>Consequently, we can also express the correlation with inner products as:</p>
<p><span class="math display">\[
cor(\mathbf{x, y}) = \frac{\mathbf{x^{\mathsf{T}} y}}{\sqrt{(\mathbf{x^{\mathsf{T}} x})} \sqrt{(\mathbf{y^{\mathsf{T}} y})}}
\]</span></p>
<p>or equivalently:</p>
<p><span class="math display">\[
cor(\mathbf{x, y}) = \frac{\mathbf{x^{\mathsf{T}} y}}{\| \mathbf{x} \| \hspace{1mm} \| \mathbf{y} \|}
\]</span></p>
<p>In the case that both <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> are standardized (mean zero
and unit variance), that is:</p>
<p><span class="math display">\[
\mathbf{x} = \begin{bmatrix}
\frac{x_1 - \bar{x}}{\sigma_{x}} \\
\frac{x_2 - \bar{x}}{\sigma_{x}} \\
\vdots \\
\frac{x_n - \bar{x}}{\sigma_{x}}
\end{bmatrix},
\hspace{5mm}
\mathbf{y} = \begin{bmatrix}
\frac{y_1 - \bar{y}}{\sigma_{y}} \\
\frac{y_2 - \bar{y}}{\sigma_{y}} \\
\vdots \\
\frac{y_n - \bar{y}}{\sigma_{y}}
\end{bmatrix}
\]</span></p>
<p>the correlation is simply the inner product:</p>
<p><span class="math display">\[
cor(\mathbf{x, y}) = \mathbf{x^{\mathsf{T}} y} \hspace{5mm} \textsf{(standardized variables)}
\]</span></p>
</div>
<div id="geometry-of-correlation" class="section level3">
<h3><span class="header-section-number">3.4.7</span> Geometry of Correlation</h3>
<p>Let’s look at two variables (i.e. vectors) from a geometric perspective.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-17"></span>
<img src="allmodelsarewrong_files/figure-html/unnamed-chunk-17-1.png" alt="Two vectors in a 2-dimensional space" width="70%" />
<p class="caption">
Figure 3.11: Two vectors in a 2-dimensional space
</p>
</div>
<p>The inner product ot two mean-centered vectors <span class="math inline">\(\langle \mathbf{x}, \mathbf{y} \rangle\)</span> is obtained with the following equation:</p>
<p><span class="math display">\[
\mathbf{x^{\mathsf{T}} y} = \|\mathbf{x}\| \hspace{1mm} \|\mathbf{y}\| \hspace{1mm} cos(\theta_{x,y})
\]</span></p>
<p>where <span class="math inline">\(cos(\theta_{x,y})\)</span> is the angle between <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span>. Rearranging the terms in the previous equation we get that:</p>
<p><span class="math display">\[
cos(\theta_{x,y}) = \frac{\mathbf{x^\mathsf{T} y}}{\|\mathbf{x}\| \|\mathbf{y}\|} = cor(\mathbf{x, y}) 
\]</span></p>
<p>which means that the correlation between mean-centered vectors <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> turns out to be the cosine of the angle between <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span>.</p>
</div>
<div id="orthogonal-projections" class="section level3">
<h3><span class="header-section-number">3.4.8</span> Orthogonal Projections</h3>
<p>Last but not least, we finish this chapter with a discussion of projections.
To be more specific, the statistical interpretation of orthogonal projections.</p>
<p>Let’s motivate this discussion with the following question: Consider two
variables <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span>. Can we approximate one of the variables
in terms of the other? This is
an asymmetric type of association since we seek to say something about the
variability of one variable, say <span class="math inline">\(\mathbf{y}\)</span>, in terms of the variability of
<span class="math inline">\(\mathbf{x}\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-18"></span>
<img src="images/duality/vector-projection0.png" alt="Two vectors in n-dimensional space" width="60%" />
<p class="caption">
Figure 3.12: Two vectors in n-dimensional space
</p>
</div>
<p>We can think of several ways to approximate <span class="math inline">\(\mathbf{y}\)</span> in terms of
<span class="math inline">\(\mathbf{x}\)</span>. The approximation of <span class="math inline">\(\mathbf{y}\)</span>, denoted by <span class="math inline">\(\mathbf{\hat{y}}\)</span>,
means finding a scalar <span class="math inline">\(b\)</span> such that:</p>
<p><span class="math display">\[
\mathbf{\hat{y}} = b \mathbf{x}
\]</span></p>
<p>The common approach to get <span class="math inline">\(\mathbf{\hat{y}}\)</span> in some optimal way is by
minimizing the square difference between <span class="math inline">\(\mathbf{y}\)</span> and <span class="math inline">\(\mathbf{\hat{y}}\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-19"></span>
<img src="images/duality/vector-projection1.png" alt="Orthogonal projection of y onto x" width="60%" />
<p class="caption">
Figure 3.13: Orthogonal projection of y onto x
</p>
</div>
<p>The answer to this question comes in the form of a projection. More precisely,
we orthogonally project <span class="math inline">\(\mathbf{y}\)</span> onto <span class="math inline">\(\mathbf{x}\)</span>:</p>
<p><span class="math display">\[
\mathbf{\hat{y}} = \mathbf{x} \left( \frac{\mathbf{y^\mathsf{T} x}}{\mathbf{x^\mathsf{T} x}} \right)
\]</span></p>
<p>or equivalently:</p>
<p><span class="math display">\[
\mathbf{\hat{y}} = \mathbf{x} \left( \frac{\mathbf{y^\mathsf{T} x}}{\| \mathbf{x} \|^2} \right)
\]</span></p>
<p>For convenience purposes, we can rewrite the above equation in a slightly different
format:</p>
<p><span class="math display">\[
\mathbf{\hat{y}} = \mathbf{x} (\mathbf{x^\mathsf{T}x})^{-1} \mathbf{x^\mathsf{T}y}
\]</span></p>
<p>If you are familiar with linear regression, you should be able to recognize
this equation. We’ll come back to this when we get to the chapter about
<a href="#ols">Linear regression</a>.</p>
</div>
<div id="the-mean-as-an-orthogonal-projection" class="section level3">
<h3><span class="header-section-number">3.4.9</span> The mean as an orthogonal projection</h3>
<p>Let’s go back to the concept of mean of a variable. As we previously mention,
a variable <span class="math inline">\(X = (x_1, \dots, x_n)\)</span>, can be thought of a vector <span class="math inline">\(\mathbf{x}\)</span>
in an <span class="math inline">\(n\)</span>-dimensional space. Furthermore, let’s also consider the constant
vector <span class="math inline">\(\mathbf{1}\)</span> of size <span class="math inline">\(n\)</span>. Here’s a conceptual diagram for this situation:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-20"></span>
<img src="images/duality/mean-projection0.png" alt="Two vectors in n-dimensional space" width="60%" />
<p class="caption">
Figure 3.14: Two vectors in n-dimensional space
</p>
</div>
<p>Out of curiosity, what happens when we ask about the orthogonal projection of
<span class="math inline">\(\mathbf{x}\)</span> onto <span class="math inline">\(\mathbf{1}\)</span>? Something like in the following picture:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-21"></span>
<img src="images/duality/mean-projection1.png" alt="Orthogonal projection of vector x onto constant vector 1" width="60%" />
<p class="caption">
Figure 3.15: Orthogonal projection of vector x onto constant vector 1
</p>
</div>
<p>This projection is expressed in vector notation as:</p>
<p><span class="math display">\[
\mathbf{\hat{x}} = \mathbf{1} \left( \frac{\mathbf{x^\mathsf{T} 1}}{\mathbf{1^\mathsf{T} 1}} \right)
\]</span></p>
<p>or equivalently:</p>
<p><span class="math display">\[
\mathbf{\hat{x}} = \mathbf{1} \left( \frac{\mathbf{x^\mathsf{T} 1}}{\| \mathbf{1} \|^2} \right)
\]</span></p>
<p>Note that the term in parenthesis is just a scalar, so we can actually express
<span class="math inline">\(\mathbf{\hat{x}}\)</span> as <span class="math inline">\(b \mathbf{1}\)</span>. This means that a projection implies
multiplying <span class="math inline">\(\mathbf{1}\)</span> by some number <span class="math inline">\(b\)</span>, such that
<span class="math inline">\(\mathbf{\hat{x}} = b \mathbf{1}\)</span> is a stretched or shrinked version of
<span class="math inline">\(\mathbf{1}\)</span>. So, what is the scalar <span class="math inline">\(b\)</span>? It is simply the mean of <span class="math inline">\(\mathbf{x}\)</span>:</p>
<p><span class="math display">\[
\mathbf{\hat{x}} = \mathbf{1} \left( \frac{\mathbf{x^\mathsf{T} 1}}{\| \mathbf{1} \|^2} \right) = \bar{x} \mathbf{1}
\]</span></p>
<p>This is better appreciated in the following figure.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-22"></span>
<img src="images/duality/mean-projection2.png" alt="Mean of x as length of its projection onto constant vector 1" width="60%" />
<p class="caption">
Figure 3.16: Mean of x as length of its projection onto constant vector 1
</p>
</div>
<p>What this tells us is that the mean of the variable <span class="math inline">\(X\)</span>, denoted by <span class="math inline">\(\bar{x}\)</span>,
has a very interesting geometric interpretation. As you can tell, <span class="math inline">\(\bar{x}\)</span> is
the length of the projected vector <span class="math inline">\(\mathbf{\hat{x}}\)</span>. Or in more formal terms,
<span class="math inline">\(\bar{x}\)</span> is the scalar projection of <span class="math inline">\(\mathbf{x}\)</span> onto <span class="math inline">\(\mathbf{1}\)</span>.</p>

</div>
</div>
</div>



            </section>

          </div>
        </div>
      </div>
<a href="intro.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="pca.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["allmodelsarewrong.pdf", "allmodelsarewrong.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
