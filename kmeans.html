<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>27 K-Means | All Models Are Wrong</title>
  <meta name="description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  <meta name="generator" content="bookdown 0.16 and GitBook 2.6.7" />

  <meta property="og:title" content="27 K-Means | All Models Are Wrong" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://allmodelsarewrong.github.io" />
  
  <meta property="og:description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  <meta name="github-repo" content="allmodelsarewrong/allmodelsarewrong.github.io" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="27 K-Means | All Models Are Wrong" />
  
  <meta name="twitter:description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  

<meta name="author" content="by Gaston Sanchez, and Ethan Marzban" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="clustering.html"/>
<link rel="next" href="hclus.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><b>All Models Are Wrong (CSL)</b><br><small>by G. Sanchez & E. Marzban</small></a></li>

<li class="divider"></li>
<li class="part"><span><b>I Welcome</b></span></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="about.html"><a href="about.html"><i class="fa fa-check"></i><b>1</b> About this book</a><ul>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#prerequisites"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="part"><span><b>II Intro</b></span></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#basic-notation"><i class="fa fa-check"></i><b>2.1</b> Basic Notation</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="duality.html"><a href="duality.html"><i class="fa fa-check"></i><b>3</b> Geometric Duality</a><ul>
<li class="chapter" data-level="3.1" data-path="duality.html"><a href="duality.html#rows-space"><i class="fa fa-check"></i><b>3.1</b> Rows Space</a></li>
<li class="chapter" data-level="3.2" data-path="duality.html"><a href="duality.html#columns-space"><i class="fa fa-check"></i><b>3.2</b> Columns Space</a></li>
<li class="chapter" data-level="3.3" data-path="duality.html"><a href="duality.html#cloud-of-individuals"><i class="fa fa-check"></i><b>3.3</b> Cloud of Individuals</a><ul>
<li class="chapter" data-level="3.3.1" data-path="duality.html"><a href="duality.html#average-individual"><i class="fa fa-check"></i><b>3.3.1</b> Average Individual</a></li>
<li class="chapter" data-level="3.3.2" data-path="duality.html"><a href="duality.html#centered-data"><i class="fa fa-check"></i><b>3.3.2</b> Centered Data</a></li>
<li class="chapter" data-level="3.3.3" data-path="duality.html"><a href="duality.html#distance-between-individuals"><i class="fa fa-check"></i><b>3.3.3</b> Distance between individuals</a></li>
<li class="chapter" data-level="3.3.4" data-path="duality.html"><a href="duality.html#distance-to-the-centroid"><i class="fa fa-check"></i><b>3.3.4</b> Distance to the centroid</a></li>
<li class="chapter" data-level="3.3.5" data-path="duality.html"><a href="duality.html#measures-of-dispersion"><i class="fa fa-check"></i><b>3.3.5</b> Measures of Dispersion</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="duality.html"><a href="duality.html#cloud-of-variables"><i class="fa fa-check"></i><b>3.4</b> Cloud of Variables</a><ul>
<li class="chapter" data-level="3.4.1" data-path="duality.html"><a href="duality.html#mean-of-a-variable"><i class="fa fa-check"></i><b>3.4.1</b> Mean of a Variable</a></li>
<li class="chapter" data-level="3.4.2" data-path="duality.html"><a href="duality.html#variance-of-a-variable"><i class="fa fa-check"></i><b>3.4.2</b> Variance of a Variable</a></li>
<li class="chapter" data-level="3.4.3" data-path="duality.html"><a href="duality.html#variance-with-vector-notation"><i class="fa fa-check"></i><b>3.4.3</b> Variance with Vector Notation</a></li>
<li class="chapter" data-level="3.4.4" data-path="duality.html"><a href="duality.html#standard-deviation-as-a-norm"><i class="fa fa-check"></i><b>3.4.4</b> Standard Deviation as a Norm</a></li>
<li class="chapter" data-level="3.4.5" data-path="duality.html"><a href="duality.html#covariance"><i class="fa fa-check"></i><b>3.4.5</b> Covariance</a></li>
<li class="chapter" data-level="3.4.6" data-path="duality.html"><a href="duality.html#correlation"><i class="fa fa-check"></i><b>3.4.6</b> Correlation</a></li>
<li class="chapter" data-level="3.4.7" data-path="duality.html"><a href="duality.html#geometry-of-correlation"><i class="fa fa-check"></i><b>3.4.7</b> Geometry of Correlation</a></li>
<li class="chapter" data-level="3.4.8" data-path="duality.html"><a href="duality.html#orthogonal-projections"><i class="fa fa-check"></i><b>3.4.8</b> Orthogonal Projections</a></li>
<li class="chapter" data-level="3.4.9" data-path="duality.html"><a href="duality.html#the-mean-as-an-orthogonal-projection"><i class="fa fa-check"></i><b>3.4.9</b> The mean as an orthogonal projection</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Unsupervised I: PCA</b></span></li>
<li class="chapter" data-level="4" data-path="pca.html"><a href="pca.html"><i class="fa fa-check"></i><b>4</b> Principal Components Analysis</a><ul>
<li class="chapter" data-level="4.1" data-path="pca.html"><a href="pca.html#low-dimensional-representations"><i class="fa fa-check"></i><b>4.1</b> Low-dimensional Representations</a></li>
<li class="chapter" data-level="4.2" data-path="pca.html"><a href="pca.html#projections"><i class="fa fa-check"></i><b>4.2</b> Projections</a><ul>
<li class="chapter" data-level="4.2.1" data-path="pca.html"><a href="pca.html#vector-and-scalar-projections"><i class="fa fa-check"></i><b>4.2.1</b> Vector and Scalar Projections</a></li>
<li class="chapter" data-level="4.2.2" data-path="pca.html"><a href="pca.html#projected-inertia"><i class="fa fa-check"></i><b>4.2.2</b> Projected Inertia</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="pca.html"><a href="pca.html#maximization-problem"><i class="fa fa-check"></i><b>4.3</b> Maximization Problem</a><ul>
<li class="chapter" data-level="4.3.1" data-path="pca.html"><a href="pca.html#eigenvectors-of-mathbfs"><i class="fa fa-check"></i><b>4.3.1</b> Eigenvectors of <span class="math inline">\(\mathbf{S}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="pca.html"><a href="pca.html#another-perspective-of-pca"><i class="fa fa-check"></i><b>4.4</b> Another Perspective of PCA</a><ul>
<li class="chapter" data-level="4.4.1" data-path="pca.html"><a href="pca.html#finding-principal-components"><i class="fa fa-check"></i><b>4.4.1</b> Finding Principal Components</a></li>
<li class="chapter" data-level="4.4.2" data-path="pca.html"><a href="pca.html#finding-the-first-pc"><i class="fa fa-check"></i><b>4.4.2</b> Finding the first PC</a></li>
<li class="chapter" data-level="4.4.3" data-path="pca.html"><a href="pca.html#finding-the-second-pc"><i class="fa fa-check"></i><b>4.4.3</b> Finding the second PC</a></li>
<li class="chapter" data-level="4.4.4" data-path="pca.html"><a href="pca.html#finding-all-pcs"><i class="fa fa-check"></i><b>4.4.4</b> Finding all PCs</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="pca.html"><a href="pca.html#data-decomposition-model"><i class="fa fa-check"></i><b>4.5</b> Data Decomposition Model</a><ul>
<li class="chapter" data-level="4.5.1" data-path="pca.html"><a href="pca.html#alternative-approaches"><i class="fa fa-check"></i><b>4.5.1</b> Alternative Approaches</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Linear Regression</b></span></li>
<li class="chapter" data-level="5" data-path="ols.html"><a href="ols.html"><i class="fa fa-check"></i><b>5</b> Linear Regression</a><ul>
<li class="chapter" data-level="5.1" data-path="ols.html"><a href="ols.html#motivation"><i class="fa fa-check"></i><b>5.1</b> Motivation</a></li>
<li class="chapter" data-level="5.2" data-path="ols.html"><a href="ols.html#the-ideaintuition-of-regression"><i class="fa fa-check"></i><b>5.2</b> The Idea/Intuition of Regression</a></li>
<li class="chapter" data-level="5.3" data-path="ols.html"><a href="ols.html#the-linear-regression-model"><i class="fa fa-check"></i><b>5.3</b> The Linear Regression Model</a></li>
<li class="chapter" data-level="5.4" data-path="ols.html"><a href="ols.html#the-error-measure"><i class="fa fa-check"></i><b>5.4</b> The Error Measure</a></li>
<li class="chapter" data-level="5.5" data-path="ols.html"><a href="ols.html#the-least-squares-algorithm"><i class="fa fa-check"></i><b>5.5</b> The Least Squares Algorithm</a></li>
<li class="chapter" data-level="5.6" data-path="ols.html"><a href="ols.html#geometries-of-ols"><i class="fa fa-check"></i><b>5.6</b> Geometries of OLS</a><ul>
<li class="chapter" data-level="5.6.1" data-path="ols.html"><a href="ols.html#rows-perspective"><i class="fa fa-check"></i><b>5.6.1</b> Rows Perspective</a></li>
<li class="chapter" data-level="5.6.2" data-path="ols.html"><a href="ols.html#columns-perspective"><i class="fa fa-check"></i><b>5.6.2</b> Columns Perspective</a></li>
<li class="chapter" data-level="5.6.3" data-path="ols.html"><a href="ols.html#parameters-perspective"><i class="fa fa-check"></i><b>5.6.3</b> Parameters Perspective</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="gradient.html"><a href="gradient.html"><i class="fa fa-check"></i><b>6</b> Gradient Descent</a><ul>
<li class="chapter" data-level="6.1" data-path="gradient.html"><a href="gradient.html#error-surface"><i class="fa fa-check"></i><b>6.1</b> Error Surface</a></li>
<li class="chapter" data-level="6.2" data-path="gradient.html"><a href="gradient.html#idea-of-gradient-descent"><i class="fa fa-check"></i><b>6.2</b> Idea of Gradient Descent</a></li>
<li class="chapter" data-level="6.3" data-path="gradient.html"><a href="gradient.html#moving-down-an-error-surface"><i class="fa fa-check"></i><b>6.3</b> Moving Down an Error Surface</a><ul>
<li class="chapter" data-level="6.3.1" data-path="gradient.html"><a href="gradient.html#the-direction-of-mathbfv"><i class="fa fa-check"></i><b>6.3.1</b> The direction of <span class="math inline">\(\mathbf{v}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="gradient.html"><a href="gradient.html#gradient-descent-and-our-model"><i class="fa fa-check"></i><b>6.4</b> Gradient Descent and our Model</a></li>
</ul></li>
<li class="part"><span><b>V Learning Concepts</b></span></li>
<li class="chapter" data-level="7" data-path="learning.html"><a href="learning.html"><i class="fa fa-check"></i><b>7</b> Theoretical Framework</a><ul>
<li class="chapter" data-level="7.1" data-path="learning.html"><a href="learning.html#mental-map"><i class="fa fa-check"></i><b>7.1</b> Mental Map</a></li>
<li class="chapter" data-level="7.2" data-path="learning.html"><a href="learning.html#kinds-of-predictions"><i class="fa fa-check"></i><b>7.2</b> Kinds of Predictions</a><ul>
<li class="chapter" data-level="7.2.1" data-path="learning.html"><a href="learning.html#two-types-of-data"><i class="fa fa-check"></i><b>7.2.1</b> Two Types of Data</a></li>
<li class="chapter" data-level="7.2.2" data-path="learning.html"><a href="learning.html#two-types-of-predictions"><i class="fa fa-check"></i><b>7.2.2</b> Two Types of Predictions</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="learning.html"><a href="learning.html#two-types-of-errors"><i class="fa fa-check"></i><b>7.3</b> Two Types of Errors</a><ul>
<li class="chapter" data-level="7.3.1" data-path="learning.html"><a href="learning.html#individual-errors"><i class="fa fa-check"></i><b>7.3.1</b> Individual Errors</a></li>
<li class="chapter" data-level="7.3.2" data-path="learning.html"><a href="learning.html#overall-errors"><i class="fa fa-check"></i><b>7.3.2</b> Overall Errors</a></li>
<li class="chapter" data-level="7.3.3" data-path="learning.html"><a href="learning.html#auxiliary-technicality"><i class="fa fa-check"></i><b>7.3.3</b> Auxiliary Technicality</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="learning.html"><a href="learning.html#noisy-targets"><i class="fa fa-check"></i><b>7.4</b> Noisy Targets</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="mse.html"><a href="mse.html"><i class="fa fa-check"></i><b>8</b> MSE of Estimator</a><ul>
<li class="chapter" data-level="8.1" data-path="mse.html"><a href="mse.html#mse-of-an-estimator"><i class="fa fa-check"></i><b>8.1</b> MSE of an Estimator</a><ul>
<li class="chapter" data-level="8.1.1" data-path="mse.html"><a href="mse.html#prototypical-cases-of-bias-and-variance"><i class="fa fa-check"></i><b>8.1.1</b> Prototypical Cases of Bias and Variance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="biasvar.html"><a href="biasvar.html"><i class="fa fa-check"></i><b>9</b> Bias-Variance Tradeoff</a><ul>
<li class="chapter" data-level="9.1" data-path="biasvar.html"><a href="biasvar.html#introduction"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="biasvar.html"><a href="biasvar.html#motivation-example"><i class="fa fa-check"></i><b>9.2</b> Motivation Example</a><ul>
<li class="chapter" data-level="9.2.1" data-path="biasvar.html"><a href="biasvar.html#two-hypotheses"><i class="fa fa-check"></i><b>9.2.1</b> Two Hypotheses</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="biasvar.html"><a href="biasvar.html#learning-from-two-points"><i class="fa fa-check"></i><b>9.3</b> Learning from two points</a></li>
<li class="chapter" data-level="9.4" data-path="biasvar.html"><a href="biasvar.html#bias-variance-derivation"><i class="fa fa-check"></i><b>9.4</b> Bias-Variance Derivation</a><ul>
<li class="chapter" data-level="9.4.1" data-path="biasvar.html"><a href="biasvar.html#out-of-sample-predictions"><i class="fa fa-check"></i><b>9.4.1</b> Out-of-Sample Predictions</a></li>
<li class="chapter" data-level="9.4.2" data-path="biasvar.html"><a href="biasvar.html#noisy-target"><i class="fa fa-check"></i><b>9.4.2</b> Noisy Target</a></li>
<li class="chapter" data-level="9.4.3" data-path="biasvar.html"><a href="biasvar.html#types-of-theoretical-mses"><i class="fa fa-check"></i><b>9.4.3</b> Types of Theoretical MSEs</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="biasvar.html"><a href="biasvar.html#the-tradeoff"><i class="fa fa-check"></i><b>9.5</b> The Tradeoff</a><ul>
<li class="chapter" data-level="9.5.1" data-path="biasvar.html"><a href="biasvar.html#bias-variance-tradeoff-picture"><i class="fa fa-check"></i><b>9.5.1</b> Bias-Variance Tradeoff Picture</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="phases.html"><a href="phases.html"><i class="fa fa-check"></i><b>10</b> Learning Phases</a><ul>
<li class="chapter" data-level="10.1" data-path="phases.html"><a href="phases.html#introduction-1"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="phases.html"><a href="phases.html#model-assessment"><i class="fa fa-check"></i><b>10.2</b> Model Assessment</a><ul>
<li class="chapter" data-level="10.2.1" data-path="phases.html"><a href="phases.html#holdout-test-set"><i class="fa fa-check"></i><b>10.2.1</b> Holdout Test Set</a></li>
<li class="chapter" data-level="10.2.2" data-path="phases.html"><a href="phases.html#why-does-a-test-set-work"><i class="fa fa-check"></i><b>10.2.2</b> Why does a test set work?</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="phases.html"><a href="phases.html#model-selection"><i class="fa fa-check"></i><b>10.3</b> Model Selection</a><ul>
<li class="chapter" data-level="10.3.1" data-path="phases.html"><a href="phases.html#three-way-holdout-method"><i class="fa fa-check"></i><b>10.3.1</b> Three-way Holdout Method</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="phases.html"><a href="phases.html#model-training"><i class="fa fa-check"></i><b>10.4</b> Model Training</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="resampling.html"><a href="resampling.html"><i class="fa fa-check"></i><b>11</b> Resample Approaches</a><ul>
<li class="chapter" data-level="11.1" data-path="resampling.html"><a href="resampling.html#general-sampling-blueprint"><i class="fa fa-check"></i><b>11.1</b> General Sampling Blueprint</a></li>
<li class="chapter" data-level="11.2" data-path="resampling.html"><a href="resampling.html#monte-carlo-cross-validation"><i class="fa fa-check"></i><b>11.2</b> Monte Carlo Cross Validation</a></li>
<li class="chapter" data-level="11.3" data-path="resampling.html"><a href="resampling.html#bootstrap-method"><i class="fa fa-check"></i><b>11.3</b> Bootstrap Method</a></li>
<li class="chapter" data-level="11.4" data-path="resampling.html"><a href="resampling.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>11.4</b> <span class="math inline">\(K\)</span>-Fold Cross-Validation</a><ul>
<li class="chapter" data-level="11.4.1" data-path="resampling.html"><a href="resampling.html#leave-one-out-cross-validation-loocv"><i class="fa fa-check"></i><b>11.4.1</b> Leave-One-Out Cross Validation (LOOCV)</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>VI Regularization</b></span></li>
<li class="chapter" data-level="12" data-path="regular.html"><a href="regular.html"><i class="fa fa-check"></i><b>12</b> Regularization Techniques</a><ul>
<li class="chapter" data-level="12.1" data-path="regular.html"><a href="regular.html#multicollinearity-issues"><i class="fa fa-check"></i><b>12.1</b> Multicollinearity Issues</a><ul>
<li class="chapter" data-level="12.1.1" data-path="regular.html"><a href="regular.html#toy-example"><i class="fa fa-check"></i><b>12.1.1</b> Toy Example</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="regular.html"><a href="regular.html#irregular-coefficients"><i class="fa fa-check"></i><b>12.2</b> Irregular Coefficients</a></li>
<li class="chapter" data-level="12.3" data-path="regular.html"><a href="regular.html#connection-to-regularization"><i class="fa fa-check"></i><b>12.3</b> Connection to Regularization</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="pcr.html"><a href="pcr.html"><i class="fa fa-check"></i><b>13</b> Principal Components Regression</a><ul>
<li class="chapter" data-level="13.1" data-path="pcr.html"><a href="pcr.html#motivation-example-1"><i class="fa fa-check"></i><b>13.1</b> Motivation Example</a></li>
<li class="chapter" data-level="13.2" data-path="pcr.html"><a href="pcr.html#the-pcr-model"><i class="fa fa-check"></i><b>13.2</b> The PCR Model</a></li>
<li class="chapter" data-level="13.3" data-path="pcr.html"><a href="pcr.html#how-does-pcr-work"><i class="fa fa-check"></i><b>13.3</b> How does PCR work?</a><ul>
<li class="chapter" data-level="13.3.1" data-path="pcr.html"><a href="pcr.html#transition-formula"><i class="fa fa-check"></i><b>13.3.1</b> Transition Formula</a></li>
<li class="chapter" data-level="13.3.2" data-path="pcr.html"><a href="pcr.html#size-of-coefficients"><i class="fa fa-check"></i><b>13.3.2</b> Size of Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="pcr.html"><a href="pcr.html#selecting-number-of-pcs"><i class="fa fa-check"></i><b>13.4</b> Selecting Number of PCs</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="pls.html"><a href="pls.html"><i class="fa fa-check"></i><b>14</b> Partial Least Squares Regression</a><ul>
<li class="chapter" data-level="14.1" data-path="pls.html"><a href="pls.html#motivation-example-2"><i class="fa fa-check"></i><b>14.1</b> Motivation Example</a></li>
<li class="chapter" data-level="14.2" data-path="pls.html"><a href="pls.html#the-plsr-model"><i class="fa fa-check"></i><b>14.2</b> The PLSR Model</a></li>
<li class="chapter" data-level="14.3" data-path="pls.html"><a href="pls.html#how-does-plsr-work"><i class="fa fa-check"></i><b>14.3</b> How does PLSR work?</a></li>
<li class="chapter" data-level="14.4" data-path="pls.html"><a href="pls.html#plsr-algorithm"><i class="fa fa-check"></i><b>14.4</b> PLSR Algorithm</a><ul>
<li class="chapter" data-level="14.4.1" data-path="pls.html"><a href="pls.html#pls-solution-with-original-variables"><i class="fa fa-check"></i><b>14.4.1</b> PLS Solution with original variables</a></li>
<li class="chapter" data-level="14.4.2" data-path="pls.html"><a href="pls.html#size-of-coefficients-1"><i class="fa fa-check"></i><b>14.4.2</b> Size of Coefficients</a></li>
<li class="chapter" data-level="14.4.3" data-path="pls.html"><a href="pls.html#some-properties"><i class="fa fa-check"></i><b>14.4.3</b> Some Properties</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="pls.html"><a href="pls.html#selecting-number-of-pls-components"><i class="fa fa-check"></i><b>14.5</b> Selecting Number of PLS Components</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="ridge.html"><a href="ridge.html"><i class="fa fa-check"></i><b>15</b> Ridge Regression</a><ul>
<li class="chapter" data-level="15.1" data-path="ridge.html"><a href="ridge.html#a-new-minimization-problem"><i class="fa fa-check"></i><b>15.1</b> A New Minimization Problem</a><ul>
<li class="chapter" data-level="15.1.1" data-path="ridge.html"><a href="ridge.html#constraining-regression-coefficients"><i class="fa fa-check"></i><b>15.1.1</b> Constraining Regression Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="ridge.html"><a href="ridge.html#a-new-minimization-solution"><i class="fa fa-check"></i><b>15.2</b> A New Minimization Solution</a></li>
<li class="chapter" data-level="15.3" data-path="ridge.html"><a href="ridge.html#what-does-rr-accomplish"><i class="fa fa-check"></i><b>15.3</b> What does RR accomplish?</a><ul>
<li class="chapter" data-level="15.3.1" data-path="ridge.html"><a href="ridge.html#how-to-find-lambda"><i class="fa fa-check"></i><b>15.3.1</b> How to find <span class="math inline">\(\lambda\)</span>?</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>VII Extending Linear Regression</b></span></li>
<li class="chapter" data-level="16" data-path="linear-extensions.html"><a href="linear-extensions.html"><i class="fa fa-check"></i><b>16</b> Beyond Linear Regression</a><ul>
<li class="chapter" data-level="16.1" data-path="linear-extensions.html"><a href="linear-extensions.html#introduction-2"><i class="fa fa-check"></i><b>16.1</b> Introduction</a></li>
<li class="chapter" data-level="16.2" data-path="linear-extensions.html"><a href="linear-extensions.html#expanding-the-regression-horizon"><i class="fa fa-check"></i><b>16.2</b> Expanding the Regression Horizon</a><ul>
<li class="chapter" data-level="16.2.1" data-path="linear-extensions.html"><a href="linear-extensions.html#linearity"><i class="fa fa-check"></i><b>16.2.1</b> Linearity</a></li>
<li class="chapter" data-level="16.2.2" data-path="linear-extensions.html"><a href="linear-extensions.html#parametric-and-nonparametric"><i class="fa fa-check"></i><b>16.2.2</b> Parametric and Nonparametric</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="linear-extensions.html"><a href="linear-extensions.html#transforming-features"><i class="fa fa-check"></i><b>16.3</b> Transforming Features</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="nonparametric.html"><a href="nonparametric.html"><i class="fa fa-check"></i><b>17</b> Nonparametric Regression</a><ul>
<li class="chapter" data-level="17.1" data-path="nonparametric.html"><a href="nonparametric.html#conditional-averages"><i class="fa fa-check"></i><b>17.1</b> Conditional Averages</a></li>
<li class="chapter" data-level="17.2" data-path="nonparametric.html"><a href="nonparametric.html#looking-at-the-neighbors"><i class="fa fa-check"></i><b>17.2</b> Looking at the Neighbors</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="knn.html"><a href="knn.html"><i class="fa fa-check"></i><b>18</b> Nearest Neighbor Estimates</a><ul>
<li class="chapter" data-level="18.1" data-path="knn.html"><a href="knn.html#introduction-3"><i class="fa fa-check"></i><b>18.1</b> Introduction</a></li>
<li class="chapter" data-level="18.2" data-path="knn.html"><a href="knn.html#k-nearest-neighbors-knn"><i class="fa fa-check"></i><b>18.2</b> <span class="math inline">\(k\)</span> Nearest Neighbors (KNN)</a><ul>
<li class="chapter" data-level="18.2.1" data-path="knn.html"><a href="knn.html#distance-measures"><i class="fa fa-check"></i><b>18.2.1</b> Distance Measures</a></li>
<li class="chapter" data-level="18.2.2" data-path="knn.html"><a href="knn.html#knn-estimator"><i class="fa fa-check"></i><b>18.2.2</b> KNN Estimator</a></li>
<li class="chapter" data-level="18.2.3" data-path="knn.html"><a href="knn.html#how-to-find-k"><i class="fa fa-check"></i><b>18.2.3</b> How to find <span class="math inline">\(k\)</span>?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html"><i class="fa fa-check"></i><b>19</b> Kernel Smoothers</a><ul>
<li class="chapter" data-level="19.1" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#introduction-4"><i class="fa fa-check"></i><b>19.1</b> Introduction</a></li>
<li class="chapter" data-level="19.2" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#kernel-smoothers-1"><i class="fa fa-check"></i><b>19.2</b> Kernel Smoothers</a><ul>
<li class="chapter" data-level="19.2.1" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#kernel-functions"><i class="fa fa-check"></i><b>19.2.1</b> Kernel Functions</a></li>
<li class="chapter" data-level="19.2.2" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#weights-from-kernels"><i class="fa fa-check"></i><b>19.2.2</b> Weights from Kernels</a></li>
<li class="chapter" data-level="19.2.3" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#kernel-estimator"><i class="fa fa-check"></i><b>19.2.3</b> Kernel Estimator</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#local-polynomial-estimators"><i class="fa fa-check"></i><b>19.3</b> Local Polynomial Estimators</a></li>
</ul></li>
<li class="part"><span><b>VIII Classification</b></span></li>
<li class="chapter" data-level="20" data-path="classif.html"><a href="classif.html"><i class="fa fa-check"></i><b>20</b> Classification Methods</a></li>
<li class="chapter" data-level="21" data-path="logistic.html"><a href="logistic.html"><i class="fa fa-check"></i><b>21</b> Logistic Regression</a><ul>
<li class="chapter" data-level="21.1" data-path="logistic.html"><a href="logistic.html#motivation-1"><i class="fa fa-check"></i><b>21.1</b> Motivation</a><ul>
<li class="chapter" data-level="21.1.1" data-path="logistic.html"><a href="logistic.html#first-approach-fitting-a-line"><i class="fa fa-check"></i><b>21.1.1</b> First Approach: Fitting a Line</a></li>
<li class="chapter" data-level="21.1.2" data-path="logistic.html"><a href="logistic.html#second-approach-harsh-thresholding"><i class="fa fa-check"></i><b>21.1.2</b> Second Approach: Harsh Thresholding</a></li>
<li class="chapter" data-level="21.1.3" data-path="logistic.html"><a href="logistic.html#third-approach-conditional-means"><i class="fa fa-check"></i><b>21.1.3</b> Third Approach: Conditional Means</a></li>
</ul></li>
<li class="chapter" data-level="21.2" data-path="logistic.html"><a href="logistic.html#logistic-regression-model"><i class="fa fa-check"></i><b>21.2</b> Logistic Regression Model</a><ul>
<li class="chapter" data-level="21.2.1" data-path="logistic.html"><a href="logistic.html#the-criterion-being-optimized"><i class="fa fa-check"></i><b>21.2.1</b> The Criterion Being Optimized</a></li>
<li class="chapter" data-level="21.2.2" data-path="logistic.html"><a href="logistic.html#another-way-to-solve-logistic-regression"><i class="fa fa-check"></i><b>21.2.2</b> Another Way to Solve Logistic Regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="22" data-path="discrim.html"><a href="discrim.html"><i class="fa fa-check"></i><b>22</b> Preamble for Discriminant Analysis</a><ul>
<li class="chapter" data-level="22.1" data-path="discrim.html"><a href="discrim.html#motivation-2"><i class="fa fa-check"></i><b>22.1</b> Motivation</a><ul>
<li class="chapter" data-level="22.1.1" data-path="discrim.html"><a href="discrim.html#distinguishing-species"><i class="fa fa-check"></i><b>22.1.1</b> Distinguishing Species</a></li>
<li class="chapter" data-level="22.1.2" data-path="discrim.html"><a href="discrim.html#sum-of-squares-decomposition"><i class="fa fa-check"></i><b>22.1.2</b> Sum of Squares Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="22.2" data-path="discrim.html"><a href="discrim.html#derived-ratios-from-sum-of-squares"><i class="fa fa-check"></i><b>22.2</b> Derived Ratios from Sum-of-Squares</a><ul>
<li class="chapter" data-level="22.2.1" data-path="discrim.html"><a href="discrim.html#correlation-ratio"><i class="fa fa-check"></i><b>22.2.1</b> Correlation Ratio</a></li>
<li class="chapter" data-level="22.2.2" data-path="discrim.html"><a href="discrim.html#f-ratio"><i class="fa fa-check"></i><b>22.2.2</b> F-Ratio</a></li>
</ul></li>
<li class="chapter" data-level="22.3" data-path="discrim.html"><a href="discrim.html#geometric-perspective"><i class="fa fa-check"></i><b>22.3</b> Geometric Perspective</a><ul>
<li class="chapter" data-level="22.3.1" data-path="discrim.html"><a href="discrim.html#clouds-from-class-structure"><i class="fa fa-check"></i><b>22.3.1</b> Clouds from Class Structure</a></li>
<li class="chapter" data-level="22.3.2" data-path="discrim.html"><a href="discrim.html#dispersion-decomposition"><i class="fa fa-check"></i><b>22.3.2</b> Dispersion Decomposition</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="23" data-path="cda.html"><a href="cda.html"><i class="fa fa-check"></i><b>23</b> Canonical Discriminant Analysis</a><ul>
<li class="chapter" data-level="23.0.1" data-path="cda.html"><a href="cda.html#looking-for-a-discriminant-axis"><i class="fa fa-check"></i><b>23.0.1</b> Looking for a discriminant axis</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="discanalysis.html"><a href="discanalysis.html"><i class="fa fa-check"></i><b>24</b> Discriminant Analysis</a><ul>
<li class="chapter" data-level="24.1" data-path="discanalysis.html"><a href="discanalysis.html#probabilistic-da"><i class="fa fa-check"></i><b>24.1</b> Probabilistic DA</a></li>
<li class="chapter" data-level="24.2" data-path="discanalysis.html"><a href="discanalysis.html#estimations"><i class="fa fa-check"></i><b>24.2</b> Estimations</a><ul>
<li class="chapter" data-level="24.2.1" data-path="discanalysis.html"><a href="discanalysis.html#univariate-data-boldsymbolp-1"><i class="fa fa-check"></i><b>24.2.1</b> Univariate Data (<span class="math inline">\(\boldsymbol{p = 1}\)</span>)</a></li>
<li class="chapter" data-level="24.2.2" data-path="discanalysis.html"><a href="discanalysis.html#multivariate-data-boldsymbolp-1"><i class="fa fa-check"></i><b>24.2.2</b> Multivariate Data (<span class="math inline">\(\boldsymbol{p &gt; 1}\)</span>)</a></li>
<li class="chapter" data-level="24.2.3" data-path="discanalysis.html"><a href="discanalysis.html#multivariate-estimations"><i class="fa fa-check"></i><b>24.2.3</b> Multivariate Estimations</a></li>
</ul></li>
<li class="chapter" data-level="24.3" data-path="discanalysis.html"><a href="discanalysis.html#discriminant-functions"><i class="fa fa-check"></i><b>24.3</b> Discriminant Functions</a></li>
<li class="chapter" data-level="24.4" data-path="discanalysis.html"><a href="discanalysis.html#quadratic-discriminant-analysis-qda"><i class="fa fa-check"></i><b>24.4</b> Quadratic Discriminant Analysis (QDA)</a></li>
<li class="chapter" data-level="24.5" data-path="discanalysis.html"><a href="discanalysis.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>24.5</b> Linear Discriminant Analysis</a></li>
<li class="chapter" data-level="24.6" data-path="discanalysis.html"><a href="discanalysis.html#canonical-discriminant-analysis"><i class="fa fa-check"></i><b>24.6</b> Canonical Discriminant Analysis</a></li>
<li class="chapter" data-level="24.7" data-path="discanalysis.html"><a href="discanalysis.html#naive-bayes"><i class="fa fa-check"></i><b>24.7</b> Naive Bayes</a></li>
<li class="chapter" data-level="24.8" data-path="discanalysis.html"><a href="discanalysis.html#fifth-case"><i class="fa fa-check"></i><b>24.8</b> Fifth Case</a></li>
<li class="chapter" data-level="24.9" data-path="discanalysis.html"><a href="discanalysis.html#comparing-the-cases"><i class="fa fa-check"></i><b>24.9</b> Comparing the Cases</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="classperformance.html"><a href="classperformance.html"><i class="fa fa-check"></i><b>25</b> Performance of Classifiers</a><ul>
<li class="chapter" data-level="25.1" data-path="classperformance.html"><a href="classperformance.html#error-for-binary-response"><i class="fa fa-check"></i><b>25.1</b> Error for Binary Response</a></li>
<li class="chapter" data-level="25.2" data-path="classperformance.html"><a href="classperformance.html#categorical-response"><i class="fa fa-check"></i><b>25.2</b> Categorical Response</a></li>
<li class="chapter" data-level="25.3" data-path="classperformance.html"><a href="classperformance.html#confusion-matrices"><i class="fa fa-check"></i><b>25.3</b> Confusion Matrices</a></li>
<li class="chapter" data-level="25.4" data-path="classperformance.html"><a href="classperformance.html#binary-response-example"><i class="fa fa-check"></i><b>25.4</b> Binary Response: Example</a><ul>
<li class="chapter" data-level="25.4.1" data-path="classperformance.html"><a href="classperformance.html#types-of-errors-application-for-savings-account"><i class="fa fa-check"></i><b>25.4.1</b> Types of Errors: Application for Savings Account</a></li>
<li class="chapter" data-level="25.4.2" data-path="classperformance.html"><a href="classperformance.html#another-example-loans"><i class="fa fa-check"></i><b>25.4.2</b> Another Example: Loans</a></li>
<li class="chapter" data-level="25.4.3" data-path="classperformance.html"><a href="classperformance.html#the-ideal-situation"><i class="fa fa-check"></i><b>25.4.3</b> The Ideal Situation</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IX Unsupervised II: Clustering</b></span></li>
<li class="chapter" data-level="26" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>26</b> Clustering</a><ul>
<li class="chapter" data-level="26.1" data-path="clustering.html"><a href="clustering.html#about-clustering"><i class="fa fa-check"></i><b>26.1</b> About Clustering</a><ul>
<li class="chapter" data-level="26.1.1" data-path="clustering.html"><a href="clustering.html#types-of-clustering"><i class="fa fa-check"></i><b>26.1.1</b> Types of Clustering</a></li>
<li class="chapter" data-level="26.1.2" data-path="clustering.html"><a href="clustering.html#hard-clustering"><i class="fa fa-check"></i><b>26.1.2</b> Hard Clustering</a></li>
</ul></li>
<li class="chapter" data-level="26.2" data-path="clustering.html"><a href="clustering.html#dispersion-measures"><i class="fa fa-check"></i><b>26.2</b> Dispersion Measures</a></li>
<li class="chapter" data-level="26.3" data-path="clustering.html"><a href="clustering.html#complexity-in-clustering"><i class="fa fa-check"></i><b>26.3</b> Complexity in Clustering</a></li>
</ul></li>
<li class="chapter" data-level="27" data-path="kmeans.html"><a href="kmeans.html"><i class="fa fa-check"></i><b>27</b> K-Means</a><ul>
<li class="chapter" data-level="27.1" data-path="kmeans.html"><a href="kmeans.html#toy-example-1"><i class="fa fa-check"></i><b>27.1</b> Toy Example</a></li>
<li class="chapter" data-level="27.2" data-path="kmeans.html"><a href="kmeans.html#what-does-k-means-do"><i class="fa fa-check"></i><b>27.2</b> What does K-means do?</a></li>
<li class="chapter" data-level="27.3" data-path="kmeans.html"><a href="kmeans.html#k-means-algorithms"><i class="fa fa-check"></i><b>27.3</b> K-Means Algorithms</a><ul>
<li class="chapter" data-level="27.3.1" data-path="kmeans.html"><a href="kmeans.html#classic-version"><i class="fa fa-check"></i><b>27.3.1</b> Classic Version</a></li>
<li class="chapter" data-level="27.3.2" data-path="kmeans.html"><a href="kmeans.html#moving-centers-algorithm"><i class="fa fa-check"></i><b>27.3.2</b> Moving Centers Algorithm</a></li>
<li class="chapter" data-level="27.3.3" data-path="kmeans.html"><a href="kmeans.html#dynamic-clouds"><i class="fa fa-check"></i><b>27.3.3</b> Dynamic Clouds</a></li>
<li class="chapter" data-level="27.3.4" data-path="kmeans.html"><a href="kmeans.html#choosing-k"><i class="fa fa-check"></i><b>27.3.4</b> Choosing <span class="math inline">\(k\)</span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="28" data-path="hclus.html"><a href="hclus.html"><i class="fa fa-check"></i><b>28</b> Hierarchical Clustering</a><ul>
<li class="chapter" data-level="28.1" data-path="hclus.html"><a href="hclus.html#agglomerative-methods"><i class="fa fa-check"></i><b>28.1</b> Agglomerative Methods</a></li>
<li class="chapter" data-level="28.2" data-path="hclus.html"><a href="hclus.html#example-single-linkage"><i class="fa fa-check"></i><b>28.2</b> Example: Single Linkage</a><ul>
<li class="chapter" data-level="28.2.1" data-path="hclus.html"><a href="hclus.html#dendrogram"><i class="fa fa-check"></i><b>28.2.1</b> Dendrogram</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>X Tree-based Methods</b></span></li>
<li class="chapter" data-level="29" data-path="trees.html"><a href="trees.html"><i class="fa fa-check"></i><b>29</b> Intro to Decision Trees</a><ul>
<li class="chapter" data-level="29.1" data-path="trees.html"><a href="trees.html#introduction-5"><i class="fa fa-check"></i><b>29.1</b> Introduction</a></li>
<li class="chapter" data-level="29.2" data-path="trees.html"><a href="trees.html#binary-trees"><i class="fa fa-check"></i><b>29.2</b> Binary Trees</a><ul>
<li class="chapter" data-level="29.2.1" data-path="trees.html"><a href="trees.html#the-process-of-building-a-tree"><i class="fa fa-check"></i><b>29.2.1</b> The Process of Building a Tree</a></li>
<li class="chapter" data-level="29.2.2" data-path="trees.html"><a href="trees.html#binary-partitions"><i class="fa fa-check"></i><b>29.2.2</b> Binary Partitions</a></li>
</ul></li>
<li class="chapter" data-level="29.3" data-path="trees.html"><a href="trees.html#measures-of-heterogeneity"><i class="fa fa-check"></i><b>29.3</b> Measures of Heterogeneity</a><ul>
<li class="chapter" data-level="29.3.1" data-path="trees.html"><a href="trees.html#entropy"><i class="fa fa-check"></i><b>29.3.1</b> Entropy</a></li>
<li class="chapter" data-level="29.3.2" data-path="trees.html"><a href="trees.html#the-math-behind-entropy"><i class="fa fa-check"></i><b>29.3.2</b> The Math Behind Entropy</a></li>
<li class="chapter" data-level="29.3.3" data-path="trees.html"><a href="trees.html#gini-impurity"><i class="fa fa-check"></i><b>29.3.3</b> Gini Impurity</a></li>
<li class="chapter" data-level="29.3.4" data-path="trees.html"><a href="trees.html#toy-example-2"><i class="fa fa-check"></i><b>29.3.4</b> Toy Example</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Made with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">All Models Are Wrong</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="kmeans" class="section level1">
<h1><span class="header-section-number">27</span> K-Means</h1>
<p>With partitioning methods, two clusters are always separated; and the
number of clusters is generally defined a priori. The main methods are:</p>
<ul>
<li><span class="math inline">\(K\)</span>-means</li>
<li><span class="math inline">\(K\)</span>-medoids</li>
<li><span class="math inline">\(K\)</span>-prototypes</li>
<li>Methods based on a concept of density</li>
<li>Kohonen networks (or maps)</li>
</ul>
<div id="toy-example-1" class="section level2">
<h2><span class="header-section-number">27.1</span> Toy Example</h2>
<p>Imagine we are in <span class="math inline">\(\mathbb{R}^n\)</span> (or <span class="math inline">\(\mathbb{R}^p\)</span>, depending on if we are
looking at things from the individuals perspective or variables perspective).
For illustrative purposes, let’s consider two clusters, that is <span class="math inline">\(K = 2\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-247"></span>
<img src="images/cluster/kmeans-step1.svg" alt="Toy data set" width="90%" />
<p class="caption">
Figure 27.1: Toy data set
</p>
</div>
<p>We start by initializing two centroids <span class="math inline">\(\mathbf{g_1}\)</span> and <span class="math inline">\(\mathbf{g_2}\)</span>.
Typically, the way centroids are initialized is by randomly selecting two
objects. Sometimes, however, a more “educated” initialization may be possible
when we have additional information about where the cluster may be located.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-248"></span>
<img src="images/cluster/kmeans-step2.svg" alt="Arbitrary centroids" width="90%" />
<p class="caption">
Figure 27.2: Arbitrary centroids
</p>
</div>
<p>Given a new data point, we compute the distances from this point to the two
centroids, and classify the point to the cluster to which the closest centroid
belongs.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-249"></span>
<img src="images/cluster/kmeans-step3.svg" alt="Distance of an object to both centroids" width="90%" />
<p class="caption">
Figure 27.3: Distance of an object to both centroids
</p>
</div>
<p>The distances of all objects to the centroids are computed as well, and the
objects get assigned to the group of the closest centroid.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-250"></span>
<img src="images/cluster/kmeans-step4.svg" alt="Assigning objects to closest centroid" width="90%" />
<p class="caption">
Figure 27.4: Assigning objects to closest centroid
</p>
</div>
<p>Now, with this new grouped data, we have to recompute the centroids:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-251"></span>
<img src="images/cluster/kmeans-step5.svg" alt="Update centroids" width="90%" />
<p class="caption">
Figure 27.5: Update centroids
</p>
</div>
<p>We repeat this procedure, that is, calculate distances of all objects to the
centroids, and re-assigning objects to clusters.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-252"></span>
<img src="images/cluster/kmeans-step6.svg" alt="Reassigning objects" width="90%" />
<p class="caption">
Figure 27.6: Reassigning objects
</p>
</div>
<p>Likewise, after assigning objects to new clusters, new centroids are updated.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-253"></span>
<img src="images/cluster/kmeans-step7.svg" alt="Recalculate centroids" width="90%" />
<p class="caption">
Figure 27.7: Recalculate centroids
</p>
</div>
<p>After enough iterations, the centroids will not move too much between each
iteration. At this point we will say the algorithm as converged, and we stop
(and the resulting clusters will be the ones we use).</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-254"></span>
<img src="images/cluster/kmeans-step8.svg" alt="Rinse and repeat until convergence" width="90%" />
<p class="caption">
Figure 27.8: Rinse and repeat until convergence
</p>
</div>
<p>Is this algorithm stable, in the sense that if we repeat this 10 times will be
obtain the same final clusters each time? The answer is “no.” The good news is
that we are guaranteed convergence. The bad news is that we may arrive at a
different final clustering. There is actually another good news, which is that
this algorithm converges quite rapidly! (The complexity is <span class="math inline">\(n \cdot k\)</span>).</p>
</div>
<div id="what-does-k-means-do" class="section level2">
<h2><span class="header-section-number">27.2</span> What does K-means do?</h2>
<p>Consider the <span class="math inline">\(s\)</span>-th step of the algorithm:</p>
<p><span class="math display">\[
\textbf{s-th step}, \quad \textbf{s} \rightarrow \textbf{s+1}, \quad \textbf{then s+1}
\]</span></p>
<p>Specifically, let us focus on the intermediate step, when step <span class="math inline">\(s\)</span> goes to step
<span class="math inline">\(s+1\)</span>. In other words, the centroids have moved but we have not reassigned
points. At step <span class="math inline">\(s\)</span> we would compute <span class="math inline">\(\mathrm{WSSD}_1\)</span> and <span class="math inline">\(\mathrm{WSSD}_2\)</span> which,
if we are using the Euclidean norm, becomes</p>
<p><span class="math display" id="eq:802-1">\[
\mathrm{WSSD}^{(s)} =  \sum_{i \in C_1^{(s)}} d^2(i, g_1^{(s)})  +  \sum_{i \in C_2^{(s)}} d^2(i, g_2^{(s)})
\tag{27.1}
\]</span></p>
<p>At the end of the <span class="math inline">\(s\)</span>-th step, we update centroids, obtaining <span class="math inline">\(g_{1}^{(s + 1)}\)</span> and
<span class="math inline">\(g_{2}^{(s + 1)}\)</span>. These are the centroids used during the assignment of step <span class="math inline">\(s+1\)</span>.
Notice that there is an intermediate step, with new centroids, but objects
clustered according to <span class="math inline">\(C^{(s)}_1\)</span> and <span class="math inline">\(C^{(s)}_2\)</span>. Technically, we can compute
within-group dispersions during this intermediate step, given by:</p>
<p><span class="math display" id="eq:802-2">\[
\mathrm{WSSD}^{(s \to s+1)} =    \sum_{i \in C_1^{(s)}} d^2(i, g_1^{(s + 1)})  +  \sum_{i \in C_2^{(s)}} d^2(i, g_1^{(s + 1)})
\tag{27.2}
\]</span></p>
<p>At the end of step <span class="math inline">\(s+1\)</span>, objects are clustered under new configurations
<span class="math inline">\(C^{(s+1)}_1\)</span> and <span class="math inline">\(C^{(s+1)}_2\)</span>. Likewise, the within-group dispersions at the
end of step <span class="math inline">\(s+1\)</span> become:</p>
<p><span class="math display" id="eq:802-3">\[
\mathrm{WSSD}^{(s+1)}  =  \sum_{i \in C_1^{(s + 1)}} d^2(i, g_1^{(s + 1)})  +  \sum_{i \in C_2^{(s + 1)}} d^2(i, g_1^{(s + 1)})
\tag{27.3}
\]</span></p>
<p>It can be proved that these three dispersions are in order of smallest to largest
(i.e. the intermediate is bigger than the <span class="math inline">\(s\)</span>-th step, the <span class="math inline">\(s+1\)</span>-th step is larger
than the intermediate). In other words,</p>
<p><span class="math display" id="eq:802-4">\[
\mathrm{WSSD}^{(s)} \geq \mathrm{WSSD}^{(s \to s+1)}  \geq \mathrm{WSSD}^{(s+1)}
\tag{27.4}
\]</span></p>
</div>
<div id="k-means-algorithms" class="section level2">
<h2><span class="header-section-number">27.3</span> K-Means Algorithms</h2>
<p>In <span class="math inline">\(K\)</span>-means clustering, we must first define the number of clusters we want
(for example, <span class="math inline">\(K = 3\)</span> clusters).</p>
<ol style="list-style-type: decimal">
<li><p>Define <span class="math inline">\(K\)</span>, the number of clusters we want (for example, <span class="math inline">\(K = 3\)</span>).</p></li>
<li><p>Initialize <span class="math inline">\(K\)</span> random centroids.</p></li>
</ol>
<div id="classic-version" class="section level3">
<h3><span class="header-section-number">27.3.1</span> Classic Version</h3>
<p>The original <span class="math inline">\(K\)</span>-means algorithm was proposed by MacQueen in 1967, and is as
follows:</p>
<ol start="0" style="list-style-type: decimal">
<li><p>Define <span class="math inline">\(K\)</span>, the number of clusters we want.</p></li>
<li><p>Start with <span class="math inline">\(K\)</span> random centroids.</p></li>
<li><p>Start examining a particular individual object, and assign it to the group of the closest centroid.</p></li>
<li><p>Recompute the centroid for this assigned cluster.</p></li>
<li><p>Assign the next object to the group of the closest centroid.</p></li>
<li><p>Recompute the centroid for this assigned cluster.</p></li>
<li><p>Repeat this procedure until all objects have been assigned.</p></li>
</ol>
<p>Note that this algorithm proceeds object-by-object, re-computing centroids after
each object has been assigned. That is, if we have <span class="math inline">\(n\)</span> objects, we will
recompute our centroid <span class="math inline">\(n\)</span> times.</p>
</div>
<div id="moving-centers-algorithm" class="section level3">
<h3><span class="header-section-number">27.3.2</span> Moving Centers Algorithm</h3>
<p>This version of <span class="math inline">\(K\)</span>-means was proposed by Forgy in 1965, and is as follows:</p>
<ol start="0" style="list-style-type: decimal">
<li><p>Define <span class="math inline">\(K\)</span>, the number of clusters we want.</p></li>
<li><p>Start with <span class="math inline">\(K\)</span> random centroids.</p></li>
<li><p>Assign each object to the nearest centroid, <em>without</em> updating the centroids until all objects have been assigned.</p></li>
<li><p>Now, recalculate the centroids.</p></li>
<li><p>Repeat steps (2) and (3) until convergence.</p></li>
</ol>
<p>Of course, this algorithm requires some criterion to define “convergence.”
Some stopping mechanisms could be:</p>
<ul>
<li><p>Stop when <span class="math inline">\(\mathrm{WSSD}^{(s)}\)</span> is equal (or roughly equal to) <span class="math inline">\(\mathrm{WSSD}^{(s + 1)}\)</span>.</p></li>
<li><p>Assign a maximum number of iterations (e.g. we stop the algorithm after 10 iterations).</p></li>
</ul>
<p>This algorithm is the most popular out of the other two discussed.</p>
</div>
<div id="dynamic-clouds" class="section level3">
<h3><span class="header-section-number">27.3.3</span> Dynamic Clouds</h3>
<p>This algorithm was proposed by Diday in 1971, and is as follows:</p>
<ol start="0" style="list-style-type: decimal">
<li><p>Instead of centroids, we work with “kernels” (i.e. a group of objects; that is, we no longer consider individual points (centroids) but rather groups of objects).</p></li>
<li><p>Repeat the steps from the other algorithms.</p></li>
</ol>
</div>
<div id="choosing-k" class="section level3">
<h3><span class="header-section-number">27.3.4</span> Choosing <span class="math inline">\(k\)</span></h3>
<p>How many groups should be pick? Well, the answer is highly dependent on our
initial centroids. The algorithm will always converge at local minima, but there
is no guarantee it will arrive at the global minimum. One idea is to run
<span class="math inline">\(K\)</span>-means multiple times, to get a sense of how stable our data is.</p>
<p>Note that cross-validation <strong>cannot</strong> be used to determine <span class="math inline">\(K^*\)</span>, the optimal
number of clusters. For an explanation of why this is, see pages 518-9 in
<a href="https://web.stanford.edu/~hastie/ElemStatLearn/">ESL</a>.</p>
<p>There is another proposal on how to choose a “good” number of clusters, but it
involves hierarchical clustering, the topic of the next chapter.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="clustering.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="hclus.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["allmodelsarewrong.pdf", "allmodelsarewrong.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
