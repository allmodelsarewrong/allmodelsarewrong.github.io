<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>28 Discriminant Analysis | All Models Are Wrong: Concepts of Statistical Learning</title>
  <meta name="description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  <meta name="generator" content="bookdown 0.16 and GitBook 2.6.7" />

  <meta property="og:title" content="28 Discriminant Analysis | All Models Are Wrong: Concepts of Statistical Learning" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://allmodelsarewrong.github.io" />
  
  <meta property="og:description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  <meta name="github-repo" content="allmodelsarewrong/allmodelsarewrong.github.io" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="28 Discriminant Analysis | All Models Are Wrong: Concepts of Statistical Learning" />
  
  <meta name="twitter:description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  

<meta name="author" content="Gaston Sanchez Ethan Marzban" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="cda.html"/>
<link rel="next" href="classperformance.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><b>All Models Are Wrong (CSL)</b><br><small>G. Sanchez & E. Marzban</small></a></li>

<li class="divider"></li>
<li class="part"><span><b>I Welcome</b></span></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="about.html"><a href="about.html"><i class="fa fa-check"></i><b>1</b> About this book</a><ul>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#prerequisites"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="part"><span><b>II Intro</b></span></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#basic-notation"><i class="fa fa-check"></i><b>2.1</b> Basic Notation</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="duality.html"><a href="duality.html"><i class="fa fa-check"></i><b>3</b> Geometric Duality</a><ul>
<li class="chapter" data-level="3.1" data-path="duality.html"><a href="duality.html#rows-space"><i class="fa fa-check"></i><b>3.1</b> Rows Space</a></li>
<li class="chapter" data-level="3.2" data-path="duality.html"><a href="duality.html#columns-space"><i class="fa fa-check"></i><b>3.2</b> Columns Space</a></li>
<li class="chapter" data-level="3.3" data-path="duality.html"><a href="duality.html#cloud-of-individuals"><i class="fa fa-check"></i><b>3.3</b> Cloud of Individuals</a><ul>
<li class="chapter" data-level="3.3.1" data-path="duality.html"><a href="duality.html#average-individual"><i class="fa fa-check"></i><b>3.3.1</b> Average Individual</a></li>
<li class="chapter" data-level="3.3.2" data-path="duality.html"><a href="duality.html#centered-data"><i class="fa fa-check"></i><b>3.3.2</b> Centered Data</a></li>
<li class="chapter" data-level="3.3.3" data-path="duality.html"><a href="duality.html#distance-between-individuals"><i class="fa fa-check"></i><b>3.3.3</b> Distance between individuals</a></li>
<li class="chapter" data-level="3.3.4" data-path="duality.html"><a href="duality.html#distance-to-the-centroid"><i class="fa fa-check"></i><b>3.3.4</b> Distance to the centroid</a></li>
<li class="chapter" data-level="3.3.5" data-path="duality.html"><a href="duality.html#measures-of-dispersion"><i class="fa fa-check"></i><b>3.3.5</b> Measures of Dispersion</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="duality.html"><a href="duality.html#cloud-of-variables"><i class="fa fa-check"></i><b>3.4</b> Cloud of Variables</a><ul>
<li class="chapter" data-level="3.4.1" data-path="duality.html"><a href="duality.html#mean-of-a-variable"><i class="fa fa-check"></i><b>3.4.1</b> Mean of a Variable</a></li>
<li class="chapter" data-level="3.4.2" data-path="duality.html"><a href="duality.html#variance-of-a-variable"><i class="fa fa-check"></i><b>3.4.2</b> Variance of a Variable</a></li>
<li class="chapter" data-level="3.4.3" data-path="duality.html"><a href="duality.html#variance-with-vector-notation"><i class="fa fa-check"></i><b>3.4.3</b> Variance with Vector Notation</a></li>
<li class="chapter" data-level="3.4.4" data-path="duality.html"><a href="duality.html#standard-deviation-as-a-norm"><i class="fa fa-check"></i><b>3.4.4</b> Standard Deviation as a Norm</a></li>
<li class="chapter" data-level="3.4.5" data-path="duality.html"><a href="duality.html#covariance"><i class="fa fa-check"></i><b>3.4.5</b> Covariance</a></li>
<li class="chapter" data-level="3.4.6" data-path="duality.html"><a href="duality.html#correlation"><i class="fa fa-check"></i><b>3.4.6</b> Correlation</a></li>
<li class="chapter" data-level="3.4.7" data-path="duality.html"><a href="duality.html#geometry-of-correlation"><i class="fa fa-check"></i><b>3.4.7</b> Geometry of Correlation</a></li>
<li class="chapter" data-level="3.4.8" data-path="duality.html"><a href="duality.html#orthogonal-projections"><i class="fa fa-check"></i><b>3.4.8</b> Orthogonal Projections</a></li>
<li class="chapter" data-level="3.4.9" data-path="duality.html"><a href="duality.html#the-mean-as-an-orthogonal-projection"><i class="fa fa-check"></i><b>3.4.9</b> The mean as an orthogonal projection</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Unsupervised I: PCA</b></span></li>
<li class="chapter" data-level="4" data-path="pca.html"><a href="pca.html"><i class="fa fa-check"></i><b>4</b> Principal Components Analysis</a><ul>
<li class="chapter" data-level="4.1" data-path="pca.html"><a href="pca.html#low-dimensional-representations"><i class="fa fa-check"></i><b>4.1</b> Low-dimensional Representations</a></li>
<li class="chapter" data-level="4.2" data-path="pca.html"><a href="pca.html#projections"><i class="fa fa-check"></i><b>4.2</b> Projections</a><ul>
<li class="chapter" data-level="4.2.1" data-path="pca.html"><a href="pca.html#vector-and-scalar-projections"><i class="fa fa-check"></i><b>4.2.1</b> Vector and Scalar Projections</a></li>
<li class="chapter" data-level="4.2.2" data-path="pca.html"><a href="pca.html#projected-inertia"><i class="fa fa-check"></i><b>4.2.2</b> Projected Inertia</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="pca.html"><a href="pca.html#maximization-problem"><i class="fa fa-check"></i><b>4.3</b> Maximization Problem</a><ul>
<li class="chapter" data-level="4.3.1" data-path="pca.html"><a href="pca.html#eigenvectors-of-mathbfs"><i class="fa fa-check"></i><b>4.3.1</b> Eigenvectors of <span class="math inline">\(\mathbf{S}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="pca.html"><a href="pca.html#another-perspective-of-pca"><i class="fa fa-check"></i><b>4.4</b> Another Perspective of PCA</a><ul>
<li class="chapter" data-level="4.4.1" data-path="pca.html"><a href="pca.html#finding-principal-components"><i class="fa fa-check"></i><b>4.4.1</b> Finding Principal Components</a></li>
<li class="chapter" data-level="4.4.2" data-path="pca.html"><a href="pca.html#finding-the-first-pc"><i class="fa fa-check"></i><b>4.4.2</b> Finding the first PC</a></li>
<li class="chapter" data-level="4.4.3" data-path="pca.html"><a href="pca.html#finding-the-second-pc"><i class="fa fa-check"></i><b>4.4.3</b> Finding the second PC</a></li>
<li class="chapter" data-level="4.4.4" data-path="pca.html"><a href="pca.html#finding-all-pcs"><i class="fa fa-check"></i><b>4.4.4</b> Finding all PCs</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="pca.html"><a href="pca.html#data-decomposition-model"><i class="fa fa-check"></i><b>4.5</b> Data Decomposition Model</a><ul>
<li class="chapter" data-level="4.5.1" data-path="pca.html"><a href="pca.html#alternative-approaches"><i class="fa fa-check"></i><b>4.5.1</b> Alternative Approaches</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Linear Regression</b></span></li>
<li class="chapter" data-level="5" data-path="ols.html"><a href="ols.html"><i class="fa fa-check"></i><b>5</b> Linear Regression</a><ul>
<li class="chapter" data-level="5.1" data-path="ols.html"><a href="ols.html#motivation"><i class="fa fa-check"></i><b>5.1</b> Motivation</a></li>
<li class="chapter" data-level="5.2" data-path="ols.html"><a href="ols.html#the-ideaintuition-of-regression"><i class="fa fa-check"></i><b>5.2</b> The Idea/Intuition of Regression</a></li>
<li class="chapter" data-level="5.3" data-path="ols.html"><a href="ols.html#the-linear-regression-model"><i class="fa fa-check"></i><b>5.3</b> The Linear Regression Model</a></li>
<li class="chapter" data-level="5.4" data-path="ols.html"><a href="ols.html#the-error-measure"><i class="fa fa-check"></i><b>5.4</b> The Error Measure</a></li>
<li class="chapter" data-level="5.5" data-path="ols.html"><a href="ols.html#the-least-squares-algorithm"><i class="fa fa-check"></i><b>5.5</b> The Least Squares Algorithm</a></li>
<li class="chapter" data-level="5.6" data-path="ols.html"><a href="ols.html#geometries-of-ols"><i class="fa fa-check"></i><b>5.6</b> Geometries of OLS</a><ul>
<li class="chapter" data-level="5.6.1" data-path="ols.html"><a href="ols.html#rows-perspective"><i class="fa fa-check"></i><b>5.6.1</b> Rows Perspective</a></li>
<li class="chapter" data-level="5.6.2" data-path="ols.html"><a href="ols.html#columns-perspective"><i class="fa fa-check"></i><b>5.6.2</b> Columns Perspective</a></li>
<li class="chapter" data-level="5.6.3" data-path="ols.html"><a href="ols.html#parameters-perspective"><i class="fa fa-check"></i><b>5.6.3</b> Parameters Perspective</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="gradient.html"><a href="gradient.html"><i class="fa fa-check"></i><b>6</b> Gradient Descent</a><ul>
<li class="chapter" data-level="6.1" data-path="gradient.html"><a href="gradient.html#error-surface"><i class="fa fa-check"></i><b>6.1</b> Error Surface</a></li>
<li class="chapter" data-level="6.2" data-path="gradient.html"><a href="gradient.html#idea-of-gradient-descent"><i class="fa fa-check"></i><b>6.2</b> Idea of Gradient Descent</a></li>
<li class="chapter" data-level="6.3" data-path="gradient.html"><a href="gradient.html#moving-down-an-error-surface"><i class="fa fa-check"></i><b>6.3</b> Moving Down an Error Surface</a><ul>
<li class="chapter" data-level="6.3.1" data-path="gradient.html"><a href="gradient.html#the-direction-of-mathbfv"><i class="fa fa-check"></i><b>6.3.1</b> The direction of <span class="math inline">\(\mathbf{v}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="gradient.html"><a href="gradient.html#gd-algorithm-for-linear-regression"><i class="fa fa-check"></i><b>6.4</b> GD Algorithm for Linear Regression</a><ul>
<li class="chapter" data-level="6.4.1" data-path="gradient.html"><a href="gradient.html#gd-algorithm-in-vector-matrix-notation"><i class="fa fa-check"></i><b>6.4.1</b> GD Algorithm in vector-matrix notation</a></li>
<li class="chapter" data-level="6.4.2" data-path="gradient.html"><a href="gradient.html#gd-algorithm-in-pointwise-notation"><i class="fa fa-check"></i><b>6.4.2</b> GD algorithm in pointwise notation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="olsml.html"><a href="olsml.html"><i class="fa fa-check"></i><b>7</b> Regression via Maximum Likelihood</a><ul>
<li class="chapter" data-level="7.1" data-path="olsml.html"><a href="olsml.html#linear-regression-reminder"><i class="fa fa-check"></i><b>7.1</b> Linear Regression Reminder</a><ul>
<li class="chapter" data-level="7.1.1" data-path="olsml.html"><a href="olsml.html#maximum-likelihood"><i class="fa fa-check"></i><b>7.1.1</b> Maximum Likelihood</a></li>
<li class="chapter" data-level="7.1.2" data-path="olsml.html"><a href="olsml.html#ml-estimator-of-sigma2"><i class="fa fa-check"></i><b>7.1.2</b> ML Estimator of <span class="math inline">\(\sigma^2\)</span></a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>V Learning Concepts</b></span></li>
<li class="chapter" data-level="8" data-path="learning.html"><a href="learning.html"><i class="fa fa-check"></i><b>8</b> Theoretical Framework</a><ul>
<li class="chapter" data-level="8.1" data-path="learning.html"><a href="learning.html#mental-map"><i class="fa fa-check"></i><b>8.1</b> Mental Map</a></li>
<li class="chapter" data-level="8.2" data-path="learning.html"><a href="learning.html#kinds-of-predictions"><i class="fa fa-check"></i><b>8.2</b> Kinds of Predictions</a><ul>
<li class="chapter" data-level="8.2.1" data-path="learning.html"><a href="learning.html#two-types-of-data"><i class="fa fa-check"></i><b>8.2.1</b> Two Types of Data</a></li>
<li class="chapter" data-level="8.2.2" data-path="learning.html"><a href="learning.html#two-types-of-predictions"><i class="fa fa-check"></i><b>8.2.2</b> Two Types of Predictions</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="learning.html"><a href="learning.html#two-types-of-errors"><i class="fa fa-check"></i><b>8.3</b> Two Types of Errors</a><ul>
<li class="chapter" data-level="8.3.1" data-path="learning.html"><a href="learning.html#individual-errors"><i class="fa fa-check"></i><b>8.3.1</b> Individual Errors</a></li>
<li class="chapter" data-level="8.3.2" data-path="learning.html"><a href="learning.html#overall-errors"><i class="fa fa-check"></i><b>8.3.2</b> Overall Errors</a></li>
<li class="chapter" data-level="8.3.3" data-path="learning.html"><a href="learning.html#probability-as-an-auxiliary-technicality"><i class="fa fa-check"></i><b>8.3.3</b> Probability as an Auxiliary Technicality</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="learning.html"><a href="learning.html#noisy-targets"><i class="fa fa-check"></i><b>8.4</b> Noisy Targets</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="mse.html"><a href="mse.html"><i class="fa fa-check"></i><b>9</b> MSE of Estimator</a><ul>
<li class="chapter" data-level="9.1" data-path="mse.html"><a href="mse.html#mse-of-an-estimator"><i class="fa fa-check"></i><b>9.1</b> MSE of an Estimator</a><ul>
<li class="chapter" data-level="9.1.1" data-path="mse.html"><a href="mse.html#prototypical-cases-of-bias-and-variance"><i class="fa fa-check"></i><b>9.1.1</b> Prototypical Cases of Bias and Variance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="biasvar.html"><a href="biasvar.html"><i class="fa fa-check"></i><b>10</b> Bias-Variance Tradeoff</a><ul>
<li class="chapter" data-level="10.1" data-path="biasvar.html"><a href="biasvar.html#introduction"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="biasvar.html"><a href="biasvar.html#motivation-example"><i class="fa fa-check"></i><b>10.2</b> Motivation Example</a><ul>
<li class="chapter" data-level="10.2.1" data-path="biasvar.html"><a href="biasvar.html#two-hypotheses"><i class="fa fa-check"></i><b>10.2.1</b> Two Hypotheses</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="biasvar.html"><a href="biasvar.html#learning-from-two-points"><i class="fa fa-check"></i><b>10.3</b> Learning from two points</a></li>
<li class="chapter" data-level="10.4" data-path="biasvar.html"><a href="biasvar.html#bias-variance-derivation"><i class="fa fa-check"></i><b>10.4</b> Bias-Variance Derivation</a><ul>
<li class="chapter" data-level="10.4.1" data-path="biasvar.html"><a href="biasvar.html#out-of-sample-predictions"><i class="fa fa-check"></i><b>10.4.1</b> Out-of-Sample Predictions</a></li>
<li class="chapter" data-level="10.4.2" data-path="biasvar.html"><a href="biasvar.html#noisy-target"><i class="fa fa-check"></i><b>10.4.2</b> Noisy Target</a></li>
<li class="chapter" data-level="10.4.3" data-path="biasvar.html"><a href="biasvar.html#types-of-theoretical-mses"><i class="fa fa-check"></i><b>10.4.3</b> Types of Theoretical MSEs</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="biasvar.html"><a href="biasvar.html#the-tradeoff"><i class="fa fa-check"></i><b>10.5</b> The Tradeoff</a><ul>
<li class="chapter" data-level="10.5.1" data-path="biasvar.html"><a href="biasvar.html#bias-variance-tradeoff-picture"><i class="fa fa-check"></i><b>10.5.1</b> Bias-Variance Tradeoff Picture</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="overfit.html"><a href="overfit.html"><i class="fa fa-check"></i><b>11</b> Overfitting</a><ul>
<li class="chapter" data-level="11.1" data-path="overfit.html"><a href="overfit.html#introduction-1"><i class="fa fa-check"></i><b>11.1</b> Introduction</a><ul>
<li class="chapter" data-level="11.1.1" data-path="overfit.html"><a href="overfit.html#bias-variance-reminder-and-pitfalls"><i class="fa fa-check"></i><b>11.1.1</b> Bias-Variance Reminder and Pitfalls</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="overfit.html"><a href="overfit.html#simulation"><i class="fa fa-check"></i><b>11.2</b> Simulation</a><ul>
<li class="chapter" data-level="11.2.1" data-path="overfit.html"><a href="overfit.html#underfitting-overfitting-and-okayfitting"><i class="fa fa-check"></i><b>11.2.1</b> Underfitting, Overfitting and Okayfitting</a></li>
<li class="chapter" data-level="11.2.2" data-path="overfit.html"><a href="overfit.html#more-learning-sets"><i class="fa fa-check"></i><b>11.2.2</b> More learning sets</a></li>
<li class="chapter" data-level="11.2.3" data-path="overfit.html"><a href="overfit.html#when-does-overfitting-occur"><i class="fa fa-check"></i><b>11.2.3</b> When does overfitting occur?</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="overfit.html"><a href="overfit.html#in-summary"><i class="fa fa-check"></i><b>11.3</b> In Summary</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="phases.html"><a href="phases.html"><i class="fa fa-check"></i><b>12</b> Learning Phases</a><ul>
<li class="chapter" data-level="12.1" data-path="phases.html"><a href="phases.html#introduction-2"><i class="fa fa-check"></i><b>12.1</b> Introduction</a></li>
<li class="chapter" data-level="12.2" data-path="phases.html"><a href="phases.html#model-assessment"><i class="fa fa-check"></i><b>12.2</b> Model Assessment</a><ul>
<li class="chapter" data-level="12.2.1" data-path="phases.html"><a href="phases.html#holdout-test-set"><i class="fa fa-check"></i><b>12.2.1</b> Holdout Test Set</a></li>
<li class="chapter" data-level="12.2.2" data-path="phases.html"><a href="phases.html#why-does-a-test-set-work"><i class="fa fa-check"></i><b>12.2.2</b> Why does a test set work?</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="phases.html"><a href="phases.html#model-selection"><i class="fa fa-check"></i><b>12.3</b> Model Selection</a><ul>
<li class="chapter" data-level="12.3.1" data-path="phases.html"><a href="phases.html#three-way-holdout-method"><i class="fa fa-check"></i><b>12.3.1</b> Three-way Holdout Method</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="phases.html"><a href="phases.html#model-training"><i class="fa fa-check"></i><b>12.4</b> Model Training</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="resampling.html"><a href="resampling.html"><i class="fa fa-check"></i><b>13</b> Resampling Approaches</a><ul>
<li class="chapter" data-level="13.1" data-path="resampling.html"><a href="resampling.html#general-sampling-blueprint"><i class="fa fa-check"></i><b>13.1</b> General Sampling Blueprint</a></li>
<li class="chapter" data-level="13.2" data-path="resampling.html"><a href="resampling.html#monte-carlo-cross-validation"><i class="fa fa-check"></i><b>13.2</b> Monte Carlo Cross Validation</a></li>
<li class="chapter" data-level="13.3" data-path="resampling.html"><a href="resampling.html#bootstrap-method"><i class="fa fa-check"></i><b>13.3</b> Bootstrap Method</a></li>
<li class="chapter" data-level="13.4" data-path="resampling.html"><a href="resampling.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>13.4</b> <span class="math inline">\(K\)</span>-Fold Cross-Validation</a><ul>
<li class="chapter" data-level="13.4.1" data-path="resampling.html"><a href="resampling.html#leave-one-out-cross-validation-loocv"><i class="fa fa-check"></i><b>13.4.1</b> Leave-One-Out Cross Validation (LOOCV)</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>VI Regularization</b></span></li>
<li class="chapter" data-level="14" data-path="regular.html"><a href="regular.html"><i class="fa fa-check"></i><b>14</b> Regularization Techniques</a><ul>
<li class="chapter" data-level="14.1" data-path="regular.html"><a href="regular.html#example-regularity-in-models"><i class="fa fa-check"></i><b>14.1</b> Example: Regularity in Models</a></li>
<li class="chapter" data-level="14.2" data-path="regular.html"><a href="regular.html#multicollinearity-issues"><i class="fa fa-check"></i><b>14.2</b> Multicollinearity Issues</a><ul>
<li class="chapter" data-level="14.2.1" data-path="regular.html"><a href="regular.html#toy-example"><i class="fa fa-check"></i><b>14.2.1</b> Toy Example</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="regular.html"><a href="regular.html#irregular-coefficients"><i class="fa fa-check"></i><b>14.3</b> Irregular Coefficients</a></li>
<li class="chapter" data-level="14.4" data-path="regular.html"><a href="regular.html#connection-to-regularization"><i class="fa fa-check"></i><b>14.4</b> Connection to Regularization</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="pcr.html"><a href="pcr.html"><i class="fa fa-check"></i><b>15</b> Principal Components Regression</a><ul>
<li class="chapter" data-level="15.1" data-path="pcr.html"><a href="pcr.html#motivation-example-1"><i class="fa fa-check"></i><b>15.1</b> Motivation Example</a></li>
<li class="chapter" data-level="15.2" data-path="pcr.html"><a href="pcr.html#the-pcr-model"><i class="fa fa-check"></i><b>15.2</b> The PCR Model</a></li>
<li class="chapter" data-level="15.3" data-path="pcr.html"><a href="pcr.html#how-does-pcr-work"><i class="fa fa-check"></i><b>15.3</b> How does PCR work?</a><ul>
<li class="chapter" data-level="15.3.1" data-path="pcr.html"><a href="pcr.html#transition-formula"><i class="fa fa-check"></i><b>15.3.1</b> Transition Formula</a></li>
<li class="chapter" data-level="15.3.2" data-path="pcr.html"><a href="pcr.html#size-of-coefficients"><i class="fa fa-check"></i><b>15.3.2</b> Size of Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="pcr.html"><a href="pcr.html#selecting-number-of-pcs"><i class="fa fa-check"></i><b>15.4</b> Selecting Number of PCs</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="pls.html"><a href="pls.html"><i class="fa fa-check"></i><b>16</b> Partial Least Squares Regression</a><ul>
<li class="chapter" data-level="16.1" data-path="pls.html"><a href="pls.html#motivation-example-2"><i class="fa fa-check"></i><b>16.1</b> Motivation Example</a></li>
<li class="chapter" data-level="16.2" data-path="pls.html"><a href="pls.html#the-plsr-model"><i class="fa fa-check"></i><b>16.2</b> The PLSR Model</a></li>
<li class="chapter" data-level="16.3" data-path="pls.html"><a href="pls.html#how-does-plsr-work"><i class="fa fa-check"></i><b>16.3</b> How does PLSR work?</a></li>
<li class="chapter" data-level="16.4" data-path="pls.html"><a href="pls.html#plsr-algorithm"><i class="fa fa-check"></i><b>16.4</b> PLSR Algorithm</a><ul>
<li class="chapter" data-level="16.4.1" data-path="pls.html"><a href="pls.html#pls-solution-with-original-variables"><i class="fa fa-check"></i><b>16.4.1</b> PLS Solution with original variables</a></li>
<li class="chapter" data-level="16.4.2" data-path="pls.html"><a href="pls.html#some-properties"><i class="fa fa-check"></i><b>16.4.2</b> Some Properties</a></li>
<li class="chapter" data-level="16.4.3" data-path="pls.html"><a href="pls.html#pls-regression-for-price-of-cars"><i class="fa fa-check"></i><b>16.4.3</b> PLS Regression for Price of cars</a></li>
<li class="chapter" data-level="16.4.4" data-path="pls.html"><a href="pls.html#size-of-coefficients-1"><i class="fa fa-check"></i><b>16.4.4</b> Size of Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="16.5" data-path="pls.html"><a href="pls.html#selecting-number-of-pls-components"><i class="fa fa-check"></i><b>16.5</b> Selecting Number of PLS Components</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="ridge.html"><a href="ridge.html"><i class="fa fa-check"></i><b>17</b> Ridge Regression</a><ul>
<li class="chapter" data-level="17.1" data-path="ridge.html"><a href="ridge.html#a-new-minimization-problem"><i class="fa fa-check"></i><b>17.1</b> A New Minimization Problem</a><ul>
<li class="chapter" data-level="17.1.1" data-path="ridge.html"><a href="ridge.html#constraining-regression-coefficients"><i class="fa fa-check"></i><b>17.1.1</b> Constraining Regression Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="ridge.html"><a href="ridge.html#a-new-minimization-solution"><i class="fa fa-check"></i><b>17.2</b> A New Minimization Solution</a></li>
<li class="chapter" data-level="17.3" data-path="ridge.html"><a href="ridge.html#what-does-rr-accomplish"><i class="fa fa-check"></i><b>17.3</b> What does RR accomplish?</a><ul>
<li class="chapter" data-level="17.3.1" data-path="ridge.html"><a href="ridge.html#how-to-find-lambda"><i class="fa fa-check"></i><b>17.3.1</b> How to find <span class="math inline">\(\lambda\)</span>?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="lasso.html"><a href="lasso.html"><i class="fa fa-check"></i><b>18</b> Lasso Regression</a><ul>
<li class="chapter" data-level="18.1" data-path="lasso.html"><a href="lasso.html#mathematical-setup"><i class="fa fa-check"></i><b>18.1</b> Mathematical Setup</a><ul>
<li class="chapter" data-level="18.1.1" data-path="lasso.html"><a href="lasso.html#closed-form"><i class="fa fa-check"></i><b>18.1.1</b> Closed Form?</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="lasso.html"><a href="lasso.html#geometric-visualization"><i class="fa fa-check"></i><b>18.2</b> Geometric Visualization</a><ul>
<li class="chapter" data-level="18.2.1" data-path="lasso.html"><a href="lasso.html#some-more-math-variable-selection-in-action"><i class="fa fa-check"></i><b>18.2.1</b> Some More Math: Variable Selection in Action</a></li>
<li class="chapter" data-level="18.2.2" data-path="lasso.html"><a href="lasso.html#example-with-mtcars"><i class="fa fa-check"></i><b>18.2.2</b> Example with <code>mtcars</code></a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="lasso.html"><a href="lasso.html#going-beyond"><i class="fa fa-check"></i><b>18.3</b> Going Beyond</a></li>
</ul></li>
<li class="part"><span><b>VII Extending Linear Regression</b></span></li>
<li class="chapter" data-level="19" data-path="linear-extensions.html"><a href="linear-extensions.html"><i class="fa fa-check"></i><b>19</b> Beyond Linear Regression</a><ul>
<li class="chapter" data-level="19.1" data-path="linear-extensions.html"><a href="linear-extensions.html#introduction-3"><i class="fa fa-check"></i><b>19.1</b> Introduction</a></li>
<li class="chapter" data-level="19.2" data-path="linear-extensions.html"><a href="linear-extensions.html#expanding-the-regression-horizon"><i class="fa fa-check"></i><b>19.2</b> Expanding the Regression Horizon</a><ul>
<li class="chapter" data-level="19.2.1" data-path="linear-extensions.html"><a href="linear-extensions.html#linearity"><i class="fa fa-check"></i><b>19.2.1</b> Linearity</a></li>
<li class="chapter" data-level="19.2.2" data-path="linear-extensions.html"><a href="linear-extensions.html#parametric-and-nonparametric"><i class="fa fa-check"></i><b>19.2.2</b> Parametric and Nonparametric</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="linear-extensions.html"><a href="linear-extensions.html#transforming-features"><i class="fa fa-check"></i><b>19.3</b> Transforming Features</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="basis.html"><a href="basis.html"><i class="fa fa-check"></i><b>20</b> Basis Expansion</a><ul>
<li class="chapter" data-level="20.1" data-path="basis.html"><a href="basis.html#basis-functions"><i class="fa fa-check"></i><b>20.1</b> Basis Functions</a></li>
<li class="chapter" data-level="20.2" data-path="basis.html"><a href="basis.html#linear-regression"><i class="fa fa-check"></i><b>20.2</b> Linear Regression</a></li>
<li class="chapter" data-level="20.3" data-path="basis.html"><a href="basis.html#polynomial-regression"><i class="fa fa-check"></i><b>20.3</b> Polynomial Regression</a></li>
<li class="chapter" data-level="20.4" data-path="basis.html"><a href="basis.html#gaussian-rbfs"><i class="fa fa-check"></i><b>20.4</b> Gaussian RBF’s</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="nonparametric.html"><a href="nonparametric.html"><i class="fa fa-check"></i><b>21</b> Nonparametric Regression</a><ul>
<li class="chapter" data-level="21.1" data-path="nonparametric.html"><a href="nonparametric.html#conditional-averages"><i class="fa fa-check"></i><b>21.1</b> Conditional Averages</a></li>
<li class="chapter" data-level="21.2" data-path="nonparametric.html"><a href="nonparametric.html#looking-at-the-neighbors"><i class="fa fa-check"></i><b>21.2</b> Looking at the Neighbors</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="knn.html"><a href="knn.html"><i class="fa fa-check"></i><b>22</b> Nearest Neighbor Estimates</a><ul>
<li class="chapter" data-level="22.1" data-path="knn.html"><a href="knn.html#introduction-4"><i class="fa fa-check"></i><b>22.1</b> Introduction</a></li>
<li class="chapter" data-level="22.2" data-path="knn.html"><a href="knn.html#k-nearest-neighbors-knn"><i class="fa fa-check"></i><b>22.2</b> <span class="math inline">\(k\)</span> Nearest Neighbors (KNN)</a><ul>
<li class="chapter" data-level="22.2.1" data-path="knn.html"><a href="knn.html#distance-measures"><i class="fa fa-check"></i><b>22.2.1</b> Distance Measures</a></li>
<li class="chapter" data-level="22.2.2" data-path="knn.html"><a href="knn.html#knn-estimator"><i class="fa fa-check"></i><b>22.2.2</b> KNN Estimator</a></li>
<li class="chapter" data-level="22.2.3" data-path="knn.html"><a href="knn.html#how-to-find-k"><i class="fa fa-check"></i><b>22.2.3</b> How to find <span class="math inline">\(k\)</span>?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="23" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html"><i class="fa fa-check"></i><b>23</b> Kernel Smoothers</a><ul>
<li class="chapter" data-level="23.1" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#introduction-5"><i class="fa fa-check"></i><b>23.1</b> Introduction</a></li>
<li class="chapter" data-level="23.2" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#kernel-smoothers-1"><i class="fa fa-check"></i><b>23.2</b> Kernel Smoothers</a><ul>
<li class="chapter" data-level="23.2.1" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#kernel-functions"><i class="fa fa-check"></i><b>23.2.1</b> Kernel Functions</a></li>
<li class="chapter" data-level="23.2.2" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#weights-from-kernels"><i class="fa fa-check"></i><b>23.2.2</b> Weights from Kernels</a></li>
<li class="chapter" data-level="23.2.3" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#kernel-estimator"><i class="fa fa-check"></i><b>23.2.3</b> Kernel Estimator</a></li>
</ul></li>
<li class="chapter" data-level="23.3" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#local-polynomial-estimators"><i class="fa fa-check"></i><b>23.3</b> Local Polynomial Estimators</a></li>
</ul></li>
<li class="part"><span><b>VIII Classification</b></span></li>
<li class="chapter" data-level="24" data-path="classif.html"><a href="classif.html"><i class="fa fa-check"></i><b>24</b> Classification</a><ul>
<li class="chapter" data-level="24.1" data-path="classif.html"><a href="classif.html#introduction-6"><i class="fa fa-check"></i><b>24.1</b> Introduction</a><ul>
<li class="chapter" data-level="24.1.1" data-path="classif.html"><a href="classif.html#credit-score-example"><i class="fa fa-check"></i><b>24.1.1</b> Credit Score Example</a></li>
<li class="chapter" data-level="24.1.2" data-path="classif.html"><a href="classif.html#toy-example-1"><i class="fa fa-check"></i><b>24.1.2</b> Toy Example</a></li>
<li class="chapter" data-level="24.1.3" data-path="classif.html"><a href="classif.html#two-class-problem"><i class="fa fa-check"></i><b>24.1.3</b> Two-class Problem</a></li>
<li class="chapter" data-level="24.1.4" data-path="classif.html"><a href="classif.html#bayes-rule-reminder"><i class="fa fa-check"></i><b>24.1.4</b> Bayes’ Rule Reminder</a></li>
</ul></li>
<li class="chapter" data-level="24.2" data-path="classif.html"><a href="classif.html#bayes-classifier"><i class="fa fa-check"></i><b>24.2</b> Bayes Classifier</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="logistic.html"><a href="logistic.html"><i class="fa fa-check"></i><b>25</b> Logistic Regression</a><ul>
<li class="chapter" data-level="25.1" data-path="logistic.html"><a href="logistic.html#motivation-1"><i class="fa fa-check"></i><b>25.1</b> Motivation</a><ul>
<li class="chapter" data-level="25.1.1" data-path="logistic.html"><a href="logistic.html#first-approach-fitting-a-line"><i class="fa fa-check"></i><b>25.1.1</b> First Approach: Fitting a Line</a></li>
<li class="chapter" data-level="25.1.2" data-path="logistic.html"><a href="logistic.html#second-approach-harsh-thresholding"><i class="fa fa-check"></i><b>25.1.2</b> Second Approach: Harsh Thresholding</a></li>
<li class="chapter" data-level="25.1.3" data-path="logistic.html"><a href="logistic.html#third-approach-conditional-means"><i class="fa fa-check"></i><b>25.1.3</b> Third Approach: Conditional Means</a></li>
</ul></li>
<li class="chapter" data-level="25.2" data-path="logistic.html"><a href="logistic.html#logistic-regression-model"><i class="fa fa-check"></i><b>25.2</b> Logistic Regression Model</a><ul>
<li class="chapter" data-level="25.2.1" data-path="logistic.html"><a href="logistic.html#the-criterion-being-optimized"><i class="fa fa-check"></i><b>25.2.1</b> The Criterion Being Optimized</a></li>
<li class="chapter" data-level="25.2.2" data-path="logistic.html"><a href="logistic.html#another-way-to-solve-logistic-regression"><i class="fa fa-check"></i><b>25.2.2</b> Another Way to Solve Logistic Regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="26" data-path="discrim.html"><a href="discrim.html"><i class="fa fa-check"></i><b>26</b> Preamble for Discriminant Analysis</a><ul>
<li class="chapter" data-level="26.1" data-path="discrim.html"><a href="discrim.html#motivation-2"><i class="fa fa-check"></i><b>26.1</b> Motivation</a><ul>
<li class="chapter" data-level="26.1.1" data-path="discrim.html"><a href="discrim.html#distinguishing-species"><i class="fa fa-check"></i><b>26.1.1</b> Distinguishing Species</a></li>
<li class="chapter" data-level="26.1.2" data-path="discrim.html"><a href="discrim.html#sum-of-squares-decomposition"><i class="fa fa-check"></i><b>26.1.2</b> Sum of Squares Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="26.2" data-path="discrim.html"><a href="discrim.html#derived-ratios-from-sum-of-squares"><i class="fa fa-check"></i><b>26.2</b> Derived Ratios from Sum-of-Squares</a><ul>
<li class="chapter" data-level="26.2.1" data-path="discrim.html"><a href="discrim.html#correlation-ratio"><i class="fa fa-check"></i><b>26.2.1</b> Correlation Ratio</a></li>
<li class="chapter" data-level="26.2.2" data-path="discrim.html"><a href="discrim.html#f-ratio"><i class="fa fa-check"></i><b>26.2.2</b> F-Ratio</a></li>
</ul></li>
<li class="chapter" data-level="26.3" data-path="discrim.html"><a href="discrim.html#geometric-perspective"><i class="fa fa-check"></i><b>26.3</b> Geometric Perspective</a><ul>
<li class="chapter" data-level="26.3.1" data-path="discrim.html"><a href="discrim.html#clouds-from-class-structure"><i class="fa fa-check"></i><b>26.3.1</b> Clouds from Class Structure</a></li>
<li class="chapter" data-level="26.3.2" data-path="discrim.html"><a href="discrim.html#dispersion-decomposition"><i class="fa fa-check"></i><b>26.3.2</b> Dispersion Decomposition</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="27" data-path="cda.html"><a href="cda.html"><i class="fa fa-check"></i><b>27</b> Canonical Discriminant Analysis</a><ul>
<li class="chapter" data-level="27.1" data-path="cda.html"><a href="cda.html#cda-semi-supervised-aspect"><i class="fa fa-check"></i><b>27.1</b> CDA: Semi-Supervised Aspect</a></li>
<li class="chapter" data-level="27.2" data-path="cda.html"><a href="cda.html#looking-for-a-discriminant-axis"><i class="fa fa-check"></i><b>27.2</b> Looking for a discriminant axis</a></li>
<li class="chapter" data-level="27.3" data-path="cda.html"><a href="cda.html#looking-for-a-compromise-criterion"><i class="fa fa-check"></i><b>27.3</b> Looking for a Compromise Criterion</a><ul>
<li class="chapter" data-level="27.3.1" data-path="cda.html"><a href="cda.html#correlation-ratio-criterion"><i class="fa fa-check"></i><b>27.3.1</b> Correlation Ratio Criterion</a></li>
<li class="chapter" data-level="27.3.2" data-path="cda.html"><a href="cda.html#f-ratio-criterion"><i class="fa fa-check"></i><b>27.3.2</b> F-ratio Criterion</a></li>
<li class="chapter" data-level="27.3.3" data-path="cda.html"><a href="cda.html#a-special-pca"><i class="fa fa-check"></i><b>27.3.3</b> A Special PCA</a></li>
</ul></li>
<li class="chapter" data-level="27.4" data-path="cda.html"><a href="cda.html#cda-supervised-aspect"><i class="fa fa-check"></i><b>27.4</b> CDA: Supervised Aspect</a><ul>
<li class="chapter" data-level="27.4.1" data-path="cda.html"><a href="cda.html#distance-behind-cda"><i class="fa fa-check"></i><b>27.4.1</b> Distance behind CDA</a></li>
<li class="chapter" data-level="27.4.2" data-path="cda.html"><a href="cda.html#predictive-idea"><i class="fa fa-check"></i><b>27.4.2</b> Predictive Idea</a></li>
<li class="chapter" data-level="27.4.3" data-path="cda.html"><a href="cda.html#cda-classifier"><i class="fa fa-check"></i><b>27.4.3</b> CDA Classifier</a></li>
<li class="chapter" data-level="27.4.4" data-path="cda.html"><a href="cda.html#limitations-of-cda-classifier"><i class="fa fa-check"></i><b>27.4.4</b> Limitations of CDA classifier</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="28" data-path="discanalysis.html"><a href="discanalysis.html"><i class="fa fa-check"></i><b>28</b> Discriminant Analysis</a><ul>
<li class="chapter" data-level="28.1" data-path="discanalysis.html"><a href="discanalysis.html#probabilistic-da"><i class="fa fa-check"></i><b>28.1</b> Probabilistic DA</a><ul>
<li class="chapter" data-level="28.1.1" data-path="discanalysis.html"><a href="discanalysis.html#normal-distributions"><i class="fa fa-check"></i><b>28.1.1</b> Normal Distributions</a></li>
<li class="chapter" data-level="28.1.2" data-path="discanalysis.html"><a href="discanalysis.html#estimating-parameters-of-normal-distributions"><i class="fa fa-check"></i><b>28.1.2</b> Estimating Parameters of Normal Distributions</a></li>
</ul></li>
<li class="chapter" data-level="28.2" data-path="discanalysis.html"><a href="discanalysis.html#discriminant-functions"><i class="fa fa-check"></i><b>28.2</b> Discriminant Functions</a></li>
<li class="chapter" data-level="28.3" data-path="discanalysis.html"><a href="discanalysis.html#quadratic-discriminant-analysis-qda"><i class="fa fa-check"></i><b>28.3</b> Quadratic Discriminant Analysis (QDA)</a></li>
<li class="chapter" data-level="28.4" data-path="discanalysis.html"><a href="discanalysis.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>28.4</b> Linear Discriminant Analysis</a><ul>
<li class="chapter" data-level="28.4.1" data-path="discanalysis.html"><a href="discanalysis.html#canonical-discriminant-analysis"><i class="fa fa-check"></i><b>28.4.1</b> Canonical Discriminant Analysis</a></li>
<li class="chapter" data-level="28.4.2" data-path="discanalysis.html"><a href="discanalysis.html#naive-bayes"><i class="fa fa-check"></i><b>28.4.2</b> Naive Bayes</a></li>
<li class="chapter" data-level="28.4.3" data-path="discanalysis.html"><a href="discanalysis.html#fifth-case"><i class="fa fa-check"></i><b>28.4.3</b> Fifth Case</a></li>
</ul></li>
<li class="chapter" data-level="28.5" data-path="discanalysis.html"><a href="discanalysis.html#comparing-the-cases"><i class="fa fa-check"></i><b>28.5</b> Comparing the Cases</a></li>
</ul></li>
<li class="chapter" data-level="29" data-path="classperformance.html"><a href="classperformance.html"><i class="fa fa-check"></i><b>29</b> Performance of Classifiers</a><ul>
<li class="chapter" data-level="29.1" data-path="classperformance.html"><a href="classperformance.html#classification-error-measures"><i class="fa fa-check"></i><b>29.1</b> Classification Error Measures</a><ul>
<li class="chapter" data-level="29.1.1" data-path="classperformance.html"><a href="classperformance.html#errors-for-binary-response"><i class="fa fa-check"></i><b>29.1.1</b> Errors for Binary Response</a></li>
<li class="chapter" data-level="29.1.2" data-path="classperformance.html"><a href="classperformance.html#error-for-categorical-response"><i class="fa fa-check"></i><b>29.1.2</b> Error for Categorical Response</a></li>
</ul></li>
<li class="chapter" data-level="29.2" data-path="classperformance.html"><a href="classperformance.html#confusion-matrices"><i class="fa fa-check"></i><b>29.2</b> Confusion Matrices</a></li>
<li class="chapter" data-level="29.3" data-path="classperformance.html"><a href="classperformance.html#binary-response-example"><i class="fa fa-check"></i><b>29.3</b> Binary Response Example</a><ul>
<li class="chapter" data-level="29.3.1" data-path="classperformance.html"><a href="classperformance.html#application-for-a-checking-account"><i class="fa fa-check"></i><b>29.3.1</b> Application for a Checking Account</a></li>
<li class="chapter" data-level="29.3.2" data-path="classperformance.html"><a href="classperformance.html#application-for-a-loan"><i class="fa fa-check"></i><b>29.3.2</b> Application for a Loan</a></li>
</ul></li>
<li class="chapter" data-level="29.4" data-path="classperformance.html"><a href="classperformance.html#decision-rules-and-errors"><i class="fa fa-check"></i><b>29.4</b> Decision Rules and Errors</a></li>
<li class="chapter" data-level="29.5" data-path="classperformance.html"><a href="classperformance.html#roc-curves"><i class="fa fa-check"></i><b>29.5</b> ROC Curves</a><ul>
<li class="chapter" data-level="29.5.1" data-path="classperformance.html"><a href="classperformance.html#graphing-roc-curves"><i class="fa fa-check"></i><b>29.5.1</b> Graphing ROC curves</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IX Unsupervised II: Clustering</b></span></li>
<li class="chapter" data-level="30" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>30</b> Clustering</a><ul>
<li class="chapter" data-level="30.1" data-path="clustering.html"><a href="clustering.html#about-clustering"><i class="fa fa-check"></i><b>30.1</b> About Clustering</a><ul>
<li class="chapter" data-level="30.1.1" data-path="clustering.html"><a href="clustering.html#types-of-clustering"><i class="fa fa-check"></i><b>30.1.1</b> Types of Clustering</a></li>
<li class="chapter" data-level="30.1.2" data-path="clustering.html"><a href="clustering.html#hard-clustering"><i class="fa fa-check"></i><b>30.1.2</b> Hard Clustering</a></li>
</ul></li>
<li class="chapter" data-level="30.2" data-path="clustering.html"><a href="clustering.html#dispersion-measures"><i class="fa fa-check"></i><b>30.2</b> Dispersion Measures</a></li>
<li class="chapter" data-level="30.3" data-path="clustering.html"><a href="clustering.html#complexity-in-clustering"><i class="fa fa-check"></i><b>30.3</b> Complexity in Clustering</a></li>
</ul></li>
<li class="chapter" data-level="31" data-path="kmeans.html"><a href="kmeans.html"><i class="fa fa-check"></i><b>31</b> K-Means</a><ul>
<li class="chapter" data-level="31.1" data-path="kmeans.html"><a href="kmeans.html#toy-example-2"><i class="fa fa-check"></i><b>31.1</b> Toy Example</a></li>
<li class="chapter" data-level="31.2" data-path="kmeans.html"><a href="kmeans.html#what-does-k-means-do"><i class="fa fa-check"></i><b>31.2</b> What does K-means do?</a></li>
<li class="chapter" data-level="31.3" data-path="kmeans.html"><a href="kmeans.html#k-means-algorithms"><i class="fa fa-check"></i><b>31.3</b> K-Means Algorithms</a><ul>
<li class="chapter" data-level="31.3.1" data-path="kmeans.html"><a href="kmeans.html#classic-version"><i class="fa fa-check"></i><b>31.3.1</b> Classic Version</a></li>
<li class="chapter" data-level="31.3.2" data-path="kmeans.html"><a href="kmeans.html#moving-centers-algorithm"><i class="fa fa-check"></i><b>31.3.2</b> Moving Centers Algorithm</a></li>
<li class="chapter" data-level="31.3.3" data-path="kmeans.html"><a href="kmeans.html#dynamic-clouds"><i class="fa fa-check"></i><b>31.3.3</b> Dynamic Clouds</a></li>
<li class="chapter" data-level="31.3.4" data-path="kmeans.html"><a href="kmeans.html#choosing-k"><i class="fa fa-check"></i><b>31.3.4</b> Choosing <span class="math inline">\(K\)</span></a></li>
<li class="chapter" data-level="31.3.5" data-path="kmeans.html"><a href="kmeans.html#comments"><i class="fa fa-check"></i><b>31.3.5</b> Comments</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="32" data-path="hclus.html"><a href="hclus.html"><i class="fa fa-check"></i><b>32</b> Hierarchical Clustering</a><ul>
<li class="chapter" data-level="32.1" data-path="hclus.html"><a href="hclus.html#agglomerative-methods"><i class="fa fa-check"></i><b>32.1</b> Agglomerative Methods</a></li>
<li class="chapter" data-level="32.2" data-path="hclus.html"><a href="hclus.html#example-single-linkage"><i class="fa fa-check"></i><b>32.2</b> Example: Single Linkage</a><ul>
<li class="chapter" data-level="32.2.1" data-path="hclus.html"><a href="hclus.html#dendrogram"><i class="fa fa-check"></i><b>32.2.1</b> Dendrogram</a></li>
</ul></li>
<li class="chapter" data-level="32.3" data-path="hclus.html"><a href="hclus.html#example-complete-linkage"><i class="fa fa-check"></i><b>32.3</b> Example: Complete Linkage</a><ul>
<li class="chapter" data-level="32.3.1" data-path="hclus.html"><a href="hclus.html#cutting-dendograms"><i class="fa fa-check"></i><b>32.3.1</b> Cutting Dendograms</a></li>
<li class="chapter" data-level="32.3.2" data-path="hclus.html"><a href="hclus.html#pros-and-coons"><i class="fa fa-check"></i><b>32.3.2</b> Pros and COons</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>X Tree-based Methods</b></span></li>
<li class="chapter" data-level="33" data-path="trees.html"><a href="trees.html"><i class="fa fa-check"></i><b>33</b> Intro to Decision Trees</a><ul>
<li class="chapter" data-level="33.1" data-path="trees.html"><a href="trees.html#introduction-7"><i class="fa fa-check"></i><b>33.1</b> Introduction</a></li>
<li class="chapter" data-level="33.2" data-path="trees.html"><a href="trees.html#some-terminology"><i class="fa fa-check"></i><b>33.2</b> Some Terminology</a><ul>
<li class="chapter" data-level="33.2.1" data-path="trees.html"><a href="trees.html#binary-trees"><i class="fa fa-check"></i><b>33.2.1</b> Binary Trees</a></li>
</ul></li>
<li class="chapter" data-level="33.3" data-path="trees.html"><a href="trees.html#space-partitions"><i class="fa fa-check"></i><b>33.3</b> Space Partitions</a><ul>
<li class="chapter" data-level="33.3.1" data-path="trees.html"><a href="trees.html#the-process-of-building-a-tree"><i class="fa fa-check"></i><b>33.3.1</b> The Process of Building a Tree</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="34" data-path="tree-impurities.html"><a href="tree-impurities.html"><i class="fa fa-check"></i><b>34</b> Binary Splits and Impurity</a><ul>
<li class="chapter" data-level="34.1" data-path="tree-impurities.html"><a href="tree-impurities.html#binary-partitions"><i class="fa fa-check"></i><b>34.1</b> Binary Partitions</a><ul>
<li class="chapter" data-level="34.1.1" data-path="tree-impurities.html"><a href="tree-impurities.html#splits-of-binary-variables"><i class="fa fa-check"></i><b>34.1.1</b> Splits of Binary variables</a></li>
<li class="chapter" data-level="34.1.2" data-path="tree-impurities.html"><a href="tree-impurities.html#splits-of-nominal-variables"><i class="fa fa-check"></i><b>34.1.2</b> Splits of Nominal Variables</a></li>
<li class="chapter" data-level="34.1.3" data-path="tree-impurities.html"><a href="tree-impurities.html#splits-of-ordinal-variables"><i class="fa fa-check"></i><b>34.1.3</b> Splits of Ordinal Variables</a></li>
<li class="chapter" data-level="34.1.4" data-path="tree-impurities.html"><a href="tree-impurities.html#splits-continuous-variables"><i class="fa fa-check"></i><b>34.1.4</b> Splits Continuous variables</a></li>
</ul></li>
<li class="chapter" data-level="34.2" data-path="tree-impurities.html"><a href="tree-impurities.html#measures-of-impurity"><i class="fa fa-check"></i><b>34.2</b> Measures of Impurity</a><ul>
<li class="chapter" data-level="34.2.1" data-path="tree-impurities.html"><a href="tree-impurities.html#entropy"><i class="fa fa-check"></i><b>34.2.1</b> Entropy</a></li>
<li class="chapter" data-level="34.2.2" data-path="tree-impurities.html"><a href="tree-impurities.html#the-math-behind-entropy"><i class="fa fa-check"></i><b>34.2.2</b> The Math Behind Entropy</a></li>
<li class="chapter" data-level="34.2.3" data-path="tree-impurities.html"><a href="tree-impurities.html#gini-impurity"><i class="fa fa-check"></i><b>34.2.3</b> Gini Impurity</a></li>
<li class="chapter" data-level="34.2.4" data-path="tree-impurities.html"><a href="tree-impurities.html#variance-based-impurity"><i class="fa fa-check"></i><b>34.2.4</b> Variance-based Impurity</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="35" data-path="tree-splits.html"><a href="tree-splits.html"><i class="fa fa-check"></i><b>35</b> Splitting Nodes</a><ul>
<li class="chapter" data-level="35.1" data-path="tree-splits.html"><a href="tree-splits.html#entropy-based-splits"><i class="fa fa-check"></i><b>35.1</b> Entropy-based Splits</a></li>
<li class="chapter" data-level="35.2" data-path="tree-splits.html"><a href="tree-splits.html#gini-index-based-splits"><i class="fa fa-check"></i><b>35.2</b> Gini-index based Splits</a></li>
<li class="chapter" data-level="35.3" data-path="tree-splits.html"><a href="tree-splits.html#looking-for-the-best-split"><i class="fa fa-check"></i><b>35.3</b> Looking for the best split</a></li>
</ul></li>
<li class="chapter" data-level="36" data-path="tree-basics.html"><a href="tree-basics.html"><i class="fa fa-check"></i><b>36</b> Building Binary Trees</a><ul>
<li class="chapter" data-level="36.1" data-path="tree-basics.html"><a href="tree-basics.html#node-splitting-stopping-criteria"><i class="fa fa-check"></i><b>36.1</b> Node-Splitting Stopping Criteria</a></li>
<li class="chapter" data-level="36.2" data-path="tree-basics.html"><a href="tree-basics.html#issues-with-trees"><i class="fa fa-check"></i><b>36.2</b> Issues with Trees</a><ul>
<li class="chapter" data-level="36.2.1" data-path="tree-basics.html"><a href="tree-basics.html#bias-variance-of-trees"><i class="fa fa-check"></i><b>36.2.1</b> Bias-Variance of Trees</a></li>
</ul></li>
<li class="chapter" data-level="36.3" data-path="tree-basics.html"><a href="tree-basics.html#pruning-a-tree"><i class="fa fa-check"></i><b>36.3</b> Pruning a Tree</a></li>
<li class="chapter" data-level="36.4" data-path="tree-basics.html"><a href="tree-basics.html#pros-and-cons-of-trees"><i class="fa fa-check"></i><b>36.4</b> Pros and Cons of Trees</a><ul>
<li class="chapter" data-level="36.4.1" data-path="tree-basics.html"><a href="tree-basics.html#advantages-of-trees"><i class="fa fa-check"></i><b>36.4.1</b> Advantages of Trees</a></li>
<li class="chapter" data-level="36.4.2" data-path="tree-basics.html"><a href="tree-basics.html#disadvantages-of-trees"><i class="fa fa-check"></i><b>36.4.2</b> Disadvantages of Trees</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="37" data-path="bagging.html"><a href="bagging.html"><i class="fa fa-check"></i><b>37</b> Bagging</a><ul>
<li class="chapter" data-level="37.1" data-path="bagging.html"><a href="bagging.html#introduction-8"><i class="fa fa-check"></i><b>37.1</b> Introduction</a><ul>
<li class="chapter" data-level="37.1.1" data-path="bagging.html"><a href="bagging.html#idea-of-bagging"><i class="fa fa-check"></i><b>37.1.1</b> Idea of Bagging</a></li>
</ul></li>
<li class="chapter" data-level="37.2" data-path="bagging.html"><a href="bagging.html#why-bother-bagging"><i class="fa fa-check"></i><b>37.2</b> Why Bother Bagging?</a></li>
</ul></li>
<li class="chapter" data-level="38" data-path="forest.html"><a href="forest.html"><i class="fa fa-check"></i><b>38</b> Random Forests</a><ul>
<li class="chapter" data-level="38.1" data-path="forest.html"><a href="forest.html#introduction-9"><i class="fa fa-check"></i><b>38.1</b> Introduction</a></li>
<li class="chapter" data-level="38.2" data-path="forest.html"><a href="forest.html#algorithm-2"><i class="fa fa-check"></i><b>38.2</b> Algorithm</a><ul>
<li class="chapter" data-level="38.2.1" data-path="forest.html"><a href="forest.html#two-sources-of-randomness"><i class="fa fa-check"></i><b>38.2.1</b> Two Sources of Randomness</a></li>
<li class="chapter" data-level="38.2.2" data-path="forest.html"><a href="forest.html#regressions-and-classification-forests"><i class="fa fa-check"></i><b>38.2.2</b> Regressions and Classification Forests</a></li>
<li class="chapter" data-level="38.2.3" data-path="forest.html"><a href="forest.html#key-advantage-of-random-forests"><i class="fa fa-check"></i><b>38.2.3</b> Key Advantage of Random Forests</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Made with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">All Models Are Wrong: Concepts of Statistical Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="discanalysis" class="section level1">
<h1><span class="header-section-number">28</span> Discriminant Analysis</h1>
<p>So far we have considered Discriminant Analysis (DA) from a largely conceptual
standpoint. In this lecture, we start to formalize our notions into a
mathematical framework in what we will call <strong>Probabilistic Discriminant Analysis</strong>.</p>
<div id="probabilistic-da" class="section level2">
<h2><span class="header-section-number">28.1</span> Probabilistic DA</h2>
<p>We start with the notion of the <strong>Bayes Classifier</strong>, which is explained in
more detail in the introductory chapter about <a href="classif.html#classif">Classification</a>.
Suppose we have a response with <span class="math inline">\(K\)</span> classes
<span class="math inline">\((\mathcal{C}_1, \dots, \mathcal{C}_K)\)</span>, and further suppose we have <span class="math inline">\(p\)</span>
predictors. The Bayes Classifier gives us the following classification rule:</p>
<p><span class="math display">\[
\text{assign } \mathbf{x_i} \text{ to the class for which } P( y_i = k \mid \mathbf{x_i}) \text{ is the largest}
\]</span></p>
<p>This rule is optimal in the sense that it minimizes the misclassification error.</p>
<p>The formula for the conditional probability is given by:</p>
<p><span class="math display" id="eq:705-01">\[
\underbrace{ P(y_i = k \mid \mathbf{x})  }_{\text{posterior}} = \frac{ \overbrace{ P(\mathbf{x_i} \mid y_i = k) }^{\text{likelihood}} \overbrace{  P(y_i = k) }^{\text{prior}} }{P(\mathbf{x_i}) }
\tag{28.1}
\]</span></p>
<p>where the denominator is obtained as:</p>
<p><span class="math display" id="eq:705-02">\[
P(\mathbf{x_i}) = \sum_{k=1}^{K} P(y_i = k) P(\mathbf{x_i} \mid y_i = k)
\tag{28.2}
\]</span></p>
<p>Changing some of the notation, let:</p>
<ul>
<li><p><span class="math inline">\(P(y = k)\)</span> = <span class="math inline">\(\pi_k\)</span>, the <strong>prior</strong> probability for class <span class="math inline">\(k\)</span>.</p></li>
<li><p><span class="math inline">\(P(X = \mathbf{x} \mid y = k)\)</span> = <span class="math inline">\(f_k(\mathbf{x})\)</span>, the <strong>class-conditional</strong>
density for inputs <span class="math inline">\(X\)</span> in class <span class="math inline">\(k\)</span>.</p></li>
</ul>
<p>Thus, the posterior probability (the conditional probability of the response
given the inputs) is:</p>
<p><span class="math display" id="eq:705-03">\[
P(y = k \mid X = \mathbf{x}) = \frac{f_k(\mathbf{x}) \hspace{1mm} \pi_k}{\sum_{k=1}^{K} f_k(\mathbf{x}) \pi_k}
\tag{28.3}
\]</span></p>
<p>Note that the numerator in the above equation is the same for all classes
<span class="math inline">\(k = 1, \dots, K\)</span>.</p>
<div id="normal-distributions" class="section level3">
<h3><span class="header-section-number">28.1.1</span> Normal Distributions</h3>
<p>Now, here comes one of the key assumptions of Discriminant Analysis:
<strong>we assume that our class-conditional probabilities follow a Gaussian distribution</strong>.</p>
<div id="univariate-data-boldsymbolp-1" class="section level4 unnumbered">
<h4>Univariate Data (<span class="math inline">\(\boldsymbol{p = 1}\)</span>)</h4>
<p>In the case of univariate data with just one input <span class="math inline">\(x\)</span>, we have that the
class-conditional density has a Normal distribution:
<span class="math inline">\(x|\mathcal{C_k} \sim N(\mu_k, \sigma_k)\)</span>.</p>
<p><span class="math display" id="eq:705-04">\[
f_k(x) = \frac{1}{\sqrt{2 \pi \sigma_k^2}} \hspace{1mm} \exp\left\{ -\frac{1}{2} \left( \frac{x - \mu_k}{\sigma_k} \right)^2 \right\}
\tag{28.4}
\]</span></p>
</div>
<div id="multivariate-data-boldsymbolp-1" class="section level4 unnumbered">
<h4>Multivariate Data (<span class="math inline">\(\boldsymbol{p &gt; 1}\)</span>)</h4>
<p>In the case of multivariate data, we use a multivariate normal distribution for
our class-conditional density:</p>
<p><span class="math display" id="eq:705-05">\[
\mathbf{x} \mid \mathcal{C}_k \sim \textit{MVN}(\boldsymbol{\mu_k}, \mathbf{\Sigma_k})
\tag{28.5}
\]</span></p>
<p>Thus, the density function is given by:</p>
<p><span class="math display" id="eq:705-06">\[
f_k(\mathbf{x}) = \frac{1}{(2 \pi)^{p/2} | \mathbf{\Sigma_k} |^{1/2} } \exp\left\{  - \frac{1}{2} (\mathbf{x} - \boldsymbol{\mu_k})^{\mathsf{T}} \mathbf{\Sigma_k}^{-1} (\mathbf{x} - \boldsymbol{\mu_k})  \right\}
\tag{28.6}
\]</span></p>
<p>where <span class="math inline">\(\mathbf{\Sigma_k}\)</span> denotes the variance-covariance matrix associated
with <span class="math inline">\(\mathcal{C_k}\)</span>, and <span class="math inline">\(\boldsymbol{\mu_k}\)</span> denotes the centroid of class
<span class="math inline">\(\mathcal{C_k}\)</span>. Note that the exponent of the density is simply the
Mahalanobis distance between <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\boldsymbol{\mu_k}\)</span>:</p>
<p><span class="math display" id="eq:705-07">\[
d^2(\mathbf{x}, \boldsymbol{\mu_k}) = (\mathbf{x} - \boldsymbol{\mu_k})^{\mathsf{T}} \mathbf{\Sigma_k}^{-1} (\mathbf{x} - \boldsymbol{\mu_k})
\tag{28.7}
\]</span></p>
<p>The following interactive plot shows an example of a (pseudo) bivariate normal
distribution:</p>
<iframe height="300" width="500" frameborder="0" src="images/classif/bivariate3d.html">
</iframe>
</div>
</div>
<div id="estimating-parameters-of-normal-distributions" class="section level3">
<h3><span class="header-section-number">28.1.2</span> Estimating Parameters of Normal Distributions</h3>
<p>Let’s parse through our equation from above. Clearly, in order for this to be
of any <em>practical</em> use, we need to estimate some of these quantities:</p>
<ul>
<li>prior probabilities: <span class="math inline">\(\hat{\pi}_k\)</span></li>
<li>mean-vectors: <span class="math inline">\(\boldsymbol{\hat{\mu}_k}\)</span></li>
<li>variance-covariance matrices: <span class="math inline">\(\mathbf{\widehat{\Sigma}_k}\)</span></li>
</ul>
<div id="priors" class="section level4 unnumbered">
<h4>Priors</h4>
<p>Estimating <span class="math inline">\(\pi_k\)</span> is relatively intuitive:</p>
<p><span class="math display" id="eq:705-08">\[
\hat{\pi}_k = \frac{n_k}{n}
\tag{28.8}
\]</span></p>
<p>where <span class="math inline">\(n_k = |\mathcal{C}_k|\)</span> denotes the size of class <span class="math inline">\(k\)</span> and <span class="math inline">\(n\)</span> denotes the
total number of data points.</p>
</div>
<div id="mean-vectors" class="section level4 unnumbered">
<h4>Mean Vectors</h4>
<p>For <span class="math inline">\(\boldsymbol{\hat{\mu}_k}\)</span>, we can use the centroid of <span class="math inline">\(\mathcal{C}_k\)</span>; i.e. the
average individual of class <span class="math inline">\(k\)</span>.</p>
<p><span class="math display" id="eq:705-09">\[
\boldsymbol{\hat{\mu}_k} = \mathbf{g_k}
\tag{28.9}
\]</span></p>
</div>
<div id="variance-covariance-matrices" class="section level4 unnumbered">
<h4>Variance-Covariance Matrices</h4>
<p>For <span class="math inline">\(\mathbf{\widehat{\Sigma}_k}\)</span>, we can use the within-variance matrix:</p>
<p><span class="math display" id="eq:705-10">\[
\mathbf{\widehat{\Sigma}_k} = \frac{1}{n-1} \mathbf{X_k}^\mathsf{T} \mathbf{X_k}
\tag{28.10}
\]</span></p>
<p>where <span class="math inline">\(\mathbf{X_k}\)</span> is the mean-centered data matrix for objects of class
<span class="math inline">\(\mathcal{C_k}\)</span>.</p>
</div>
</div>
</div>
<div id="discriminant-functions" class="section level2">
<h2><span class="header-section-number">28.2</span> Discriminant Functions</h2>
<p>Given all of our above estimations, we can now find an estimate for the
posterior probability <span class="math inline">\(P(y_i = k \mid \mathbf{x_i})\)</span>:</p>
<p><span class="math display" id="eq:705-11">\[
\widehat{P(y_i = k \mid \mathbf{x} )} = \frac{\hat{\pi}_k \hspace{1mm} \hat{f_k} (\mathbf{x}) }{\sum_{k=1}^{K} \hat{\pi}_k \hat{f_k} (\mathbf{x}) }
\tag{28.11}
\]</span></p>
<p>Now, note that the denominator remains constant across classes. Furthermore,
we have a Gaussian expression in our numerator; hence, it makes sense to take
logarithms:</p>
<p><span class="math display" id="eq:705-12">\[
\ln\left[ P(y_i = k \mid \mathbf{x} ) \right] \propto \ln( \hat{\pi}_k) + \ln\left[ \hat{f_k} (\mathbf{x}) \right]
\tag{28.12}
\]</span></p>
<p>We now substitute the Multivariate Normal p.d.f. into our expression above, to see:</p>
<p><span class="math display" id="eq:705-13">\[\begin{align*}
\ln\left[ \hat{f_k}(x) \right] &amp; = \ln \left[  (2\pi)^{-p/2} | \mathbf{\hat{\Sigma}_k} |^{-1/2}  \exp\left\{  - \frac{1}{2} (\mathbf{x} - \boldsymbol{\hat{\mu}_k})^{\mathsf{T}} \mathbf{\hat{\Sigma}_k}^{-1} (\mathbf{x} - \boldsymbol{\hat{\mu}_k})  \right\}   \right] \\
&amp; \to \ln\left( | \mathbf{\hat{\Sigma}_k} |^{-1/2} \right) - \frac{1}{2} (\mathbf{x} - \boldsymbol{\hat{\mu}_k})^{\mathsf{T}} \mathbf{\hat{\Sigma}_k}^{-1} (\mathbf{x} - \boldsymbol{\hat{\mu}_k})  \\
&amp; = -\frac{1}{2} \ln\left( | \mathbf{\hat{\Sigma}_k} |  \right) - \frac{1}{2} \left[ \mathbf{x}^\mathsf{T} \mathbf{\hat{\Sigma}_k^{-1}} \mathbf{x} - 2 \mathbf{x}^\mathsf{T} \mathbf{\hat{\Sigma}_k^{-1}} \boldsymbol{\hat{\mu}_k} + \boldsymbol{\hat{\mu}_k}^{\mathsf{T}} \mathbf{\hat{\Sigma}_k^{-1}} \boldsymbol{\hat{\mu}_k}  \right] 
\tag{28.13}
\end{align*}\]</span></p>
<p>Substituting this back into our expression for <span class="math inline">\(\ln[P(y_i = k \mid \mathbf{x})]\)</span> leads us to the so-called <strong>discriminant functions</strong>:</p>
<p><span class="math display" id="eq:705-14">\[
\delta_k(\mathbf{x}) = -\frac{1}{2} \ln\left( | \mathbf{\hat{\Sigma}_k} |  \right) - \frac{1}{2} \left[ \mathbf{x}^\mathsf{T} \mathbf{\hat{\Sigma}_k^{-1}} \mathbf{x}^\mathsf{T} - 2 \mathbf{x}^\mathsf{T} \mathbf{\hat{\Sigma}_k^{-1}} \boldsymbol{\hat{\mu}_k} + \boldsymbol{\hat{\mu}_k}^{\mathsf{T}} \mathbf{\hat{\Sigma}_k^{-1}} \boldsymbol{\hat{\mu}_k}  \right]  + \ln\left(  \hat{\pi}_k \right)
\tag{28.14}
\]</span></p>
<p>In general, our classification rule is as follows:</p>
<p><span class="math display">\[
\text{assign } \mathbf{x}  \text{ to the class for which } \delta_k(\mathbf{x}) \text{ is the largest} 
\]</span></p>
</div>
<div id="quadratic-discriminant-analysis-qda" class="section level2">
<h2><span class="header-section-number">28.3</span> Quadratic Discriminant Analysis (QDA)</h2>
<p>Let us classify the order of each term in <span class="math inline">\(\delta_K(\mathbf{x})\)</span>, w.r.t.
<span class="math inline">\(\mathbf{x}\)</span>:</p>
<p><span class="math display" id="eq:705-15">\[
\delta_k(\mathbf{x}) =  -\frac{1}{2}   \underbrace{ \ln\left( | \mathbf{\hat{\Sigma}_k} |  \right) }_{\text{constant}}- \frac{1}{2} \left[ \underbrace{ \mathbf{x}^\mathsf{T} \mathbf{\hat{\Sigma}_k^{-1}} \mathbf{x} }_{\text{quadratic}}  - \underbrace{ 2 \mathbf{x}^\mathsf{T} \mathbf{\hat{\Sigma}_k^{-1}} \boldsymbol{\hat{\mu}_k} }_{\text{linear}} + \underbrace{  \boldsymbol{\hat{\mu}_k}^{\mathsf{T}} \mathbf{\hat{\Sigma}_k^{-1}} \boldsymbol{\hat{\mu}_k} }_{\text{constant}}  \right]  + \underbrace{ \ln\left(  \hat{\pi}_k \right) }_{\text{constant}}
\tag{28.15}
\]</span></p>
<p>Of course, we could group terms to obtain an expression of the form
<span class="math inline">\(\text{const} + \text{linear} + \text{quadratic}\)</span>. The important thing is that
we obtain a quadratic function of <span class="math inline">\(\mathbf{x}\)</span>; this leads us to
<strong>Quadratic Discriminant Analysis (QDA)</strong>.</p>
<p>Having a quadratic discriminant function causes the decision boundaries in QDA
to be quadratic surfaces.</p>
</div>
<div id="linear-discriminant-analysis" class="section level2">
<h2><span class="header-section-number">28.4</span> Linear Discriminant Analysis</h2>
<p>What if, in our expression for <span class="math inline">\(\delta_K(\mathbf{x})\)</span>, we have that all
covariance matrices are the same:</p>
<p><span class="math display" id="eq:705-16">\[
\mathbf{\hat{\Sigma}_1} = \mathbf{\hat{\Sigma}_2} = \dots = \mathbf{\hat{\Sigma}_K} = \mathbf{\hat{\Sigma}}
\tag{28.16}
\]</span></p>
<p>In this case, the discriminant function becomes</p>
<p><span class="math display" id="eq:705-17">\[
\delta_k(\mathbf{x}) =  \underbrace{ \ln\left(  \hat{\pi}_k \right) }_{\text{constant}} -   \underbrace{ \frac{1}{2} \ln\left( | \mathbf{\hat{\Sigma}} |  \right) }_{\text{no k dependency}} - \frac{1}{2} \left[ \underbrace{ \mathbf{x}^\mathsf{T} \mathbf{\hat{\Sigma}^{-1}} \mathbf{x} }_{\text{no k dependency}}  - \underbrace{ 2 \mathbf{x}^\mathsf{T} \mathbf{\hat{\Sigma}^{-1}} \boldsymbol{\hat{\mu}_k} }_{\text{linear}} + \underbrace{  \boldsymbol{\hat{\mu}_k}^{\mathsf{T}} \mathbf{\hat{\Sigma}^{-1}} \boldsymbol{\hat{\mu}_k} }_{\text{constant}}  \right]
\tag{28.17}
\]</span></p>
<p>In other words, ignoring terms that do not depend on <span class="math inline">\(k\)</span>, we obtain
(again, w.r.t. <span class="math inline">\(\mathbf{x}\)</span>)</p>
<p><span class="math display" id="eq:705-18">\[
\delta_k(\mathbf{x}) =  \underbrace{ \ln\left(  \hat{\pi}_k \right) }_{\text{constant}}  - \frac{1}{2} \left[ - \underbrace{ 2 \mathbf{x}^\mathsf{T} \mathbf{\hat{\Sigma}^{-1}} \boldsymbol{\hat{\mu}_k} }_{\text{linear}} + \underbrace{  \boldsymbol{\hat{\mu}_k}^{\mathsf{T}} \mathbf{\hat{\Sigma}^{-1}} \boldsymbol{\hat{\mu}_k} }_{\text{constant}}  \right]
\tag{28.18}
\]</span></p>
<p>That is, our discriminant function is now linear w.r.t. <span class="math inline">\(\mathbf{x}\)</span>; hence,
we end up with <strong>Linear Discriminant Analysis (LDA)</strong> in its most general form.</p>
<div id="canonical-discriminant-analysis" class="section level3">
<h3><span class="header-section-number">28.4.1</span> Canonical Discriminant Analysis</h3>
<p>Let us continue adding more assumptions. We again assume that all covariance
matrices are the same across classes, as well as the prior probabilities:</p>
<p><span class="math display" id="eq:705-19">\[
\mathbf{\hat{\Sigma}_1} = \mathbf{\hat{\Sigma}_2} = \dots = \mathbf{\hat{\Sigma}_K} = \mathbf{\hat{\Sigma}} \quad \text{and} \quad \hat{\pi}_1 = \dots = \hat{\pi}_k = \hat{\pi}
\tag{28.19}
\]</span></p>
<p>Then, the discriminant functions <span class="math inline">\(\delta_k(\mathbf{x})\)</span> become:</p>
<p><span class="math display" id="eq:705-20">\[
\delta_k(\mathbf{x}) =  \underbrace{ \ln\left(  \hat{\pi} \right) }_{\text{no k dependency}} +  \underbrace{  \mathbf{x}^\mathsf{T} \mathbf{\hat{\Sigma}^{-1}} \boldsymbol{\hat{\mu}_k} }_{\text{linear}} - \frac{1}{2}  \underbrace{  \boldsymbol{\hat{\mu}_k}^{\mathsf{T}} \mathbf{\hat{\Sigma}^{-1}} \boldsymbol{\hat{\mu}_k} }_{\text{constant}}
\tag{28.20}
\]</span></p>
<p>which becomes, after ignoring <span class="math inline">\(k\)</span>-independent terms,</p>
<p><span class="math display" id="eq:705-21">\[
\delta_k(\mathbf{x}) =  \underbrace{  \mathbf{x}^\mathsf{T} \mathbf{\hat{\Sigma}^{-1}} \boldsymbol{\hat{\mu}_k} }_{\text{linear}} - \frac{1}{2}  \underbrace{  \boldsymbol{\hat{\mu}_k}^{\mathsf{T}} \mathbf{\hat{\Sigma}^{-1}} \boldsymbol{\hat{\mu}_k} }_{\text{constant}}
\tag{28.21}
\]</span></p>
<p>Now, what is going in with this first term? It is simply the distance between
<span class="math inline">\(\mathbf{x}\)</span> and the centroid of <span class="math inline">\(\mathcal{C_k}\)</span>, using the within-class
variance matrix as a metric matrix. In other words, <em>the above expression is our good old friend CDA!</em></p>
</div>
<div id="naive-bayes" class="section level3">
<h3><span class="header-section-number">28.4.2</span> Naive Bayes</h3>
<p>Let’s add one more supposition to the list of assumptions considered so far.
Equal covariance matrices, equal priors, and now also assume that
<span class="math inline">\(\mathbf{\hat{\Sigma}}\)</span> is diagonal which means that the predictors are
uncorrelated:</p>
<p><span class="math display" id="eq:705-22">\[
\mathbf{\hat{\Sigma}} = \begin{pmatrix} Var(X_1) &amp; 0 &amp; \dots &amp; 0 \\ 0 &amp; Var(X_2) &amp; \dots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \dots &amp; Var(X_p) \\ \end{pmatrix}
\tag{28.22}
\]</span></p>
<p>So we have the following assumptions:</p>
<p><span class="math display" id="eq:705-22">\[
\mathbf{\hat{\Sigma}_1} = \mathbf{\hat{\Sigma}_2} = \dots = \mathbf{\hat{\Sigma}_K} = \mathbf{\hat{\Sigma}}, \qquad \hat{\pi}_1 = \dots = \hat{\pi}_k = \hat{\pi} \\
\quad \text{and} \quad \text{diagonal } \mathbf{\hat{\Sigma}}
\tag{28.22}
\]</span></p>
<p>Then, the discriminant function becomes (with diagonal <span class="math inline">\(\mathbf{\hat{\Sigma}}\)</span>):</p>
<p><span class="math display" id="eq:705-23">\[
\delta_k(\mathbf{x}) =  \underbrace{  \mathbf{x}^\mathsf{T} \mathbf{\hat{\Sigma}^{-1}} \boldsymbol{\hat{\mu}_k} }_{\text{linear}} - \frac{1}{2}  \underbrace{  \boldsymbol{\hat{\mu}_k}^{\mathsf{T}} \mathbf{\hat{\Sigma}^{-1}} \boldsymbol{\hat{\mu}_k} }_{\text{constant}}
\tag{28.23}
\]</span></p>
<p>This leads to what is known as <strong>Naive Bayes</strong>.</p>
</div>
<div id="fifth-case" class="section level3">
<h3><span class="header-section-number">28.4.3</span> Fifth Case</h3>
<p>Again assume that <span class="math inline">\(\mathbf{\hat{\Sigma}_1} = \mathbf{\hat{\Sigma}_2} = \dots = \mathbf{\hat{\Sigma}_K} = \mathbf{\hat{\Sigma}}\)</span> as well as
<span class="math inline">\(\hat{\pi}_1 = \dots = \hat{\pi}_k =: \hat{\pi}\)</span>. Also assume that
<span class="math inline">\(\mathbf{\hat{\Sigma}} = \mathbf{I_p}\)</span>, the <span class="math inline">\(p \times p\)</span> identity matrix.
In other words, our Mahalanobis distance collapses to the Euclidean distance.</p>
</div>
</div>
<div id="comparing-the-cases" class="section level2">
<h2><span class="header-section-number">28.5</span> Comparing the Cases</h2>
<p>Here’s a summary table of the five cases discussed above. All cases
assume that the class-conditional distributions <span class="math inline">\(f_k(\mathbf{x})\)</span> follow
a Normal or Multivariate-Normal Distribution, depending on the number of
input features <span class="math inline">\(p\)</span>.</p>
<table>
<colgroup>
<col width="32%" />
<col width="11%" />
<col width="11%" />
<col width="17%" />
<col width="18%" />
<col width="8%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Cov Matrix</th>
<th align="left">Priors</th>
<th align="left">Method</th>
<th align="left">Bayes Rule</th>
<th align="left">Flexibility</th>
<th align="left">Bias</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Unequal across classes</td>
<td align="left">Unequal across classes</td>
<td align="left">QDA</td>
<td align="left">quadratic</td>
<td align="left">+++++</td>
<td align="left">+</td>
</tr>
<tr class="even">
<td align="left">Equal across classes</td>
<td align="left">Unequal across classes</td>
<td align="left">LDA</td>
<td align="left">linear</td>
<td align="left">++++</td>
<td align="left">++</td>
</tr>
<tr class="odd">
<td align="left">Equal across classes</td>
<td align="left">Equal across classes</td>
<td align="left">CDA</td>
<td align="left">linear</td>
<td align="left">+++</td>
<td align="left">+++</td>
</tr>
<tr class="even">
<td align="left">Equal across classes, diagonal</td>
<td align="left">Equal across classes</td>
<td align="left">Naive Bayes</td>
<td align="left">linear</td>
<td align="left">++</td>
<td align="left">++++</td>
</tr>
<tr class="odd">
<td align="left">Equal across classes, identity</td>
<td align="left">Equal across classes</td>
<td align="left">Euclid. Dist.</td>
<td align="left">linear</td>
<td align="left">+</td>
<td align="left">+++++</td>
</tr>
</tbody>
</table>
<p>Notice that the cases are listed from more flexible to less flexible.
Discriminant methods such as Naive Bayes imposes fairly restrictive assumptions.
Also, notice that you can have discriminant methods with linear decision
boundaries of different degrees of flexibility.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="cda.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="classperformance.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["allmodelsarewrong.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
