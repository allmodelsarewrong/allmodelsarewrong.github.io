<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>16 Ridge Regression | All Models Are Wrong: Concepts of Statistical Learning</title>
  <meta name="description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  <meta name="generator" content="bookdown 0.16 and GitBook 2.6.7" />

  <meta property="og:title" content="16 Ridge Regression | All Models Are Wrong: Concepts of Statistical Learning" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://allmodelsarewrong.github.io" />
  
  <meta property="og:description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  <meta name="github-repo" content="allmodelsarewrong/allmodelsarewrong.github.io" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="16 Ridge Regression | All Models Are Wrong: Concepts of Statistical Learning" />
  
  <meta name="twitter:description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  

<meta name="author" content="Gaston Sanchez Ethan Marzban" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="pls.html"/>
<link rel="next" href="linear-extensions.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><b>All Models Are Wrong (CSL)</b><br><small>G. Sanchez & E. Marzban</small></a></li>

<li class="divider"></li>
<li class="part"><span><b>I Welcome</b></span></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="about.html"><a href="about.html"><i class="fa fa-check"></i><b>1</b> About this book</a><ul>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#prerequisites"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="part"><span><b>II Intro</b></span></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#basic-notation"><i class="fa fa-check"></i><b>2.1</b> Basic Notation</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="duality.html"><a href="duality.html"><i class="fa fa-check"></i><b>3</b> Geometric Duality</a><ul>
<li class="chapter" data-level="3.1" data-path="duality.html"><a href="duality.html#rows-space"><i class="fa fa-check"></i><b>3.1</b> Rows Space</a></li>
<li class="chapter" data-level="3.2" data-path="duality.html"><a href="duality.html#columns-space"><i class="fa fa-check"></i><b>3.2</b> Columns Space</a></li>
<li class="chapter" data-level="3.3" data-path="duality.html"><a href="duality.html#cloud-of-individuals"><i class="fa fa-check"></i><b>3.3</b> Cloud of Individuals</a><ul>
<li class="chapter" data-level="3.3.1" data-path="duality.html"><a href="duality.html#average-individual"><i class="fa fa-check"></i><b>3.3.1</b> Average Individual</a></li>
<li class="chapter" data-level="3.3.2" data-path="duality.html"><a href="duality.html#centered-data"><i class="fa fa-check"></i><b>3.3.2</b> Centered Data</a></li>
<li class="chapter" data-level="3.3.3" data-path="duality.html"><a href="duality.html#distance-between-individuals"><i class="fa fa-check"></i><b>3.3.3</b> Distance between individuals</a></li>
<li class="chapter" data-level="3.3.4" data-path="duality.html"><a href="duality.html#distance-to-the-centroid"><i class="fa fa-check"></i><b>3.3.4</b> Distance to the centroid</a></li>
<li class="chapter" data-level="3.3.5" data-path="duality.html"><a href="duality.html#measures-of-dispersion"><i class="fa fa-check"></i><b>3.3.5</b> Measures of Dispersion</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="duality.html"><a href="duality.html#cloud-of-variables"><i class="fa fa-check"></i><b>3.4</b> Cloud of Variables</a><ul>
<li class="chapter" data-level="3.4.1" data-path="duality.html"><a href="duality.html#mean-of-a-variable"><i class="fa fa-check"></i><b>3.4.1</b> Mean of a Variable</a></li>
<li class="chapter" data-level="3.4.2" data-path="duality.html"><a href="duality.html#variance-of-a-variable"><i class="fa fa-check"></i><b>3.4.2</b> Variance of a Variable</a></li>
<li class="chapter" data-level="3.4.3" data-path="duality.html"><a href="duality.html#variance-with-vector-notation"><i class="fa fa-check"></i><b>3.4.3</b> Variance with Vector Notation</a></li>
<li class="chapter" data-level="3.4.4" data-path="duality.html"><a href="duality.html#standard-deviation-as-a-norm"><i class="fa fa-check"></i><b>3.4.4</b> Standard Deviation as a Norm</a></li>
<li class="chapter" data-level="3.4.5" data-path="duality.html"><a href="duality.html#covariance"><i class="fa fa-check"></i><b>3.4.5</b> Covariance</a></li>
<li class="chapter" data-level="3.4.6" data-path="duality.html"><a href="duality.html#correlation"><i class="fa fa-check"></i><b>3.4.6</b> Correlation</a></li>
<li class="chapter" data-level="3.4.7" data-path="duality.html"><a href="duality.html#geometry-of-correlation"><i class="fa fa-check"></i><b>3.4.7</b> Geometry of Correlation</a></li>
<li class="chapter" data-level="3.4.8" data-path="duality.html"><a href="duality.html#orthogonal-projections"><i class="fa fa-check"></i><b>3.4.8</b> Orthogonal Projections</a></li>
<li class="chapter" data-level="3.4.9" data-path="duality.html"><a href="duality.html#the-mean-as-an-orthogonal-projection"><i class="fa fa-check"></i><b>3.4.9</b> The mean as an orthogonal projection</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Unsupervised I: PCA</b></span></li>
<li class="chapter" data-level="4" data-path="pca.html"><a href="pca.html"><i class="fa fa-check"></i><b>4</b> Principal Components Analysis</a><ul>
<li class="chapter" data-level="4.1" data-path="pca.html"><a href="pca.html#low-dimensional-representations"><i class="fa fa-check"></i><b>4.1</b> Low-dimensional Representations</a></li>
<li class="chapter" data-level="4.2" data-path="pca.html"><a href="pca.html#projections"><i class="fa fa-check"></i><b>4.2</b> Projections</a><ul>
<li class="chapter" data-level="4.2.1" data-path="pca.html"><a href="pca.html#vector-and-scalar-projections"><i class="fa fa-check"></i><b>4.2.1</b> Vector and Scalar Projections</a></li>
<li class="chapter" data-level="4.2.2" data-path="pca.html"><a href="pca.html#projected-inertia"><i class="fa fa-check"></i><b>4.2.2</b> Projected Inertia</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="pca.html"><a href="pca.html#maximization-problem"><i class="fa fa-check"></i><b>4.3</b> Maximization Problem</a><ul>
<li class="chapter" data-level="4.3.1" data-path="pca.html"><a href="pca.html#eigenvectors-of-mathbfs"><i class="fa fa-check"></i><b>4.3.1</b> Eigenvectors of <span class="math inline">\(\mathbf{S}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="pca.html"><a href="pca.html#another-perspective-of-pca"><i class="fa fa-check"></i><b>4.4</b> Another Perspective of PCA</a><ul>
<li class="chapter" data-level="4.4.1" data-path="pca.html"><a href="pca.html#finding-principal-components"><i class="fa fa-check"></i><b>4.4.1</b> Finding Principal Components</a></li>
<li class="chapter" data-level="4.4.2" data-path="pca.html"><a href="pca.html#finding-the-first-pc"><i class="fa fa-check"></i><b>4.4.2</b> Finding the first PC</a></li>
<li class="chapter" data-level="4.4.3" data-path="pca.html"><a href="pca.html#finding-the-second-pc"><i class="fa fa-check"></i><b>4.4.3</b> Finding the second PC</a></li>
<li class="chapter" data-level="4.4.4" data-path="pca.html"><a href="pca.html#finding-all-pcs"><i class="fa fa-check"></i><b>4.4.4</b> Finding all PCs</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="pca.html"><a href="pca.html#data-decomposition-model"><i class="fa fa-check"></i><b>4.5</b> Data Decomposition Model</a><ul>
<li class="chapter" data-level="4.5.1" data-path="pca.html"><a href="pca.html#alternative-approaches"><i class="fa fa-check"></i><b>4.5.1</b> Alternative Approaches</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Linear Regression</b></span></li>
<li class="chapter" data-level="5" data-path="ols.html"><a href="ols.html"><i class="fa fa-check"></i><b>5</b> Linear Regression</a><ul>
<li class="chapter" data-level="5.1" data-path="ols.html"><a href="ols.html#motivation"><i class="fa fa-check"></i><b>5.1</b> Motivation</a></li>
<li class="chapter" data-level="5.2" data-path="ols.html"><a href="ols.html#the-ideaintuition-of-regression"><i class="fa fa-check"></i><b>5.2</b> The Idea/Intuition of Regression</a></li>
<li class="chapter" data-level="5.3" data-path="ols.html"><a href="ols.html#the-linear-regression-model"><i class="fa fa-check"></i><b>5.3</b> The Linear Regression Model</a></li>
<li class="chapter" data-level="5.4" data-path="ols.html"><a href="ols.html#the-error-measure"><i class="fa fa-check"></i><b>5.4</b> The Error Measure</a></li>
<li class="chapter" data-level="5.5" data-path="ols.html"><a href="ols.html#the-least-squares-algorithm"><i class="fa fa-check"></i><b>5.5</b> The Least Squares Algorithm</a></li>
<li class="chapter" data-level="5.6" data-path="ols.html"><a href="ols.html#geometries-of-ols"><i class="fa fa-check"></i><b>5.6</b> Geometries of OLS</a><ul>
<li class="chapter" data-level="5.6.1" data-path="ols.html"><a href="ols.html#rows-perspective"><i class="fa fa-check"></i><b>5.6.1</b> Rows Perspective</a></li>
<li class="chapter" data-level="5.6.2" data-path="ols.html"><a href="ols.html#columns-perspective"><i class="fa fa-check"></i><b>5.6.2</b> Columns Perspective</a></li>
<li class="chapter" data-level="5.6.3" data-path="ols.html"><a href="ols.html#parameters-perspective"><i class="fa fa-check"></i><b>5.6.3</b> Parameters Perspective</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="gradient.html"><a href="gradient.html"><i class="fa fa-check"></i><b>6</b> Gradient Descent</a><ul>
<li class="chapter" data-level="6.1" data-path="gradient.html"><a href="gradient.html#error-surface"><i class="fa fa-check"></i><b>6.1</b> Error Surface</a></li>
<li class="chapter" data-level="6.2" data-path="gradient.html"><a href="gradient.html#idea-of-gradient-descent"><i class="fa fa-check"></i><b>6.2</b> Idea of Gradient Descent</a></li>
<li class="chapter" data-level="6.3" data-path="gradient.html"><a href="gradient.html#moving-down-an-error-surface"><i class="fa fa-check"></i><b>6.3</b> Moving Down an Error Surface</a><ul>
<li class="chapter" data-level="6.3.1" data-path="gradient.html"><a href="gradient.html#the-direction-of-mathbfv"><i class="fa fa-check"></i><b>6.3.1</b> The direction of <span class="math inline">\(\mathbf{v}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="gradient.html"><a href="gradient.html#gradient-descent-and-our-model"><i class="fa fa-check"></i><b>6.4</b> Gradient Descent and our Model</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="olsml.html"><a href="olsml.html"><i class="fa fa-check"></i><b>7</b> Regression via Maximum Likelihood</a><ul>
<li class="chapter" data-level="7.1" data-path="olsml.html"><a href="olsml.html#linear-regression-reminder"><i class="fa fa-check"></i><b>7.1</b> Linear Regression Reminder</a><ul>
<li class="chapter" data-level="7.1.1" data-path="olsml.html"><a href="olsml.html#maximum-likelihood"><i class="fa fa-check"></i><b>7.1.1</b> Maximum Likelihood</a></li>
<li class="chapter" data-level="7.1.2" data-path="olsml.html"><a href="olsml.html#ml-estimator-of-sigma2"><i class="fa fa-check"></i><b>7.1.2</b> ML Estimator of <span class="math inline">\(\sigma^2\)</span></a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>V Learning Concepts</b></span></li>
<li class="chapter" data-level="8" data-path="learning.html"><a href="learning.html"><i class="fa fa-check"></i><b>8</b> Theoretical Framework</a><ul>
<li class="chapter" data-level="8.1" data-path="learning.html"><a href="learning.html#mental-map"><i class="fa fa-check"></i><b>8.1</b> Mental Map</a></li>
<li class="chapter" data-level="8.2" data-path="learning.html"><a href="learning.html#kinds-of-predictions"><i class="fa fa-check"></i><b>8.2</b> Kinds of Predictions</a><ul>
<li class="chapter" data-level="8.2.1" data-path="learning.html"><a href="learning.html#two-types-of-data"><i class="fa fa-check"></i><b>8.2.1</b> Two Types of Data</a></li>
<li class="chapter" data-level="8.2.2" data-path="learning.html"><a href="learning.html#two-types-of-predictions"><i class="fa fa-check"></i><b>8.2.2</b> Two Types of Predictions</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="learning.html"><a href="learning.html#two-types-of-errors"><i class="fa fa-check"></i><b>8.3</b> Two Types of Errors</a><ul>
<li class="chapter" data-level="8.3.1" data-path="learning.html"><a href="learning.html#individual-errors"><i class="fa fa-check"></i><b>8.3.1</b> Individual Errors</a></li>
<li class="chapter" data-level="8.3.2" data-path="learning.html"><a href="learning.html#overall-errors"><i class="fa fa-check"></i><b>8.3.2</b> Overall Errors</a></li>
<li class="chapter" data-level="8.3.3" data-path="learning.html"><a href="learning.html#auxiliary-technicality"><i class="fa fa-check"></i><b>8.3.3</b> Auxiliary Technicality</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="learning.html"><a href="learning.html#noisy-targets"><i class="fa fa-check"></i><b>8.4</b> Noisy Targets</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="mse.html"><a href="mse.html"><i class="fa fa-check"></i><b>9</b> MSE of Estimator</a><ul>
<li class="chapter" data-level="9.1" data-path="mse.html"><a href="mse.html#mse-of-an-estimator"><i class="fa fa-check"></i><b>9.1</b> MSE of an Estimator</a><ul>
<li class="chapter" data-level="9.1.1" data-path="mse.html"><a href="mse.html#prototypical-cases-of-bias-and-variance"><i class="fa fa-check"></i><b>9.1.1</b> Prototypical Cases of Bias and Variance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="biasvar.html"><a href="biasvar.html"><i class="fa fa-check"></i><b>10</b> Bias-Variance Tradeoff</a><ul>
<li class="chapter" data-level="10.1" data-path="biasvar.html"><a href="biasvar.html#introduction"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="biasvar.html"><a href="biasvar.html#motivation-example"><i class="fa fa-check"></i><b>10.2</b> Motivation Example</a><ul>
<li class="chapter" data-level="10.2.1" data-path="biasvar.html"><a href="biasvar.html#two-hypotheses"><i class="fa fa-check"></i><b>10.2.1</b> Two Hypotheses</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="biasvar.html"><a href="biasvar.html#learning-from-two-points"><i class="fa fa-check"></i><b>10.3</b> Learning from two points</a></li>
<li class="chapter" data-level="10.4" data-path="biasvar.html"><a href="biasvar.html#bias-variance-derivation"><i class="fa fa-check"></i><b>10.4</b> Bias-Variance Derivation</a><ul>
<li class="chapter" data-level="10.4.1" data-path="biasvar.html"><a href="biasvar.html#out-of-sample-predictions"><i class="fa fa-check"></i><b>10.4.1</b> Out-of-Sample Predictions</a></li>
<li class="chapter" data-level="10.4.2" data-path="biasvar.html"><a href="biasvar.html#noisy-target"><i class="fa fa-check"></i><b>10.4.2</b> Noisy Target</a></li>
<li class="chapter" data-level="10.4.3" data-path="biasvar.html"><a href="biasvar.html#types-of-theoretical-mses"><i class="fa fa-check"></i><b>10.4.3</b> Types of Theoretical MSEs</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="biasvar.html"><a href="biasvar.html#the-tradeoff"><i class="fa fa-check"></i><b>10.5</b> The Tradeoff</a><ul>
<li class="chapter" data-level="10.5.1" data-path="biasvar.html"><a href="biasvar.html#bias-variance-tradeoff-picture"><i class="fa fa-check"></i><b>10.5.1</b> Bias-Variance Tradeoff Picture</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="phases.html"><a href="phases.html"><i class="fa fa-check"></i><b>11</b> Learning Phases</a><ul>
<li class="chapter" data-level="11.1" data-path="phases.html"><a href="phases.html#introduction-1"><i class="fa fa-check"></i><b>11.1</b> Introduction</a></li>
<li class="chapter" data-level="11.2" data-path="phases.html"><a href="phases.html#model-assessment"><i class="fa fa-check"></i><b>11.2</b> Model Assessment</a><ul>
<li class="chapter" data-level="11.2.1" data-path="phases.html"><a href="phases.html#holdout-test-set"><i class="fa fa-check"></i><b>11.2.1</b> Holdout Test Set</a></li>
<li class="chapter" data-level="11.2.2" data-path="phases.html"><a href="phases.html#why-does-a-test-set-work"><i class="fa fa-check"></i><b>11.2.2</b> Why does a test set work?</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="phases.html"><a href="phases.html#model-selection"><i class="fa fa-check"></i><b>11.3</b> Model Selection</a><ul>
<li class="chapter" data-level="11.3.1" data-path="phases.html"><a href="phases.html#three-way-holdout-method"><i class="fa fa-check"></i><b>11.3.1</b> Three-way Holdout Method</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="phases.html"><a href="phases.html#model-training"><i class="fa fa-check"></i><b>11.4</b> Model Training</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="resampling.html"><a href="resampling.html"><i class="fa fa-check"></i><b>12</b> Resample Approaches</a><ul>
<li class="chapter" data-level="12.1" data-path="resampling.html"><a href="resampling.html#general-sampling-blueprint"><i class="fa fa-check"></i><b>12.1</b> General Sampling Blueprint</a></li>
<li class="chapter" data-level="12.2" data-path="resampling.html"><a href="resampling.html#monte-carlo-cross-validation"><i class="fa fa-check"></i><b>12.2</b> Monte Carlo Cross Validation</a></li>
<li class="chapter" data-level="12.3" data-path="resampling.html"><a href="resampling.html#bootstrap-method"><i class="fa fa-check"></i><b>12.3</b> Bootstrap Method</a></li>
<li class="chapter" data-level="12.4" data-path="resampling.html"><a href="resampling.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>12.4</b> <span class="math inline">\(K\)</span>-Fold Cross-Validation</a><ul>
<li class="chapter" data-level="12.4.1" data-path="resampling.html"><a href="resampling.html#leave-one-out-cross-validation-loocv"><i class="fa fa-check"></i><b>12.4.1</b> Leave-One-Out Cross Validation (LOOCV)</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>VI Regularization</b></span></li>
<li class="chapter" data-level="13" data-path="regular.html"><a href="regular.html"><i class="fa fa-check"></i><b>13</b> Regularization Techniques</a><ul>
<li class="chapter" data-level="13.1" data-path="regular.html"><a href="regular.html#multicollinearity-issues"><i class="fa fa-check"></i><b>13.1</b> Multicollinearity Issues</a><ul>
<li class="chapter" data-level="13.1.1" data-path="regular.html"><a href="regular.html#toy-example"><i class="fa fa-check"></i><b>13.1.1</b> Toy Example</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="regular.html"><a href="regular.html#irregular-coefficients"><i class="fa fa-check"></i><b>13.2</b> Irregular Coefficients</a></li>
<li class="chapter" data-level="13.3" data-path="regular.html"><a href="regular.html#connection-to-regularization"><i class="fa fa-check"></i><b>13.3</b> Connection to Regularization</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="pcr.html"><a href="pcr.html"><i class="fa fa-check"></i><b>14</b> Principal Components Regression</a><ul>
<li class="chapter" data-level="14.1" data-path="pcr.html"><a href="pcr.html#motivation-example-1"><i class="fa fa-check"></i><b>14.1</b> Motivation Example</a></li>
<li class="chapter" data-level="14.2" data-path="pcr.html"><a href="pcr.html#the-pcr-model"><i class="fa fa-check"></i><b>14.2</b> The PCR Model</a></li>
<li class="chapter" data-level="14.3" data-path="pcr.html"><a href="pcr.html#how-does-pcr-work"><i class="fa fa-check"></i><b>14.3</b> How does PCR work?</a><ul>
<li class="chapter" data-level="14.3.1" data-path="pcr.html"><a href="pcr.html#transition-formula"><i class="fa fa-check"></i><b>14.3.1</b> Transition Formula</a></li>
<li class="chapter" data-level="14.3.2" data-path="pcr.html"><a href="pcr.html#size-of-coefficients"><i class="fa fa-check"></i><b>14.3.2</b> Size of Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="pcr.html"><a href="pcr.html#selecting-number-of-pcs"><i class="fa fa-check"></i><b>14.4</b> Selecting Number of PCs</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="pls.html"><a href="pls.html"><i class="fa fa-check"></i><b>15</b> Partial Least Squares Regression</a><ul>
<li class="chapter" data-level="15.1" data-path="pls.html"><a href="pls.html#motivation-example-2"><i class="fa fa-check"></i><b>15.1</b> Motivation Example</a></li>
<li class="chapter" data-level="15.2" data-path="pls.html"><a href="pls.html#the-plsr-model"><i class="fa fa-check"></i><b>15.2</b> The PLSR Model</a></li>
<li class="chapter" data-level="15.3" data-path="pls.html"><a href="pls.html#how-does-plsr-work"><i class="fa fa-check"></i><b>15.3</b> How does PLSR work?</a></li>
<li class="chapter" data-level="15.4" data-path="pls.html"><a href="pls.html#plsr-algorithm"><i class="fa fa-check"></i><b>15.4</b> PLSR Algorithm</a><ul>
<li class="chapter" data-level="15.4.1" data-path="pls.html"><a href="pls.html#pls-solution-with-original-variables"><i class="fa fa-check"></i><b>15.4.1</b> PLS Solution with original variables</a></li>
<li class="chapter" data-level="15.4.2" data-path="pls.html"><a href="pls.html#size-of-coefficients-1"><i class="fa fa-check"></i><b>15.4.2</b> Size of Coefficients</a></li>
<li class="chapter" data-level="15.4.3" data-path="pls.html"><a href="pls.html#some-properties"><i class="fa fa-check"></i><b>15.4.3</b> Some Properties</a></li>
</ul></li>
<li class="chapter" data-level="15.5" data-path="pls.html"><a href="pls.html#selecting-number-of-pls-components"><i class="fa fa-check"></i><b>15.5</b> Selecting Number of PLS Components</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="ridge.html"><a href="ridge.html"><i class="fa fa-check"></i><b>16</b> Ridge Regression</a><ul>
<li class="chapter" data-level="16.1" data-path="ridge.html"><a href="ridge.html#a-new-minimization-problem"><i class="fa fa-check"></i><b>16.1</b> A New Minimization Problem</a><ul>
<li class="chapter" data-level="16.1.1" data-path="ridge.html"><a href="ridge.html#constraining-regression-coefficients"><i class="fa fa-check"></i><b>16.1.1</b> Constraining Regression Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="ridge.html"><a href="ridge.html#a-new-minimization-solution"><i class="fa fa-check"></i><b>16.2</b> A New Minimization Solution</a></li>
<li class="chapter" data-level="16.3" data-path="ridge.html"><a href="ridge.html#what-does-rr-accomplish"><i class="fa fa-check"></i><b>16.3</b> What does RR accomplish?</a><ul>
<li class="chapter" data-level="16.3.1" data-path="ridge.html"><a href="ridge.html#how-to-find-lambda"><i class="fa fa-check"></i><b>16.3.1</b> How to find <span class="math inline">\(\lambda\)</span>?</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>VII Extending Linear Regression</b></span></li>
<li class="chapter" data-level="17" data-path="linear-extensions.html"><a href="linear-extensions.html"><i class="fa fa-check"></i><b>17</b> Beyond Linear Regression</a><ul>
<li class="chapter" data-level="17.1" data-path="linear-extensions.html"><a href="linear-extensions.html#introduction-2"><i class="fa fa-check"></i><b>17.1</b> Introduction</a></li>
<li class="chapter" data-level="17.2" data-path="linear-extensions.html"><a href="linear-extensions.html#expanding-the-regression-horizon"><i class="fa fa-check"></i><b>17.2</b> Expanding the Regression Horizon</a><ul>
<li class="chapter" data-level="17.2.1" data-path="linear-extensions.html"><a href="linear-extensions.html#linearity"><i class="fa fa-check"></i><b>17.2.1</b> Linearity</a></li>
<li class="chapter" data-level="17.2.2" data-path="linear-extensions.html"><a href="linear-extensions.html#parametric-and-nonparametric"><i class="fa fa-check"></i><b>17.2.2</b> Parametric and Nonparametric</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="linear-extensions.html"><a href="linear-extensions.html#transforming-features"><i class="fa fa-check"></i><b>17.3</b> Transforming Features</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="nonparametric.html"><a href="nonparametric.html"><i class="fa fa-check"></i><b>18</b> Nonparametric Regression</a><ul>
<li class="chapter" data-level="18.1" data-path="nonparametric.html"><a href="nonparametric.html#conditional-averages"><i class="fa fa-check"></i><b>18.1</b> Conditional Averages</a></li>
<li class="chapter" data-level="18.2" data-path="nonparametric.html"><a href="nonparametric.html#looking-at-the-neighbors"><i class="fa fa-check"></i><b>18.2</b> Looking at the Neighbors</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="knn.html"><a href="knn.html"><i class="fa fa-check"></i><b>19</b> Nearest Neighbor Estimates</a><ul>
<li class="chapter" data-level="19.1" data-path="knn.html"><a href="knn.html#introduction-3"><i class="fa fa-check"></i><b>19.1</b> Introduction</a></li>
<li class="chapter" data-level="19.2" data-path="knn.html"><a href="knn.html#k-nearest-neighbors-knn"><i class="fa fa-check"></i><b>19.2</b> <span class="math inline">\(k\)</span> Nearest Neighbors (KNN)</a><ul>
<li class="chapter" data-level="19.2.1" data-path="knn.html"><a href="knn.html#distance-measures"><i class="fa fa-check"></i><b>19.2.1</b> Distance Measures</a></li>
<li class="chapter" data-level="19.2.2" data-path="knn.html"><a href="knn.html#knn-estimator"><i class="fa fa-check"></i><b>19.2.2</b> KNN Estimator</a></li>
<li class="chapter" data-level="19.2.3" data-path="knn.html"><a href="knn.html#how-to-find-k"><i class="fa fa-check"></i><b>19.2.3</b> How to find <span class="math inline">\(k\)</span>?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="20" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html"><i class="fa fa-check"></i><b>20</b> Kernel Smoothers</a><ul>
<li class="chapter" data-level="20.1" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#introduction-4"><i class="fa fa-check"></i><b>20.1</b> Introduction</a></li>
<li class="chapter" data-level="20.2" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#kernel-smoothers-1"><i class="fa fa-check"></i><b>20.2</b> Kernel Smoothers</a><ul>
<li class="chapter" data-level="20.2.1" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#kernel-functions"><i class="fa fa-check"></i><b>20.2.1</b> Kernel Functions</a></li>
<li class="chapter" data-level="20.2.2" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#weights-from-kernels"><i class="fa fa-check"></i><b>20.2.2</b> Weights from Kernels</a></li>
<li class="chapter" data-level="20.2.3" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#kernel-estimator"><i class="fa fa-check"></i><b>20.2.3</b> Kernel Estimator</a></li>
</ul></li>
<li class="chapter" data-level="20.3" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#local-polynomial-estimators"><i class="fa fa-check"></i><b>20.3</b> Local Polynomial Estimators</a></li>
</ul></li>
<li class="part"><span><b>VIII Classification</b></span></li>
<li class="chapter" data-level="21" data-path="classif.html"><a href="classif.html"><i class="fa fa-check"></i><b>21</b> Classification</a><ul>
<li class="chapter" data-level="21.1" data-path="classif.html"><a href="classif.html#introduction-5"><i class="fa fa-check"></i><b>21.1</b> Introduction</a><ul>
<li class="chapter" data-level="21.1.1" data-path="classif.html"><a href="classif.html#credit-score-example"><i class="fa fa-check"></i><b>21.1.1</b> Credit Score Example</a></li>
<li class="chapter" data-level="21.1.2" data-path="classif.html"><a href="classif.html#toy-example-1"><i class="fa fa-check"></i><b>21.1.2</b> Toy Example</a></li>
<li class="chapter" data-level="21.1.3" data-path="classif.html"><a href="classif.html#two-class-problem"><i class="fa fa-check"></i><b>21.1.3</b> Two-class Problem</a></li>
<li class="chapter" data-level="21.1.4" data-path="classif.html"><a href="classif.html#bayes-rule-reminder"><i class="fa fa-check"></i><b>21.1.4</b> Bayes’ Rule Reminder</a></li>
</ul></li>
<li class="chapter" data-level="21.2" data-path="classif.html"><a href="classif.html#bayes-classifier"><i class="fa fa-check"></i><b>21.2</b> Bayes Classifier</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="logistic.html"><a href="logistic.html"><i class="fa fa-check"></i><b>22</b> Logistic Regression</a><ul>
<li class="chapter" data-level="22.1" data-path="logistic.html"><a href="logistic.html#motivation-1"><i class="fa fa-check"></i><b>22.1</b> Motivation</a><ul>
<li class="chapter" data-level="22.1.1" data-path="logistic.html"><a href="logistic.html#first-approach-fitting-a-line"><i class="fa fa-check"></i><b>22.1.1</b> First Approach: Fitting a Line</a></li>
<li class="chapter" data-level="22.1.2" data-path="logistic.html"><a href="logistic.html#second-approach-harsh-thresholding"><i class="fa fa-check"></i><b>22.1.2</b> Second Approach: Harsh Thresholding</a></li>
<li class="chapter" data-level="22.1.3" data-path="logistic.html"><a href="logistic.html#third-approach-conditional-means"><i class="fa fa-check"></i><b>22.1.3</b> Third Approach: Conditional Means</a></li>
</ul></li>
<li class="chapter" data-level="22.2" data-path="logistic.html"><a href="logistic.html#logistic-regression-model"><i class="fa fa-check"></i><b>22.2</b> Logistic Regression Model</a><ul>
<li class="chapter" data-level="22.2.1" data-path="logistic.html"><a href="logistic.html#the-criterion-being-optimized"><i class="fa fa-check"></i><b>22.2.1</b> The Criterion Being Optimized</a></li>
<li class="chapter" data-level="22.2.2" data-path="logistic.html"><a href="logistic.html#another-way-to-solve-logistic-regression"><i class="fa fa-check"></i><b>22.2.2</b> Another Way to Solve Logistic Regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="23" data-path="discrim.html"><a href="discrim.html"><i class="fa fa-check"></i><b>23</b> Preamble for Discriminant Analysis</a><ul>
<li class="chapter" data-level="23.1" data-path="discrim.html"><a href="discrim.html#motivation-2"><i class="fa fa-check"></i><b>23.1</b> Motivation</a><ul>
<li class="chapter" data-level="23.1.1" data-path="discrim.html"><a href="discrim.html#distinguishing-species"><i class="fa fa-check"></i><b>23.1.1</b> Distinguishing Species</a></li>
<li class="chapter" data-level="23.1.2" data-path="discrim.html"><a href="discrim.html#sum-of-squares-decomposition"><i class="fa fa-check"></i><b>23.1.2</b> Sum of Squares Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="23.2" data-path="discrim.html"><a href="discrim.html#derived-ratios-from-sum-of-squares"><i class="fa fa-check"></i><b>23.2</b> Derived Ratios from Sum-of-Squares</a><ul>
<li class="chapter" data-level="23.2.1" data-path="discrim.html"><a href="discrim.html#correlation-ratio"><i class="fa fa-check"></i><b>23.2.1</b> Correlation Ratio</a></li>
<li class="chapter" data-level="23.2.2" data-path="discrim.html"><a href="discrim.html#f-ratio"><i class="fa fa-check"></i><b>23.2.2</b> F-Ratio</a></li>
</ul></li>
<li class="chapter" data-level="23.3" data-path="discrim.html"><a href="discrim.html#geometric-perspective"><i class="fa fa-check"></i><b>23.3</b> Geometric Perspective</a><ul>
<li class="chapter" data-level="23.3.1" data-path="discrim.html"><a href="discrim.html#clouds-from-class-structure"><i class="fa fa-check"></i><b>23.3.1</b> Clouds from Class Structure</a></li>
<li class="chapter" data-level="23.3.2" data-path="discrim.html"><a href="discrim.html#dispersion-decomposition"><i class="fa fa-check"></i><b>23.3.2</b> Dispersion Decomposition</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="24" data-path="cda.html"><a href="cda.html"><i class="fa fa-check"></i><b>24</b> Canonical Discriminant Analysis</a><ul>
<li class="chapter" data-level="24.1" data-path="cda.html"><a href="cda.html#cda-semi-supervised-aspect"><i class="fa fa-check"></i><b>24.1</b> CDA: Semi-Supervised Aspect</a></li>
<li class="chapter" data-level="24.2" data-path="cda.html"><a href="cda.html#looking-for-a-discriminant-axis"><i class="fa fa-check"></i><b>24.2</b> Looking for a discriminant axis</a></li>
<li class="chapter" data-level="24.3" data-path="cda.html"><a href="cda.html#looking-for-a-compromise-criterion"><i class="fa fa-check"></i><b>24.3</b> Looking for a Compromise Criterion</a><ul>
<li class="chapter" data-level="24.3.1" data-path="cda.html"><a href="cda.html#correlation-ratio-criterion"><i class="fa fa-check"></i><b>24.3.1</b> Correlation Ratio Criterion</a></li>
<li class="chapter" data-level="24.3.2" data-path="cda.html"><a href="cda.html#f-ratio-criterion"><i class="fa fa-check"></i><b>24.3.2</b> F-ratio Criterion</a></li>
<li class="chapter" data-level="24.3.3" data-path="cda.html"><a href="cda.html#a-special-pca"><i class="fa fa-check"></i><b>24.3.3</b> A Special PCA</a></li>
</ul></li>
<li class="chapter" data-level="24.4" data-path="cda.html"><a href="cda.html#cda-supervised-aspect"><i class="fa fa-check"></i><b>24.4</b> CDA: Supervised Aspect</a><ul>
<li class="chapter" data-level="24.4.1" data-path="cda.html"><a href="cda.html#distance-behind-cda"><i class="fa fa-check"></i><b>24.4.1</b> Distance behind CDA</a></li>
<li class="chapter" data-level="24.4.2" data-path="cda.html"><a href="cda.html#predictive-idea"><i class="fa fa-check"></i><b>24.4.2</b> Predictive Idea</a></li>
<li class="chapter" data-level="24.4.3" data-path="cda.html"><a href="cda.html#cda-classifier"><i class="fa fa-check"></i><b>24.4.3</b> CDA Classifier</a></li>
<li class="chapter" data-level="24.4.4" data-path="cda.html"><a href="cda.html#limitations-of-cda-classifier"><i class="fa fa-check"></i><b>24.4.4</b> Limitations of CDA classifier</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="25" data-path="discanalysis.html"><a href="discanalysis.html"><i class="fa fa-check"></i><b>25</b> Discriminant Analysis</a><ul>
<li class="chapter" data-level="25.1" data-path="discanalysis.html"><a href="discanalysis.html#probabilistic-da"><i class="fa fa-check"></i><b>25.1</b> Probabilistic DA</a><ul>
<li class="chapter" data-level="25.1.1" data-path="discanalysis.html"><a href="discanalysis.html#normal-distributions"><i class="fa fa-check"></i><b>25.1.1</b> Normal Distributions</a></li>
<li class="chapter" data-level="25.1.2" data-path="discanalysis.html"><a href="discanalysis.html#estimating-parameters-of-normal-distributions"><i class="fa fa-check"></i><b>25.1.2</b> Estimating Parameters of Normal Distributions</a></li>
</ul></li>
<li class="chapter" data-level="25.2" data-path="discanalysis.html"><a href="discanalysis.html#discriminant-functions"><i class="fa fa-check"></i><b>25.2</b> Discriminant Functions</a></li>
<li class="chapter" data-level="25.3" data-path="discanalysis.html"><a href="discanalysis.html#quadratic-discriminant-analysis-qda"><i class="fa fa-check"></i><b>25.3</b> Quadratic Discriminant Analysis (QDA)</a></li>
<li class="chapter" data-level="25.4" data-path="discanalysis.html"><a href="discanalysis.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>25.4</b> Linear Discriminant Analysis</a><ul>
<li class="chapter" data-level="25.4.1" data-path="discanalysis.html"><a href="discanalysis.html#canonical-discriminant-analysis"><i class="fa fa-check"></i><b>25.4.1</b> Canonical Discriminant Analysis</a></li>
<li class="chapter" data-level="25.4.2" data-path="discanalysis.html"><a href="discanalysis.html#naive-bayes"><i class="fa fa-check"></i><b>25.4.2</b> Naive Bayes</a></li>
<li class="chapter" data-level="25.4.3" data-path="discanalysis.html"><a href="discanalysis.html#fifth-case"><i class="fa fa-check"></i><b>25.4.3</b> Fifth Case</a></li>
</ul></li>
<li class="chapter" data-level="25.5" data-path="discanalysis.html"><a href="discanalysis.html#comparing-the-cases"><i class="fa fa-check"></i><b>25.5</b> Comparing the Cases</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="classperformance.html"><a href="classperformance.html"><i class="fa fa-check"></i><b>26</b> Performance of Classifiers</a><ul>
<li class="chapter" data-level="26.1" data-path="classperformance.html"><a href="classperformance.html#classification-error-measures"><i class="fa fa-check"></i><b>26.1</b> Classification Error Measures</a><ul>
<li class="chapter" data-level="26.1.1" data-path="classperformance.html"><a href="classperformance.html#errors-for-binary-response"><i class="fa fa-check"></i><b>26.1.1</b> Errors for Binary Response</a></li>
<li class="chapter" data-level="26.1.2" data-path="classperformance.html"><a href="classperformance.html#error-for-categorical-response"><i class="fa fa-check"></i><b>26.1.2</b> Error for Categorical Response</a></li>
</ul></li>
<li class="chapter" data-level="26.2" data-path="classperformance.html"><a href="classperformance.html#confusion-matrices"><i class="fa fa-check"></i><b>26.2</b> Confusion Matrices</a></li>
<li class="chapter" data-level="26.3" data-path="classperformance.html"><a href="classperformance.html#binary-response-example"><i class="fa fa-check"></i><b>26.3</b> Binary Response Example</a><ul>
<li class="chapter" data-level="26.3.1" data-path="classperformance.html"><a href="classperformance.html#application-for-checking-account"><i class="fa fa-check"></i><b>26.3.1</b> Application for Checking Account</a></li>
<li class="chapter" data-level="26.3.2" data-path="classperformance.html"><a href="classperformance.html#application-for-loan"><i class="fa fa-check"></i><b>26.3.2</b> Application for Loan</a></li>
</ul></li>
<li class="chapter" data-level="26.4" data-path="classperformance.html"><a href="classperformance.html#decision-rules-and-errors"><i class="fa fa-check"></i><b>26.4</b> Decision Rules and Errors</a></li>
<li class="chapter" data-level="26.5" data-path="classperformance.html"><a href="classperformance.html#roc-curves"><i class="fa fa-check"></i><b>26.5</b> ROC Curves</a><ul>
<li class="chapter" data-level="26.5.1" data-path="classperformance.html"><a href="classperformance.html#graphing-roc-curves"><i class="fa fa-check"></i><b>26.5.1</b> Graphing ROC curves</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IX Unsupervised II: Clustering</b></span></li>
<li class="chapter" data-level="27" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>27</b> Clustering</a><ul>
<li class="chapter" data-level="27.1" data-path="clustering.html"><a href="clustering.html#about-clustering"><i class="fa fa-check"></i><b>27.1</b> About Clustering</a><ul>
<li class="chapter" data-level="27.1.1" data-path="clustering.html"><a href="clustering.html#types-of-clustering"><i class="fa fa-check"></i><b>27.1.1</b> Types of Clustering</a></li>
<li class="chapter" data-level="27.1.2" data-path="clustering.html"><a href="clustering.html#hard-clustering"><i class="fa fa-check"></i><b>27.1.2</b> Hard Clustering</a></li>
</ul></li>
<li class="chapter" data-level="27.2" data-path="clustering.html"><a href="clustering.html#dispersion-measures"><i class="fa fa-check"></i><b>27.2</b> Dispersion Measures</a></li>
<li class="chapter" data-level="27.3" data-path="clustering.html"><a href="clustering.html#complexity-in-clustering"><i class="fa fa-check"></i><b>27.3</b> Complexity in Clustering</a></li>
</ul></li>
<li class="chapter" data-level="28" data-path="kmeans.html"><a href="kmeans.html"><i class="fa fa-check"></i><b>28</b> K-Means</a><ul>
<li class="chapter" data-level="28.1" data-path="kmeans.html"><a href="kmeans.html#toy-example-2"><i class="fa fa-check"></i><b>28.1</b> Toy Example</a></li>
<li class="chapter" data-level="28.2" data-path="kmeans.html"><a href="kmeans.html#what-does-k-means-do"><i class="fa fa-check"></i><b>28.2</b> What does K-means do?</a></li>
<li class="chapter" data-level="28.3" data-path="kmeans.html"><a href="kmeans.html#k-means-algorithms"><i class="fa fa-check"></i><b>28.3</b> K-Means Algorithms</a><ul>
<li class="chapter" data-level="28.3.1" data-path="kmeans.html"><a href="kmeans.html#classic-version"><i class="fa fa-check"></i><b>28.3.1</b> Classic Version</a></li>
<li class="chapter" data-level="28.3.2" data-path="kmeans.html"><a href="kmeans.html#moving-centers-algorithm"><i class="fa fa-check"></i><b>28.3.2</b> Moving Centers Algorithm</a></li>
<li class="chapter" data-level="28.3.3" data-path="kmeans.html"><a href="kmeans.html#dynamic-clouds"><i class="fa fa-check"></i><b>28.3.3</b> Dynamic Clouds</a></li>
<li class="chapter" data-level="28.3.4" data-path="kmeans.html"><a href="kmeans.html#choosing-k"><i class="fa fa-check"></i><b>28.3.4</b> Choosing <span class="math inline">\(K\)</span></a></li>
<li class="chapter" data-level="28.3.5" data-path="kmeans.html"><a href="kmeans.html#comments"><i class="fa fa-check"></i><b>28.3.5</b> Comments</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="29" data-path="hclus.html"><a href="hclus.html"><i class="fa fa-check"></i><b>29</b> Hierarchical Clustering</a><ul>
<li class="chapter" data-level="29.1" data-path="hclus.html"><a href="hclus.html#agglomerative-methods"><i class="fa fa-check"></i><b>29.1</b> Agglomerative Methods</a></li>
<li class="chapter" data-level="29.2" data-path="hclus.html"><a href="hclus.html#example-single-linkage"><i class="fa fa-check"></i><b>29.2</b> Example: Single Linkage</a><ul>
<li class="chapter" data-level="29.2.1" data-path="hclus.html"><a href="hclus.html#dendrogram"><i class="fa fa-check"></i><b>29.2.1</b> Dendrogram</a></li>
</ul></li>
<li class="chapter" data-level="29.3" data-path="hclus.html"><a href="hclus.html#example-complete-linkage"><i class="fa fa-check"></i><b>29.3</b> Example: Complete Linkage</a><ul>
<li class="chapter" data-level="29.3.1" data-path="hclus.html"><a href="hclus.html#cutting-dendograms"><i class="fa fa-check"></i><b>29.3.1</b> Cutting Dendograms</a></li>
<li class="chapter" data-level="29.3.2" data-path="hclus.html"><a href="hclus.html#pros-and-coons"><i class="fa fa-check"></i><b>29.3.2</b> Pros and COons</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>X Tree-based Methods</b></span></li>
<li class="chapter" data-level="30" data-path="trees.html"><a href="trees.html"><i class="fa fa-check"></i><b>30</b> Intro to Decision Trees</a><ul>
<li class="chapter" data-level="30.1" data-path="trees.html"><a href="trees.html#introduction-6"><i class="fa fa-check"></i><b>30.1</b> Introduction</a></li>
<li class="chapter" data-level="30.2" data-path="trees.html"><a href="trees.html#some-terminology"><i class="fa fa-check"></i><b>30.2</b> Some Terminology</a><ul>
<li class="chapter" data-level="30.2.1" data-path="trees.html"><a href="trees.html#binary-trees"><i class="fa fa-check"></i><b>30.2.1</b> Binary Trees</a></li>
</ul></li>
<li class="chapter" data-level="30.3" data-path="trees.html"><a href="trees.html#space-partitions"><i class="fa fa-check"></i><b>30.3</b> Space Partitions</a><ul>
<li class="chapter" data-level="30.3.1" data-path="trees.html"><a href="trees.html#the-process-of-building-a-tree"><i class="fa fa-check"></i><b>30.3.1</b> The Process of Building a Tree</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="31" data-path="tree-impurities.html"><a href="tree-impurities.html"><i class="fa fa-check"></i><b>31</b> Binary Splits and Impurity</a><ul>
<li class="chapter" data-level="31.1" data-path="tree-impurities.html"><a href="tree-impurities.html#binary-partitions"><i class="fa fa-check"></i><b>31.1</b> Binary Partitions</a><ul>
<li class="chapter" data-level="31.1.1" data-path="tree-impurities.html"><a href="tree-impurities.html#splits-of-binary-variables"><i class="fa fa-check"></i><b>31.1.1</b> Splits of Binary variables</a></li>
<li class="chapter" data-level="31.1.2" data-path="tree-impurities.html"><a href="tree-impurities.html#splits-of-nominal-variables"><i class="fa fa-check"></i><b>31.1.2</b> Splits of Nominal Variables</a></li>
<li class="chapter" data-level="31.1.3" data-path="tree-impurities.html"><a href="tree-impurities.html#splits-of-ordinal-variables"><i class="fa fa-check"></i><b>31.1.3</b> Splits of Ordinal Variables</a></li>
<li class="chapter" data-level="31.1.4" data-path="tree-impurities.html"><a href="tree-impurities.html#splits-continuous-variables"><i class="fa fa-check"></i><b>31.1.4</b> Splits Continuous variables</a></li>
</ul></li>
<li class="chapter" data-level="31.2" data-path="tree-impurities.html"><a href="tree-impurities.html#measures-of-impurity"><i class="fa fa-check"></i><b>31.2</b> Measures of Impurity</a><ul>
<li class="chapter" data-level="31.2.1" data-path="tree-impurities.html"><a href="tree-impurities.html#entropy"><i class="fa fa-check"></i><b>31.2.1</b> Entropy</a></li>
<li class="chapter" data-level="31.2.2" data-path="tree-impurities.html"><a href="tree-impurities.html#the-math-behind-entropy"><i class="fa fa-check"></i><b>31.2.2</b> The Math Behind Entropy</a></li>
<li class="chapter" data-level="31.2.3" data-path="tree-impurities.html"><a href="tree-impurities.html#gini-impurity"><i class="fa fa-check"></i><b>31.2.3</b> Gini Impurity</a></li>
<li class="chapter" data-level="31.2.4" data-path="tree-impurities.html"><a href="tree-impurities.html#variance-based-impurity"><i class="fa fa-check"></i><b>31.2.4</b> Variance-based Impurity</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="32" data-path="tree-splits.html"><a href="tree-splits.html"><i class="fa fa-check"></i><b>32</b> Splitting Nodes</a><ul>
<li class="chapter" data-level="32.1" data-path="tree-splits.html"><a href="tree-splits.html#entropy-based-splits"><i class="fa fa-check"></i><b>32.1</b> Entropy-based Splits</a></li>
<li class="chapter" data-level="32.2" data-path="tree-splits.html"><a href="tree-splits.html#gini-index-based-splits"><i class="fa fa-check"></i><b>32.2</b> Gini-index based Splits</a></li>
<li class="chapter" data-level="32.3" data-path="tree-splits.html"><a href="tree-splits.html#looking-for-the-best-split"><i class="fa fa-check"></i><b>32.3</b> Looking for the best split</a></li>
</ul></li>
<li class="chapter" data-level="33" data-path="tree-basics.html"><a href="tree-basics.html"><i class="fa fa-check"></i><b>33</b> Building Binary Trees</a><ul>
<li class="chapter" data-level="33.1" data-path="tree-basics.html"><a href="tree-basics.html#node-splitting-stopping-criteria"><i class="fa fa-check"></i><b>33.1</b> Node-Splitting Stopping Criteria</a></li>
<li class="chapter" data-level="33.2" data-path="tree-basics.html"><a href="tree-basics.html#issues-with-trees"><i class="fa fa-check"></i><b>33.2</b> Issues with Trees</a><ul>
<li class="chapter" data-level="33.2.1" data-path="tree-basics.html"><a href="tree-basics.html#bias-variance-of-trees"><i class="fa fa-check"></i><b>33.2.1</b> Bias-Variance of Trees</a></li>
</ul></li>
<li class="chapter" data-level="33.3" data-path="tree-basics.html"><a href="tree-basics.html#pruning-a-tree"><i class="fa fa-check"></i><b>33.3</b> Pruning a Tree</a></li>
<li class="chapter" data-level="33.4" data-path="tree-basics.html"><a href="tree-basics.html#pros-and-cons-of-trees"><i class="fa fa-check"></i><b>33.4</b> Pros and Cons of Trees</a><ul>
<li class="chapter" data-level="33.4.1" data-path="tree-basics.html"><a href="tree-basics.html#advantages-of-trees"><i class="fa fa-check"></i><b>33.4.1</b> Advantages of Trees</a></li>
<li class="chapter" data-level="33.4.2" data-path="tree-basics.html"><a href="tree-basics.html#disadvantages-of-trees"><i class="fa fa-check"></i><b>33.4.2</b> Disadvantages of Trees</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="34" data-path="bagging.html"><a href="bagging.html"><i class="fa fa-check"></i><b>34</b> Bagging</a><ul>
<li class="chapter" data-level="34.1" data-path="bagging.html"><a href="bagging.html#introduction-7"><i class="fa fa-check"></i><b>34.1</b> Introduction</a><ul>
<li class="chapter" data-level="34.1.1" data-path="bagging.html"><a href="bagging.html#idea-of-bagging"><i class="fa fa-check"></i><b>34.1.1</b> Idea of Bagging</a></li>
</ul></li>
<li class="chapter" data-level="34.2" data-path="bagging.html"><a href="bagging.html#why-bother-bagging"><i class="fa fa-check"></i><b>34.2</b> Why Bother Bagging?</a></li>
</ul></li>
<li class="chapter" data-level="35" data-path="forest.html"><a href="forest.html"><i class="fa fa-check"></i><b>35</b> Random Forests</a><ul>
<li class="chapter" data-level="35.1" data-path="forest.html"><a href="forest.html#introduction-8"><i class="fa fa-check"></i><b>35.1</b> Introduction</a></li>
<li class="chapter" data-level="35.2" data-path="forest.html"><a href="forest.html#algorithm-1"><i class="fa fa-check"></i><b>35.2</b> Algorithm</a><ul>
<li class="chapter" data-level="35.2.1" data-path="forest.html"><a href="forest.html#two-sources-of-randomness"><i class="fa fa-check"></i><b>35.2.1</b> Two Sources of Randomness</a></li>
<li class="chapter" data-level="35.2.2" data-path="forest.html"><a href="forest.html#regressions-and-classification-forests"><i class="fa fa-check"></i><b>35.2.2</b> Regressions and Classification Forests</a></li>
<li class="chapter" data-level="35.2.3" data-path="forest.html"><a href="forest.html#key-advantage-of-random-forests"><i class="fa fa-check"></i><b>35.2.3</b> Key Advantage of Random Forests</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Made with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">All Models Are Wrong: Concepts of Statistical Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ridge" class="section level1">
<h1><span class="header-section-number">16</span> Ridge Regression</h1>
<p>From ordinary least squares regression, we know that the predicted response is
given by:</p>
<p><span class="math display" id="eq:504-01">\[
\mathbf{\hat{y}} = \mathbf{X} (\mathbf{X^\mathsf{T} X})^{-1} \mathbf{X^\mathsf{T} y}
\tag{16.1}
\]</span></p>
<p>(provided the inverse exists). We also know that if <span class="math inline">\(\mathbf{X^\mathsf{T} X}\)</span>
is near-singular, finding its inverse could lead to trouble. For example,
near-perfect multicollinearity corresponds to small eigenvalues of
<span class="math inline">\(\mathbf{X^\mathsf{T} X}\)</span>, which will cause estimated regression coefficients
to have large variance.</p>
<p>Hence, a natural idea arises: let’s try to modify <span class="math inline">\(\mathbf{X^\mathsf{T} X}\)</span>
to avoid this danger of near-singularity. Specifically, let us add a constant
positive term <span class="math inline">\(\lambda\)</span> to the diagonal of <span class="math inline">\(\mathbf{X^\mathsf{T} X}\)</span>, here’s how:</p>
<p><span class="math display" id="eq:504-02">\[
\mathbf{X^\mathsf{T} X} + \lambda \mathbf{I}
\tag{16.2}
\]</span></p>
<p>Note that <span class="math inline">\(\lambda\)</span> is <em>not</em> an eigenvalue here; it is simply a positive constant. In what way the addition of <span class="math inline">\(\lambda &gt; 0\)</span> to the diagonal of
<span class="math inline">\(\mathbf{X^\mathsf{T} X}\)</span> changes the structure of this matrix?</p>
<p>By properties of eigenstructures, it turns out that
<span class="math inline">\(\mathbf{X^\mathsf{T} X} + \lambda \mathbf{I}\)</span> and <span class="math inline">\(\mathbf{X^\mathsf{T} X}\)</span>
have the same eigenvectors; however, the eigenvalues
of the former matrix will never be <span class="math inline">\(0\)</span>.</p>
<p>Specifically, say <span class="math inline">\(\mathbf{X^\mathsf{T} X}\)</span> has eigenvalues
<span class="math inline">\(\lambda_1, \lambda_2, \dots, \lambda_p\)</span>. The eigenvalues of
<span class="math inline">\(\mathbf{X^\mathsf{T} X} + \lambda \mathbf{I}\)</span> will therefore be
<span class="math inline">\(\overset{*}{\lambda_1}, \overset{*}{\lambda_2}, \dots, \overset{*}{\lambda_p}\)</span>
where <span class="math inline">\(\overset{*}{\lambda_j} = \lambda_j + \lambda\)</span>.</p>
<p>Another interesting aspect is that there is actually a minimization problem
behind this method (of appending an extra term to the eigenvalues). In Ridge
Regression, we have that the predicted output is:</p>
<p><span class="math display" id="eq:504-03">\[
\mathbf{\hat{y}}_{RR}  = \mathbf{X} (\mathbf{X^\mathsf{T} X} + \lambda \mathbf{I} )^{-1} \mathbf{X^\mathsf{T} y}
\tag{16.3}
\]</span></p>
<p>To understand where this estimator comes from, let’s consider the geometry of
the MSE function in a regression context.</p>
<div id="a-new-minimization-problem" class="section level2">
<h2><span class="header-section-number">16.1</span> A New Minimization Problem</h2>
<p>As you know, in linear regression the overall error function <span class="math inline">\(E_{in}\)</span> that we
are interested in minimizing is the mean squared error (<span class="math inline">\(\text{MSE}\)</span>).
Moreover, we can look at the geometry of such error measure from the perspective
of the parameters (i.e. the regression coefficients).
From this perspective, we denote this error function as <span class="math inline">\(E_{in}(\mathbf{b})\)</span>,
making explicit its dependency on the vector of coefficients <span class="math inline">\(\mathbf{b}\)</span>.
For illustration purposes, let’s consider two inputs <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>, and their corresponding parameters <span class="math inline">\(b_1\)</span> and <span class="math inline">\(b_2\)</span>. The error function <span class="math inline">\(E_{in}(\mathbf{b})\)</span>
will generate a convex error surface with the shape of a bowl, or paraboloid
(see figure below).</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-154"></span>
<img src="images/penalized/ridge-error-surface.svg" alt="Error Surface" width="95%" />
<p class="caption">
Figure 16.1: Error Surface
</p>
</div>
<p>As usual we can express MSE for <span class="math inline">\(E_{in}(\mathbf{b})\)</span> as follows:</p>
<p><span class="math display" id="eq:504-04">\[
E_{in}(\mathbf{b}) = \frac{1}{n} (\mathbf{Xb} - \mathbf{y})^{\mathsf{T}} (\mathbf{Xb} - \mathbf{y})
\tag{16.4}
\]</span></p>
<p>In OLS, we minimize <span class="math inline">\(E(\mathbf{b})\)</span> unconditionally; that is, without any type
of restriction. The associated solution, is indicated with a blue dot at the
center of the elliptical contours of constant error.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-155"></span>
<img src="images/penalized/ridge-constraint0.svg" alt="Contour Errors with OLS solution" width="70%" />
<p class="caption">
Figure 16.2: Contour Errors with OLS solution
</p>
</div>
<div id="constraining-regression-coefficients" class="section level3">
<h3><span class="header-section-number">16.1.1</span> Constraining Regression Coefficients</h3>
<p>An alternative approach to minimize <span class="math inline">\(E_{in}(\mathbf{b})\)</span> unconditionally is to
impose a restriction on the squared magnitude of the regression coefficients.
In other words, we still want to minimize the mean squared error
<span class="math inline">\(E_{in}(\mathbf{b})\)</span> but now we don’t let <span class="math inline">\(\mathbf{b}\)</span> take any kind of values.
Instead, we are going to require the following condition on <span class="math inline">\(b_1, \dots, b_p\)</span>:</p>
<p><span class="math display">\[
\textsf{constraining coefficients:} \quad \sum_{j=1}^{p} b_{j}^2 \leq c
\]</span></p>
<p>Here’s an easy way to think about—and remember—this condition: think of
<span class="math inline">\(c\)</span> as a <em>budget</em> for how much you can spend on the size of all coefficients
<span class="math inline">\(b_1, \dots, b_p\)</span> (or their squared values to be more precise).</p>
<p>Adding this restriction to the MSE, we now have the following constrained
minimization of <span class="math inline">\(E_{in}(\mathbf{b})\)</span>:</p>
<p><span class="math display" id="eq:504-05">\[
\min_{\mathbf{b}} \left\{ \frac{1}{n} (\mathbf{Xb} - \mathbf{y})^{\mathsf{T}} (\mathbf{Xb} - \mathbf{y})  \right\}  \quad \mathrm{s.t.} \quad \| \mathbf{b} \|_{2}^{2} = \mathbf{b^\mathsf{T} b} \leq c
\tag{16.5}
\]</span></p>
<p>for some “budget” <span class="math inline">\(c\)</span>.</p>
<p>On the <span class="math inline">\(b_1, b_2\)</span> plane, what is the locus of points such that
<span class="math inline">\(\mathbf{b^\mathsf{T}b} \leq c\)</span>? The answer is a disk of radius <span class="math inline">\(c\)</span>,
centered at the origin. Hence, sketching the level curves (curves of constant
errors) as well as this restriction, we obtain the following picture
(for some value of <span class="math inline">\(c\)</span>):</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-156"></span>
<img src="images/penalized/ridge-constraint1.svg" alt="Error Contours with Constraint" width="70%" />
<p class="caption">
Figure 16.3: Error Contours with Constraint
</p>
</div>
<p>Depending on the chosen value for <span class="math inline">\(c\)</span>, we could end up with a big enough
constraint that includes the OLS solution, like in the following picture.
As you would expect, in this case the solution with the restriction is not
constraining anything whatsoever.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-157"></span>
<img src="images/penalized/ridge-constraint2.svg" alt="Error contours with a large budget" width="70%" />
<p class="caption">
Figure 16.4: Error contours with a large budget
</p>
</div>
<p>Of course, we could make our budget stricter by reducing the value of <span class="math inline">\(c\)</span> we
choose, something depicted in the next figure.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-158"></span>
<img src="images/penalized/ridge-constraint3.svg" alt="Error Contours with small budget" width="70%" />
<p class="caption">
Figure 16.5: Error Contours with small budget
</p>
</div>
</div>
</div>
<div id="a-new-minimization-solution" class="section level2">
<h2><span class="header-section-number">16.2</span> A New Minimization Solution</h2>
<p>Let’s consider one generic elliptical contour of constant error, a given
budget <span class="math inline">\(c\)</span>, and a point <span class="math inline">\(\mathbf{b}\)</span> satisfying the budget constraint, like in
the picture below.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-159"></span>
<img src="images/penalized/ridge-constraint4.svg" alt="Generic error contour with candidate point" width="70%" />
<p class="caption">
Figure 16.6: Generic error contour with candidate point
</p>
</div>
<p>As you can tell from the above diagram, the candidate point <span class="math inline">\(\mathbf{b}\)</span> is
such that <span class="math inline">\(\mathbf{b^\mathsf{T} b} = c\)</span>. However, this point is not fully
minimizing <span class="math inline">\(E(\mathbf{b})\)</span>, given the constraint. Visually speaking, we could
find other candidate values for <span class="math inline">\(\mathbf{b}\)</span> along the red circumference that
would give us smaller <span class="math inline">\(E(\mathbf{b})\)</span>.</p>
<p>How do we know that the shown candidate point <span class="math inline">\(\mathbf{b}\)</span> can be improved?
To answer this question we should visualize the gradient <span class="math inline">\(\nabla E(\mathbf{b})\)</span>
which, as we know, will be pointing in the direction orthogonal to the contour
ellipse—i.e. direction of largest change of <span class="math inline">\(E(\mathbf{b})\)</span>. In addition, we
should also pay attention to the direction of the candidate point, illustrated
with the red vector. Notice that this vector is normal (i.e. orthogonal) to the
circumference of the constraint.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-160"></span>
<img src="images/penalized/ridge-constraint5.svg" alt="Gradient of error surface associated to candidate point" width="70%" />
<p class="caption">
Figure 16.7: Gradient of error surface associated to candidate point
</p>
</div>
<p>Notice also that the angle between the normal vector and the gradient is less
than 180 degrees. This is an indication that we can find better <span class="math inline">\(\mathbf{b}\)</span>
points that make the error smaller. But is there an optimal <span class="math inline">\(\mathbf{b^*}\)</span> that
achieves the smallest <span class="math inline">\(E(\mathbf{b})\)</span> while satisfying the budget constraint?
With this visualization, we get the intuition that <span class="math inline">\(\mathbf{b}^{*}\)</span> must lie on
the boundary of the constraint region (i.e., in the 2-D case, on the circle of
radius <span class="math inline">\(c\)</span> centered at the origin).</p>
<p>It turns out that the optimal vector <span class="math inline">\(\mathbf{b^*}\)</span> corresponds to the one that
is exactly the opposite of the gradient.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-161"></span>
<img src="images/penalized/ridge-constraint6.svg" alt="Optimal point opposite to the gradient" width="70%" />
<p class="caption">
Figure 16.8: Optimal point opposite to the gradient
</p>
</div>
<p>At this point, the gradient and normal vectors will be exactly antiparallel
which, in mathematical terms, means that the normal vector is proportional to
the gradient:</p>
<p><span class="math display" id="eq:504-06">\[
\nabla E_{in}(\mathbf{b}^*) \propto - \mathbf{b}^{*}
\tag{16.6}
\]</span></p>
<p>For convenience purposes, let us choose a proportionality constant of
<span class="math inline">\(-2 (\lambda / n)\)</span>. That is, we want the vector <span class="math inline">\(\mathbf{b}^{*}\)</span> such that</p>
<p><span class="math display" id="eq:504-07">\[
\nabla E_{in}(\mathbf{b}^*) = - 2 \frac{\lambda}{n} \mathbf{b}^{*}
\tag{16.7}
\]</span></p>
<p>Rearranging the terms the above equation becomes:</p>
<p><span class="math display" id="eq:504-08">\[
\nabla E_{in}(\mathbf{b}^*) + 2 \frac{\lambda}{n} \mathbf{b}^{*} = \mathbf{0}
\tag{16.8}
\]</span></p>
<p>Now, notice that the above equation looks like the gradient of something…
But what is that something? The answer turns out to be:</p>
<p><span class="math display" id="eq:504-09">\[
f(\mathbf{b}) = E_{in}(\mathbf{b}) + (\lambda/n) \mathbf{b}^\mathsf{T} \mathbf{b}
\tag{16.9}
\]</span></p>
<p>You can verify directly that <span class="math inline">\(\nabla f(\mathbf{b})\)</span> is precisely the left hand
side of equation <a href="ridge.html#eq:504-08">(16.8)</a>:</p>
<p><span class="math display" id="eq:504-10">\[
\nabla f(\mathbf{b}^*) = \nabla E_{in}(\mathbf{b}^*) + 2 \frac{\lambda}{n} \mathbf{b}^{*}
\tag{16.10}
\]</span></p>
<p>Let us rephrase what we have seen so far. From our initial minimization
problem <a href="ridge.html#eq:504-05">(16.5)</a>, we saw that its solution is given by the vector
<span class="math inline">\(\mathbf{b}^{*}\)</span> that solves <em>another</em> minimization problem:</p>
<p><span class="math display" id="eq:504-11">\[
\min_{\mathbf{b}} \left\{ E_{in}(\mathbf{b}) + \frac{\lambda}{n} \mathbf{b}^\mathsf{T} \mathbf{b} \right\} 
\tag{16.11}
\]</span></p>
<p>In other words, the minimizations <a href="ridge.html#eq:504-05">(16.5)</a> and <a href="ridge.html#eq:504-11">(16.11)</a> are
exactly equivalent.</p>
<p>From <a href="ridge.html#eq:504-11">(16.11)</a>, we can obtain the solution,
namely, the ridge regression estimate of <span class="math inline">\(\mathbf{b}\)</span>:</p>
<p><span class="math display">\[
\textsf{ridge coefficients:} \quad \mathbf{b}_{RR} = (\mathbf{X}^\mathsf{T} \mathbf{X} + \lambda \mathbf{I})^{-1} \mathbf{X}^\mathsf{T} \mathbf{y}
\]</span></p>
<p>As you can imagine, the optimal vector <span class="math inline">\(\mathbf{b}\)</span> that minimizes <span class="math inline">\(E_{in}\)</span> and
satisfies the constraint, will be on a given contour of constant error. This
situation is illustrated in the image below. Also, this image depicts the
general case in which the estimated rigde parameters are biased. That is, the
direction of the optimal vector <span class="math inline">\(\mathbf{b}\)</span> is not pointing in the direction
of the OLS solution.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-162"></span>
<img src="images/penalized/ridge-constraint7.svg" alt="Error contour intersecting the constraint circle" width="70%" />
<p class="caption">
Figure 16.9: Error contour intersecting the constraint circle
</p>
</div>
<p>Note that, from the discussion at the beginning of this chapter, we see that
the ridge regression estimate always exists, and is always unique (unlike the
OLS estimate).</p>
</div>
<div id="what-does-rr-accomplish" class="section level2">
<h2><span class="header-section-number">16.3</span> What does RR accomplish?</h2>
<p>In ridge regression, the key ingredient is the hyperparameter <span class="math inline">\(\lambda\)</span>.
Setting <span class="math inline">\(\lambda = 0\)</span> causes the ridge solution to be the same as the OLS
solution:</p>
<p><span class="math display" id="eq:504-12">\[\begin{align*}
\mathbf{b}_{RR} &amp;= (\mathbf{X}^\mathsf{T} \mathbf{X} + 0 \mathbf{I})^{-1} \mathbf{X}^\mathsf{T} \mathbf{y} \\
&amp;= (\mathbf{X}^\mathsf{T} \mathbf{X})^{-1} \mathbf{X}^\mathsf{T} \mathbf{y} \\
&amp;= \mathbf{b}_{OLS}
\tag{16.12}
\end{align*}\]</span></p>
<p>But what about specifying a very large value for <span class="math inline">\(\lambda\)</span>? Let’s think about
this case. If <span class="math inline">\(\lambda\)</span> becomes larger and larger, the terms on the diagonal of
<span class="math inline">\((\mathbf{X}^\mathsf{T} \mathbf{X} + \lambda \mathbf{I})\)</span> will be dominated by
such <span class="math inline">\(\lambda\)</span> value. Consequently, the matrix inverse,
<span class="math inline">\((\mathbf{X}^\mathsf{T} \mathbf{X} + \lambda \mathbf{I})^{-1}\)</span>, will basically be
dominated by the inverse of the terms on its diagonal:</p>
<p><span class="math display" id="eq:504-13">\[
\lambda &gt;&gt; 0  \quad \Longrightarrow \quad (\mathbf{X}^\mathsf{T} \mathbf{X} + \lambda \mathbf{I})^{-1} \rightarrow \frac{1}{\lambda} \mathbf{I}
\tag{16.13}
\]</span></p>
<p>Taking this idea to the extreme with very large <span class="math inline">\(\lambda\)</span>, will cause
<span class="math inline">\(1/\lambda\)</span> to be zero, essentially “collapsing”
<span class="math inline">\((\mathbf{X}^\mathsf{T} \mathbf{X} + \lambda \mathbf{I})^{-1}\)</span> into a
<span class="math inline">\(p \times p\)</span> matrix full of zeros:</p>
<p><span class="math display" id="eq:504-14">\[
\lambda \rightarrow \infty \quad \Longrightarrow \quad (\mathbf{X}^\mathsf{T} \mathbf{X} + \lambda \mathbf{I})^{-1} \rightarrow \mathbf{0}_{(p,p)}
\tag{16.14}
\]</span></p>
<p>thus, the ridge regression coefficients will also collapse to zero:</p>
<p><span class="math display" id="eq:504-15">\[
\lambda \rightarrow \infty \quad \Longrightarrow \quad \mathbf{b}_{RR} = \mathbf{0}
\tag{16.15}
\]</span></p>
<div id="relation-between-lambda-and-c" class="section level4 unnumbered">
<h4>Relation between <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(c\)</span>?</h4>
<p>What is the relationship between the budget <span class="math inline">\(c\)</span> and the non-negative constant
<span class="math inline">\(\lambda\)</span>? Both of these scalars are intimately related. Imposing a large budget
constraint <span class="math inline">\(c\)</span>, results in a small <span class="math inline">\(\lambda\)</span>. The smaller the <span class="math inline">\(\lambda\)</span>, the
closer <span class="math inline">\(\mathbf{b}_{RR}\)</span> to the OLS solution. Conversely, imposing a small
budget <span class="math inline">\(c\)</span>, causes <span class="math inline">\(\lambda\)</span> to become large. The larger the <span class="math inline">\(\lambda\)</span>, the
closer the ridge coefficients to zero.</p>
<p><span class="math display">\[
\uparrow c \quad \Longrightarrow \quad \downarrow \lambda \\
\downarrow c \quad \Longrightarrow \quad \uparrow \lambda
\]</span></p>
</div>
<div id="how-to-find-lambda" class="section level3">
<h3><span class="header-section-number">16.3.1</span> How to find <span class="math inline">\(\lambda\)</span>?</h3>
<p>In ridge regression, <span class="math inline">\(\lambda\)</span> is a tuning parameter, and therefore it cannot
be found analytically. Instead, you have to implement a trial and error process
with various values for <span class="math inline">\(\lambda\)</span>, and determine which seems to be a good one.
How? Typically with cross-validation, or other type of resampling approach.
Here’s a description of the steps to be carried out.</p>
<p>Assumet that we have a training data set consisting of <span class="math inline">\(n\)</span> data points:
<span class="math inline">\(\mathcal{D}_{train} = (\mathbf{x_1}, y_1), \dots, (\mathbf{x_n}, y_n)\)</span>.
Using <span class="math inline">\(K\)</span>-fold cross-validation, we (randomly) split the data into <span class="math inline">\(K\)</span> folds:</p>
<p><span class="math display">\[
\mathcal{D}_{train} = \mathcal{D}_{fold-1} \cup \mathcal{D}_{fold-2} \dots \cup \mathcal{D}_{fold-K}
\]</span></p>
<p>Each fold set <span class="math inline">\(\mathcal{D}_{fold-k}\)</span> will play the role of an evaluation set <span class="math inline">\(\mathcal{D}_{eval-k}\)</span>.
Having defined the <span class="math inline">\(k\)</span> fold sets, we form the corresponding <span class="math inline">\(K\)</span> retraining sets:</p>
<ul>
<li><span class="math inline">\(\mathcal{D}_{train-1} = \mathcal{D}_{train} \setminus \mathcal{D}_{fold-1}\)</span></li>
<li><span class="math inline">\(\mathcal{D}_{train-2} = \mathcal{D}_{train} \setminus \mathcal{D}_{fold-2}\)</span></li>
<li><span class="math inline">\(\dots\)</span></li>
<li><span class="math inline">\(\mathcal{D}_{train-K} = \mathcal{D}_{train} \setminus \mathcal{D}_{fold-K}\)</span></li>
</ul>
<p>The cross-validation procedure then repeats the following loops:</p>
<ul>
<li>For <span class="math inline">\(\lambda_b = 0.001, 0.002, \dots, \lambda_B\)</span>
<ul>
<li>For <span class="math inline">\(k = 1, \dots, K\)</span>
<ul>
<li>fit RR model <span class="math inline">\(h_{b,k}\)</span> with <span class="math inline">\(\lambda_b\)</span> on <span class="math inline">\(\mathcal{D}_{train-k}\)</span></li>
<li>compute and store <span class="math inline">\(E_{eval-k} (h_{b,k})\)</span> using <span class="math inline">\(\mathcal{D}_{eval-k}\)</span></li>
</ul></li>
<li>end for <span class="math inline">\(k\)</span></li>
<li>compute and store <span class="math inline">\(E_{cv_{b}} = \frac{1}{K} \sum_k E_{eval-k}(h_{b,k})\)</span></li>
</ul></li>
<li>end for <span class="math inline">\(\lambda_b\)</span></li>
<li>Compare all cross-validation errors <span class="math inline">\(E_{cv_1}, E_{cv_2}, \dots, E_{cv_B}\)</span> and choose the smallest of them, say <span class="math inline">\(E_{cv_{b^*}}\)</span></li>
<li>Use <span class="math inline">\(\lambda^*\)</span> to fit the (finalist) Ridge Regression model:
<span class="math inline">\(\mathbf{\hat{y}} = (\mathbf{X}^\mathsf{T} \mathbf{X} + \lambda^* \mathbf{I})^{-1} \mathbf{X}^\mathsf{T} \mathbf{y}\)</span></li>
</ul>

</div>
</div>
</div>



            </section>

          </div>
        </div>
      </div>
<a href="pls.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="linear-extensions.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["allmodelsarewrong.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
