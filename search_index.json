[
["overfit.html", "11 Overfitting 11.1 Introduction 11.2 Simulation", " 11 Overfitting In this chapter we discuss the notions of underfitting and overfitting. 11.1 Introduction In supervised learning, one of the major risks we run when fitting a model is to overestimate how well it will do when we use it. This risk has its own name: overfitting, and it is something that every statistical learning user should learn about. The phenomenon of overfitting is very similar to when we feel overconfident before taking a test, and then finding out that we were not as well prepared for the actual test as we presumptively thought. Following on this metaphor of studying for a test, we are pretty sure you have experienced the following situations: You studied for a test, and were able to grasp the general idea for some topics, but lacked many of the important details. You studied for a test, focusing too much on certain topics, memorizing most or even all of their details, at the expense of other topics. Despite your best disposition to study for a test, you were not able to study properly because of major distractions, for example: you got distracted with all the notifications in your phone, or an emergency happened that affected your studying plans, or you got distracted with the noise produced by those loud reparations in your building. Those of us who have experienced these pitfalls, know very well that they typically lead to poor performance on the test, or at least they result in a lower than expected performance. It turns out that these issues are also shared by supervised learning systems, and we have special names for each of them: Bias: how much of the target model’s behavior your class of model \\(\\bar{h}()\\) can approximate. Variance: how much variation the fitted model \\(h()\\) experiments when exposed to new learning data. Noise: random or irreducible noise \\(\\sigma^2\\) in the data. This, of course, brings us back to the bias-variance decomposition and its famous tradeoff: \\[ \\mathbb{E}_{\\mathcal{D}} \\left [ \\left (h^{(\\mathcal{D})}(x_0) - y_0 \\right)^2 \\right ] = \\text{var} + \\text{bias}^2 + \\sigma^2 \\tag{11.1} \\] We have two characteristic situations: Underfitting: when the model has limited learning capacity, and gets only the “general ideal” Overfitting: when the model shows good performance in-sample, but when applied to out-of-sample data, it performs poorly We should say that Overfitting is a bit more complex. Think of overfitting as a phenomenon or process. 11.2 Simulation For illustration purposes, let’s carry out a simple simulation. To keep things simple, we consider a target function with some noise. \\[ f(x) = sin(1 + x^2) + \\varepsilon \\] with the input variable \\(x\\) in the interval \\([-1,1]\\), and the noise term \\(\\varepsilon\\) having mean zero and constant variance \\(\\sigma^2 = 0.09\\). The signal \\(sin(1 + x^2)\\) is depicted in the figure below: We randomly generate 10 in-sample points (blue circles), and 10 out-of-sample points (red crosses) Linear Model The first fitted model is a linear model \\[ h(x) = b_0 + b_1 x \\] Quadratic Model The second fitted model is a quadratic model (i.e. polynomial of degree 2) \\[ h(x) = b_0 + b_1 x + b_2 x^2 \\] Nonic Model Because we have \\(n = 10\\) data points, we can fit a polynomial up to degree 9, which is called a nonic: \\[ h(x) = b_0 + b_1 x + b_2 x^2 + \\dots + b_8 x^8 + b_9 x^9 \\] Various Polynomials We fit polynomials of various degress, from degree 1 to 9. and compute both the in-sample error and the out-of-sample error: degree Ein Eout 1 1 0.00147 0.00215 2 2 0.00093 0.00137 3 3 0.00029 0.00081 4 4 0.00012 0.00116 5 5 0.00011 0.00110 6 6 0.00007 0.00108 7 7 0.00004 0.00118 8 8 0.00003 0.00151 9 9 0.00000 0.00231 which we can display graphically "]
]
