[
["index.html", "All Models Are Wrong Concepts of Statistical Learning (CSL) Preface", " All Models Are Wrong Concepts of Statistical Learning (CSL) by Gaston Sanchez, and Ethan Marzban Preface This is a work in progress for an introductory text about concepts of Statistical Learning, covering some of the common supervised as well as unsupervised methods. How to cite this book: Sanchez, G., Marzban, E. (2019) All Models Are Wrong: Concepts of Statistical Learning. https://allmodelsarewrong.github.io © 2019 Sanchez, Marzban. All Rights Reserved. "],
["about.html", "1 About this book", " 1 About this book This is a book about concepts of Statistical Learning. The book has an overarching take-home message that we use as the title of this work: All Models Are Wrong. This comes from a famous quote credited to great British statistician George Edward Pelham Box: “All models are wrong, but some are useful.” Because Statistical Learning is inherently related to model building, we must never forget that every proposed model will be wrong. This is why we need to measure the generalization error of a model. We may never be able to come up with a perfect model that gives zero error. And that is okay. It doesn’t have to be perfect. The important—and challenging—thing is to find useful models (yes, we know, easier said than done). Knowing that the field(s) of Machine Learning, Statisticial Learning (SL), and any other name about learning from data, is a very broad subject, we should warn you that this book is not intended to be the ultimate compilation of every single SL technique ever devised. Instead, we focus on the concepts that we consider the building blocks that any user or practitioner needs to make sense of most common SL techniques. A big shortcoming of the book: we don’t cover neural networks. At least not in this first round of iterations. Sorry. On the plus side: We’ve tried hard to keep the notation as simple and consistent as possible. And we’ve also made a titanic effort to make it extremely visual (lots of diagrams, pictures, plots, graphs, figures, …, you name it). Prerequisites We are assuming that you already have some knowledge under your belt. You will better understand (and hopefully enjoy) the book if you’ve taken one or more courses on the following subjects: linear or matrix algebra multivariable calculus statistics probability programming or scripting Acknowledgements Many thanks to the UC Berkeley students, and teaching staff, of Stat 154 Modern Statistical Prediction and Machine Learning (Fall 2017, Spring 2018, Fall 2019). In particular, thank you to Jin Kweon, and Jiyoon Jeong for catching many typos in the first iteration of the course slides. Also, thanks to Anita Silver, Joyce Yip, Jingwei Guan, Skylar Liang, and Raymond Chang for being amazing and committed note takers. Likewise, thanks to Johnny Hong, Omid Shams Solari, Ryan Theisen, and Frank Qiu, for their collaboration as TAs. "],
["intro.html", "2 Introduction 2.1 Basic Notation", " 2 Introduction Picture a data set containing scores of several courses for college students. For example, courses like matrix algebra, multivariable calculus, statistics, and probability. And say we also have historical data about a course in Statistical Learning. In particular we have final scores measured on a scale from 0 to 100, we also have final grades (in letter grade scale), as well as a third interesting variable “Pass - Non-Pass” indicating whether the student passed statistical learning. Some data like that fits perfectly well in a tabular format. The rows contain the records for a bunch of students, and the columns refer to the variables. Student LinAlg Calculus Statistics StatLearn Grade P/NP 1 \\(\\bigcirc\\) \\(\\bigcirc\\) \\(\\bigcirc\\) 92 A P 2 \\(\\bigcirc\\) \\(\\bigcirc\\) \\(\\bigcirc\\) 85 B P 3 \\(\\bigcirc\\) \\(\\bigcirc\\) \\(\\bigcirc\\) 40 F NP New \\(\\bigcirc\\) \\(\\bigcirc\\) \\(\\bigcirc\\) ? ? ? Suppose that, based on this historical data, we wish to predict the score of a new student (whose Linear Algebra, Calculus, and Statistics grades are known) in Statistical Learning. To do so, we would fit some sort of model to our data; i.e. we would perform regression. This is a form of supervised learning, since our model is trained using known inputs (i.e. LinAlg, Calculus, and Statistics grades) as well as known responses (i.e. the Statistical Learning grades of the previous students). Likewise, we may be interested in studying the data not from a prediction oriented perspective but from a purely exploratory perspective. For example, maybe we want to investigate what is the relationship between the courses Linear Algebra, Calculus, and Statistics; that is: explore the relationship between the features. Or maybe we want to study the resemblance among individuals and see what kind of students have similar scores, or if there are “natural” groups of individuals based on their features. Both of these tasks are examples of unsupervised learning. We use the information in the data to discover patterns, without focusing on any single variable as a target response. In summary, we will focus on two types of learning paradigms: Unsupervised Learning: where we have inputs, but not response variables. Supervised Learning: where we have inputs, and one (or more) response variable(s). Figure 2.1: Inputs and outputs in supervised and unsupervised learning By the way, there are other types of Learning paradigms (e.g. deep learning, reinforcement learning), but we won’t discuss them in this book. To visualize the different types of learning, the different types of variables, and the methodology associated with each combination of learning/data types, we can use the following graphic: Figure 2.2: Supervised and Unsupervised Corners 2.1 Basic Notation In this book we are going to use a fair amount of math notation. Becoming familiar with the meaning of all the different symbols as soon as possible, should allow you to keep the learning curve a little bit less steep. The starting point is always the data, which we will assume to be in a tabular format, that can be translated into a mathematical matrix object. Here’s an example of a data matrix \\(\\mathbf{X}\\) of size \\(n \\times p\\) \\[ \\mathbf{X} = \\ \\begin{bmatrix} x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1j} &amp; \\cdots &amp; x_{1p} \\\\ x_{21} &amp; x_{22} &amp; \\cdots &amp; x_{2j} &amp; \\cdots &amp; x_{2p} \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots &amp; &amp; \\vdots \\\\ x_{i1} &amp; x_{i2} &amp; \\cdots &amp; x_{ij} &amp; \\cdots &amp; x_{ip} \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots &amp; &amp; \\vdots \\\\ x_{n1} &amp; x_{n2} &amp; \\cdots &amp; x_{nj} &amp; \\cdots &amp; x_{np} \\\\ \\end{bmatrix} \\tag{2.1} \\] By default, we will assume that the rows of a data matrix correspond to the individuals or objects. Likewise, we will also assume that the columns of a data matrix correspond to the variables or features observed on the individuals. In this sense, the symbol \\(x_{ij}\\) represents the value observed for the \\(j\\)-th variable on the \\(i\\)-th individual. Throughout this book, every time you see the letter \\(i\\), either alone or as an index associated with any other symbol (superscript or subscript), it means that such term corresponds to an individual or a row of some data matrix. For instance, symbols like \\(x_i\\), \\(\\mathbf{x_i}\\), and \\(\\alpha_i\\) are all examples that refer to—or denote a connection with—individuals. In turn, we will always use the letter \\(j\\) to convey association with variables or columns of some data matrix. For instance, \\(x_j\\), \\(\\mathbf{x_j}\\), and \\(\\alpha_j\\) are examples that refer to—or denote a connection with—variables. For better or for worse, we’ve made the decision to represent both the rows and the columns of a matrix as vectors using the same notation: as bold lower case letters such as \\(\\mathbf{x_i}\\) and \\(\\mathbf{x_j}\\). Because we know that there’s a risk of confusing a vector that corresponds to a row with a vector that corresponds to a column, sometimes we will use the arrow notation for row vectors: \\(\\mathbf{\\vec{x}_i}\\). So, going back to the above data matrix \\(\\mathbf{X}\\), we can represent the first variable as a vector \\(\\mathbf{x_1} = (x_{11}, x_{21}, \\dots, x_{n1})\\). Likewise, we can represent the first individual with the vector \\(\\mathbf{\\vec{x}_1} = (x_{11}, x_{12}, \\dots, x_{1n})\\). Here’s a reference table with the most common symbols and notation used throughout the book. Symbol Description \\(n\\) number of objects or individuals \\(p\\) number of variables or features \\(i\\) running index for rows or individuals \\(j\\) running index for columns or variables \\(k\\) running index determined by context \\(\\ell, m, q\\) other auxiliary indexes \\(f()\\), \\(h()\\), \\(d()\\) functions \\(\\lambda, \\mu, \\gamma, \\alpha\\) greek letters represent scalars \\(\\mathbf{x}\\), \\(\\mathbf{y}\\) variables, size determined by context \\(\\mathbf{w}\\), \\(\\mathbf{a}\\), \\(\\mathbf{b}\\) vectors of weight coefficients \\(\\mathbf{z}\\), \\(\\mathbf{t}\\), \\(\\mathbf{u}\\) components or latent variables \\(\\mathbf{X} : n \\times p\\) data matrix with \\(n\\) rows and \\(p\\) columns \\(x_{ij}\\) element of a matrix in \\(i\\)-th row and \\(j\\)-th column \\(\\mathbf{1}\\) vector of ones, size determined by context \\(\\mathbf{I}\\) identity matrix, size determined by context By the way, there are many more symbols that will appear in later chapters. But for now these are the fundamental ones. Likewise, the table below contains some of the most common operators that we will use in subsequent chapters: Symbol Description \\(\\mathbb{E}[X]\\) expected value of a random variable \\(X\\) \\(\\|\\mathbf{a}\\|\\) euclidean norm of a vector \\(\\mathbf{a}^{\\mathsf{T}}\\) transpose of a vector (or matrix) \\(\\mathbf{a^{\\mathsf{T}}b}\\) inner product of two vectors \\(\\langle \\mathbf{a}, \\mathbf{b} \\rangle\\) inner product of two vectors \\(det(\\mathbf{A})\\) determinant of a square matrix \\(tr(\\mathbf{A})\\) trace of a square matrix \\(\\mathbf{A}^{-1}\\) inverse of a square matrix \\(diag(\\mathbf{A})\\) diagonal of a square matrix \\(E()\\) overall error function \\(err()\\) pointwise error function \\(sign()\\) sign function \\(var()\\) variance function \\(cov()\\) covariance function \\(cor()\\) correlation function "],
["duality.html", "3 Geometric Duality 3.1 Rows Space 3.2 Columns Space 3.3 Cloud of Individuals 3.4 Cloud of Variables", " 3 Geometric Duality Before discussing unsupervised as well as supervised learning methods, we prefer to give you a prelude by talking and thinking about data in a geometric sense. This chapter will set the stage for most of the topics covered in later chapters. Let’s suppose we have some data in the form of a data matrix. For convenience purposes, let’s also suppose that all variables are measured in a real-value scale. Obviously not all data is expressed or even encoded numerically. You may have categorical or symbolic data. But for this illustration, let’s assume that any categorical and symbolic data has already been transformed into a numeric scale (e.g. dummy indicators, optimal scaling). It’s very enlightening to think of a data matrix as viewed from the glass of Geometry. The key idea is to think of the data in a matrix as elements living in a multidimensional space. Actually, we can regard a data matrix from two apparently different perspectives that, in reality, are intimately connected: the rows perspective and the columns perspective. In order to explain these perspectives, let me use the following diagram of a data matrix \\(\\mathbf{X}\\) with \\(n\\) rows and \\(p\\) columns, with \\(x_{ij}\\) representing the element in the \\(i\\)-th row and \\(j\\)-th column. Figure 3.1: Duality of a data matrix When we look at a data matrix from the columns perpective what we are doing is focusing on the \\(p\\) variables. In a similar way, when looking at a data matrix from its rows perspective, we are focusing on the \\(n\\) individuals. Like a coin, though, this matrix has two sides: a rows side, and a columns side. That is, we could look at the data from the rows point of view, or the columns point of view. These two views are (of course) not completely independent. This double perspective or duality for short, is like the two sides of the same coin. 3.1 Rows Space We know that human vision is limited to three-dimensions, but pretend that you had superpowers that let you visualize a space with any number of dimensions. Because each row of the data matrix has \\(p\\) elements, we can regard individuals as objects that live in a \\(p\\)-dimensional space. For visualization purposes, think of each variable as playing the role of a dimension associated to a given axis in this space; likewise, consider each of the \\(n\\) individuals as being depicted as a point (or particle) in such space, like in the following diagram: Figure 3.2: Rows space In the figure above, even though I’m showing only three axes, you should pretend that you are visualizing a \\(p\\)-dimensional space (imaging that there are \\(p\\) axes). Each point in this space corresponds to a single individual, and they all form what we can call a cloud of points. 3.2 Columns Space We can do the same visual exercise with the columns of a data matrix. Since each variable has \\(n\\) elements, we can regard the set of \\(p\\) variables as objects that live in an \\(n\\)-dimensional space. However, instead of representing each variable with a dot, it’s better to graphically represent them with an arrow (or vector). Why? Because of two reasons: one is to distinguish them from the individuals (dots). But more important, because the esential thing with a variable is not really its magnitude (and therefore its position) but its direction. Often, as part of the preprocessing steps we apply transformations on variables that change their scales (e.g. shrinking them, or stretching them) without modifying their directions. Figure 3.3: Columns space Analogously to the rows space and its cloud of individuals, you should also pretend that the image above is displaying an \\(n\\)-dimensional space with a bunch of blue arrows pointing in various directions. What’s next? Now that we know how to think of data from a geometric perspective, the next step is to discuss a handful of common operations that can be performed with points and vectors that live in some geometric space. 3.3 Cloud of Individuals In the previous chapter we introduce the powerful idea of looking at the rows and columns of a data matrix from the lens of geometry. We are assuming in general that the rows have to do with \\(n\\) individuals that lie in a \\(p\\)-dimensional space. Figure 3.4: Cloud of points Let’s start describing a set of common operations that we can apply on the individuals (living in a \\(p\\)-dimensional space). 3.3.1 Average Individual We can ask about the typical or average individual. If you only have one variable, then all the individual points lie in a one-dimensional space, which is basically a line: Figure 3.5: Points in one dimension In this case, the average individual is simply the average of the values, which geometrically corresponds to the balancing point: Figure 3.6: Average individual Algebraically we have: individuals \\(x_1, x_2, \\dots, x_n\\), and the average is: \\[ \\bar{x} = \\frac{x_1 + \\dots + x_n}{n} = \\frac{1}{n} \\sum_{i=1}^{n} x_i \\] In vector notation, the average can be calculated with an inner product between \\(\\mathbf{x} = (x_1, x_2, \\dots, x_n)\\), and a constant vector of \\(n\\)-ones \\(\\mathbf{1}\\): \\[ \\bar{x} = \\mathbf{x^\\mathsf{T}1} \\] What about the multivariate case? It turns out that we can also ask about the average individual of a cloud of points, like in the following figure: Figure 3.7: Cloud of points with centroid (i.e. average individual) The average individual, in a \\(p\\)-dimensional space is the point \\(\\mathbf{\\vec{g}}\\) containing as coordiantes the averages of all the variables: \\[ \\mathbf{\\vec{g}} = (\\bar{x}_1, \\bar{x}_2, \\dots, \\bar{x}_j) \\] where \\(\\bar{x}_j\\) is the average of the \\(j\\)-th variable. This average individual \\(\\mathbf{\\vec{g}}\\) is also known as the centroid, barycenter, or center of gravity of the cloud of points. 3.3.2 Centered Data Often, it is convenient to transform the data in such a way that the centroid of a data set becomes the origin of the cloud of points. Geometrically, this type of transformation involves a shif of the axes in the \\(p\\)-dimensional space. Algebraically, this transformation corresponds to expresing the values of each variable in terms of deviations from their means. Figure 3.8: Cloud of points of mean-centered data 3.3.3 Distance between individuals Another common operation that we may be interested in is the distance between two individuals. Obviously the notion of distance is not unique, since you can choose different types of distance measures. Perhaps the most comon type of distance is the (squared) Euclidean distance. Unless otherwise mentioned, this will be the default distance used in this book. Figure 3.9: Distance between two individuals If you have one variable \\(X\\), then the squared distance \\(d^2(i,\\ell)\\) between two individuals \\(x_i\\) and \\(x_\\ell\\) is: \\[ d^2(i,\\ell) = (x_i - x_\\ell)^2 \\] In general, with \\(p\\) variables, the squared distance between the \\(i\\)-th individual and the \\(\\ell\\)-th individual is: \\[\\begin{align*} d^2(i,\\ell) &amp;= (x_{i1} - x_{\\ell 1})^2 + (x_{i2} - x_{\\ell 2})^2 + \\dots + (x_{ip} - x_{\\ell p})^2 \\\\ &amp;= (\\mathbf{\\vec{x}_i} - \\mathbf{\\vec{x}_\\ell})^\\mathsf{T} (\\mathbf{\\vec{x}_i} - \\mathbf{\\vec{x}_\\ell}) \\end{align*}\\] 3.3.4 Distance to the centroid A special case is the distance between any individual \\(i\\) and the average individual: \\[\\begin{align*} d^2(i,g) &amp;= (x_{i1} - \\bar{x}_1)^2 + (x_{i2} - \\bar{x}_2)^2 + \\dots + (x_{ip} - \\bar{x}_p)^2 \\\\ &amp;= (\\mathbf{\\vec{x}_i} - \\mathbf{\\vec{g}})^\\mathsf{T} (\\mathbf{\\vec{x}_i} - \\mathbf{\\vec{g}}) \\end{align*}\\] 3.3.5 Measures of Dispersion What else can we calculate with the individuals? Think about it. So far we’ve seen how to calculate the average individual, as well as distances between individuals. The average individual or centroid plays the role of a measure of center. And everytime you get a measure of center, it makes sense to get a measure of spread. Overall Dispersion One way to compute a measure of scatter among individuals is to consider all the squared distances between pairs of individuals. For instance, say you have three individuals \\(a\\), \\(b\\), and \\(c\\). We can calculate all pairwise distances and add them up: \\[ d^2(a,b) + d^2(b,a) + d^2(a,c) + d^2(c,a) + d^2(b,c) + d^2(c,b) \\] In general, when you have \\(n\\) individuals, you can obtain up to \\(n^2\\) squared distances. We will give the generic name of Overall Dispersion to the sum of all squared pairwise distances: \\[ \\text{overall dispersion} = \\sum_{i=1}^{n} \\sum_{\\ell=1}^{n} d^2(i,\\ell) \\] Inertia Another measure of scatter among individuals can be computed by adding the distances between all individuals and the centroid. Figure 3.10: Inertia The sum of squared distances from each point to the centroid then becomes \\[ \\frac{1}{n} \\sum_{i=1}^{n} d^2(i,g) = \\frac{1}{n} \\sum_{i=1}^{n} (\\mathbf{\\vec{x}_i} - \\mathbf{\\vec{g}})^\\mathsf{T} (\\mathbf{\\vec{x}_i} - \\mathbf{\\vec{g}}) \\] We will name this measure Inertia, borrowing this term from the concept of inertia used in mechanics (in physics). \\[ \\text{Inertia} = \\frac{1}{n} \\sum_{i=1}^{n} d^2(i,g) \\] What is the motivation behind this measure? Consider the \\(p = 1\\) case; i.e. when \\(\\mathbf{X}\\) is simply a column vector \\[ \\mathbf{X} = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\\\ \\end{pmatrix} \\] The centroid will simply be the mean of these points: i.e. \\(\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\\). The sum of squared-distances from each point to the centroid then becomes: \\[ (x_1 - \\bar{x})^2 + (x_2 - \\bar{x})^2 + \\dots + (x_n - \\bar{x})^2 = \\sum_{i=1}^{n} (x_i - \\bar{x})^2 \\] Does the above formula look familiar? What if we take the average of the squared distances: \\[ \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})^2 = \\frac{(x_1 - \\bar{x})^2 + (x_2 - \\bar{x})^2 + \\dots + (x_n - \\bar{x})^2}{n} \\] Same question: Do you recognize this formula? You better do… This is nothing else than the formula of the variance of \\(X\\). And yes, we are dividing by \\(n\\) (not by \\(n-1\\)). Hence, you can think of inertia as a multidimensional extension of variance, which gives the typical squared distance around the centroid. Overall Dispersion and Inertia Interestingly, the overall dispersion and the inertia are connected through the following relation: \\[\\begin{align*} \\text{overall dispersion} &amp;= \\sum_{i=1}^{n} \\sum_{\\ell=1}^{n} d^2(i,\\ell) \\\\ &amp;= 2n \\sum_{i=1}^{n} d^2(i,g) \\\\ &amp;= (2n^2) \\text{Inertia} \\end{align*}\\] The proof of this relation is left as a homework exercise. 3.4 Cloud of Variables The starting point when analyzing variables involves computing various summary measures—such as means, and variances—to get an idea of the common or central values, and the amount of variability of each variable. In this chapter we will review how concepts like the mean of a variable, the variance, covariance, and correlation, can be interpreted in a geometric sense, as well as their expressions in terms of vector-matrix operations. 3.4.1 Mean of a Variable To measure variation, we usually begin by calculating a “typical” value. The idea is to summarize the values of a variable with one or two representative values. You will find this notion under several terms like measures of center, location, central tendency, or centrality. The prototypical summary value of center is the mean, sometimes referred to as average. The mean of an \\(n-\\)element variable \\(X = (x_1, x_2, \\dots, x_n)\\), represented by \\(\\bar{x}\\), is obtained by adding all the \\(x_i\\) values and then dividing by their total number \\(n\\): \\[ \\bar{x} = \\frac{x_1 + x_2 + \\dots + x_n}{n} \\] Using summation notation we can express \\(\\bar{x}\\) in a very compact way as: \\[ \\bar{x} = \\frac{1}{n} \\sum_{i = 1}^{n} x_i \\] If you associate a constant weight of \\(1/n\\) to each observation \\(x_i\\), you can look at the formula of the mean as a weighted sum: \\[ \\bar{x} = \\frac{1}{n} x_1 + \\frac{1}{n} x_2 + \\dots + \\frac{1}{n} x_n \\] This is a slightly different way of looking at the mean that will allow you to generalize the concept of an “average” as a weighted aggregation of information. For example, if we denote the weight of the \\(i\\)-th individual as \\(w_i\\), then the average can be expressed as: \\[ \\bar{x} = w_1 x_1 + w_2 x_2 + \\dots + w_n x_n = \\sum_{i=1}^{n} w_i x_i = \\mathbf{w^\\mathsf{T} x} \\] 3.4.2 Variance of a Variable A measure of center such as the mean is not enoguh to summarize the information of a variable. We also need a measure of the amount of variability. Synonym terms are variation, spread, scatter, and dispersion. Because of its relevance and importance for statistical learning methods, we will focus on one particular measure of spread: the variance (and its square root the standard deviation). Simply put, the variance is a measure of spread around the mean. The main idea behind the calculation of the variance is to quantify the typical concentration of values around the mean. The way this is done is by averaging the squared deviations from the mean. \\[ var(X) = \\frac{(x_1 - \\bar{x})^2 + \\dots + (x_n - \\bar{x})^2}{n} = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})^2 \\] Let’s disect the terms and operations involved in the formula of the variance. the main terms are the deviations from the mean \\((x_i - \\bar{x})\\), that is, the difference between each observation \\(x_i\\) and the mean \\(\\bar{x}\\). conceptually speaking, we want to know what is the average size of the deviations around the mean. simply averaging the deviations won’t work because their sum is zero (i.e. the sum of deviations around the mean will cancel out because the mean is the balancing point). this is why we square each deviation: \\((x_i - \\bar{x})^2\\), which literally means getting the squared distance from \\(x_i\\) to \\(\\bar{x}\\). having squared all the deviations, then we average them to get the variance. Because the variance has squared units, we need to take the square root to “recover” the original units in which \\(X\\) is expressed. This gives us the standard deviation \\[ sd(X) = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})^2} \\] In this sense, you can say that the standard deviation is roughly the average distance that the data points vary from the mean. Sample Variance In practice, you will often find two versions of the formula for the variance: one in which the sum of squared deviations is divided by \\(n\\), and another one in which the division is done by \\(n-1\\). Each version is associated to the statistical inference view of variance in terms of whether the data comes from the population or from a sample of the population. The population variance is obtained dividing by \\(n\\): \\[ \\textsf{population variance:} \\quad \\frac{1}{(n)} \\sum_{i=1}^{n} (x_i - \\bar{x})^2 \\] The sample variance is obtained dividing by \\(n - 1\\) instead of dividing by \\(n\\). The reason for doing this is to get an unbiased estimor of the population variance: \\[ \\textsf{sample variance:} \\quad \\frac{1}{(n-1)} \\sum_{i=1}^{n} (x_i - \\bar{x})^2 \\] It is important to note that most statistical software compute the variance with the unbiased version. If you implement your own functions and are planning to compare them against other software, then it is crucial to known what other programmers are using for computing the variance. Otherwise, your results might be a bit different from the ones with other people’s code. In this book, unless indicated otherwise, we will use the factor \\(\\frac{1}{n}\\) when introducing concepts of variance, and related measures. If needed, we will let you know when a formula needs to use the factor \\(\\frac{1}{n-1}\\). 3.4.3 Variance with Vector Notation In a similar way to expressing the mean with vector notation, you can also formulate the variance in terms of vector-matrix notation. First, notice that the formula of the variance consists of the addition of squared terms. Second, recall that a sum of numbers can be expressed with an inner product by using the unit vector (or summation operator). If we denote a vector of ones of size \\(n\\) as \\(\\mathbf{1}_{n}\\), then the variance of a vector \\(\\mathbf{x}\\) can be obtained with the following inner product: \\[ var(\\mathbf{x}) = \\frac{1}{n} (\\mathbf{x} - \\mathbf{\\bar{x}})^\\mathsf{T} (\\mathbf{x} - \\mathbf{\\bar{x}}) \\] where \\(\\mathbf{\\bar{x}}\\) is an \\(n\\)-element vector of mean values \\(\\bar{x}\\). Assuming that \\(\\mathbf{x}\\) is already mean-centered, then the variance is proportional to the squared norm of \\(\\mathbf{x}\\) \\[ var(\\mathbf{x}) = \\frac{1}{n} \\hspace{1mm} \\mathbf{x}^{\\mathsf{T}} \\mathbf{x} = \\frac{1}{n} \\| \\mathbf{x} \\|^2 \\] This means that we can formulate the variance with the general notion of an inner product: \\[ var(\\mathbf{x}) = \\frac{1}{n} \\langle \\mathbf{x}, \\mathbf{x} \\rangle \\] 3.4.4 Standard Deviation as a Norm If we use a metric matrix \\(\\mathbf{D} = diag(1/n)\\) then we have that the variance is given by a special type of inner product: \\[ var(\\mathbf{x}) = \\langle \\mathbf{x}, \\mathbf{x} \\rangle_{D} = \\mathbf{x}^{\\mathsf{T}} \\mathbf{D x} \\] From this point of view, we can say that the variance of \\(\\mathbf{x}\\) is equivalent to its squared norm when the vector space is endowed with a metric \\(\\mathbf{D}\\). Consequently, the standard deviation is simply the length of \\(\\mathbf{x}\\) in this particular geometric space. \\[ sd(\\mathbf{x}) = \\| \\mathbf{x} \\|_{D} \\] When looking at the standard deviation from this perspective, you can actually say that the amount of spread of a vector \\(\\mathbf{x}\\) is actually its length (under the metric \\(\\mathbf{D}\\)). 3.4.5 Covariance The covariance generalizes the concept of variance for two variables. Recall that the formula for the covariance between \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) is: \\[ cov(\\mathbf{x, y}) = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x}) (y_i - \\bar{y}) \\] where \\(\\bar{x}\\) is the mean value of \\(\\mathbf{x}\\) obtained as: \\[ \\bar{x} = \\frac{1}{n} (x_1 + x_2 + \\dots + x_n) = \\frac{1}{n} \\sum_{i = 1}^{n} x_i \\] and \\(\\bar{y}\\) is the mean value of \\(\\mathbf{y}\\): \\[ \\bar{y} = \\frac{1}{n} (y_1 + y_2 + \\dots + y_n) = \\frac{1}{n} \\sum_{i = 1}^{n} y_i \\] Basically, the covariance is a statistical summary that is used to assess the linear association between pairs of variables. Assuming that the variables are mean-centered, we can get a more compact expression of the covariance in vector notation: \\[ cov(\\mathbf{x, y}) = \\frac{1}{n} (\\mathbf{x^{\\mathsf{T}} y}) \\] Properties of covariance: the covariance is a symmetric index: \\(cov(X,Y) = cov(Y,X)\\) the covariance can take any real value (negative, null, positive) the covariance is linked to variances under the name of the Cauchy-Schwarz inequality: \\[cov(X,Y)^2 \\leq var(X) var(Y) \\] 3.4.6 Correlation Although the covariance indicates the direction—positive or negative—of a possible linear relation, it does not tell us how big or small the relation might be. To have a more interpretable index, we must transform the convariance into a unit-free measure. To do this we must consider the standard deviations of the variables so we can normalize the covariance. The result of this normalization is the coefficient of linear correlation defined as: \\[ cor(X, Y) = \\frac{cov(X, Y)}{\\sqrt{var(X)} \\sqrt{var(Y)}} \\] Representing \\(X\\) and \\(Y\\) as vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\), we can express the correlation as: \\[ cor(\\mathbf{x}, \\mathbf{y}) = \\frac{cov(\\mathbf{x}, \\mathbf{y})}{\\sqrt{var(\\mathbf{x})} \\sqrt{var(\\mathbf{y})}} \\] Assuming that \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are mean-centered, we can express the correlation as: \\[ cor(\\mathbf{x, y}) = \\frac{\\mathbf{x^{\\mathsf{T}} y}}{\\|\\mathbf{x}\\| \\hspace{1mm} \\|\\mathbf{y}\\|} \\] As it turns out, the norm of a mean-centered variable \\(\\mathbf{x}\\) is proportional to the square root of its variance (or standard deviation): \\[ \\| \\mathbf{x} \\| = \\sqrt{\\mathbf{x^{\\mathsf{T}} x}} = \\sqrt{n} \\sqrt{var(\\mathbf{x})} \\] Consequently, we can also express the correlation with inner products as: \\[ cor(\\mathbf{x, y}) = \\frac{\\mathbf{x^{\\mathsf{T}} y}}{\\sqrt{(\\mathbf{x^{\\mathsf{T}} x})} \\sqrt{(\\mathbf{y^{\\mathsf{T}} y})}} \\] or equivalently: \\[ cor(\\mathbf{x, y}) = \\frac{\\mathbf{x^{\\mathsf{T}} y}}{\\| \\mathbf{x} \\| \\hspace{1mm} \\| \\mathbf{y} \\|} \\] In the case that both \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are standardized (mean zero and unit variance), that is: \\[ \\mathbf{x} = \\begin{bmatrix} \\frac{x_1 - \\bar{x}}{\\sigma_{x}} \\\\ \\frac{x_2 - \\bar{x}}{\\sigma_{x}} \\\\ \\vdots \\\\ \\frac{x_n - \\bar{x}}{\\sigma_{x}} \\end{bmatrix}, \\hspace{5mm} \\mathbf{y} = \\begin{bmatrix} \\frac{y_1 - \\bar{y}}{\\sigma_{y}} \\\\ \\frac{y_2 - \\bar{y}}{\\sigma_{y}} \\\\ \\vdots \\\\ \\frac{y_n - \\bar{y}}{\\sigma_{y}} \\end{bmatrix} \\] the correlation is simply the inner product: \\[ cor(\\mathbf{x, y}) = \\mathbf{x^{\\mathsf{T}} y} \\hspace{5mm} \\textsf{(standardized variables)} \\] 3.4.7 Geometry of Correlation Let’s look at two variables (i.e. vectors) from a geometric perspective. Figure 3.11: Two vectors in a 2-dimensional space The inner product ot two mean-centered vectors \\(\\langle \\mathbf{x}, \\mathbf{y} \\rangle\\) is obtained with the following equation: \\[ \\mathbf{x^{\\mathsf{T}} y} = \\|\\mathbf{x}\\| \\hspace{1mm} \\|\\mathbf{y}\\| \\hspace{1mm} cos(\\theta_{x,y}) \\] where \\(cos(\\theta_{x,y})\\) is the angle between \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\). Rearranging the terms in the previous equation we get that: \\[ cos(\\theta_{x,y}) = \\frac{\\mathbf{x^\\mathsf{T} y}}{\\|\\mathbf{x}\\| \\hspace{1mm} \\|\\mathbf{y}\\|} = cor(\\mathbf{x, y}) \\] which means that the correlation between mean-centered vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) turns out to be the cosine of the angle between \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\). 3.4.8 Orthogonal Projections Last but not least, we finish this chapter with a discussion of projections. To be more specific, the statistical interpretation of orthogonal projections. Let’s motivate this discussion with the following question: Consider two variables \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\). Can we approximate one of the variables in terms of the other? This is an asymmetric type of association since we seek to say something about the variability of one variable, say \\(\\mathbf{y}\\), in terms of the variability of \\(\\mathbf{x}\\). Figure 3.12: Two vectors in n-dimensional space We can think of several ways to approximate \\(\\mathbf{y}\\) in terms of \\(\\mathbf{x}\\). The approximation of \\(\\mathbf{y}\\), denoted by \\(\\mathbf{\\hat{y}}\\), means finding a scalar \\(b\\) such that: \\[ \\mathbf{\\hat{y}} = b \\mathbf{x} \\] The common approach to get \\(\\mathbf{\\hat{y}}\\) in some optimal way is by minimizing the square difference between \\(\\mathbf{y}\\) and \\(\\mathbf{\\hat{y}}\\). Figure 3.13: Orthogonal projection of y onto x The answer to this question comes in the form of a projection. More precisely, we orthogonally project \\(\\mathbf{y}\\) onto \\(\\mathbf{x}\\): \\[ \\mathbf{\\hat{y}} = \\mathbf{x} \\left( \\frac{\\mathbf{y^\\mathsf{T} x}}{\\mathbf{x^\\mathsf{T} x}} \\right) \\] or equivalently: \\[ \\mathbf{\\hat{y}} = \\mathbf{x} \\left( \\frac{\\mathbf{y^\\mathsf{T} x}}{\\| \\mathbf{x} \\|^2} \\right) \\] For convenience purposes, we can rewrite the above equation in a slightly different format: \\[ \\mathbf{\\hat{y}} = \\mathbf{x} (\\mathbf{x^\\mathsf{T}x})^{-1} \\mathbf{x^\\mathsf{T}y} \\] If you are familiar with linear regression, you should be able to recognize this equation. We’ll come back to this when we get to the chapter about Linear regression. 3.4.9 The mean as an orthogonal projection Let’s go back to the concept of mean of a variable. As we previously mention, a variable \\(X = (x_1, \\dots, x_n)\\), can be thought of a vector \\(\\mathbf{x}\\) in an \\(n\\)-dimensional space. Furthermore, let’s also consider the constant vector \\(\\mathbf{1}\\) of size \\(n\\). Here’s a conceptual diagram for this situation: Figure 3.14: Two vectors in n-dimensional space Out of curiosity, what happens when we ask about the orthogonal projection of \\(\\mathbf{x}\\) onto \\(\\mathbf{1}\\)? Something like in the following picture: Figure 3.15: Orthogonal projection of vector x onto constant vector 1 This projection is expressed in vector notation as: \\[ \\mathbf{\\hat{x}} = \\mathbf{1} \\left( \\frac{\\mathbf{x^\\mathsf{T} 1}}{\\mathbf{1^\\mathsf{T} 1}} \\right) \\] or equivalently: \\[ \\mathbf{\\hat{x}} = \\mathbf{1} \\left( \\frac{\\mathbf{x^\\mathsf{T} 1}}{\\| \\mathbf{1} \\|^2} \\right) \\] Note that the term in parenthesis is just a scalar, so we can actually express \\(\\mathbf{\\hat{x}}\\) as \\(b \\mathbf{1}\\). This means that a projection implies multiplying \\(\\mathbf{1}\\) by some number \\(b\\), such that \\(\\mathbf{\\hat{x}} = b \\mathbf{1}\\) is a stretched or shrinked version of \\(\\mathbf{1}\\). So, what is the scalar \\(b\\)? It is simply the mean of \\(\\mathbf{x}\\): \\[ \\mathbf{\\hat{x}} = \\mathbf{1} \\left( \\frac{\\mathbf{x^\\mathsf{T} 1}}{\\| \\mathbf{1} \\|^2} \\right) = \\bar{x} \\mathbf{1} \\] This is better appreciated in the following figure. Figure 3.16: Mean of x and its relation with the projection onto constant vector 1 What this tells us is that the mean of the variable \\(X\\), denoted by \\(\\bar{x}\\), has a very interesting geometric interpretation. As you can tell, \\(\\bar{x}\\) is the scalar by which you would multiply \\(\\mathbf{1}\\) in order to obtain the vector projection \\(\\mathbf{\\hat{x}}\\). "],
["pca.html", "4 Principal Components Analysis 4.1 Low-dimensional Representations 4.2 Projections 4.3 Maximization Problem 4.4 Another Perspective of PCA 4.5 Data Decomposition Model", " 4 Principal Components Analysis Our first unsupervised method of the book is Principal Components Analysis, commonly referred to as PCA. Principal Components Analysis (PCA) is the workhorse method of multivariate data analysis. Simply put, PCA helps us study and explore a data set of quantitative variables measured on a set of objects. One way to look at the purpose of principal components analysis is to get the best low-dimensional representation of the variation in data. Among the various appealing features of PCA is that it allows us to obtain a visualization of the objects in order to see their proximities. Likewise, it also provides us results to get a graphic representation of the variables in terms of their correlations. Overall, PCA is a multivariate technique that allows us to summarize the systematic patterns of variations in a data set. The classic reference for PCA is the work by the eminent British biostatistician Karl Pearson “On Lines and Planes of Closest Fit to Systems of Points in Space,” from 1901. This publication presents the PCA problem under a purely geometric standpoint, describing how to find low-dimensional subspaces that best fit—in the least squares sense—a cloud of points. The other seminal work of PCA is the one by the American mathematician and economic theorist Harold Hotelling with “Analysis of a Complex of Statistical Variables into Principal Components,” from 1933. Unlike Pearson, Hotelling finds the principal components as orthogonal linear combinations of the variables of maximum variance. PCA is one of those methods that can be approached from multiple, seemingly unrelated, perspectives. The way we are going to introduce PCA is not the typical way in which PCA is discussed in most books published in English. However, our introduction is actually based on the ideas and concepts originally published in Karl Pearson’s 1901 paper On lines and planes of closest fit to systems of points in space. This is what can be considered to be the first paper on PCA, although keep in mind that Karl Pearson never used the term principal components analysis. That term was coined by Harold Hotelling, who formalized the method by giving it a more mature statistical perspective. 4.1 Low-dimensional Representations Let’s play the following game. Imagine for a minute that you have the superpower to see any type of multidimensional space (not just three-dimensions). As we mentioned before, we think of the individuals as forming a cloud of points in a \\(p\\)-dim space, and the variables forming a cloud of arrows in an \\(n\\)-dim space. Pretend that you have some data in which its cloud of points has the shape of a mug, like in the following diagram: Figure 4.1: Cloud of points in the form of a mug This mug is supposed to be high-dimensional, and something that you are not supposed to ever see in real life. So the question is: Is there a way in which we can get a low-dimensional representation of this data? Luckily, the answer is: YES, we can! How? Well, the name of the game is projections: we can look for projections of the data into sub-spaces of lower dimension, like in the diagram below. Figure 4.2: Various projections onto subspaces Think of projections as taking photographs or x-rays of the mug. You can take a photo of the mug from different angles. For instance, you can take a picture in which the lens of the camera is on the top of the mug, or another picture in which the lens is below the mug (from the bottom), and so on. If you have to take the “best” photograph of the mug, from what angle would you take such picture? To answer this question we need to be more precise about what do we mean by “best”. Here, we are talking about getting a picture in which the image of the mug is as similar as possible to the original object. As you can tell from the above figure, we have three candidate subspaces: \\(\\mathbb{H}_A\\), \\(\\mathbb{H}_B\\), and \\(\\mathbb{H}_C\\). Among the three possible projections, subspace \\(\\mathbb{H}_C\\) is the one that provides the best low dimensional representation, in the sense that the projected silhouette is the most similar to the original mug shape. Figure 4.3: The shape of the projection is similar to the original mug shape. We can say that the “photo” from projecting onto subspace \\(\\mathbb{H}_C\\) is the one that most resembles the original object. Now, keep in mind that the resulting image in the low-dimensional space is not capturing the whole pattern. In other words, there is some loss of information. However, by chosing the right projection, we hope to minimize such loss. 4.2 Projections Following the idea of projections, let’s now discuss with more detail this concept, and its implications. Pretend that we zoom in to see some of the individuals of the cloud of points that form the mug (see figure below). Keep in mind that these data points are in a \\(p\\)-dimensional space, which will also have its centroid (i.e. average individual). Figure 4.4: Set of individuals in p-dim space. Because our goal is to look for a low-dimensional represention, we can start by considering a one-dimensional space, that is, some axis. In the above diagram (as well as the one below) this dimension is depicted with the yellow line, labeled as \\(dim_{\\mathbf{v}}\\). Figure 4.5: Set of individuals in p-dim space. We should note that we don’t really manipulate \\(dim_{\\mathbf{v}}\\) directly. Instead, what we manipulate is a vector \\(\\mathbf{v}\\) along this dimension. Figure 4.6: Dimension spanned by vector v At the end of the day, we want to project the individuals onto this dimension. In particular, the type of projections that we are interested in are orthogonal projections. Figure 4.7: Projections onto one dimension 4.2.1 Vector and Scalar Projections Let’s consider a specific individual, for example the \\(i\\)-th individual. And let’s take the centroid as the origin of the cloud of points. In this way, the dimension that we are looking for has to pass thorugh the centroid of this cloud. Obtaining the orthogonal projection of the \\(i\\)-th individual onto the dimension \\(dim_{\\mathbf{v}}\\) involves projecting \\(\\mathbf{x_i}\\) onto \\(\\mathbf{v}\\). Figure 4.8: Projection of an individual onto one dimension Recall that an orthogonal projection can be split into two types of projections: (1) the vector projection, and (2) the scalar projection. The vector projection of \\(\\mathbf{x_i}\\) onto the \\(\\mathbf{v}\\) is defined as: \\[ proj_{\\mathbf{v}} (\\mathbf{x_i}) = \\frac{\\mathbf{x_{i}^\\mathsf{T} v}}{\\mathbf{v^\\mathsf{T}v}} \\hspace{1mm} \\mathbf{v} = \\mathbf{\\hat{v}} \\] The scalar projection of \\(\\mathbf{x_i}\\) onto \\(\\mathbf{v}\\) is defined as: \\[ comp_{\\mathbf{v}} (\\mathbf{x_i}) = \\frac{\\mathbf{x_{i}^\\mathsf{T}v}}{\\|\\mathbf{v}\\|} = z_{ik} \\] The following diagram displays both types of projections: Figure 4.9: Scalar and vector projections of i-th individual onto vector v We are not really interested in obtaining the vector projection \\(proj_{\\mathbf{v}} (\\mathbf{x_i})\\). Instead, what we care about is the scalar projection \\(comp_{\\mathbf{v}} (\\mathbf{x_i})\\). In other words, we just want to obtain the coordinate of the \\(i\\)-th individual along this axis. 4.2.2 Projected Inertia As we said before, our goal is to find the angle that gives us the “best” photo of the mug (i.e. cloud of points). This can be translated as finding a subspace in which the distances between the points is most similar to those on the actual mug. Figure 4.10: Projection Goal Now, we need to define what we mean by “similar” distances in \\(\\mathbb{H}\\). Consider the overall dispersion of our original system (the one with all \\(p\\) dimensions): \\(\\sum_{j} \\sum_{\\ell} d^2 (i, \\ell)\\). This is a fixed number; and we cannot change it. However, we can try fine-tune our subset \\(\\mathbb{H}\\) such that \\(\\sum_{j} \\sum_{\\ell} d^2 (i, \\ell) \\approx \\sum_{i} \\sum_{\\ell} d_H^2(i, \\ell)\\). \\[ \\sum_{j} \\sum_{\\ell} d^2 (i, \\ell) = \\ 2n \\sum_i d^2 (i, g) \\] However, it will be easier to simply consider the inertia of the system (as opposed to the overall dispersion). \\[ 2n \\sum_i d^2 (i, g) \\ \\to \\ \\frac{1}{n} \\sum_{i=1}^{n} d^2 (i, g) \\] In mathematical terms, finding the subspace \\(\\mathbb{H}\\) that gives us “similar” distances to those of the original space corresponds to maximizing the projected inertia: \\[ \\max_{\\mathbb{H}} \\left\\{ \\frac{1}{n} \\sum_{i=1}^{n} d_{\\mathbb{H}}^2 (i, g) \\right\\} \\] Now, as we discuss before, the simplest subspace will be one dimension, that is, we consider the case where \\(\\mathbb{H} \\subseteq \\mathbb{R}^1\\). So the first approach is to project our data points onto a vector \\(\\mathbf{v}\\). Hence, the projected inertia becomes: \\[ \\frac{1}{n} \\sum_{i=1}^{n} d_\\mathbb{H}^2 (i, g) = \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\mathbf{x_i}^\\mathsf{T} \\mathbf{v} \\right)^2 = \\frac{1}{n} \\sum_{i=1}^{n} z_i^2 \\] and our maximization problem becomes \\[ \\max_{\\mathbf{v}} \\left\\{ \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\mathbf{x_i}^\\mathsf{T} \\mathbf{v} \\right)^2 \\right\\} \\quad \\mathrm{s.t.} \\quad \\mathbf{v}^\\mathsf{T} \\mathbf{v} = 1 \\] For convenience, we add the restriction that \\(\\mathbf{v}\\) is a unit-vector (vector of unit norm). Why do we need the constraint? If we didn’t have the constraint, we could always find a \\(\\mathbf{v}\\) that obtains higher inertia, and things would explode. Furthermore, as we will see in the next sectoin, this helps in the algebra we will use when we actually perform the maximization. 4.3 Maximization Problem We now need to compute the projected Inertia. Since we have projected onto a line spanned by \\(\\mathbf{v}\\), the projected inertia \\(I_{\\mathbb{H}}\\) will simply be the variance of the projected data points: \\[ I_{\\mathbb{H}} = \\frac{1}{n} \\sum_{i=1}^{n} d_\\mathbb{H}^2(i, 0) = \\frac{1}{n} \\sum_{i=1}^{n} z_i^2 \\] (note that \\(g = 0\\) since we are assuming our data is mean-centered). We can simplify notation as follows: \\[ \\mathbf{z} := \\begin{pmatrix} z_1 \\\\ z_2 \\\\ \\vdots \\\\ z_n \\\\ \\end{pmatrix} \\ \\Longrightarrow \\ I_\\mathbb{H} = \\frac{1}{n} \\mathbf{z}^\\mathsf{T} \\mathbf{z} = \\frac{1}{n} \\mathbf{v}^\\mathsf{T} \\mathbf{X}^\\mathsf{T} \\mathbf{X} \\mathbf{v} \\] Hence, the maximization problem becomes \\[ \\max_{\\mathbf{v}} \\left\\{ \\frac{1}{n} \\mathbf{v}^\\mathsf{T} \\mathbf{X}^\\mathsf{T} \\mathbf{X} \\mathbf{v} \\right\\} \\qquad \\text{s.t.} \\qquad \\mathbf{v}^\\mathsf{T} \\mathbf{v} = 1 \\] To solve this maximization problem, we utilize the method of Lagrange Multipliers: \\[ \\mathcal{L} = \\frac{1}{n} \\mathbf{v}^\\mathsf{T} \\mathbf{X}^\\mathsf{T} \\mathbf{X} \\mathbf{v} - \\lambda \\left( \\mathbf{v}^\\mathsf{T} \\mathbf{v} - 1 \\right) \\] Calculating the derivative of \\(mathcal{L}\\) with respect to \\(mathbf{v}\\): \\[ \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{v}} = \\frac{2}{n} \\mathbf{X}^\\mathsf{T} \\mathbf{X} \\mathbf{v} - 2 \\lambda \\mathbf{v} = 0 \\ \\Rightarrow \\ \\underbrace{ \\frac{1}{n} \\mathbf{X}^\\mathsf{T} \\mathbf{X} \\mathbf{v} }_{:= \\mathbf{S} } = \\lambda \\mathbf{v} \\ \\Rightarrow \\ \\mathbf{S} \\mathbf{v} = \\lambda \\mathbf{v} \\] That is, we obtain the equation \\(\\mathbf{S} \\mathbf{v} = \\lambda \\mathbf{v}\\) where \\(\\mathbf{S} \\in \\mathbb{R}^{p \\times p}\\) is symmetric. This means that \\(\\mathbf{v}\\) is an eigenvector (with eigenvalue \\(\\lambda\\)) of \\(\\mathbf{S}\\). 4.3.1 Eigenvectors of \\(\\mathbf{S}\\) Assume that \\(\\mathbf{X}\\) has full rank (i.e. \\(\\mathrm{rank}(\\mathbf{X}) = p\\)). We then obtain \\(p\\) eigenvectors, which together form the matrix \\(\\mathbf{V}\\): \\[ \\mathbf{V} = \\begin{bmatrix} \\mathbf{v_1} &amp; \\mathbf{v_2} &amp; \\dots &amp; \\mathbf{v_k} &amp; \\dots &amp; \\mathbf{v_p} \\\\ \\end{bmatrix} \\] We can also obtain \\(\\mathbf{\\Lambda}_{p \\times p} := \\mathrm{diag}\\left\\{ \\lambda_i \\right\\}_{i=1}^{n}\\) (i.e. the matrix of eigenvalues): that is, \\[ \\mathbf{\\Lambda} = \\begin{bmatrix} \\lambda_1 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; \\lambda_2 &amp; \\dots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; \\lambda_p \\\\ \\end{bmatrix} \\] Finally, we can also obtain the matrix \\(\\mathbf{Z}_{n \\times p}\\) which is the matrix consisting of the vectors \\(\\mathbf{z_i}\\): \\[ \\mathbf{Z} = \\begin{bmatrix} \\mathbf{z_1} &amp; \\mathbf{z_2} &amp; \\dots &amp; \\mathbf{z_k} &amp; \\dots &amp; \\mathbf{z_p} \\\\ \\end{bmatrix} \\] The matrix \\(\\mathbf{V}\\) consists of the loadings The matrix \\(\\mathbf{Z}\\) contains the principal components (PC’s), also known as scores. Let us examine the \\(k\\)th principal component \\(\\mathbf{z_k}\\): \\[ \\mathbf{z_k} = \\mathbf{X v_k} = v_{k1} \\mathbf{x_1} + v_{k2} \\mathbf{x_2} + \\dots + v_{kp} \\mathbf{x_p} \\] (where \\(\\mathbf{x_k}\\) denotes columns of \\(\\mathbf{X}\\)). Note that \\(\\mathrm{mean}(\\mathbf{z_k}) = 0\\); since we are assuming that the data is mean-centered, we have that \\(\\mathrm{mean}(\\mathbf{x_1}) = 0\\). What about variance? \\[\\begin{align*} Var(\\mathbf{z_k}) &amp; = \\frac{1}{n} \\mathbf{z_{k}^\\mathsf{T}} \\mathbf{z_k} \\\\ &amp; = \\frac{1}{n}\\left( \\mathbf{X} \\mathbf{v_k} \\right)^{\\mathsf{T}} \\left( \\mathbf{X} \\mathbf{v_k} \\right) \\\\ &amp; = \\frac{1}{n} \\mathbf{v_{k}^\\mathsf{T}} \\mathbf{X}^\\mathsf{T} \\mathbf{X} \\mathbf{v_k} \\\\ &amp; = \\mathbf{v_k}^\\mathsf{T} \\mathbf{S} \\mathbf{v_k} \\\\ &amp; = \\mathbf{v_k}^\\mathsf{T} \\left( \\lambda_k \\mathbf{v_k} \\right) = \\lambda_k \\left( \\mathbf{v_k}^\\mathsf{T} \\mathbf{v_k} \\right) \\\\ &amp; = \\lambda_k \\end{align*}\\] Hence, we obtain the following interesting result: the \\(k\\)-th eigenvalue of \\(\\mathbf{S}\\) is simply the variance of the \\(k\\)-th principal component. If \\(\\mathbf{X}\\) is mean centered, then \\(\\mathbf{S} = \\frac{1}{n} \\mathbf{X^\\mathsf{T} X}\\) is nothing but the variance-covariance matrix of our data. If \\(\\mathbf{X}\\) is standardized (i.e. mean-centered and scaled by the variance), then \\(\\mathbf{S}\\) becomes the correlation matrix. In summary: \\(\\mathrm{mean}(\\mathbf{z_k}) = 0\\) \\(Var(\\mathbf{z_k}) = \\lambda_k\\) \\(\\mathrm{sd}(\\mathbf{z_k}) = \\sqrt{\\lambda_k}\\) We have the following fact (the proof is omitted, and may be assigned as homework or as a test question): \\[ I = \\frac{1}{n} \\sum_{i=1}^{n} d^2(i, g) = \\sum_{k} \\lambda_k = \\mathrm{tr}\\left( \\frac{1}{n} \\mathbf{X^\\mathsf{T} X} \\right) \\] The dimensions that we find in our analysis (through \\(\\mathbf{v_k}\\)) relates directly to \\(\\mathbf{z_k}\\). \\(\\sum_{k=1}^{p} \\lambda_k\\) relates directly to the total amount of variability of our data. Remark: The principal components capture different parts of the variability of the full data. 4.4 Another Perspective of PCA The overall idea behind PCA is the following. Given a set of \\(p\\) variables \\(\\mathbf{x_1}, \\mathbf{x_2}, \\dots, \\mathbf{x_p}\\), we want to obtain new \\(k\\) variables \\(\\mathbf{z_1}, \\mathbf{z_2}, \\dots, \\mathbf{z_k}\\), called the Principal Components (PCs). A principal component is a linear combination: \\(\\mathbf{z} = \\mathbf{Xv}\\). The first PC is a linear mix: Figure 4.11: PCs as linear combinations of X-variables The second PC is another linear mix: Figure 4.12: PCs as linear combinations of X-variables We want to compute the PCs as linear combinations of the original variables. \\[ \\begin{array}{c} \\mathbf{z_1} = v_{11} \\mathbf{x_1} + \\dots + v_{p1} \\mathbf{x_p} \\\\ \\mathbf{z_2} = v_{12} \\mathbf{x_1} + \\dots + v_{p2} \\mathbf{x_p} \\\\ \\vdots \\\\ \\mathbf{z_k} = v_{1k} \\mathbf{x_1} + \\dots + v_{pk} \\mathbf{x_p} \\\\ \\end{array} \\] Or in matrix notation: \\[ \\mathbf{Z} = \\mathbf{X V} \\] where \\(\\mathbf{Z}\\) is an \\(n \\times k\\) matrix of principal components, and \\(\\mathbf{V}\\) is a \\(p \\times k\\) matrix of weights, also known as directional vectors of the principal axes. The following figure shows a graphical representation of a PCA problem in diagram notation: Figure 4.13: PCs as linear combinations of X-variables We look to transform the original variables into a smaller set of new variables, the Principal Components (PCs), that summarize the variation in data. The PCs are obtained as linear combinations (i.e. weighted sums) of the original variables. We look for PCs is in such a way that they have maximum variance, and being mutually orthogonal. 4.4.1 Finding Principal Components The way to find principal components is to construct them as weighted sums of the original variables, looking to optimize some criterion and following some constraints. One way in which we can express the criterion is to require components \\(\\mathbf{z_1}, \\mathbf{z_2}, \\dots, \\mathbf{z_k}\\) that capture most of the variation in the data \\(\\mathbf{X}\\). “Capturing most of the variation,” implies looking for a vector \\(\\mathbf{v_h}\\) such that a component \\(\\mathbf{z_h} = \\mathbf{X v_h}\\) has maximum variance: \\[ \\max_{\\mathbf{v_h}} \\; var(\\mathbf{z_h}) \\quad \\Rightarrow \\quad \\max_{\\mathbf{v_h}} \\; var(\\mathbf{X v_h}) \\] that is \\[ \\max_{\\mathbf{v_h}} \\; \\frac{1}{n} \\mathbf{v_{h}^\\mathsf{T} X^\\mathsf{T} X v_h} \\] As you can tell, this is a maximization problem. Without any constraints, this problem is unbounded, not to mention useless. We could take \\(\\mathbf{v_h}\\) as bigger as we want without being able to reach any maximum. To get a feasible solution we need to impose some kind of restriction. The standard adopted constraint is to require \\(\\mathbf{v_h}\\) to be of unit norm: \\[ \\| \\mathbf{v_h} \\| = 1 \\; \\hspace{1mm} \\Rightarrow \\; \\hspace{1mm} \\mathbf{v_{h}^\\mathsf{T} v_h} = 1 \\] Note that \\((1/n) \\mathbf{X^\\mathsf{T} X}\\) is the variance-covariance matrix. If we denote \\(\\mathbf{S} = (1/n) \\mathbf{X^\\mathsf{T} X}\\) then the criterion to be maximized is: \\[ \\max_{\\mathbf{v_h}} \\; \\mathbf{v_{h}^\\mathsf{T} S v_h} \\] subject to \\(\\mathbf{v_{h}^\\mathsf{T} v_h} = 1\\) To avoid a PC \\(\\mathbf{z_h}\\) from capturing the same variation as other PCs \\(\\mathbf{z_l}\\) (i.e. avoiding redundant information), we also require them to be mutually orthogonal so they are uncorrelated with each other. Formally, we impose the restriction \\(\\mathbf{z_h}\\) to be perpendicular to other components: \\(\\mathbf{z_{h}^\\mathsf{T} z_l} = 0; (h \\neq l)\\). 4.4.2 Finding the first PC In order to get the first principal component \\(\\mathbf{z_1} = \\mathbf{X v_1}\\), we need to find \\(\\mathbf{v_1}\\) such that: \\[ \\max_{\\mathbf{v_1}} \\; \\mathbf{v_{1}^\\mathsf{T} S v_1} \\] subject to \\(\\mathbf{v_{1}^\\mathsf{T} v_1} = 1\\) Being a maximization problem, the typical procedure to find the solution is by using the Lagrangian multiplier method. Using Lagrange multipliers we get: \\[ \\mathbf{v_{1}^\\mathsf{T} S v_1} - \\lambda (\\mathbf{v_{1}^\\mathsf{T} v_1} - 1) = 0 \\] Differentiation with respect to \\(\\mathbf{v_1}\\), and equating to zero gives: \\[ \\mathbf{S v_1} - \\lambda_1 \\mathbf{v_1} = \\mathbf{0} \\] Rearranging some terms we get: \\[ \\mathbf{S v_1} = \\lambda_1 \\mathbf{v_1} \\] What does this mean? It means that \\(\\lambda_1\\) is an eigenvalue of \\(\\mathbf{S}\\), and \\(\\mathbf{v_1}\\) is the corresponding eigenvector. 4.4.3 Finding the second PC In order to find the second principal component \\(\\mathbf{z_2} = \\mathbf{X v_2}\\), we need to find \\(\\mathbf{v_2}\\) such that \\[ \\max_{\\mathbf{v_2}} \\; \\mathbf{v_{2}^\\mathsf{T} S v_2} \\] subject to \\(\\| \\mathbf{v_2} \\| = 1\\) and \\(\\mathbf{z_{1}^\\mathsf{T} z_2} = 0\\). Remember that \\(\\mathbf{z_2}\\) must be uncorrelated to \\(\\mathbf{z_1}\\). Applying the Lagrange multipliers, it can be shown that the desired \\(\\mathbf{v_2}\\) is such that \\[ \\mathbf{S v_2} = \\lambda_2 \\mathbf{v_2} \\] In other words. \\(\\lambda_2\\) is an eigenvalue of \\(\\mathbf{S}\\) and \\(\\mathbf{v_2}\\) is the corresponding eigenvector. 4.4.4 Finding all PCs All PCs can be found simultaneously by diagonalizing \\(\\mathbf{S}\\). Diagonalizing \\(\\mathbf{S}\\) involves expressing it as the product: \\[ \\mathbf{S} = \\mathbf{V \\Lambda V^\\mathsf{T}} \\] where: \\(\\mathbf{D}\\) is a diagonal matrix the elements in the diagonal of \\(\\mathbf{D}\\) are the eigenvalues of \\(\\mathbf{S}\\) the columns of \\(\\mathbf{V}\\) are orthonormal: \\(\\mathbf{V^\\mathsf{T} V= I}\\) the columns of \\(\\mathbf{V}\\) are the eigenvectors of \\(\\mathbf{S}\\) \\(\\mathbf{V^\\mathsf{T}} = \\mathbf{V^{-1}}\\) Diagonalizing a symmetric matrix is nothing more than obtaining its eigenvalue decomposition (a.k.a. spectral decomposition). A \\(p \\times p\\) symmetric matrix \\(\\mathbf{S}\\) has the following properties: \\(\\mathbf{S}\\) has \\(p\\) real eigenvalues (counting multiplicites) the eigenvectors corresponding to different eigenvalues are orthogonal \\(\\mathbf{S}\\) is orthogonally diagonalizable (\\(\\mathbf{S} = \\mathbf{V \\Lambda V^\\mathsf{T}}\\)) the set of eigenvalues of \\(\\mathbf{S}\\) is called the spectrum of \\(\\mathbf{S}\\) In summary: The PCA solution can be obtained with an Eigenvalue Decomposition of the matrix \\(\\mathbf{S} = (1/n) \\mathbf{X^\\mathsf{T}X}\\) 4.5 Data Decomposition Model Formally, PCA involves finding scores and loadings such that the data can be expressed as a product of two matrices: \\[ \\underset{n \\times p}{\\mathbf{X}} = \\underset{n \\times r}{\\mathbf{Z}} \\underset{r \\times p}{\\mathbf{V^\\mathsf{T}}} \\] where \\(\\mathbf{Z}\\) is the matrix of PCs or scores, and \\(\\mathbf{V}\\) is the matrix of loadings. We can obtain as many different eigenvalues as the rank of \\(\\mathbf{S}\\) denoted by \\(r\\). Ideally, we expect \\(r\\) to be smaller than \\(p\\) so we get a convenient data reduction. But usually we will only retain just a few PCs (i.e. \\(k \\ll p\\)) expecting not to lose too much information: \\[ \\underset{n \\times p}{\\mathbf{X}} \\approx \\underset{n \\times k}{\\mathbf{Z}} \\hspace{1mm} \\underset{k \\times p}{\\mathbf{V^\\mathsf{T}}} + \\text{Residual} \\] The previous expression means that just a few PCs will optimally summarize the main structure of the data 4.5.1 Alternative Approaches Finding \\(\\mathbf{z_h} = \\mathbf{X v_h}\\) with maximum variance has another important property that it is not always mentioned in multivariate textbooks but that we find worth mentioning. \\(\\mathbf{z_h}\\) is such that \\[ \\max \\sum_{j = 1}^{p} cor^2(\\mathbf{z_h, x_j}) \\] What this expression implies is that principal components \\(\\mathbf{z_h}\\) are computed to be the best representants in terms of maximizing the sum of squared correlations with the variables \\(\\mathbf{x_1}, \\mathbf{x_2}, \\dots, \\mathbf{x_j}\\). Interestingly, you can think of PCs as predictors of the variables in \\(\\mathbf{X}\\). Under this perspective, we can reverse the relations and see PCA from a regression-like model perspective: \\[ \\mathbf{x_j} = v_{jh} \\mathbf{z_h} + \\mathbf{e_h} \\] Notice that the regression coefficient is the \\(j\\)-th element of the \\(h\\)-th eigenvector. "],
["ols.html", "5 Linear Regression 5.1 Motivation 5.2 The Idea/Intuition of Regression 5.3 The Linear Regression Model 5.4 The Error Measure 5.5 The Least Squares Algorithm 5.6 Geometries of OLS", " 5 Linear Regression Before entering supervised learning territory, we want to discuss the general framework of linear regression. We will introduce this topic from a pure model-fitting point of view. In other words, we will postpone the learning aspect (the prediction aspect) after the chapter of theory of learning. The reason to cover linear regression in this way is for us to have something to work with when we start talking about the theory of supervised learning. 5.1 Motivation Consider, again, the NBA dataset example from previous chapters. Suppose we want to use this data to predict the salary of NBA players, in terms of certain variables like player’s team, player’s height, player’s weight, player’s position, player’s years of professional experience, player’s number of 2pts, player’ number of 3 points, number of blocks, etc. Of course, we need information on the salaries of some current NBA player’s: Player Height Weight Yrs Expr 2 Points 3 Points 1 \\(\\bigcirc\\) \\(\\bigcirc\\) \\(\\bigcirc\\) \\(\\bigcirc\\) \\(\\bigcirc\\) 2 \\(\\bigcirc\\) \\(\\bigcirc\\) \\(\\bigcirc\\) \\(\\bigcirc\\) \\(\\bigcirc\\) 3 \\(\\bigcirc\\) \\(\\bigcirc\\) \\(\\bigcirc\\) \\(\\bigcirc\\) \\(\\bigcirc\\) … … … … … … As usual, we use the symbol \\(\\mathbf{x_i}\\) to denote the vector of measurements of player \\(i\\)’s statistics (e.g. height, weight, etc.); in turn, the salary of the \\(i\\)-th player is represented with \\(y_i\\). Ideally, there exists some function \\(f : \\mathcal{X} \\to \\mathcal{y}\\) (i.e. a function that takes values from \\(\\mathcal{X}\\) space and maps them to a single value \\(y\\)). We will refer to this function the ideal “formula” for salary. Here we are using the word formula in a very lose sense, and not necessarily using the word “formula” in the mathematical sense. We now seek a hypothesized model (which we call \\(\\widehat{f} : \\mathcal{X} \\to y\\)), which we select from some set of candidate functions \\(h_1, h_2, \\dots, h_m\\). Our task is to obtain \\(\\widehat{f}\\) in a way that we can claim that it is a good approximation of the (unknown) function \\(f\\). 5.2 The Idea/Intuition of Regression Consider again the NBA dataset, and again let \\(y_i\\) denote the salary for the \\(i\\)-th player (ignoring inflation). Say we now have a new prospective player from Europe; and we are tasked with predicting their salary. Scenario 1. Suppose we have no information on this new player. How would we compute \\(y_{0}\\) (i.e. the salary of this new player)? One possibility is to guesstimate \\(y_0\\) using the historical average salary \\(\\bar{y}\\) of NBA players. In other words, we would simply calculate: \\(\\hat{y}_0 = \\bar{y}\\). In this case we are using \\(\\bar{y}\\) as the typical score (e.g. a measure of center) as a plausible guestimate for \\(y_0\\). We could also look at the median of the existing salaries, if we are concerned about outliers. Scenario 2. Now, suppose we know that this new player will sign on to the LA Lakers. We could then use \\(\\hat{y}_0 = \\text{avg}(\\text{Laker&#39;s Salaries})\\): that is, the average salary of all Laker’s players. Figure 5.1: Average salary by team Scenario 3. Similarly, if we know this new player’s years of experience (e.g. 6 years), we would look at the average of salaries corresponding to players with the same level of experience. Figure 5.2: Average salary by years of experience What do the three previous scenarios correspond to? In all of these examples, the prediction is basically a conditional mean: \\(\\hat{y}_0 = \\text{ave}(y_i|x_i = x_0)\\). Of course, the previous strategy only makes sense when we have data points \\(x_i\\) that are equal to \\(x_0\\). But what if none of the available \\(x_i\\) values are equal to \\(x_0\\)? We’ll talk about this later. The previous hypothetical scenarios illustrate the core idea of regression: we predict using quantities of the form: \\[ \\mathbb{E}(y_i \\mid x_{i1}^{*} , x_{i2}^{*}, \\dots, x_{ip}^{*}) \\] where \\(x_{ij}^{*}\\) represents the \\(i\\)-th measurement of the \\(j\\)-th variable. The above equation is what we call the regression function; note that the regression function is nothing more than a conditional expectation! 5.3 The Linear Regression Model A regression model: use one or more features \\(X\\) to say something about the reponse \\(Y\\). A linear regression model tells us to combine our features in a linear way in order to approximate the response: \\[ \\hat{Y} = b_0 + b_1 X \\tag{5.1} \\] In pointwise format, that is for a given individual \\(i\\), we have: \\[ \\hat{y}_i = b_0 + b_1 x_i \\tag{5.2} \\] In vector notation: \\[ \\mathbf{\\hat{y}} = b_0 + b_1 \\mathbf{x} \\tag{5.3} \\] To simplify notation, sometimes we prefer to add an auxiliary constant feature in the form of a vector of 1’s with \\(n\\) elements. \\[ \\mathbf{X} = \\ \\begin{bmatrix} 1 &amp; x_{1} \\\\ 1 &amp; x_{2} \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; x_{n} \\\\ \\end{bmatrix}, \\qquad \\mathbf{\\hat{y}} = \\begin{bmatrix} \\hat{y}_{1} \\\\ \\hat{y}_{2} \\\\ \\vdots \\\\ \\hat{y}_{n} \\\\ \\end{bmatrix}, \\qquad \\mathbf{b} = \\begin{bmatrix} b_{0} \\\\ b_{1} \\\\ \\end{bmatrix} \\] In the multidimensional case when we have \\(p&gt;1\\) predictors: \\[ \\mathbf{X} = \\ \\begin{bmatrix} 1 &amp; x_{11} &amp; \\dots &amp; x_{1p} \\\\ 1 &amp; x_{21} &amp; \\dots &amp; x_{2p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; x_{n1} &amp; \\dots &amp; x_{np} \\\\ \\end{bmatrix}, \\qquad \\mathbf{\\hat{y}} = \\begin{bmatrix} \\hat{y}_{1} \\\\ \\hat{y}_{2} \\\\ \\vdots \\\\ \\hat{y}_{n} \\\\ \\end{bmatrix}, \\qquad \\mathbf{b} = \\begin{bmatrix} b_{0} \\\\ b_{1} \\\\ \\vdots \\\\ b_{p} \\\\ \\end{bmatrix} \\] With the matrix of features, the response, and the coefficients we have: \\[ \\mathbf{\\hat{y}} = \\mathbf{Xb} \\] In path diagram form, the linear model looks like this: Figure 5.3: Linear combination with constant term If we assume centered predictors then we don’t have to worry about the constant term: Figure 5.4: Linear combination without constant term Obviously the question becomes: how do we obtain the vector of coefficients \\(\\mathbf{b}\\)? 5.4 The Error Measure We would like to get \\(\\hat{y}_i\\) to be “as close as” possible to \\(y_i\\). This requires to come up with some type of measure of closeness. Among the various functions that we can use to measure how close \\(\\hat{y}_i\\) and \\(y_i\\) are, the most common option is to use the squared distance between such values: \\[ d^2(y_i, \\hat{y}_i) = (y_i - \\hat{y}_i)^2 = (\\hat{y}_i - y_i)^2 \\tag{5.4} \\] Replacing \\(\\hat{y}_i\\) with \\(\\mathbf{b^\\mathsf{T}\\vec{x}_i}\\) we have: \\[ d^2(y_i, \\hat{y}_i) = (\\mathbf{b^\\mathsf{T}\\vec{x}_i} - y_i)^2 \\tag{5.5} \\] Notice that \\(d^2(y_i, \\hat{y}_i)\\) is a pointwise error measure. But we need to define a global measure of error. This is typically done by adding all the pointwise error measures \\(e_{i}^{2} = (\\hat{y}_i - y_i)^2\\). There are two flavors of overall error measures based on squared pointwise differences: (1) the sum of squared errors or \\(\\text{SSE}\\), and (2) the mean squared error or \\(\\text{MSE}\\). The sum of squared errors, \\(\\text{SSE}\\), is defined as: \\[ \\text{SSE} = \\sum_{i=1}^{n} \\text{err}_i \\tag{5.6} \\] The mean squared error, \\(\\text{MSE}\\), is defined as: \\[ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} \\text{err}_i \\tag{5.7} \\] As you can tell, \\(\\text{SSE} = n \\text{MSE}\\), and viceversa, \\(\\text{MSE} = \\text{SSE} / n\\) Throughout this book, unless mentioned otherwise, when dealing with regression problems, we will consider the \\(\\text{MSE}\\) as the default overall error function to be minimized (you could also take \\(\\text{SSE}\\) instead). Doing some algebra, it’s easy to see that: \\[\\begin{align} \\text{MSE} &amp;= \\frac{1}{n} \\sum_{i=1}^{n} e_{i}^{2} \\\\ &amp;= \\frac{1}{n} \\sum_{i=1}^{n} (\\mathbf{b^\\mathsf{T}\\vec{x}_i} - y_i)^2 \\\\ &amp;= \\frac{1}{n} (\\mathbf{Xb} - \\mathbf{y})^\\mathsf{T} (\\mathbf{Xb} - \\mathbf{y}) \\\\ &amp;= \\frac{1}{n} \\| \\mathbf{Xb} - \\mathbf{y} \\|^2 \\tag{5.8} \\end{align}\\] 5.5 The Least Squares Algorithm We want to minimize the mean of squared errors. So we compute the derivative of \\(\\text{MSE}\\) with respect to \\(\\mathbf{b}\\): \\[ \\frac{\\partial }{\\partial \\mathbf{b}}\\text{MSE}(\\mathbf{b}) = \\frac{\\partial }{\\partial \\mathbf{b}} (\\mathbf{b^\\mathsf{T}X^\\mathsf{T}Xb - 2 b^\\mathsf{T}X^\\mathsf{T}y + \\mathbf{y^\\mathsf{T}y}}) \\tag{5.9} \\] which becomes: \\[ 2 \\mathbf{X^\\mathsf{T}Xb} - 2 \\mathbf{X^\\mathsf{T}y} \\tag{5.10} \\] Equating to zero we get that: \\[ \\mathbf{X^\\mathsf{T}Xb} = \\mathbf{X^\\mathsf{T}y} \\quad (\\text{normal equations}) \\tag{5.11} \\] These are the so-called Normal equations. It is a system of \\(n\\) equations with \\(p+1\\) unknowns. If the cross-product matrix \\(\\mathbf{X^\\mathsf{T}X}\\) is invertible, which is not a minor assumption, then the vector of regression coefficients \\(\\mathbf{b}\\) that we are looking for is given by: \\[ \\mathbf{b} = (\\mathbf{X^\\mathsf{T}X})^{-1} \\mathbf{X^\\mathsf{T}y} \\tag{5.12} \\] Having obtained \\(\\mathbf{b}\\), we can easily compute the response vector: \\[\\begin{align*} \\mathbf{\\hat{y}} &amp;= \\mathbf{Xb} \\\\ &amp;= \\mathbf{X} (\\mathbf{X^\\mathsf{T}X})^{-1} \\mathbf{X^\\mathsf{T} y} \\tag{5.13} \\end{align*}\\] If we denote \\(\\mathbf{H} = \\mathbf{X} (\\mathbf{X^\\mathsf{T}X})^{-1} \\mathbf{X^\\mathsf{T}}\\), then the predicted response is: \\(\\mathbf{\\hat{y}} = \\mathbf{Hy}\\). This matrix is better known as the hat matrix, because it puts the hat on the response. More importantly, the matrix \\(\\mathbf{H}\\) is an orthogonal projector. From linear algebra, orthogonal projectors have very interesting properties: they are symmetric they are idempotent their eigenvalues are either 0 or 1 5.6 Geometries of OLS Now that we’ve seen the algebra, it’s time to look at the geometric interpretation of all the action that is going on within linear regression via OLS. We will discuss three geometric perspectives: OLS from the individuals point of view (i.e. rows of the data matrix). OLS from the variables point of view (i.e. columns of the data matrix). OLS from the parameters point of view, and the error surface. 5.6.1 Rows Perspective This is probably the most popular perspective covered in most textbooks. Consider a \\((p + 1)\\)-dimensional space. For illustration purposes let’s assume that our data has just \\(p=1\\) predictor. In other words, we have the response \\(Y\\) and one predictor \\(X\\). We can depict individuals as points in this space: Figure 5.5: Scatterplot of individuals In linear regression, we want to predict \\(y_i\\) by linearly mixing the inputs \\(\\hat{y}_{i} = b_0 + b_1 x_i\\). In two dimensions, the fitted model corresponds to a line. In three dimensions it would correspond to a plane. And in higher dimensions this would corresponds to a hyperplane. Figure 5.6: Scatterplot with regression line With a fitted line, we obtain predicted values \\(\\hat{y}_i\\). Some predicted values may be equal to the observed values. Other predicted values will be greater than the observed values. And some predicted values will be smaller than the observed values. Figure 5.7: Observed values and predicted values As you can imagine, given a set of data points, you can fit an infinite number of lines (in general). So which line are we looking for? We want to obtain the line that minimizes the square of the errors \\(e_i = \\hat{y}_{i} - y_{i}\\). In the figure below, these errors are represented by the vertical difference between the observed values \\(y_i\\) and the predicted values \\(\\hat{y}_i\\). Figure 5.8: OLS focuses on minimizing the squared errors Combining all residuals, we want to obtain parameters \\(b_0, \\dots, b_p\\) that minimize the squared norm of the residuals: \\[ \\sum_{i=1}^{n} e_{i}^{2} = \\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2 = \\sum_{i=1}^{n} (b_0 + b_1 x_i - y_i)^2 \\tag{5.14} \\] In vector-matrix form we have: \\[\\begin{align*} \\| \\mathbf{e} \\|^2 &amp;= \\| \\mathbf{\\hat{y}} - \\mathbf{y} \\|^2 \\\\ &amp;= \\| \\mathbf{Xb} - \\mathbf{y} \\|^2 \\\\ &amp;= (\\mathbf{Xb} - \\mathbf{y})^\\mathsf{T} (\\mathbf{Xb} - \\mathbf{y}) \\tag{5.15} \\end{align*}\\] which is equivalent to minimize the mean squared error (MSE). 5.6.2 Columns Perspective This is less common than the rows perspective. Imagine variables in an \\(n\\)-dimensional space, both the response and the predictors. In this space, the \\(X\\) variables will span some subspace \\(\\mathbb{S}_{X}\\). This subspace is not supposed to contain the response—unless \\(Y\\) happens to be a linear combination of \\(X_1, \\dots, X_p\\). Figure 5.9: Features and Response view What are we looking for? We’re looking for a linear combination \\(\\mathbf{Xb}\\). As you can tell, there’s an infinite number of linear combinations that can be formed with \\(X_1, \\dots, X_p\\). Figure 5.10: Linear combination of features The mix of features that we are interested in, \\(\\mathbf{\\hat{y}} = \\mathbf{Xb}\\), is the one that gives us the closest approximation to \\(\\mathbf{y}\\). Figure 5.11: Linear combination to be as close as possible to response Now, what do we mean by closest approximation? How do we determine the closeness between \\(\\mathbf{\\hat{y}}\\) and \\(\\mathbf{y}\\)? By looking at the difference, which results in a vector \\(\\mathbf{e} = \\mathbf{\\hat{y}} - \\mathbf{y}\\). And then measuring the size or norm of this vector. Well, the squared norm to be precise. In other words, we want to obtain \\(\\mathbf{\\hat{y}}\\) such that the squared norm \\(\\| \\mathbf{e} \\|^2\\) is as small as possible. \\[ \\text{Minimize} \\quad \\| \\mathbf{e} \\|^2 = \\| \\mathbf{\\hat{y}} - \\mathbf{y} \\|^2 \\tag{5.16} \\] 5.6.3 Parameters Perspective We could also visualize the regression problem from the point of view of the parameters \\(\\mathbf{b}\\) and the error surface. This is the least common perspective discussed in the literature that has to do with linear regression in general. However, it is not that uncommon within the Statistical Learning literature. For illustration purposes, assume that we have only two predictors \\(X_1\\) and \\(X_2\\). Recall that the Mean Squared Error (MSE) is: \\[ E(\\mathbf{y}, \\mathbf{\\hat{y}}) = \\frac{1}{n} \\left( \\mathbf{b^\\mathsf{T} X^\\mathsf{T}} \\mathbf{X b} - 2 \\mathbf{b^\\mathsf{T} X^\\mathsf{T} y} + \\mathbf{y^\\mathsf{T}y} \\right) \\tag{5.17} \\] Now, from the point of view of \\(\\mathbf{b} = (b_1, b_2)\\), we can classify the order of each term: \\[ E(\\mathbf{y}, \\mathbf{\\hat{y}}) = \\frac{1}{n} ( \\underbrace{\\mathbf{b^\\mathsf{T} X^\\mathsf{T}} \\mathbf{X b}}_{\\text{Quadratic Form}} - \\underbrace{ 2 \\mathbf{b^\\mathsf{T} X^\\mathsf{T} y}}_{\\text{Linear}} + \\underbrace{ \\mathbf{y^\\mathsf{T}y}}_{\\text{Constant}} ) \\tag{5.18} \\] Since \\(\\mathbf{X^\\mathsf{T} X}\\) is positive semidefinite, we know that \\(\\mathbf{b^\\mathsf{T} X^\\mathsf{T} Xb} \\geq 0\\). Furthermore, we know that (from vector calculus) it will be a paraboloid (bowl-shaped surface) in the \\((E, b_1, b_2)\\) space. The following diagram depicts this situation. Figure 5.12: Error Surface Imagine that we get horizontal slices of the error surface. For any of those slices, we can project them onto the plane spanned by the parameters \\(b_1\\) and \\(b_2\\). The resulting projections will be like a topographic map, with error contours on this plane. In general, those contours will be ellipses. Figure 5.13: Error Surface with slices, and their projections With quadratic error surfaces like this, they have a minimum value, and we are guaranteed the existence of \\(\\mathbf{b}^* = (b_1^{*}, b_2^{*})\\) s.t. \\(\\mathbf{b^\\mathsf{T} X^\\mathsf{T} X b}\\) is minimized. This is a powerful result! Consider, for example, a parabolic cylinder. Such a shape has no unique minimum; rather, it has an infinite number of points (all lying on a line) that minimize the function. The point being; with positive semi-definite matrices, we never have this latter case. Figure 5.14: Error Surface with contour errors and the minimum The minimum of the error surface occurs at the point \\((b_{1}^{*}, b_{2}^{*})\\). This is the precisely the OLS solution. "],
["gradient.html", "6 Gradient Descent 6.1 Error Surface 6.2 Idea of Gradient Descent 6.3 Moving Down an Error Surface 6.4 Gradient Descent and our Model", " 6 Gradient Descent Before moving to the next part of the book which deals with the theory of learning, we want to introduce a very popular optimization technique that is commonly used in many statistical learning methods: the famous gradient descent algorithm. 6.1 Error Surface Consider the overall error measure of a linear regression problem, for example the mean squared error (MSE)—or if you prefer the sum of squared errors (SSE). \\[ E(\\mathbf{y}, \\mathbf{\\hat{y}}) = \\frac{1}{n} \\left( \\mathbf{b^\\mathsf{T} X^\\mathsf{T}} \\mathbf{X b} - 2 \\mathbf{b^\\mathsf{T} X^\\mathsf{T} y} + \\mathbf{y^\\mathsf{T}y} \\right) \\] As we saw in the previous chapter, we can look at such error measure from the perspective of the parameters (i.e. the regression coefficients). From this perspective, we denote this error function as \\(E(\\mathbf{b})\\), making explicit its dependency on the vector of coefficients \\(\\mathbf{b}\\). \\[ E(\\mathbf{b}) = \\frac{1}{n} ( \\underbrace{\\mathbf{b^\\mathsf{T} X^\\mathsf{T}} \\mathbf{X b}}_{\\text{Quadratic Form}} - \\underbrace{ 2 \\mathbf{b^\\mathsf{T} X^\\mathsf{T} y}}_{\\text{Linear}} + \\underbrace{ \\mathbf{y^\\mathsf{T}y}}_{\\text{Constant}} ) \\] As you can tell, \\(E(\\mathbf{b})\\) is a quadratic function with respect to \\(\\mathbf{b}\\). Moreover, \\(E(\\mathbf{b})\\) is a positive semidefinite quadratic form which implies that it is a convex function. What does this all mean? For illustration purposes, let’s consider again a linear regression with two inputs \\(X_1\\) and \\(X_2\\), and assume that there is no constant term. In this case, the error function \\(E(\\mathbf{b})\\) will generate a convex error surface with the shape of a bowl: Figure 6.1: Error Surface In general, with a convex error function, we know that there is a minimum, and the challenge is to find such value. Figure 6.2: Error Surface With OLS we can use direct methods to obtain the minimum. All we need to do is to compute the derivative of the error function, set it equal to zero, and find this minimum point \\(\\mathbf{\\overset{*}{b}}\\). Assuming that the matrix \\(\\mathbf{X^\\mathsf{T} X}\\) is invertible, the OLS minimum is easily calculated as: \\(\\mathbf{b} = (\\mathbf{X^\\mathsf{T}X})^{-1} \\mathbf{X^\\mathsf{T} y}\\). Interestingly, we can also use iterative methods to compute this minimum. The iterative method that we will discuss in this chapter is called gradient descent. 6.2 Idea of Gradient Descent The main idea of gradient descent is as follows: we start with an arbitrary point \\(\\mathbf{b}^{(0)} = ( b_1^{(0)}, b_2^{(0)} )\\), and we evaluate the error function at this point: \\(E(\\mathbf{b}^{(0)})\\). This gives us a location somewhere on the error surface. Figure 6.3: Starting position on error surface We then get a new vector \\(\\mathbf{b}^{(1)}\\) in a way that we “move down” the surface to obtain a new position \\(E(\\mathbf{b}^{(1)})\\): Figure 6.4: Begin to move down on error surface We keep “moving down” the surface, obtaining subsequent new vectors \\(\\mathbf{b}^{(s)}\\) that result in an error \\(E(\\mathbf{b}^{(s)})\\) that is closer to the minimum of \\(E(\\mathbf{b})\\). Figure 6.5: Keep moving down on error surface Eventually, we should get very, very close to the minimizing point, and ideally, arrive at the minimum \\(\\mathbf{\\overset{*}{b}}\\). One important thing to always keep in mind when dealing with minimization (and maximization) problems is that, in practice, we don’t get to see the surface. Instead, we only have local information at the current point we evaluate the error function. Here’s a useful metaphor: Imagine that you are on the top a mountain (or some similar landscape) and your goal is to get to the bottom of the valley. The issue is that it is a foggy day (or night), and the visibility conditions are so poor that you only get to see/feel what is very near you (a couple of inches around you). What would you do to get to the bottom of the valley? Well, you start touching your surroundings trying to feel in which direction the slope of the terrain goes down. The key is to identify the direction in which the slope gives you the steepest descent. This is the conceptual idea behind optimization algorithms like gradient descent. 6.3 Moving Down an Error Surface What do we mean by “moving down the error surface”? Well, mathematically, this means we generate the new vector \\(\\mathbf{b}^{(1)}\\) using the following formula: \\[ \\mathbf{b}^{(1)} = \\mathbf{b}^{(0)} + \\alpha \\mathbf{v} \\] As you can tell, in addition to the values of parameter vectors \\(\\mathbf{b}^{(0)}\\) and \\(\\mathbf{b}^{(1)}\\), we have two extra ingredients: \\(\\alpha\\) and \\(\\mathbf{v}\\). We call \\(\\alpha\\) the step size. Intuitively, \\(\\alpha\\) tells us how far down the surface we are moving. In turn, \\(\\mathbf{v}\\) is the vector indicating the direction in which we need to move. Because we are interested in this direction, we can simply consider \\(\\mathbf{v}\\) to be a unit vector. We will discuss how to find the direction of \\(\\mathbf{v}\\) in a little bit. Right now let’s just focus on generating new vectors in this manner: \\[\\begin{align*} \\mathbf{b}^{(2)} &amp; = \\mathbf{b}^{(1)} + \\alpha \\mathbf{v}^{(1)} \\\\ \\mathbf{b}^{(3)} &amp; = \\mathbf{b}^{(2)} + \\alpha \\mathbf{v}^{(2)} \\\\ \\vdots &amp; \\hspace{10mm} \\vdots \\\\ \\mathbf{b}^{(s+1)} &amp; = \\mathbf{b}^{(s)} + \\alpha \\mathbf{v}^{(s)} \\end{align*}\\] Note that we are assuming a constant step size \\(\\alpha\\); that is, note that \\(\\alpha\\) remains the same at each iteration. We should say that there are more sophisticated versions of gradient descent that allow a variable step size, however we will not consider that case. As we can see in the series of figures above, the direction in which we travel will change at each step of the process. We will also see this mathematically in the next subsection. 6.3.1 The direction of \\(\\mathbf{v}\\) How do we find the direction of \\(\\mathbf{v}\\)? Consider the gradient of our error function. The gradient always points in the direction of steepest ascent (i.e. largest positive change). Hence, we want to travel in the exact opposite direction of the gradient. Let’s “prove” this mathematically. In terms of the error function itself, what does it mean for our vectors \\(\\mathbf{b}^{(s+1)}\\) to be “getting closer” to the minimizing point? Well, it means that the error at point \\(\\mathbf{b}^{(s+1)}\\) is less than the error at point \\(\\mathbf{b}^{(s)}\\). Hence, we examine \\(\\Delta E_{\\mathbf{b}}\\), the difference between the errors at these two points: \\[\\begin{align*} \\Delta E_{\\mathbf{b}} &amp; := E\\big( \\mathbf{b}^{(s + 1)} \\big) - E\\big( \\mathbf{b}^{(s)} \\big) \\\\ &amp; = E \\big( \\mathbf{b}^{(s)} + \\alpha \\mathbf{v} \\big) - E\\big( \\mathbf{b}^{(s)} \\big) \\end{align*}\\] In order for our vector \\(\\mathbf{b}^{(s+1)}\\) to be closer to the minimizing point that \\(\\mathbf{b}^{(s)}\\), we want this quantity to be as negative as possible. To find the \\(\\mathbf{v}\\) that makes this true, we need to use a trick: Taylor Expand \\(E(\\mathbf{b}^{(s)} + \\alpha \\mathbf{v})\\). Doing so, we obtain: \\[\\begin{align*} \\Delta E_{\\mathbf{b}} &amp;= E\\big( \\mathbf{b}^{(s)} \\big) + \\nabla E\\big( \\mathbf{b}^{(s)} \\big)^{\\mathsf{T}} \\big(\\alpha \\mathbf{v} \\big) + O \\big( \\alpha^2 \\big) - E\\big( \\mathbf{b}^{(s)} \\big) \\\\ &amp; = \\alpha \\hspace{1mm} \\nabla E \\big( \\mathbf{b}^{(s)} \\big)^{\\mathsf{T}} \\mathbf{v} + O(\\alpha^2) \\end{align*}\\] Hence, we must examine the term of the inner product between the gradient and the unit vector \\(\\mathbf{v}\\), that is: \\([ \\nabla E(\\mathbf{b}^{(s)})]^{\\mathsf{T}} \\mathbf{v}\\). For notational convenience, let us (temporarily) define \\(\\mathbf{u} : = \\nabla E ( \\mathbf{b}^{(s)} )\\). In this way, we need to examine the inner product: \\(\\mathbf{u^\\mathsf{T}v}\\). With respect to the orientation of \\(\\mathbf{v}\\) and \\(\\mathbf{u}\\), we can consider three prototypical cases: \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are either antiparallel, parallel, or orthogonal. Figure 6.6: Three prototypical cases When both vectors are parallel, \\(\\mathbf{u} \\parallel \\mathbf{v}\\) (i.e. the second case above), then \\(\\mathbf{u}^\\mathsf{T}\\mathbf{v} = \\| \\mathbf{u} \\|\\). When both vectors are antiparallel, \\(\\mathbf{u} \\not\\parallel \\mathbf{v}\\) (i.e. the first case above), then \\(\\mathbf{u}^\\mathsf{T} = - \\| \\mathbf{u} \\|\\). When both vectors are orthogonal, \\(\\mathbf{u} \\perp \\mathbf{v}\\), then \\(\\mathbf{u}^\\mathsf{T} \\mathbf{v} = 0\\). Therefore, in any of the three cases, we have that \\[ \\mathbf{u}^\\mathsf{T} \\mathbf{v} \\geq - \\left\\| \\mathbf{u} \\right\\| \\] Hence, recalling that \\(\\mathbf{u} := \\nabla E(\\mathbf{b}^{(s)})\\), we can plug this result into our error computation to obtain: \\[\\begin{align*} \\Delta E_{\\mathbf{b}} &amp; := E\\big( \\mathbf{b}^{(s + 1)} \\big) - E\\big( \\mathbf{b}^{(s)} \\big) \\\\ &amp; = E \\big( \\mathbf{b}^{(s)} + \\alpha \\mathbf{v} \\big) - E\\big( \\mathbf{b}^{(s)} \\big) \\\\ &amp; = \\alpha \\hspace{1mm} \\nabla E \\big( \\mathbf{b}^{(s)} \\big)^{\\mathsf{T}} \\mathbf{v} + O(\\alpha^2) \\\\ &amp; \\geq - \\alpha \\left\\| \\nabla E\\big( \\mathbf{b}^{(s)} \\big) \\right\\| \\mathbf{v} \\end{align*}\\] Thus, to make \\(\\Delta E_{\\mathbf{b}}\\) as negative as possible, we should take \\(\\mathbf{v}\\) parallel to \\(( - \\| \\nabla E(\\mathbf{b}^{(s)}) \\| )\\). The Moral: We want the following: \\[ \\mathbf{v} = - \\frac{\\nabla E(\\mathbf{b}^{(s)}) }{\\left\\| \\nabla E(\\mathbf{b}^{(s)}) \\right\\| } \\] which means we want to move in the direction opposite to that of the gradient. Keep in mind that we divided by the norm because we previously defined \\(\\mathbf{v}\\) to be a unit vector. This also reveals the meaning behind the name gradient descent; we are descending in the direction opposite to the gradient of the error function. 6.4 Gradient Descent and our Model Before we present the full algorithm for gradient descent in the context of regression, let us investigate the actual gradient further. Since we have a formula for \\(E(\\mathbf{b})\\), we can find a closed form for its gradient \\(\\nabla E(\\mathbf{b})\\): \\[\\begin{align*} E(\\mathbf{b}) &amp; = \\frac{1}{n} \\left( \\mathbf{y} - \\mathbf{X} \\mathbf{b} \\right)^{\\mathsf{T}} \\left( \\mathbf{y} - \\mathbf{X} \\mathbf{b} \\right) \\\\ \\\\ &amp; = \\frac{1}{n} \\left( \\mathbf{b}^\\mathsf{T} \\mathbf{X}^\\mathsf{T} \\mathbf{X} \\mathbf{b} - 2 \\mathbf{b}^\\mathsf{T} \\mathbf{X}^\\mathsf{T} \\mathbf{y} + \\mathbf{y}^\\mathsf{T} \\mathbf{y} \\right) \\\\ \\\\ \\nabla E(\\mathbf{b}) &amp; = \\frac{1}{n} \\left( 2 \\mathbf{X}^\\mathsf{T} \\mathbf{X} \\mathbf{b} - 2 \\mathbf{X}^\\mathsf{T} \\mathbf{y} \\right) \\end{align*}\\] This was from the columns point of view; we can also find a different formula from the row’s perspective: \\[\\begin{align*} E(\\mathbf{b}) &amp; = \\frac{1}{n} \\sum_{i = 1}^{n} \\left( y_i - \\mathbf{b}^\\mathsf{T} \\mathbf{x_i} \\right)^2 = \\frac{1}{n} \\sum_{i=1}^{n} \\left( y_i - \\hat{y}_i \\right) \\\\ &amp; = \\frac{1}{n} \\sum_{i=1}^{n} \\left( y_i - b_0 x_{i0} - b_1 x_{i1} - \\dots - b_{j} x_{ij} - \\dots - b_{p} x_{ip} \\right)^2 \\\\ \\frac{\\partial}{\\partial b_j} E(\\mathbf{b}) &amp; = - \\frac{2}{n} \\sum_{i=1}^{n} \\left( y_i - b_0 x_{i0} - b_1 x_{i1} - \\dots - b_{j} x_{ij} - \\dots - b_{p} x_{ip} \\right) x_{ij} \\\\ \\frac{\\partial}{\\partial b_j} E(\\mathbf{b}^{(0)}) &amp; = - \\frac{2}{n} \\sum_{i=1}^{n} \\left( y_i - b_0 x_{i0} - b_1 x_{i1} - \\dots - b_{j} x_{ij} - \\dots - b_{p} x_{ip} \\right) x_{ij} \\\\ &amp; = - \\frac{2}{n} \\sum_{i=1}^{n} \\left( y_i - \\left[ \\mathbf{b}^{(0)} \\right]^{\\mathsf{T}} \\mathbf{x_i} \\right) x_{ij} \\end{align*}\\] This will be a better formula to use in our iterative algorithm, posited below. Algorithm Initialize a vector \\(\\mathbf{b}^{(0)} = \\left( b_{0}^{(0)}, b_{1}^{(0)}, \\dots, b_{p}^{(0)} \\right)\\) Repeat the following over \\(s\\) (starting with \\(s = 0\\)), until convergence: \\(b_{j}^{(s+1)}:= b_{j}^{(s)} + \\alpha \\cdot \\frac{2}{n} \\sum_{i=1}^{n} \\left( y_i - \\left[\\mathbf{b}^{(s)}\\right]^{\\mathsf{T}} \\mathbf{x_i} \\right) x_{ij}\\) for all \\(j = 0, \\dots, p\\) simultaneously. In more compact notation, \\[ b_{j}^{(s+1)} = b_{j}^{(s)} + \\alpha \\cdot \\frac{\\partial }{\\partial b_j} E(\\mathbf{b}^{(s)}) \\] Store these elements in the vector \\(\\mathbf{b}^{(s+1)} = \\left( b_{0}^{(s+1)} , b_{1}^{(s+1)} , \\dots, b_{p}^{(s+1)} \\right)\\) When there is little change between \\(\\mathbf{b}^{(k + 1)}\\) and \\(\\mathbf{b}^{(k)}\\) (for some integer \\(k\\)), the algorithm will have converged and \\(\\mathbf{b}^{*} = \\mathbf{b}^{(k+1)}\\). Note that in the algorithm, we have dropped the condition that \\(\\mathbf{v}\\) is a unit vector. Indeed, if you look on the Wikipedia page for Gradient Descent, the algorithm listed there also omits the unit-length condition. "],
["learning.html", "7 Theoretical Framework 7.1 Mental Map 7.2 Kinds of Predictions 7.3 Two Types of Errors 7.4 Noisy Targets", " 7 Theoretical Framework Finally we have arrived to the part of the book in which we provide a framework for the theory of learning. Well, to be more precise, the framework is really about the theory of supervised learning. Keep in mind that most of what will be covered in this chapter is highly theoretical. It has to do with the concepts and principles that ideally we expect to find in a perfect world. Having said, the real world, more often than not, is far from being ideal. So we will also need to discuss what to do in practice to overcome the idealistic limitations of our theoretical assumptions. 7.1 Mental Map So far, we have seen an example of Unsupervised Learning (PCA), as well as one method of Supervised Learning (regression). Now, we begin discussing learning ideas at an abstract level. Let’s return to our example of predicting NBA players’ salaries. We have a series of inputs: height, weight, 2PTS, 3PTS, fouls, etc. From these inputs, we obtain an output: the salary of a player. We also have a target function \\(f : \\mathcal{X} \\to \\mathcal{Y}\\). (i.e. a function mapping from the column spaces of \\(\\mathbf{X}\\) to the output space \\(\\mathbf{y}\\)). Keep in mind that this function is an ideal, and remains unknown throughout our computations. Here’s a metaphor that we like to use. Pretend that the target function is some sort of mythical creature, like a unicorn (or your preferred creature). We are trying to find this elusive guy. More generally, we have a dataset \\(\\mathcal{D}: (\\mathbf{x_1}, y_1), ( \\mathbf{x_2}, y_2), \\dots, (\\mathbf{x_n}, y_n)\\), where \\(\\mathbf{x_i}\\) represents the vector of observations for the \\(i\\)-th player/individual. From this data, we wish to obtain a hypothesis model \\(\\widehat{f}: \\mathcal{X} \\to \\mathcal{Y}\\) that is an approximation to the unknown function \\(f\\). We can sketch these basic ideas in a sort of “mental map”. We will refer to this picture as the “diagram for supervised learning”. Figure 7.1: Supervised Learning Diagram (ver 1) Here’s how to read this diagram. We are using orange clouds around those concepts that are more intangible than those appearing within rectangular or oval shapes. One of these clouds is the unknown target function \\(f\\), which as its name indicates it is unknown. This implies that we never really “discover” the function \\(f\\) in its entirety; rather, we just find a good enough approximation to it by estimating \\(\\widehat{f}\\). Now, as you can tell from the mental map, the other orange clound has to do with precisely this idea of good approximation: \\(\\widehat{f} \\approx f\\). It is also theoretical because of what we just said: that we don’t know \\(f\\). We should let you know that as we modify our diagram, we will encounter more orange concepts as well of highly theoretical nature. Now let’s turn our attention to the blue rectangular elements. One of them is the data set \\(\\mathcal{D}\\) which is influenced by the unknown target function. The other blue rectangle has to do with a set of candidate models \\(\\mathcal{H}\\), which is sometimes referred to as the hypothesis set. Both the data set and the set of models are tangible ingredients. Moreover, the set of candidate models is totally under out control. We get to dedice what type of models we want try out (e.g. linear model, polynomial models, non-parametric models). Then we go to the yellow oval shape which right now is simply labeled as the learning algorithm \\(\\mathcal{A}\\). This corresponds to the set of instructions and steps to be carried out when learning from data. It is also the stage of the diagram in which most computations take place. Finally, we arrive at the yellow rectangle containing the final model \\(\\widehat{f}\\). This is supposed to be the selected model by the learning algorithm from the set of hypothesis models. Ideally, this model is the one that provides a good approximation for the target function \\(f\\). Going back to the holy grail of supervised learning, our goal is to find a model \\(\\widehat{f}\\) that gives “good” or accurate predictions. Before discussing what exactly do we mean by “accurate?”, we first need to talk about predictions. 7.2 Kinds of Predictions What is the ultimate goal in supervised learning? Quick answer, we want a “good” model. What does “good” model mean? Simply put, it means that we want to estimate an unknown model \\(f\\) with some model \\(\\widehat{f}\\) that gives “good” predictions. What do we mean by “good” predictions? Loosely speaking, it means that we want to obtain “accurate” predictions. Before clarifying the notion of accurate predictions, let’s discuss first the concept of predictions. Think of a simple linear regression model (e.g. with one predictor). Having a fitted model \\(\\widehat{f}(x)\\), we can use it to make two types of predictions. On one hand, for an observed point \\(x_i\\), we can compute \\(\\hat{y}_i = \\widehat{f}(x_i)\\). By observed point we mean that \\(x_i\\) was part of the data used to find \\(\\widehat{f}\\). On the other hand, we can also compute \\(\\hat{y}_0 = \\widehat{f}(x_0)\\) for a point \\(x_0\\) what was not part of the data used when deriving \\(\\widehat{f}\\). 7.2.1 Two Types of Data The two distinct types of predictions involve two slightly different kinds of data. The data points \\(x_i\\) that we use to fit a model is what we call training or learning data. The data points \\(x_0\\) that we use to assess the performance of a model are points NOT supposed to be part of the training set. This implies that, at least in theory, we need two kinds of data sets: In-sample data, denoted \\(\\mathcal{D}_{in}\\), used to fit a model Out-of-sample data, denoted \\(\\mathcal{D}_{out}\\), used to measure the predictive quality of a model 7.2.2 Two Types of Predictions In other words, we have two types of predictions: predictions \\(\\hat{y}_i\\) of observed/seen values \\(x_i\\) predicitons \\(\\hat{y}_0\\) of unobserved/unseed values \\(x_0\\) Each type of prediction is associated with a certain behavioral feature of a model. The predictions of observed data, \\(\\hat{y}_i\\), have to do with the memorizing aspect (apparent error, resubstitution error). The predictions of unobserved data, \\(\\hat{y}_0\\), have to do with the generalization aspect (generalization error, prediction error). Both kinds of predictions are important, and each of them is interesting in its own right. However, from the supervised learning standpoint, it is the second type of predictions that we are ultimately interested in. That is, we want to find models that are able to give predictions \\(\\hat{y}_0\\) as accurate as possible for the real value \\(y_0\\). Don’t get us wrong. Having good predictions \\(\\hat{y}_i\\) of observed values is important and desirable. And to a large extent, it is a necessary condition for a good model. However, it is not a sufficient condition. It is not enough to fit the observed data well, in order to get a good predictive model. Sometimes, you can perfectly fit the observed data, but have a terrible performance for unobserved values \\(x_0\\). 7.3 Two Types of Errors In theory, we are dealing with two types of predictions, each of which is associated to certain types of data points. Because we are interested in obtaining models that give accurate predictions, we need a way to measure the accuracy of such predictions. At the conceptual level we need some mechanism to quantify how different the fitted model is from the target function \\(f\\): \\[ \\widehat{f} \\text{ -vs- } f \\] It would be nice to have some measure of how much discrepancy exists between the estimated model and the target model. This means that we need a function that summarizes, somehow, the total amount of error. We will denote such term as an Overall Measure of Error: \\[ \\text{Overall Measure of Error:} \\quad E(\\widehat{f},f) \\] The typical way in which an overall measure of error is defined is in terms of individual or pointwise errors \\(err_i(\\hat{y}_i, y_i)\\) that quantify the difference between an observed value \\(y_i\\) and its predicted value \\(\\hat{y}_i\\). As a matter of fact, most overall errors focus on the addition of the pointwise errors: \\[ E(\\widehat{f},f) = \\text{measure} \\left( \\sum err_i(\\hat{y}_i, y_i) \\right ) \\] Unless otherwise said, in this book we will use the mean sum of errors as the default overall error measure: \\[ E(\\widehat{f},f) = \\frac{1}{n} \\left( \\sum_i err_i (\\hat{y}_i, y_i) \\right) \\] 7.3.1 Individual Errors What form does the individual error function, \\(err()\\), take? In theory, they can take any form you want. This means that you can invent your own individual error function. However, the most common ones are: squared error: \\(\\quad err(\\widehat{f}, f) = \\left( \\hat{y}_i - y_i \\right)^2\\) absolute error: \\(\\quad err(\\widehat{f}, f) = \\left| \\hat{y}_i - y_i \\right|\\) misclassification error: \\(\\quad err(\\widehat{f}, f) = [\\![ \\hat{y}_i \\neq y_i ]\\!]\\) In the machine learning literature, these individual errors are formally known as loss functions. 7.3.2 Overall Errors As you can imagine, there are actually two types of overall error measures, based on the type of data that is used to assess the individual errors: In-sample Error, denoted \\(E_{in}\\) Out-of-sample Error, denoted \\(E_{out}\\) The in-sample error is the average of pointwise errors from data points of the in-sample data \\(D_{in}\\): \\[ E_{in} (\\widehat{f}, f) = \\frac{1}{n} \\sum_{i} err_i \\] The out-of-sample error is the theoretical mean, or expected value, of the pointwise errors: \\[ E_{out} (\\widehat{f}, f) = \\mathbb{E}_{\\mathcal{X}} \\left[ err \\left( \\widehat{f}(x), f(x) \\right) \\right] \\] The expectation is taken over the input space \\(\\mathcal{X}\\). The point \\(x\\) denotes a general data point in such space \\(\\mathcal{X}\\). Notice the theoretical nature of \\(E_{out}\\). In practice, you will never, never, be able to compute this quantity. In the machine learning literature, these overall measures of error are formally known as cost functions or risks. Let’s update our supervised learning diagram to include error measures: Figure 7.2: Supervised Learning Diagram (ver 2) 7.3.3 Auxiliary Technicality We need to assume some probability distribution \\(P\\) on \\(\\mathcal{X}\\). That is, we assume our vectors \\(\\mathbf{x_1}, \\dots, \\mathbf{x_n}\\) are independent identically distributed (i.i.d.) samples from this distribution \\(P\\). (Exactly what distribution you pick - normal, chi-squared, \\(t\\), etc. - is, for the moment, irrelevant). Recall that out-of-sample data is highly theoretical; we will never be able to obtain it in its entirety. The best we can do is obtain a subset of the out-of-sample data (the test data), and estimate the rest of the data. Our imposition of a distributional structure on \\(\\mathcal{X}\\) enables us to link the in-sample error with the out-of-sample data. Recall that our ultimate goal is to get a good function \\(\\widehat{f} \\approx f\\). What do we mean by the symbol “\\(\\approx\\)”? Technically speaking, we want \\(E_{\\mathrm{out}}(\\widehat{f}) \\approx 0\\). If this is the case, we can safely say that our model has been successfully trained. However, we can never check if this is the case, since we don’t have access to \\(E_{\\mathrm{out}}\\). To solve this, we break our goal into two sub-goals: \\[ E_{\\mathrm{out}} (\\widehat{f}) \\approx 0 \\ \\Rightarrow \\begin{cases} E_{\\mathrm{in}}(\\widehat{f}) \\approx 0 &amp; \\text{practical result} \\\\ E_{\\mathrm{out}}(\\widehat{f}) \\approx E_{\\mathrm{in}}(\\widehat{f}) &amp; \\text{technical/theoretical result} \\\\ \\end{cases} \\] The first condition is easy to check. How do we check the second? We check the second condition by invoking our distributional assumption \\(P\\) on \\(\\mathcal{X}\\). Using our assumption, we can cite various theorems to assert that the second result indeed holds true. We will later find ways to estimate \\(E_{\\mathrm{out}}(\\widehat{f})\\). Figure 7.3: Supervised Learning Diagram (ver 3) 7.4 Noisy Targets In practice, our function won’t necessarily be a nice (or smooth) function. Rather, there will be some noise. Hence, instead of saying \\(y = f(x)\\) where \\(f : \\mathcal{X} \\to \\mathcal{Y}\\), a better statement might be something like \\(y = f(x) + \\varepsilon\\). But even this notation has some flaws; for example, we could have multiple inputs mapping to the same output (which cannot happen if \\(f\\) is a proper “function”). That is, we may have two individuals with the exact same inputs \\(\\mathbf{x_A} = \\mathbf{x_B}\\) but with different response variables \\(y_A \\neq y_B\\). Instead, it makes more sense to consider some target distribution \\(P(y \\mid x)\\). In this way, we can think of our data as forming a joint probability distribution \\(P(\\mathbf{x}, y)\\). That is because \\(P(\\mathbf{x}, y) = P(\\mathbf{x}) P(y \\mid \\mathbf{x})\\). Figure 7.4: Supervised Learning Diagram (ver 4) In supervised learning, we want to learn the conditional distribution \\(P(y \\mid \\mathbf{x})\\). Again, we can think of this probability as \\(y = f() + \\text{noise}\\). Also, sometimes the Hypothesis Sets and Learning Algorithm boxes are combined into one, called the Learning Model. "],
["mse.html", "8 MSE of Estimator 8.1 MSE of an Estimator", " 8 MSE of Estimator In this chapter we provide a preliminary review of the Mean Squared Error (MSE) of an estimator. This will allow us to have a more gentle introduction to the next chapter about the famous Bias-Variance tradeoff. 8.1 MSE of an Estimator In order to discuss the bias-variance decomposition of a regression function and its expected MSE, we would like to first review the concept of the mean squared error of an estimator. Recall that estimation consists of providing an approximate value to the parameter of a population, using a ( random) sample of observations drawn from such population. Say we have a population of \\(n\\) objects and we are interested in describing them with some numeric characteristic \\(\\theta\\). For example, our population may be formed by all students in some college, and we want to know their average height. We call this (theoretical) average the parameter. Figure 8.1: Population described by some parameter of interest. To estimate the value of the parameter, we may draw a random sample of \\(m &lt; n\\) students from the population and compute a statistic \\(\\hat{\\theta}\\). Ideally, we would use some statistic \\(\\hat{\\theta}\\) that approximates well the parameter \\(\\theta\\). Figure 8.2: Random sample from a population In practice, this is the typical process that you would carry out: Get a random sample from a population. Use the limited amount of data in the sample to estimate \\(\\theta\\) using some formula to compute \\(\\hat{\\theta}\\). Make a statement about how reliable of an estimator \\(\\hat{\\theta}\\) is. Now, for illustration purposes, let’s do the following mental experiment. Pretend that you can draw multiple random samples, all of the same size \\(m\\), from the population. In fact, you should pretend that you can get an infinite number of samples. And suppose that for each sample you will compute a statistic \\(\\hat{\\theta}\\). A first random sample of size \\(m\\) would result in \\(\\hat{\\theta}_1\\). A second random sample of size \\(m\\) would result in \\(\\hat{\\theta}_2\\). And so on. Figure 8.3: Various random samples of equal size and their statistics A couple of important things to notice: An estimator is a random variable\" A first sample will result in \\(\\hat{\\theta}_1\\) A second sample will result in \\(\\hat{\\theta}_2\\) A third sample will result in \\(\\hat{\\theta}_3\\) and so on … Some samples will yield a \\(\\hat{\\theta}_k\\) that overestimates \\(\\theta\\) Other samples will yield a \\(\\hat{\\theta}_k\\) that underestimates \\(\\theta\\) Some samples will yield a \\(\\hat{\\theta}_k\\) matching \\(\\theta\\) In theory, we could get a very large number of samples, and visualize the distribution of \\(\\hat{\\theta}\\), like in the figure below: Figure 8.4: Distribution of an estimator As you would expect, some estimators will be close to the parameter \\(\\theta\\), while others not so much. Under general assumptions, we can also assume that the estimator has expected value \\(\\mathbb{E}(\\hat{\\theta})\\), with finite variance \\(var(\\hat{\\theta})\\). Figure 8.5: Distribution of an estimator An interesting question to consider is: In general, how much different—or similar—is \\(\\hat{\\theta}\\) from \\(\\theta\\)? To be more concrete: on average, how close we expect the estimator to be from the parameter? To answer this question we can look for a measure to assess the typical distance of estimators from the parameter. This involves looking at the difference: \\(\\hat{\\theta} - \\theta\\), which is commonly referred to as the estimation error: \\[ \\text{estimation error} = \\hat{\\theta} - \\theta \\] We would like to measure the “size” of such difference. Notice that the estimation error is also a random variable: A first sample will result in an error \\(\\hat{\\theta}_1 - \\theta\\) A second sample will result in an error \\(\\hat{\\theta}_2 - \\theta\\) A third sample will result in an error \\(\\hat{\\theta}_3 - \\theta\\) and so on … So how do we measure the “size” of the estimation errors? The typical way to quantify the amount of estimation error is by calculating the squared errors, and then averaging over all the possible values of the estimators. This is known as the Mean Squared Error (MSE) of \\(\\hat{\\theta}\\): \\[ \\text{MSE}(\\hat{\\theta}) = \\mathbb{E} [(\\hat{\\theta} - \\theta)^2] \\] MSE is the squared distance from our estimator \\(\\hat{\\theta}\\) to the true value \\(\\theta\\), averaged over all possible samples. It is convenient to regard the estimation error, \\(\\hat{\\theta} - \\theta\\), with respect to \\(\\mathbb{E}(\\hat{\\theta})\\). In other words, the distance between \\(\\hat{\\theta}\\) and \\(\\theta\\) can be expressed with respect to the expected value \\(\\mathbb{E}(\\hat{\\theta})\\): Figure 8.6: Estimator, its mean, and the parameter Let’s rewrite \\((\\hat{\\theta} - \\theta)^2\\) as \\(( \\hat{\\theta} - \\mathbb{E}(\\hat{\\theta}) + \\mathbb{E}(\\hat{\\theta}) - \\theta)^2\\), and let \\(\\mathbb{E}(\\hat{\\theta}) = \\mu_{\\hat{\\theta}}\\). Then: \\[\\begin{align*} (\\hat{\\theta} - \\theta)^2 &amp;= \\left ( \\hat{\\theta} - \\mathbb{E}(\\hat{\\theta}) + \\mathbb{E}(\\hat{\\theta}) - \\theta \\right )^2 \\\\ &amp;= ( \\hat{\\theta} - \\mu_{\\hat{\\theta}} + \\mu_{\\hat{\\theta}} - \\theta )^2 \\\\ &amp;= (\\underbrace{\\hat{\\theta} - \\mu_{\\hat{\\theta}}}_{a} + \\underbrace{\\mu_{\\hat{\\theta}} - \\theta}_{b})^2 \\\\ &amp;= a^2 + b^2 + 2ab \\\\ \\Longrightarrow \\mathbb{E} \\left [ (\\hat{\\theta} - \\theta)^2 \\right ] &amp;= \\mathbb{E}[a^2 + b^2 + 2ab] \\end{align*}\\] We have that \\(\\text{MSE}(\\hat{\\theta}) = \\mathbb{E} [(\\hat{\\theta} - \\theta)^2]\\) can be decomposed as: \\[\\begin{align*} \\mathbb{E} \\left [ (\\hat{\\theta} - \\theta)^2 \\right ] &amp;= \\mathbb{E}[a^2 + b^2 + 2ab] \\\\ &amp;= \\mathbb{E}(a^2) + \\mathbb{E}(b^2) + 2\\mathbb{E}(ab) \\\\ &amp;= \\mathbb{E} [ (\\hat{\\theta} - \\mu_{\\hat{\\theta}})^2 ] + \\mathbb{E} [ (\\mu_{\\hat{\\theta}} - \\theta)^2 ] + 2\\mathbb{E}(ab) \\end{align*}\\] Notice that \\(\\mathbb{E}(ab)\\): \\[ \\mathbb{E}(ab) = \\mathbb{E}[ (\\hat{\\theta} - \\mu_{\\hat{\\theta}}) (\\mu_{\\hat{\\theta}} - \\theta) ] = 0 \\] Consequently \\[\\begin{align*} \\text{MSE}(\\hat{\\theta}) &amp;= \\mathbb{E} \\left [ (\\hat{\\theta} - \\theta)^2 \\right ] \\\\ &amp; \\\\ &amp;= \\mathbb{E} [ (\\hat{\\theta} - \\mu_{\\hat{\\theta}})^2 ] + \\mathbb{E} [ (\\mu_{\\hat{\\theta}} - \\theta)^2 ] \\\\ &amp; \\\\ &amp;= \\mathbb{E} [(\\hat{\\theta} - \\mu_{\\hat{\\theta}})^2] + \\mathbb{E} [ (\\mu_{\\hat{\\theta}} - \\theta) ]^2 \\\\ &amp; \\\\ &amp;= \\underbrace{\\mathbb{E} [(\\hat{\\theta} - \\mu_{\\hat{\\theta}})^2]}_{\\text{Variance}} + (\\underbrace{\\mu_{\\hat{\\theta}} - \\theta}_{\\text{Bias}})^2 \\\\ &amp; \\\\ &amp;= \\text{Var}(\\hat{\\theta}) + \\text{Bias}^{2} (\\hat{\\theta}) \\end{align*}\\] The MSE of an estimator can be decomposed in terms of Bias and Variance. Bias, \\(\\mu_{\\hat{\\theta}} - \\theta\\), is the tendency of \\(\\hat{\\theta}\\) to overestimate or underestimate \\(\\theta\\) over all possible samples. Variance, \\(\\text{Var}(\\hat{\\theta})\\), simply measures the average variability of the estimators around their mean \\(\\mathbb{E}(\\hat{\\theta})\\). 8.1.1 Prototypical Cases of Bias and Variance Depending on the type of estimator \\(\\hat{\\theta}\\), and the sample size \\(m\\), we can get statistics having different behaviors. The following diagram illustrates four classic scenarios contrasting low and high values for both the bias and the variance. Figure 8.7: Prototypical scenarios for Bias-Variance "],
["biasvar.html", "9 Bias-Variance Tradeoff 9.1 Bias-Variance Tradeoff 9.2 Motivation Example 9.3 Learning from two points 9.4 Bias-Variance Derivation 9.5 The Tradeoff", " 9 Bias-Variance Tradeoff In this chapter we discuss one of the theoretical dogmas of statistical learning: the famous Bias-Variance tradeoff. 9.1 Bias-Variance Tradeoff In the previous chapter we reviewed the concept of Mean Squared Error of a statistic (or estimator) \\(\\hat{\\theta}\\). As we saw, we can decompose \\(\\text{MSE}(\\hat{\\theta})\\) as the sum of two components: Bias-squared and Variance. \\[ \\text{MSE}(\\hat{\\theta}) = \\text{Var}(\\hat{\\theta}) + \\text{Bias}^{2} (\\hat{\\theta}) \\] In this chapter we extend this notion to the MSE of a regression model. 9.2 Motivation Example Let’s start with a toy example. Consider a noiseless target function \\(f(x) = sin(\\pi x)\\), with the input variable \\(x\\) in the interval \\([-1,1]\\), like in the following picture: (#fig:plot_target)Target function 9.2.1 Two Hypotheses Let’s assume a learning scenario in which, given a data set of \\(n\\) points, we fit the data using one of two models (see the idealized figure shown below): \\(\\mathcal{H}_0\\): Set of all lines of the form \\(h(x) = b\\) \\(\\mathcal{H}_1\\): Set of all lines of the form \\(h(x) = b_0 + b_1 x\\) (#fig:plot_two_hypotheses)Two Learning hypothesis models 9.3 Learning from two points In this case study, we will assume a data set of size \\(n = 2\\). That is, we sample \\(x\\) uniformly in \\([-1,1]\\) to generate a data set of two points \\((x_1, y_1), (x_2, y_2)\\); and fit the data using the two models \\(\\mathcal{H}_0\\) and \\(\\mathcal{H}_1\\). For \\(\\mathcal{H}_0\\), we choose the constant hypothesis that best fits the data (the horizontal line at the midpoint \\(b = \\frac{y_1 + y_2}{2}\\)). For \\(\\mathcal{H}_1\\), we choose the line that passes through the two data points \\((x_1, y_1)\\) and \\((x_2, y_2)\\). Here’s an example in R of two \\(x\\)-points randomly sampled from a uniform distribution in the interval \\([-1,1]\\), and their corresponding \\(y\\)-points: \\(p_1(x_1, y_1) = (0.0949158, 0.2937874)\\) \\(p_2(x_2, y_2) = (0.4880941, 0.9993006)\\) With the given points above, the two fitted models are: \\(h_0(x) = 0.646544\\) \\(h_1(x) = 0.123472 + 1.794385 \\hspace{1mm} x\\) 9.4 Bias-Variance Derivation Bias-Variance (BV) depends on MSE, and is typically discussed within the confines of regression. BV is a good theoretical device; i.e. we can never compute it in practice (if we could, we would have access to \\(f\\), in which case we wouldn’t really need Statistical Learning in the first place!) Given a data set \\(\\mathcal{D}\\) of \\(n\\) points, and a hypothesis \\(h(x)\\), the expectation of the Squared Error for a given out-of-sample point \\(x_o\\), over all possible training sets, is expressed as: \\[ \\mathbb{E}_{\\mathcal{D}} \\left [ \\left( h^{(\\mathcal{D})}(x_0) - f(x_0) \\right)^2 \\right ] \\] Assume the target \\(f(x)\\) is noiseless. Here, \\(h^{(\\mathcal{D})}(x_0)\\) plays the role of \\(\\hat{\\theta}\\) and \\(f(x_0)\\) plays the role of \\(\\theta\\). Hence, we need to introduce \\(\\bar{h} := \\mathbb{E}_{\\mathcal{D}} \\left[ h^{(\\mathcal{D})}(x_0) \\right]\\); i.e. the “average hypothesis”. Then: \\[\\begin{align*} \\mathbb{E}_{\\mathcal{D}} \\left [ \\left( h^{(\\mathcal{D})}(x_0) - f(x_0) \\right)^2 \\right ] &amp;= \\mathbb{E}_{\\mathcal{D}} \\left [ \\left (h^{(\\mathcal{D})}(x_0) - \\bar{h} + \\bar{h} - f(x_0) \\right)^2 \\right ] \\\\ &amp;= \\mathbb{E}_{\\mathcal{D}} \\Big [ \\Big (\\underbrace{h^{(\\mathcal{D})}(x_0) - \\bar{h}}_{a} + \\underbrace{\\bar{h} - f(x_0)}_{b} \\Big)^2 \\Big ] \\\\ &amp;= \\mathbb{E}_{\\mathcal{D}} \\left [ (a + b)^2 \\right ] \\\\ &amp;= \\mathbb{E}_{\\mathcal{D}} \\left [ a^2 + 2ab + b^2 \\right ] \\\\ &amp;= \\mathbb{E}_{\\mathcal{D}} [a^2] + \\mathbb{E}_{\\mathcal{D}} [b^2] + \\mathbb{E}_{\\mathcal{D}} [2ab] \\\\ \\end{align*}\\] Let’s examine the first two terms: \\(\\mathbb{E}_{\\mathcal{D}} \\left [ \\left (h^{(\\mathcal{D})}(x_0) - \\bar{h} \\right)^2 \\right ] = \\text{variance} \\left( h(x_0) \\right)\\) \\(\\mathbb{E}_{\\mathcal{D}} \\left [ \\left (\\bar{h} - f(x_0) \\right)^2 \\right ] = \\text{Bias}^2 \\left( h(x_0) \\right)\\) Now, what about the cross-term: \\(\\mathbb{E}_{\\mathcal{D}} [2ab]\\)? \\[\\begin{align*} \\mathbb{E}_{\\mathcal{D}} \\left[ 2 \\left( h^{(\\mathcal{D})}(x_0) - \\bar{h} \\right) \\left( \\bar{h} - f(x_0) \\right) \\right] &amp; \\propto \\mathbb{E}_{\\mathcal{D}} \\left[ h^{(\\mathcal{D})}(x_0) \\right] - \\bar{h} \\\\ &amp; = \\bar{g} - \\bar{g} = 0 \\end{align*}\\] Hence, under the assumption of noiseless \\(f\\), we have that the expectation of the Squared Error for a given out-of-sample point \\(x_0\\), over all possible training sets, is expressed as: \\[ \\mathbb{E}_{\\mathcal{D}} \\left [ \\left( h^{(\\mathcal{D})}(x_0) - f(x_0) \\right)^2 \\right ] = \\underbrace{\\mathbb{E}_{\\mathcal{D}} \\left [ \\left (h^{(\\mathcal{D})}(x_0) - \\bar{h}(x_0) \\right)^2 \\right ]}_{\\text{variance}} + \\underbrace{\\left [ (\\bar{h}(x_0) - f(x_0))^2 \\right ] }_{\\text{bias}^2} \\] 9.4.1 Noisy Target Now, when there is noise in the data we have that: \\(y = f(x) + \\epsilon\\). If \\(\\epsilon\\) is a zero-mean noise random variable with variance \\(\\sigma^2\\), the bias-variance decomposition becomes: \\[ \\mathbb{E}_{\\mathcal{D}} \\left [ \\left (h^{(\\mathcal{D})}(x_o) - y_o \\right)^2 \\right ] = \\text{bias}^2 + \\text{var} + \\sigma^2 \\] Notice that the above equation assumes that the squared error corresponds to just one out-of-sample (i.e. test) point \\((x_0, y_0) = (x_0, f(x_0))\\). 9.4.2 Types of Theoretical MSEs Keep in mind that there are different flavors of (theoretical) MSE: \\(\\mathbb{E}_{\\mathcal{X}} \\left[ \\left( h(x_0) - f(x_0) \\right)^2 \\right]\\) \\(\\mathbb{E}_\\mathcal{D} \\left[ \\left( h^{\\mathcal{D}}(x_0) - f(x_0) \\right)^2 \\right]\\) \\(\\mathbb{E}_{\\mathcal{X}} \\left[ \\mathbb{E}_\\mathcal{D} \\left\\{ \\left( h^{\\mathcal{D}}(x_0) - f(x_0) \\right)^2 \\right\\} \\right]\\) 9.5 The Tradeoff We finish this chapter with a brief discussion of the so-called bias-variance tradeoff. Bias Let’s first focus on the bias, \\(\\bar{h} - f(x)\\). The average \\(\\bar{h}\\) comes from a class of hypotheses (e.g. constant models, linear models, etc.) In other words, \\(\\bar{h}\\) is a prototypical example of a certain class of hypotheses. The bias term thus can be interpreted as a measure of how well a particlar type of hypothesis \\(\\mathcal{H}\\) (e.g. constant model, linear model, quadratic model, etc.) approximates the target function \\(f\\). Variance Let’s focus on the variance, \\(\\mathbb{E}_\\mathcal{D} \\left[ (h^{(\\mathcal{D})}(x) - \\bar{g} )^2 \\right ]\\). This is a measure of how close a particular \\(h^{(\\mathcal{D})}(x)\\) can get to the average functional form \\(\\bar{h}\\). Put ir in other terms, how precise is our particular function compared to the average function? Tradeoff Ideally, a model should have small variance and small bias. But, of course, there is a tradeoff between these two (hence the Bias-Variance tradeoff). That is, decreasing bias tends to result in increases variance, and vice-versa. To decrease bias, a model has to be more flexible/complex. How complex? In theory, to decrease bias, one needs “insider” information. That is, to truly decrease bias, we need some information on the form of \\(f\\). Hence, it is nearly impossible to decrease bias. We therefore put our efforts towards decreasing variance. One way to decrease variance would be to add more training data. However, there is a price to pay: we will have less test data. We could reduce the dimensions of our data (i.e. play with lower-rank data matrices through, for example, PCA). We could also apply regularization. At its core, this notion relates to making the size of a model’s parameters small. Consequently, there is a reduction in the variance of a model. "],
["validation.html", "10 Validation 10.1 Model Assessment 10.2 Holdout Method 10.3 Repeated Holdout Method 10.4 Bootstrap Method 10.5 Model Selection 10.6 Cross-Validation", " 10 Validation In this chapter we discuss the topic of validation. We cover how to measure the predictive performance of a model (i.e. model assessment). Also, how to select a model between a set of candidate models (i.e. model selection). Likewise, how to make the most of a limitted data (resampling techniques). 10.1 Model Assessment Model Assessment has to do with: How can we estimate \\(E_{out}\\)? 10.2 Holdout Method Let us start with a theoretical approach: the so-called Holdout Method. In practice, we start with some available data \\(\\mathcal{D} = (\\mathbf{x_1}, y_1), \\dots, (\\mathbf{x_n}, y_n)\\). We then split \\(\\mathcal{D}\\) into two subsets: (1) the training set \\(\\mathcal{D}_{\\mathrm{train}}\\), and (2) the test set \\(\\mathcal{D}_{\\mathrm{test}}\\). How do we actually obtain these two sets? Usually by taking random samples, without replacement, from \\(\\mathcal{D}\\). \\[ \\mathcal{D} \\to \\begin{cases} \\text{training } \\mathcal{D}_{train} &amp; \\to \\text{size } n - k \\\\ \\text{test } \\mathcal{D}_{test} &amp; \\to \\text{size } k \\\\ \\end{cases} \\] We then fit a particular model using \\(\\mathcal{D}_{train}\\); call this model \\(h^{-}(x)\\), “\\(h\\)-minus”, because it is a model fitted with the training \\(\\mathcal{D}_{train}\\), which is a subset of the available data \\(\\mathcal{D}\\). Using \\(\\mathcal{D}_{test}\\), we can measure the performance of the model: \\[ E_{\\mathrm{test}}(h^{-}) = \\frac{1}{k} \\sum_{i=1}^{k} err_i\\left( h^{-}(x_i) , y_i \\right) ; \\hspace{5mm} (x_i, y_i) \\in \\mathcal{D}_{test} \\] \\(E_{\\mathrm{test}}\\) will give (hopefully) a good estimate of \\(E_{out}\\). However, the model that you ultimately deliver will not be \\(h^{-}\\); rather, you need to refit \\(h^{-}\\) using the entire data \\(\\mathcal{D}\\). This yields the final hypothesis model \\(\\widehat{f}\\). Holdout Algorithm Here’s the conceptual algorithm for the holdout method: Compile the available data into a set \\(\\mathcal{D} = \\{(\\mathbf{x_1}, y_1), \\dots, (\\mathbf{x_n}, y_n) \\}\\). Choose \\(k \\in \\mathbb{Z}^{+}\\) elements from \\(\\mathcal{D}\\) to comprise a test set \\(\\mathcal{D}_{test}\\), and place the remaining \\(n - k\\) points into the training set \\(\\mathcal{D}_{train}\\). Use \\(\\mathcal{D}_{train}\\) to fit a particular model \\(h^{-}(x)\\). Measure the performance of \\(h^{-}\\) using \\(\\mathcal{D}_{test}\\); specifically, compute \\[ E_{test}(h^{-}) = \\frac{1}{k} \\sum_{i=1}^{k} err_i[ h^{-}(\\mathbf{x_i}), y_i ] \\] where \\((\\mathbf{x_i}, y_i) \\in \\mathcal{D}_{test}\\), and \\(err_i\\) is some measure of pointwise error. Generate the final model \\(\\widehat{f}\\) by refitting \\(h^{-}\\) to the entire dataset \\(\\mathcal{D}\\). There are many different conventions as to how to pick \\(k\\): one common rule-of-thumb is to assign 80% of your data to the training set, and the remaining 20% to the test set. 10.2.1 Rationale Behind Holdout Method Suppose we have only one test point \\((x_0, y_0)\\). In a regression context, we would have \\[ err_0 \\big( h^{-}(x_0), y_0 \\big) = \\big( h^{-}(x_0) - y_0 \\big) ^2 \\] If we took the expectation of this over all \\(\\mathcal{X}\\), we should obtain \\(E_{out}\\): \\[ \\mathbb{E}_{\\mathcal{X}} [err_0] = E_{out} ; \\hspace{5mm} Var(err_0) = s_{e}^2 \\] Now, having only one test point is quite limiting. If we instead take \\(k\\) test points, then \\[ \\frac{1}{k} \\sum_{i=1}^{k} \\left[ h^{-}(x_i) - y_i \\right]^2 = E_{test} \\] We still have that \\(\\mathbb{E}[E_{test}] = E_{out}\\). Now, however, what about the variance? Let’s assume that the errors across points are independent (this may not be the case in practice, but we make this assumption to ease computation): \\[ Var(E_{\\mathrm{test}}) = \\frac{1}{k^2} \\sum_{i=1}^{k} Var(err_i) = \\frac{1}{k^2} (k \\cdot s_{e}^2) = \\frac{s_{e}^2 }{k} \\] Hence, the larger the test set, the smaller the variance of \\(E_{test}\\) will be. Of course, \\(k\\) is not freely selectable; the larger \\(k\\) is, the smaller our training dataset will be. 10.3 Repeated Holdout Method Our next validation procedure is the “Repeated Holdout Method”, also known as Monte-Carlo Cross-Validation. Repeated Holdout Algorithm Here’s the conceptual algorithm for the repeated holdout method: Compile the available data into a set \\(\\mathcal{D} = \\{(\\mathbf{x_1}, y_1), \\dots, (\\mathbf{x_n}, y_n) \\}\\). Repeat the Holdout method \\(B\\) times, where \\(B\\) is a very large integer (for example, 500). At each iteration, obtain the following elements: \\(\\mathcal{D}^{b}_{train}\\), the \\(b\\)-th training set. \\(h^{-}_{b}\\), the \\(b\\)-th fitted model (fitted to \\(\\mathcal{D}_{train}^{b}\\)) \\(E_{test}^{b} = \\frac{1}{k} \\sum_{i=1}^{n} err_i[ h^{-}_{b}(\\mathbf{x_j}), y_j]\\) where \\((\\mathbf{x_i}, y_i) \\in \\mathcal{D}_{test}^{b}\\) Obtain an overall value for \\(E_{test}\\) by averaging the \\(E_{test}^{b}\\) values: \\[ E_{test} = \\frac{1}{B} \\sum_{b=1}^{B} E_{test}^{b} \\] 10.4 Bootstrap Method Another interesting validation procedure is the “Bootstrap Method”. Bootstrap Algorithm Here’s the conceptual algorithm for the repeated holdout method: Compile the available data into a set \\(\\mathcal{D} = \\{(\\mathbf{x_1}, y_1), \\dots, (\\mathbf{x_n}, y_n) \\}\\). Repeat the following steps for \\(b = 1, 2, \\dots B\\) where \\(B\\) is a very large integer (for example, 500): Generate \\(\\mathcal{D}_{train}^{b}\\) by sampling \\(n\\) times with replacement from \\(\\mathcal{D}\\). (Note that \\(\\mathcal{D}_{train}^{b}\\) could contain repeated elements). Generate \\(\\mathcal{D}_{test}^{b}\\) by combining all of the elements in \\(\\mathcal{D}\\) that were not captured in \\(\\mathcal{D}_{train}\\). Hence, the size of \\(\\mathcal{D}_{test}\\) will change at each iteration. (Note that \\(\\mathcal{D}_{train}\\) should not contain repeated values). Generate \\(h^{-}_b\\) by fitting a model to \\(\\mathcal{D}_{train}^{b}\\). Compute \\(E_{test}^b = \\frac{1}{k_b} \\sum_{i=1}^{n} err_i \\big( h^{-}_i(\\mathbf{x_i}), y_i \\big)\\) where \\((\\mathbf{x_i}, y_i) \\in \\mathcal{D}_{test}^b\\) and \\(k_b\\) is the size of \\(\\mathcal{D}_{test}^b\\). Obtain an overall value for \\(E_{test}\\) by averaging the \\(E_{test}^b\\) values: \\[ E_{boot} = \\frac{1}{B} \\sum_{b=1}^{B} E_{test}^{b} \\] Bootstrapping enables us to construct confidence intervals for \\(E_{out}\\). We have that: \\[ \\mathrm{SE}_{boot} = \\sqrt{ \\frac{1}{B - 1} \\sum_{b=1}^{B} \\left(E_{test}^{b} - E_{boot} \\right)^2 } \\] Empirical quantiles can be used. 10.5 Model Selection Say we have \\(M\\) models to choose from. Consider, for example, the following three cases (each with \\(M = 3\\)): We could have three different types of hypotheses: \\(h_1:\\) linear model \\(h_2:\\) neural network \\(h_3:\\) regression tree Or we could also have a particular type of model, e.g. polynomials, with three different degrees: \\(h_1:\\) quadratic model \\(h_2:\\) cubic model \\(h_3:\\) 15th-degree model Or maybe a principal components regression with three options for the number of componenets to retain: \\(h_1:\\) PCR \\((c = 1)\\) \\(h_2:\\) PCR \\((c = 2)\\) \\(h_3:\\) PCR \\((c = 3)\\) The schematic for validation can be viewed in the following visual diagram: INSERT DIAGRAM Note that in the diagram, \\(\\mathcal{H}_m\\) represents the \\(m\\)-th hypothesis, and \\(h^{-}_m\\) represents the \\(m\\)-th fit to the \\(m\\)-th hypothesis. Why are we calling \\(\\mathcal{D}_{val}\\) a “validation” set, when it appears to be serving the same role as a “test” set? Indeed, ISL makes no distinction between the validation set and the test set (see, for instance, pages 176-177 in the textbook). However, in making this distinction explicit, we hope to make one point clear: choosing the “best” model (i.e. the model with the smallest \\(E_{val}\\)) is a learning decision. Had we stopped our procedure before making this choice (i.e. if we had stopped after considering \\(E_{val}^m\\) for \\(m = 1, 2, ..., M\\)), we could have plausibly called these errors “test errors.” However, we went one step further and as a result obtain a biased estimate of \\(E_{out}\\); namely, \\(E_{val}^{m^*}\\). 10.5.1 Three-way Holdout Method We can use the idea of validation in conjunction with the holdout method. Namely, instead of splitting \\(\\mathcal{D}\\) into two sets (training and test), we split it into three sets: training, validation, and test. \\[ \\mathcal{D} \\to \\begin{cases} \\mathcal{D}_{train} \\\\ \\mathcal{D}_{test} \\\\ \\mathcal{D}_{val} \\\\ \\end{cases} \\] We perform validation as above, then use \\(\\mathcal{D}_{train}\\) to get \\(E_{test}(h_{m}^{*})\\). Then finally refit the model \\(h_{m}\\) using the winning class (e.g. the \\(m\\)-th hypothesis) as well as the entire dataset. Now we have an unbiased estimate of error to report: \\(E_{test}(h_{m}^{*})\\). Of course, there is still a tradeoff: now we need to split our data into three! The larger our test and validations sets are, the smaller our training set will be. 10.6 Cross-Validation We will focus our attention to Leave-One Out CV. 10.6.1 Leave-One-Out Cross Validation (LOOCV) This is a particular case for \\(k\\)-fold CV. "],
["regular.html", "11 Regularization Techniques 11.1 Multicollinearity Issues 11.2 Irregular Coefficients 11.3 Regularization Metaphor", " 11 Regularization Techniques In this part of the book we will talk about the notion of regularization (what is regularization, what is the purpose of regularization, what approaches are used for regularization) all of this within the context of linear models. If you look at the dictionary definition of the term regularization you should find something like this: regularization; the act of bringing to uniformity; make (something) regular What is regularization? Simply put: Making things regular. In the context of regression, this means reducing the variability (and therefore the size) of the vector of predicted coefficients. What we have seen is that if there is multicollinearity, some of the elements of \\(\\mathbf{b}_{\\text{ols}}\\) will have high variance. In other words, \\(\\| \\mathbf{b}_{\\text{ols}} \\|_{\\ell_p}\\) will be very large (in this case, we say that the coefficients are irregular). In regularization, we try to mitigate the effects of these irregular coefficients. We will discuss two main types of approaches to achieve regularization: Dimension Reduction: Principal Components Regression (PCR), Partial Least Squares regression (PLSR) Penalized Methods: Ridge Regression, Lasso regression. 11.1 Multicollinearity Issues One of the issues when fitting regression models is due to multicollinearity: the condition that arises when two or more predictors are highly correlated. How does this affect OLS regression? When one or more predictors are linear combinations of other predictors, then \\(\\mathbf{X^\\mathsf{T} X}\\) is singular. This is known as exact collinearity. When this happends, there is no unique LS estimate \\(\\mathbf{b}\\). A more challenging problem arises when \\(\\mathbf{X^\\mathsf{T} X}\\) is close to singular but not exactly. This is usually referred to as near perfect collinearity or simply multicollinearity. Multicollinearity leads to imprecise (unstable) estimates \\(\\mathbf{b}\\). What are the typical causes of multicollinearity? One or more predictors are linear combinations of other predictors One or more predictors are almost perfect linear combinations of other predictors More predictors than observations \\(p &gt; n\\) 11.1.1 Toy Example Let’s play with mtcars, one of the built-in data sets in R. mtcars[1:10, ] #&gt; mpg cyl disp hp drat wt qsec vs am gear carb #&gt; Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 #&gt; Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 #&gt; Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 #&gt; Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 #&gt; Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 #&gt; Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 #&gt; Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 #&gt; Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 #&gt; Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 #&gt; Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 Let’s use mpg as response, and disp, hp, and wt as predictors. # response mpg &lt;- mtcars$mpg # predictors disp &lt;- mtcars$disp hp &lt;- mtcars$hp wt &lt;- mtcars$wt # standardized predictors X &lt;- scale(cbind(disp, hp, wt)) Let’s inspect the correlation matrix: \\[ \\mathbf{R} = \\frac{1}{n-1} \\mathbf{X^\\mathsf{T} X} \\] # correlation matrix cor(X) #&gt; disp hp wt #&gt; disp 1.0000000 0.7909486 0.8879799 #&gt; hp 0.7909486 1.0000000 0.6587479 #&gt; wt 0.8879799 0.6587479 1.0000000 If we look at the circle of correlations from a principal components analysis, we see the following pattern: Carrying out an ordinary least squares regression: #&gt; #&gt; Call: #&gt; lm(formula = mpg ~ disp + hp + wt) #&gt; #&gt; Coefficients: #&gt; (Intercept) disp hp wt #&gt; 37.105505 -0.000937 -0.031157 -3.800891 #&gt; #&gt; Call: #&gt; lm(formula = mpg ~ disp + hp + wt) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -3.891 -1.640 -0.172 1.061 5.861 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 37.105505 2.110815 17.579 &lt; 2e-16 *** #&gt; disp -0.000937 0.010350 -0.091 0.92851 #&gt; hp -0.031157 0.011436 -2.724 0.01097 * #&gt; wt -3.800891 1.066191 -3.565 0.00133 ** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 2.639 on 28 degrees of freedom #&gt; Multiple R-squared: 0.8268, Adjusted R-squared: 0.8083 #&gt; F-statistic: 44.57 on 3 and 28 DF, p-value: 8.65e-11 What about \\(\\mathbf{(X^\\mathsf{T} X)^{-1}}\\) #&gt; disp hp wt #&gt; disp 0.23627475 -0.08598357 -0.15316573 #&gt; hp -0.08598357 0.08827847 0.01819843 #&gt; wt -0.15316573 0.01819843 0.15627798 Let’s introduce exact collinearity. For example, let’s create a fourth variable disp1 that is a multiple of disp, and then let’s add this variable to the data matrix X. What happens if we try to compute the inverse of \\(\\mathbf{X^\\mathsf{T}X}\\)? disp1 &lt;- 10 * disp X1 &lt;- scale(cbind(disp, disp1, hp, wt)) solve(t(X1) %*% X1) #&gt; Error in solve.default(t(X1) %*% X1): system is computationally singular: reciprocal condition number = 1.55757e-17 As you can tell, R detected perfect collinearity, and the computation of the inverse shows that the matrix if singular. Let’s introduce near-exact collinearity set.seed(123) disp2 &lt;- disp + rnorm(length(disp)) X2 &lt;- scale(cbind(disp, disp2, hp, wt)) solve(t(X2) %*% X2) #&gt; disp disp2 hp wt #&gt; disp 588.167214 -590.721826 1.01055902 1.99383316 #&gt; disp2 -590.721826 593.525960 -1.10174784 -2.15719062 #&gt; hp 1.010559 -1.101748 0.09032362 0.02220277 #&gt; wt 1.993833 -2.157191 0.02220277 0.16411837 Let’s make things even more extreme! set.seed(123) disp3 &lt;- disp + rnorm(length(disp), mean = 0, sd = 0.1) X3 &lt;- scale(cbind(disp, disp3, hp, wt)) cor(disp, disp3) #&gt; [1] 0.9999997 And let’s introduce a minor, tiny modification, that in theory, would hardly have any effect: # small changes may have a &quot;butterfly&quot; effect disp31 &lt;- disp3 # change just one observation disp31[1] &lt;- disp3[1] * 1.01 X31 &lt;- scale(cbind(disp, disp31, hp, wt)) cor(disp, disp31) #&gt; [1] 0.9999973 When we calculate the inverse of \\(\\mathbf{X}_{31}^\\mathsf{T} \\mathbf{X}_{31}\\), something weird happens: solve(t(X3) %*% X3) #&gt; disp disp3 hp wt #&gt; disp 59175.36325 -59202.97211 10.91501090 21.38646548 #&gt; disp3 -59202.97211 59230.83035 -11.00617104 -21.54976679 #&gt; hp 10.91501 -11.00617 0.09032362 0.02220277 #&gt; wt 21.38647 -21.54977 0.02220277 0.16411837 solve(t(X31) %*% X31) #&gt; disp disp31 hp wt #&gt; disp 5941.5946101 -5942.3977358 0.30661752 0.64947961 #&gt; disp31 -5942.3977358 5943.4373181 -0.39266978 -0.80278577 #&gt; hp 0.3066175 -0.3926698 0.08830442 0.01825147 #&gt; wt 0.6494796 -0.8027858 0.01825147 0.15638642 This is what we may call a “butterfly effect.” By modifying just one cell in \\(\\mathbf{X}_{31}\\) by adding a little amount of random noise, the inverses \\((\\mathbf{X}_{3}^\\mathsf{T} \\mathbf{X}_{3})^{-1}\\) and \\(\\mathbf{X}_{31}^\\mathsf{T} \\mathbf{X}_{31}\\) have changed dramatically. 11.2 Irregular Coefficients In OLS regression, the theoretical variance-covariance matrix of the regression coefficients is given by: \\[ Var(\\mathbf{\\boldsymbol{\\hat{\\beta}}}) = \\ \\begin{bmatrix} Var(\\hat{\\beta}_1) &amp; Cov(\\hat{\\beta}_1, \\hat{\\beta}_2) &amp; \\cdots &amp; Cov(\\hat{\\beta}_1, \\hat{\\beta}_p) \\\\ Cov(\\hat{\\beta}_2, \\hat{\\beta}_1) &amp; Var(\\hat{\\beta}_2) &amp; \\cdots &amp; Cov(\\hat{\\beta}_2, \\hat{\\beta}_p) \\\\ \\vdots &amp; &amp; \\ddots &amp; \\vdots \\\\ Cov(\\hat{\\beta}_p, \\hat{\\beta}_1) &amp; Cov(\\hat{\\beta}_p, \\hat{\\beta}_2) &amp; \\cdots &amp; Var(\\hat{\\beta}_p) \\\\ \\end{bmatrix} \\] \\[ Var(\\mathbf{\\boldsymbol{\\hat{\\beta}}}) = \\sigma^2 (\\mathbf{X^\\mathsf{T} X})^{-1} \\] Doing some algebra, \\[ Var(\\boldsymbol{\\hat{\\beta}}) = \\sigma^2 (\\mathbf{X^\\mathsf{T} X})^{-1} \\] The variance of a particular coefficient \\(\\hat{\\beta}_j\\) is given by: \\[ Var(\\hat{\\beta}_j) = \\sigma^2 \\left [ (\\mathbf{X^\\mathsf{T} X})^{-1} \\right ]_{jj} \\] where \\(\\left [ (\\mathbf{X^\\mathsf{T} X})^{-1} \\right ]_{jj}\\) is the \\(j\\)-th diagonal element of \\((\\mathbf{X^\\mathsf{T} X})^{-1}\\) A couple of remarks: Recall again that we don’t know \\(\\sigma^2\\). How can we find an estimator \\(\\hat{\\sigma}^2\\)? We don’t observe the error terms \\(\\boldsymbol{\\varepsilon}\\) but we do have the residuals \\(\\mathbf{e = y - \\hat{y}}\\) As well as the Residual Sum of Squares (RSS) \\[ \\text{RSS} = \\sum_{i=1}^{n} e_{i}^{2} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 \\] 11.2.1 Effect of Multicollinearity Assuming standardized variables, \\(\\mathbf{X^\\mathsf{T} X} = n \\mathbf{R}\\) It can be shown that: \\[ Var(\\boldsymbol{\\hat{\\beta}}) = \\sigma^2 \\left ( \\frac{\\mathbf{R}^{-1}}{n} \\right ) \\] and \\(Var(\\hat{\\beta}_j)\\) can then be expressed as: \\[ Var(\\hat{\\beta}_j) = \\frac{\\sigma^2}{n} [\\mathbf{R}^{-1}]_{jj} \\] It turns out that: \\[ [\\mathbf{R}^{-1}]_{jj} = \\frac{1}{1 - R_{j}^{2}} \\] is known as the Variance Inflation Factor or VIF. If \\(R_{j}^{2}\\) is close to 1, then VIF will be large, and so \\(Var(\\hat{\\beta})\\) will also be large. If we write the eigenvalue decomposition of \\(\\mathbf{R}\\) as: \\[ \\mathbf{R = V \\boldsymbol{\\Lambda} V^\\mathsf{T}} \\] then the inverse of \\(\\mathbf{R}\\) becomes: \\[ \\mathbf{R^{-1} = V \\boldsymbol{\\Lambda}^{-1} V^\\mathsf{T}} \\] It can be shown that: \\[ Var(\\hat{\\beta}_j) = \\left ( \\frac{\\sigma^2}{n} \\right ) \\sum_{l=1}^{p} \\frac{v^{2}_{jl}}{\\lambda_l} \\] As you can tell, the variance of the estimators depends on the inverses of the eigenvalues of \\(\\mathbf{R}\\). With very small eigenvalues, the larger the variance of the estimates. In summary: the standard errors of \\(\\hat{\\beta}_j\\) are inflated. the fit is unstable, and becomes very sensitive to small perturbations. small changes in \\(Y\\) can lead to large changes in the coefficients. What would you do to overcome multicollinearity? Reduce number of predictors If \\(p &gt; n\\), then try to get more observations (increase \\(n\\)) Find an orthogonal basis for the predictors Impose constraints on the estimated coefficients A mix of some or all of the above? Other ideas? 11.3 Regularization Metaphor In OLS, we want to find parameters that give us a linear combination of our response variables. Think of this as if you go “shopping” for parameters. We assume (initially) that we have a blank check, and can pick whatever parameters we want. In regularization, we impose some sort of restriction on what coefficients we can buy. We could use penalized methods or we could use a slightly more lenient method: dimension reduction. In dimension reduction, there are no explicit penalties, but we still impose a “cost” on the regressors that are highly collinear. We reduce the number of parameters that we can buy. In penalized methods, we explicitly impose a budget on how much money we can spend when buying parameters. "],
["pcr.html", "12 Principal Components Regression 12.1 Motivation Example 12.2 The PCR Model 12.3 How does PCR work?", " 12 Principal Components Regression The first dimension reduction method that we will describe to regularize a model is Principal Components Regression (PCR). 12.1 Motivation Example To introduce PCR we are going to use a subset of the “2004 New Car and Truck Data” curated by Roger W. Johnson using records from Kiplinger’s Personal Finance. You can find more information about this data in the following url: http://jse.amstat.org/datasets/04cars.txt The data file, cars2004.csv, is available in the following github repository: https://github.com/allmodelsarewrong/data The data set consists of 10 variables measured on 385 cars. Here’s what the first six rows (and ten columns) look like: #&gt; price engine cyl hp city_mpg #&gt; Acura 3.5 RL 4dr 43755 3.5 6 225 18 #&gt; Acura 3.5 RL w/Navigation 4dr 46100 3.5 6 225 18 #&gt; Acura MDX 36945 3.5 6 265 17 #&gt; Acura NSX coupe 2dr manual S 89765 3.2 6 290 17 #&gt; Acura RSX Type S 2dr 23820 2.0 4 200 24 #&gt; Acura TL 4dr 33195 3.2 6 270 20 #&gt; hwy_mpg weight wheel length width #&gt; Acura 3.5 RL 4dr 24 3880 115 197 72 #&gt; Acura 3.5 RL w/Navigation 4dr 24 3893 115 197 72 #&gt; Acura MDX 23 4451 106 189 77 #&gt; Acura NSX coupe 2dr manual S 24 3153 100 174 71 #&gt; Acura RSX Type S 2dr 31 2778 101 172 68 #&gt; Acura TL 4dr 28 3575 108 186 72 In this example we take the variable price as the response, and the rest of the columns as input or predictor variables: engine cyl hp city_mpg hw_mpg weight wheel length width The regression model is: \\[ \\texttt{price} = b_0 + b_1 \\texttt{cyl} + b_2 \\texttt{hp} + \\dots + b_9 \\texttt{width} + \\boldsymbol{\\varepsilon} \\] For exploration purposes, let’s examine the matrix of correlations among all variables: #&gt; engine cyl hp city_mpg hwy_mpg weight wheel length width #&gt; price 0.6 0.654 0.836 -0.485 -0.469 0.476 0.204 0.210 0.314 #&gt; engine 0.912 0.778 -0.706 -0.708 0.812 0.631 0.624 0.727 #&gt; cyl 0.792 -0.670 -0.664 0.731 0.553 0.547 0.621 #&gt; hp -0.672 -0.652 0.631 0.396 0.381 0.500 #&gt; city_mpg 0.941 -0.736 -0.481 -0.468 -0.590 #&gt; hwy_mpg -0.789 -0.455 -0.390 -0.585 #&gt; weight 0.751 0.653 0.808 #&gt; wheel 0.867 0.760 #&gt; length 0.752 And let’s also take a look at the circle of correlations, from the output of a PCA on the entire data set: #&gt; Error in text.default(cars_pca$var$coord[1, 1], cars_pca$var$coord[1, : plot.new has not been called yet Computing the OLS solution for the regression model of price onto the other nine predictors we obtain: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 32536.025 17777.488 1.8302 6.802e-02 #&gt; engine -3273.053 1542.595 -2.1218 3.451e-02 #&gt; cyl 2520.927 896.202 2.8129 5.168e-03 #&gt; hp 246.595 13.201 18.6797 1.621e-55 #&gt; city_mpg -229.987 332.824 -0.6910 4.900e-01 #&gt; hwy_mpg 979.967 345.558 2.8359 4.817e-03 #&gt; weight 9.937 2.045 4.8584 1.741e-06 #&gt; wheel -695.392 172.896 -4.0220 6.980e-05 #&gt; length 33.690 89.660 0.3758 7.073e-01 #&gt; width -635.382 306.344 -2.0741 3.875e-02 Out of curiosity, let’s compare the correlations and the coefficients: #&gt; correlation coefficient #&gt; engine 0.5997873 -3273.05304 #&gt; cyl 0.6544123 2520.92691 #&gt; hp 0.8360930 246.59496 #&gt; city_mpg -0.4854130 -229.98735 #&gt; hwy_mpg -0.4694315 979.96656 #&gt; weight 0.4760867 9.93652 #&gt; wheel 0.2035464 -695.39157 #&gt; length 0.2096682 33.69009 #&gt; width 0.3135383 -635.38224 As you can tell from the above output, some correlation signs don’t match the signs of their corresponding regression coefficients. For example, engine is positively correlated with price but it turns out to have a negative regression coefficient. Or look at hwy_mpg which is negatively correlated with price but it has a positive regression coefficient. 12.2 The PCR Model In PCR, we seek principal components \\(\\mathbf{z_1}, \\dots, \\mathbf{z_k}\\), linear combinations of the inputs: \\(\\mathbf{z_k} = \\mathbf{Xv_k}\\). Figure 12.1: PCs as linear combinations of input variables If we retain all principal components, then we know that we can factorize the input matrix \\(\\mathbf{X}\\) as: \\[ \\mathbf{X} = \\mathbf{Z V^\\mathsf{T}} \\] where: \\(\\mathbf{Z}\\) is the matrix of principal components \\(\\mathbf{V}\\) is the matrix of loadings If we only keep a subset of \\(k &lt; p\\) PCs, then we have a decomposition of the data matrix into a signal part captured by \\(k\\) components, and a residual or noise part: \\[ \\underset{n \\times p}{\\mathbf{X}} = \\underset{n \\times k}{\\mathbf{Z}} \\hspace{1mm} \\underset{k \\times k}{\\mathbf{V^\\mathsf{T}}} + \\underset{n \\times k}{\\mathbf{E}} \\] Figure 12.2: Matrix diagram for inputs The idea is to use the components \\(\\mathbf{Z}\\) as predictors of \\(\\mathbf{y}\\). More specifically, the idea is to fit a linear regression in order to find coefficients \\(\\mathbf{b}\\): \\[ \\mathbf{y} = \\mathbf{Zb} + \\mathbf{e} \\] Figure 12.3: Matrix diagram for response Usually, you don’t use all \\(p\\) PCs, but just a few of them. In other words, if we only keep a subset of \\(k &lt; p\\) PCs, then the idea of PCR remains constant: use the \\(k\\) components in \\(\\mathbf{Z}\\) as predictors of \\(\\mathbf{y}\\): \\[ \\mathbf{\\hat{y}} = \\mathbf{Z b} \\] Without loss of generality suppose the predictors and response are standardized. In Principal Components Regression we regress \\(\\mathbf{y}\\) onto the PC’s: \\[ \\mathbf{\\hat{y}} = b_1 \\mathbf{z_1} + b \\mathbf{z_2} + \\dots + b_p \\mathbf{z_k} \\] The vector of PCR coefficients is obtained via ordinary least squares (OLS): \\[ \\mathbf{b} = \\mathbf{(Z^\\mathsf{T} Z)^{-1} Z^\\mathsf{T} y} \\] Using the cars2004 data set, with the PCA of the inputs, we can run a linear regression of price onto all nine PCs: #&gt; Regression coefficients for all PCs #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; PC1 -4470.761 205.1288 -21.7949 0.0000 #&gt; PC2 7608.419 468.4759 16.2408 0.0000 #&gt; PC3 -9650.324 660.1829 -14.6177 0.0000 #&gt; PC4 -1768.547 980.6487 -1.8034 0.0721 #&gt; PC5 10528.146 1115.0825 9.4416 0.0000 #&gt; PC6 -5593.736 1177.6700 -4.7498 0.0000 #&gt; PC7 -5746.452 1721.5725 -3.3379 0.0009 #&gt; PC8 -7606.196 1926.3769 -3.9484 0.0001 #&gt; PC9 5473.090 2660.3834 2.0573 0.0404 If we only take the first two PCs, then the regression coefficients are: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; PC1 -4470.761 205.1288 -21.7949 0 #&gt; PC2 7608.419 468.4759 16.2408 0 Likewise, if take only the first three PCs, then the regression coefficients are: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; PC1 -4470.761 205.1288 -21.7949 0 #&gt; PC2 7608.419 468.4759 16.2408 0 #&gt; PC3 -9650.324 660.1829 -14.6177 0 Because of uncorrelatedness, the contributions and estimated coefficient of a PC are unaffected by which other PCs are also included in the regression. 12.3 How does PCR work? Start with \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times p}\\), assuming standardized data. Then, perform PCA on \\(\\mathbf{X}\\): either a EVD of a multiple of \\(\\mathbf{X^\\mathsf{T} X}\\) (e.g. \\((n-1)^{-1} \\mathbf{X^\\mathsf{T} X}\\)), or the SVD of \\(\\mathbf{X} = \\mathbf{U D V^\\mathsf{T}}\\). In either case, \\(\\mathbf{X} = \\mathbf{Z V^\\mathsf{T}}\\) (the matrix of PC’s times the transpose of the matrix of loadings). From OLS, we have: \\[ \\mathbf{\\hat{y}} = \\mathbf{X (X^\\mathsf{T} X)^{-1} X^\\mathsf{T} y} = \\mathbf{Xb}_\\text{ols} \\] We can replace \\(\\mathbf{X}\\) by \\(\\mathbf{Z V^\\mathsf{T}}\\): \\[\\begin{align*} \\mathbf{\\hat{y}} &amp;= \\mathbf{X (X^\\mathsf{T} X)^{-1} X^\\mathsf{T} y} \\\\ &amp;= \\mathbf{Z V^\\mathsf{T}} \\left (\\mathbf{(Z V^\\mathsf{T})^\\mathsf{T} Z V^\\mathsf{T}} \\right )^{-1} \\mathbf{(Z V^\\mathsf{T})^\\mathsf{T} y} \\\\ &amp;= \\mathbf{Z V^\\mathsf{T}} \\left (\\mathbf{V Z^\\mathsf{T} Z V^\\mathsf{T}} \\right )^{-1} \\mathbf{V Z^\\mathsf{T} y} \\\\ &amp;= \\mathbf{Z V^\\mathsf{T}} (\\mathbf{V \\Lambda V^\\mathsf{T}})^{-1} \\mathbf{V Z^\\mathsf{T} y} \\\\ &amp;= \\mathbf{Z V^\\mathsf{T}} (\\mathbf{V} \\mathbf{\\Lambda}^{-1} \\mathbf{V^\\mathsf{T}}) \\mathbf{V Z^\\mathsf{T} y} \\\\ &amp;= \\mathbf{Z} \\mathbf{\\Lambda}^{-1} \\mathbf{Z^\\mathsf{T} y} \\\\ &amp;= \\mathbf{Zb}_\\text{pcr} \\end{align*}\\] 12.3.1 Transition Formula In PCR, if \\(k = p\\) (and assuming \\(\\mathbf{X}\\) is of full-rank), which means that if you keep all PC’s, what happens is: \\[\\begin{align*} \\mathbf{\\hat{y}} &amp;= \\mathbf{X} \\mathbf{b}_{\\text{ols}} \\\\ &amp;= \\mathbf{Z V^\\mathsf{T}} \\mathbf{b}_{\\text{ols}} \\\\ &amp;= \\mathbf{Z} \\mathbf{b}_{\\text{pcr}} \\end{align*}\\] We can reexpress the PCR coefficients in terms of the original variables. \\[\\begin{align*} \\mathbf{\\hat{y}} &amp;= \\mathbf{Z} \\mathbf{b}_{\\text{pcr}} \\\\ &amp;= \\mathbf{X V} \\mathbf{b}_{\\text{pcr}} \\\\ &amp;= \\mathbf{X} \\mathbf{b}_{\\text{ols}} \\end{align*}\\] In summary, we can go back and forth between regression coefficients for PC scores, and regression coefficients for original input features: \\(\\mathbf{\\hat{y}} = \\mathbf{X} \\mathbf{b}_\\text{ols} = \\mathbf{Z V^\\mathsf{T}} \\mathbf{b}_\\text{ols} = \\mathbf{Z b}_\\text{pcr}\\). \\(\\mathbf{\\hat{y}} = \\mathbf{Z} \\mathbf{b}_\\text{pcr} = \\mathbf{X V} \\mathbf{b}_\\text{pcr} = \\mathbf{X b}_\\text{ols}\\). The following output shows the regression coefficients of all nine regression equations in terms of the original input variables: #&gt; Z_1:1 Z_1:2 Z_1:3 Z_1:4 Z_1:5 Z_1:6 Z_1:7 Z_1:8 Z_1:9 #&gt; engine 1641 2345 5487 5203 1887 1064 2168.5 -3551 -3327 #&gt; cyl 1544 2881 7312 7289 2213 1401 -295.5 3856 3766 #&gt; hp 1377 4148 8402 8755 16016 17348 17585.7 17700 17352 #&gt; city_mpg -1487 -3864 459 -149 -470 999 1891.3 2151 -1213 #&gt; hwy_mpg -1472 -4140 583 516 1633 1468 1477.9 1607 5536 #&gt; weight 1642 1261 -510 -1187 -2266 707 3557.7 5507 7033 #&gt; wheel 1385 -2392 -2860 -2422 -3009 -187 -3315.7 -4832 -4938 #&gt; length 1332 -2634 -2202 -1346 -883 -2778 28.2 1260 447 #&gt; width 1501 -738 -1731 -2811 1464 -904 -2413.8 -1978 -2142 Obviously, if you keep all components, you aren’t changing anything: you’re spending the same amount of money as you were in regular least-squares regression. #&gt; engine cyl hp city_mpg hwy_mpg weight wheel #&gt; -3326.7904 3766.1495 17352.2185 -1213.0142 5535.9355 7032.5383 -4937.8106 #&gt; length width #&gt; 446.9752 -2141.8196 compare with the regression coefficients of OLS: #&gt; Xengine Xcyl Xhp Xcity_mpg Xhwy_mpg Xweight Xwheel #&gt; -3326.7904 3766.1495 17352.2185 -1213.0142 5535.9355 7032.5383 -4937.8106 #&gt; Xlength Xwidth #&gt; 446.9752 -2141.8196 The idea is to keep only a few components. Hence, the goal is to find \\(k\\) principal components (with \\(k \\ll p\\); \\(k\\) is called the tuning parameter or the hyperparameter). How do we determine \\(k\\)? The typical way is to use cross-validation. 12.3.2 Size of Coefficients Let’s look at the evolution of the PCA regression coefficients. This is a very interesting plot that allows us to see how the size of the coefficients grow as we add more and more PCs into the regression equation: Remarks The catch: those PC’s you choose to keep may not be good predictors of \\(\\mathbf{y}\\). That is, there is a chance that some of those PC’s you discarded actually capture a fair amount of the signal. Unfortunately, there is no way of knowing this in a real-life setting. Partial Least Squares was developed as a cure for this, which is the topic of the next chapter. "],
["pls.html", "13 Partial Least Squares Regression 13.1 Motivation Example 13.2 The PLSR Model 13.3 How does PLSR work? 13.4 PLSR Algorithm", " 13 Partial Least Squares Regression Another dimension reduction method that we can use to regularize a model is Partial Least Squares Regression (PLSR). Before we dive deep into the nuts and bolts of PLSR, we should let you know that PLS methods form a very big family of methods. While the regression method is probably the most popular PLS technique, it is by no means the only one. And even within PLSR, there’s a wide array of different algorithms that let you obtain the core solution. PLS Regression was mainly developed in the early 1980s by Scandinavian chemometricians Svante Wold and Harald Martens. The theoretical background was based on the PLS Modeling framework of Herman Wold (Svante’s father). PLS Regression was developed as an algorithmic solution, with no optimization criterion explicitly defined. It was introduced in the fields of chemometrics (where almost immediately became a hit). It slowly attracted the attention of curious applied statisticians. It took a couple of years (late 1980s - early 1990s) for applied mathematicians to discover its properties. Nowadays there are several versions (flavors) of the algorithm to compute a PLS regression. 13.1 Motivation Example To introduce PLSR we are using the same data set of the previous chapter: a subset of the “2004 New Car and Truck Data”. The data set consists of 10 variables measured on 385 cars. Here’s what the first six rows look like: #&gt; price engine cyl hp city_mpg #&gt; Acura 3.5 RL 4dr 43755 3.5 6 225 18 #&gt; Acura 3.5 RL w/Navigation 4dr 46100 3.5 6 225 18 #&gt; Acura MDX 36945 3.5 6 265 17 #&gt; Acura NSX coupe 2dr manual S 89765 3.2 6 290 17 #&gt; Acura RSX Type S 2dr 23820 2.0 4 200 24 #&gt; Acura TL 4dr 33195 3.2 6 270 20 #&gt; hwy_mpg weight wheel length width #&gt; Acura 3.5 RL 4dr 24 3880 115 197 72 #&gt; Acura 3.5 RL w/Navigation 4dr 24 3893 115 197 72 #&gt; Acura MDX 23 4451 106 189 77 #&gt; Acura NSX coupe 2dr manual S 24 3153 100 174 71 #&gt; Acura RSX Type S 2dr 31 2778 101 172 68 #&gt; Acura TL 4dr 28 3575 108 186 72 The response variable is price, and there are nine predictors: engine cyl hp city_mpg hw_mpg weight wheel length width The regression model involves predicting price in terms of the nine inputs: \\(\\texttt{price} = b_0 + b_1 \\texttt{cyl} + b_2 \\texttt{hp} + \\dots + b_9 \\texttt{width} + \\boldsymbol{\\varepsilon}\\) Computing the OLS solution for the regression model of price onto the other nine predictors we obtain the following coefficients (see first column): #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 32536.025 17777.488 1.8302 6.802e-02 #&gt; engine -3273.053 1542.595 -2.1218 3.451e-02 #&gt; cyl 2520.927 896.202 2.8129 5.168e-03 #&gt; hp 246.595 13.201 18.6797 1.621e-55 #&gt; city_mpg -229.987 332.824 -0.6910 4.900e-01 #&gt; hwy_mpg 979.967 345.558 2.8359 4.817e-03 #&gt; weight 9.937 2.045 4.8584 1.741e-06 #&gt; wheel -695.392 172.896 -4.0220 6.980e-05 #&gt; length 33.690 89.660 0.3758 7.073e-01 #&gt; width -635.382 306.344 -2.0741 3.875e-02 13.2 The PLSR Model In PLS regression, like in PCR, we seek components \\(\\mathbf{z_1}, \\dots, \\mathbf{z_k}\\), linear combinations of the inputs (e.g. \\(\\mathbf{z_1} = \\mathbf{Xw_1}\\)), such that they are good predictors for both the response \\(\\mathbf{y}\\) as well as the inputs \\(\\mathbf{x_j}\\) for \\(j = 1,\\dots, p\\). Moreover, there is an implicit assupmtion in the PLS regression model: both \\(\\mathbf{X}\\) and \\(\\mathbf{y}\\) are assumed to be functions of a reduced (\\(k &lt; p\\)) number of components \\(\\mathbf{Z} = [\\mathbf{z_1}, \\dots, \\mathbf{z_k}]\\) that can be used to decompone the inputs and the response: \\[ \\mathbf{X} = \\mathbf{Z V^\\mathsf{T}} + \\mathbf{E} \\] Figure 13.1: Matrix diagram for inputs and \\[ \\mathbf{y} = \\mathbf{Z b} + \\mathbf{e} \\] Figure 13.2: Matrix diagram for response where: \\(\\mathbf{Z}\\) is a matrix of PLS components \\(\\mathbf{V}\\) is a matrix of PLS loadings \\(\\mathbf{E}\\) is a matrix of \\(X\\)-residuals \\(\\mathbf{b}\\) is a vector of PLS regression coefficients \\(\\mathbf{e}\\) is a vector of \\(y\\)-residuals By taking a few number of components \\(\\mathbf{Z} = [\\mathbf{z_1}, \\dots, \\mathbf{z_k}]\\), we can have approximations for the inputs and the response. The model for the \\(X\\)-space is: \\[ \\mathbf{\\hat{X}} = \\mathbf{Z V^{\\mathsf{T}}} \\] In turn, the prediction model for \\(\\mathbf{y}\\) is given by: \\[ \\mathbf{\\hat{y}} = \\mathbf{Z b} \\] 13.3 How does PLSR work? So how do we obtain the PLS components, aka PLS scores? We use an iterative algorithm, obtaining one component at a time. Here’s how to get the first PLS component \\(\\mathbf{z_1}\\). Assume that both the inputs and the response are mean-centered (and possibly standardized). We compute the covariances between all the input variables and the response: \\(\\mathbf{\\tilde{w}_1} = (cov(\\mathbf{x_1}, \\mathbf{y}), \\dots, cov(\\mathbf{x_p}, \\mathbf{y}))\\). Figure 13.3: Towards the first PLS component. In vector-matrix notation we have: \\[ \\mathbf{\\tilde{w}_1} = \\mathbf{X^\\mathsf{T}y} \\] For convenience purposes, we normalize the vector of covariances \\(\\mathbf{\\tilde{w}_1}\\), which gives us a unit-vector \\(\\mathbf{w_1}\\): \\[ \\mathbf{w_1} = \\frac{\\mathbf{\\tilde{w}_1}}{\\|\\mathbf{\\tilde{w}_1}\\|} \\] #&gt; first PLS weight #&gt; [,1] #&gt; engine 0.001782118 #&gt; cyl 0.002857956 #&gt; hp 0.171985612 #&gt; city_mpg -0.007484109 #&gt; hwy_mpg -0.007752089 #&gt; weight 0.984987298 #&gt; wheel 0.004225081 #&gt; length 0.008131684 #&gt; width 0.003089621 We use these weights to compute the first component \\(\\mathbf{z_1}\\) as a linear combination of the inputs: \\(\\mathbf{z_1} = w_{11} \\mathbf{x_1} + \\dots + w_{p1} \\mathbf{x_p}\\), Figure 13.4: First PLS component. or in vector-matrix notation: \\[ \\mathbf{z_1} = \\mathbf{Xw_1} \\] Here are the first 10 elements of \\(\\mathbf{z_1}\\) #&gt; first PLS component (10 elements only) #&gt; [,1] #&gt; Acura 3.5 RL 4dr 344.24572 #&gt; Acura 3.5 RL w/Navigation 4dr 357.05055 #&gt; Acura MDX 913.48050 #&gt; Acura NSX coupe 2dr manual S -360.90753 #&gt; Acura RSX Type S 2dr -745.89228 #&gt; Acura TL 4dr 51.39841 #&gt; Acura TSX 4dr -300.53740 #&gt; Audi A4 1.8T 4dr -284.07748 #&gt; Audi A4 3.0 4dr -68.58479 #&gt; Audi A4 3.0 convertible 2dr 278.15085 We use this component to regress both the inputs and the response onto it. The regression coefficient \\(v_{1j}\\) of each simple linear regression between \\(\\mathbf{z_1}\\) and the \\(j\\)-th predictor is called a PLS loading. The entire vector of loadings \\(\\mathbf{v_1}\\) associated to first component is: \\[ \\mathbf{v_1} = \\mathbf{X^\\mathsf{T}z_1} / \\mathbf{z_{1}^\\mathsf{T}z_1} \\] #&gt; first PLS loading #&gt; [,1] #&gt; engine 0.001176718 #&gt; cyl 0.001561745 #&gt; hp 0.064016991 #&gt; city_mpg -0.005536001 #&gt; hwy_mpg -0.006343509 #&gt; weight 1.003819205 #&gt; wheel 0.007551534 #&gt; length 0.012276141 #&gt; width 0.003862309 In turn, the regression coefficient \\(b_1\\) obtained by regressing the response onto the first component is: \\[ b_1 = \\mathbf{y^\\mathsf{T} z_1} / \\mathbf{z_{1}^\\mathsf{T}z_1} \\] In our example, the regression coefficient \\(b_1\\) with first PLS score \\(\\mathbf{z_1}\\) is: #&gt; first PLS regression coefficient #&gt; [1] 13.61137 We can get a first one-rank approximation \\(\\mathbf{\\hat{X}} = \\mathbf{z_1 v^{\\mathsf{T}}_1}\\), and then obtain a residual matrix \\(\\mathbf{X_1}\\) by removing the variation in the inputs captured by \\(\\mathbf{z_1}\\): \\[ \\mathbf{X_1} = \\mathbf{X} - \\mathbf{z_1 v^{\\mathsf{T}}_1} \\qquad \\mathsf{(deflation)} \\] We can do the same with the response: \\[ \\mathbf{y_1} = \\mathbf{y} - b_1 \\mathbf{z_1} \\] We can obtain further PLS components \\(\\mathbf{z_2}\\), \\(\\mathbf{z_3}\\), etc. by repeating the process described above on the residual data matrices \\(\\mathbf{X_1}\\), \\(\\mathbf{X_2}\\), etc., and the residual response vectors \\(\\mathbf{y_1}\\), \\(\\mathbf{y_2}\\), etc. For example, a second PLS score \\(\\mathbf{z_2}\\) will be formed by a linear combination of first \\(X\\)-residuals: Figure 13.5: Second PLS component. 13.4 PLSR Algorithm We will describe what we consider the “standard” algorithm. However, keep in mind that there are a handful of slightly different versions that may find in other places. We assume mean-centered and standardized variables \\(\\mathbf{X}\\) and \\(\\mathbf{y}\\). We start by setting \\(\\mathbf{X_0} = \\mathbf{X}\\), and \\(\\mathbf{y_0} = \\mathbf{y}\\). Repeat for \\(h = 1, \\dots, r = \\text{rank}(\\mathbf{X})\\): Start with weights \\(\\mathbf{\\tilde{w}_h} = \\mathbf{X_{h-1}^\\mathsf{T}} \\mathbf{y_{h-1}} / \\mathbf{y_{h-1}}^\\mathsf{T} \\mathbf{y_{h-1}}\\) Normalize weights: \\(\\mathbf{w_h} = \\mathbf{\\tilde{w}_h} / \\| \\mathbf{\\tilde{w}_h} \\|\\) Compute PLS component: \\(\\mathbf{z_h} = \\mathbf{X_{h-1} w_h} / \\mathbf{w_{h}^\\mathsf{T} w_h}\\) Regress \\(\\mathbf{y_h}\\) onto \\(\\mathbf{z_h}\\): \\(b_h = \\mathbf{y_{h}^{\\mathsf{T}} z_h / z_{h}^{\\mathsf{T}} z_h}\\) Regress \\(\\mathbf{x_j}\\) onto \\(\\mathbf{z_h}\\): \\(\\mathbf{v_h} = \\mathbf{X_{h-1}}^{\\mathsf{T}} \\mathbf{z_h} / \\mathbf{z_{h}}^{\\mathsf{T}} \\mathbf{z_h}\\) Deflate (residual) predictors: \\(\\mathbf{X_h} = \\mathbf{X_{h-1}} - \\mathbf{z_h} \\mathbf{v_{h}^{\\mathsf{T}}}\\) Deflate (residual) response: \\(\\mathbf{y_h} = \\mathbf{y_{h-1}} - b_h \\mathbf{z_h}\\) Notice that steps 1, 3, 4, and 5 involve simple OLS regressions. If you think about it, what PLS is doing is calculating all the different ingredients (e.g. \\(\\mathbf{w_h}, \\mathbf{z_h}, \\mathbf{v_h}, b_h\\)) separately, using least squares regressions. Hence the reason for its name partial least squares. By default, we can obtain up to \\(r = rank(\\mathbf{X})\\) different PLS components. In the cars example, we can actually obtain nine scores. The following output shows the regression coefficients of all nine regression equations: #&gt; Z_1:1 Z_1:2 Z_1:3 Z_1:4 Z_1:5 Z_1:6 Z_1:7 Z_1:8 #&gt; engine 0.0243 1.44 -3.34 -15.09 -33.59 -113.70 -284.84 -1148.41 #&gt; cyl 0.0389 3.03 6.87 55.93 166.51 471.47 1056.23 2073.22 #&gt; hp 2.3410 250.04 248.74 262.63 254.81 251.35 243.73 238.81 #&gt; city_mpg -0.1019 -4.69 50.66 368.94 210.79 -69.52 -412.43 -171.42 #&gt; hwy_mpg -0.1055 -3.49 48.59 464.50 528.56 811.07 1177.28 933.19 #&gt; weight 13.4070 -2.27 1.80 6.44 8.21 9.61 9.77 9.08 #&gt; wheel 0.0575 -7.32 -125.75 -387.68 -797.42 -669.88 -680.47 -676.98 #&gt; length 0.1107 -9.00 -196.71 -90.85 83.57 59.30 2.26 17.07 #&gt; width 0.0421 -1.61 -43.73 -181.27 -427.09 -940.70 -729.49 -725.37 #&gt; Z_1:9 #&gt; engine -3273.05 #&gt; cyl 2520.93 #&gt; hp 246.59 #&gt; city_mpg -229.99 #&gt; hwy_mpg 979.97 #&gt; weight 9.94 #&gt; wheel -695.39 #&gt; length 33.69 #&gt; width -635.38 If we keep all components, we get the OLS solution (using standardized data): #&gt; Regression coefficients using all 9 PLS components #&gt; engine cyl hp city_mpg hwy_mpg weight #&gt; -3273.05304 2520.92691 246.59496 -229.98735 979.96656 9.93652 #&gt; wheel length width #&gt; -695.39157 33.69009 -635.38224 Compare with the regression coefficients of OLS: #&gt; Xengine Xcyl Xhp Xcity_mpg Xhwy_mpg Xweight Xwheel #&gt; -3326.7904 3766.1495 17352.2185 -1213.0142 5535.9355 7032.5383 -4937.8106 #&gt; Xlength Xwidth #&gt; 446.9752 -2141.8196 Obviously, reatining all PLS scores does not provide an improvement over the OLS regression. This is because we are not changing anything. Using our metaphor about “shopping for coefficients”, by keeping all PLS scores we are spending the same amount of money that we spend in ordinary least-squares regression. The dimension reduction idea involves keeping only a few components \\(k \\ll r\\), such that the fitted model has a better generalization ability than the full model. The number of components \\(k\\) is called the tuning parameter or hyperparameter. How do we determine \\(k\\)? The typical way is to use cross-validation. 13.4.1 PLS Solution with original variables The PLS regression equation is typically expressed in terms of the original variables \\(\\mathbf{X}\\) instead of using the deflated matrices \\(\\mathbf{X_{h-1}}\\). This re-arrangement is more convenient for prediction purposes. \\[ \\mathbf{y = X b}_{\\text{pls}} + \\mathbf{e} \\] Note that the PLS \\(\\mathbf{b}_{\\text{pls}}\\)-coefficients of the regression equation are not parameters of the PLS regression model. Instead, these are post-hoc calculations for making things more maneagable. Transforming Regression Coefficients PLS components are obtained as linear combinations of residual matrices \\(\\mathbf{X_h}\\), but they can also be expressed in terms of the original variables: \\[\\begin{align*} \\mathbf{z_1} &amp;= \\mathbf{X w_1 = X \\overset{*}{w}_{1}} \\\\ \\mathbf{z_2} &amp;= \\mathbf{X_1 w_2 = X (I - z_1 v_{1}^{\\mathsf{T}}) w_2 = X \\overset{*}{w}_{2}} \\\\ \\mathbf{z_3} &amp;= \\mathbf{X_2 w_3 = X (I - z_2 v_{2}^{\\mathsf{T}}) w_3 = X \\overset{*}{w}_{3}} \\\\ &amp; \\vdots \\\\ \\mathbf{z_h} &amp;= \\mathbf{X_{h-1} w_h = X (I - z_{h-1} v_{h-1}^{\\mathsf{T}}) w_h = X \\overset{*}{w}_{h}} \\\\ \\end{align*}\\] Consequently, \\(\\mathbf{Z_h} = [\\mathbf{z_1}, \\mathbf{z_2}, \\dots, \\mathbf{z_h}] = \\mathbf{X \\overset{*}{W}_{h}}\\) We know that \\(\\mathbf{\\hat{y}} = b_1 \\mathbf{z_1} + \\dots + b_h \\mathbf{z_h} = \\mathbf{Z b}\\). And because the PLS components can be expressed in terms of the original variables, we can conveniently reexpress the solution in terms of the original predictors: \\[ \\mathbf{\\hat{y}} = \\mathbf{Z b} = \\mathbf{X \\overset{*}{W} b} = \\mathbf{X \\overset{*}{b}}_\\text{pls} \\] where \\(\\overset{*}{\\mathbf{b}}_\\text{pls}\\) are the derived PLS-coefficients (not OLS). Note that these PLS star-coefficients of the regression equation are NOT parameters of the PLS regression model. Instead, these are post-hoc calculations for making things more maneagable. Figure 13.6: PLS components in terms of original inputs. Interestingly, if you retain all \\(h = rank(\\mathbf{X})\\) PLS components, the \\(\\overset{*}{\\mathbf{b}}_\\text{pls}\\) coefficients will be equal to the OLS coefficients. 13.4.2 Size of Coefficients Let’s look at the evolution of the PLS regression coefficients. This is a very interesting plot that allows us to see how the size of the coefficients grow as we add more and more PLS components into the regression equation: 13.4.3 Some Properties Some interesting properties of the different elements derived in PLS Regression: \\(\\mathbf{z_h^{\\mathsf{T}} z_l} = 0, \\quad l &gt; h\\) \\(\\mathbf{w_h^{\\mathsf{T}} p_h} = 1\\) \\(\\mathbf{w_h^{\\mathsf{T}} X_{l}^{\\mathsf{T}}} = 0, \\quad l \\geq h\\) \\(\\mathbf{w_h^{\\mathsf{T}} p_l} = 0, \\quad l &gt; h\\) \\(\\mathbf{w_h^{\\mathsf{T}} w_l} = 0, \\quad l &gt; h\\) \\(\\mathbf{z_h^{\\mathsf{T}} X_l} = 0, \\quad l \\geq h\\) \\(\\mathbf{X_h} = \\mathbf{X} \\prod_{j=1}^{p} (\\mathbf{I - w_j v_{j}^{\\mathsf{T}}}), \\quad h \\geq 1\\) Remarks PLS regression is somewhat close to Principal Components regression (PCR). Like PCR, PLSR involves projecting the response onto uncorrelated components (i.e. linear combinations of predictors). Unlike PCR, the way PLS components are extracted is by taking into account the response variable. We can conveniently reexpress the solution in terms of the original predictors. PLSR is not based on any optimization criterion. Rather it is based on an interative algorithm (which converges) Simplicity in its algorithm: no need to invert any matrix, no need to diagonalize any matrix. All you need to do is compute simple regressions. In other words, you just need inner products. Missing data is allowed (but you need to modify the algorithm). Easily extendable to the multivariate case of various responses. Handles cases where we have more predictors than observations (\\(p \\gg n\\)). "],
["ridge.html", "14 Ridge Regression", " 14 Ridge Regression From OLS: \\(\\mathbf{y} = \\mathbf{X} (\\mathbf{X^\\mathsf{T} X})^{-1} \\mathbf{X^\\mathsf{T} y}\\) (provided the inverse exists). If \\(\\mathbf{X^\\mathsf{T} X}\\) is near-singular, finding its inverse could lead to trouble. For example, near-perfect multicollinearity corresponds to small eigenvalues, which corresponds to large variance (of the corresponding estimated coefficients). Hence, a natural idea arises: let’s try to modify \\(\\mathbf{X^\\mathsf{T} X}\\) to avoid this danger of near-singularity. Specifically, let us append an extra term to \\(\\mathbf{X^\\mathsf{T} X}\\): that is, consider \\(\\mathbf{X^\\mathsf{T} X} + \\lambda \\mathbf{I}\\) . Note that \\(\\lambda\\) is not an eigenvalue here. Note that \\(\\mathbf{X^\\mathsf{T} X} + \\lambda \\mathbf{I}\\) and \\(\\mathbf{X^\\mathsf{T} X}\\) have the same eigenvectors; however, the eigenvalues of the former matrix will never be \\(0\\). Specifically, say \\(\\mathbf{X^\\mathsf{T} X}\\) has eigenvalues \\(\\lambda_1, \\lambda_2, \\dots, \\lambda_p\\). The eigenvalues of \\(\\mathbf{X^\\mathsf{T} X} + \\lambda \\mathbf{I}\\) will therefore be \\(\\overset{*}{\\lambda_1}, \\overset{*}{\\lambda_2}, \\dots, \\overset{*}{\\lambda_p}\\) where \\(\\overset{*}{\\lambda_j} = \\lambda_j + \\lambda\\). It turns out there is a minimization problem behind this method (of appending an extra term to the eigenvalues). In Ridge Regression, we have that \\[ \\mathbf{y}_{RR} = \\mathbf{X} (\\mathbf{X^\\mathsf{T} X} + \\lambda \\mathbf{I} )^{-1} \\mathbf{X^\\mathsf{T} y} \\] Using the MSE for \\(E_{in}[h(\\mathbf{x})]\\), we have \\(E_{in}[h(\\mathbf{x})] = \\frac{1}{n} (\\mathbf{Xb} - \\mathbf{y})^{\\mathsf{T}} (\\mathbf{Xb} - \\mathbf{y})\\). Let us examine the geometric perspective of \\(E_{in}\\) with respect to \\(\\mathbf{b}\\) (for simplicity, assume we have only two predictors \\(b_1\\) and \\(b_2\\)): INSERT DIAGRAM In OLS, we minimize \\(E_{in}\\) unconditionally; that is, without restriction. In Ridge Regression, we are minimizing \\(E_{in}(\\mathbf{b})\\) subject to an additional constraint: \\[ \\text{Ridge Regression} \\rightarrow \\min_{\\mathbf{b}} \\left\\{ \\frac{1}{n} (\\mathbf{Xb} - \\mathbf{y})^{\\mathsf{T}} (\\mathbf{Xb} - \\mathbf{y}) \\right\\} \\quad \\mathrm{s.t.} \\quad \\| \\mathbf{b} \\|_{2}^{2} = \\mathbf{b^\\mathsf{T} b} \\leq c \\] for some “budget” \\(c\\). On the \\(b_1, b_2\\) plane, what is the locus of points such that \\(\\mathbf{b^\\mathsf{T}b} \\leq c\\)? The answer is a disk of radius \\(c\\), centered at the origin. Hence, sketching the level curves (curves of constant RSS) as well as this restriction, we obtain the following picture (initially choosing a very large value of \\(c\\)): INSERT DIAGRAM Of course, we could make our budget stricter by reducing the value of \\(c\\) we choose: INSERT DIAGRAM Now, consider two points: \\(\\mathbf{b_1}\\) and \\(\\mathbf{b_2}\\), both pictured below. Also consider the gradients of the error surface (\\(\\nabla E_{in}(\\mathbf{b}_j)\\)) and the constraint surfaces at each point. We know that the gradient of the red circle (i.e. a normal vector) will point radially away from \\(\\mathbf{\\overset{*}{b}}\\), and we know that \\(\\nabla E_{in}(\\mathbf{b}_j)\\) will point in the direction of steepest ascent along the error surface: INSERT DIAGRAM "],
["classif.html", "15 Classification Methods", " 15 Classification Methods In this part of the book we discuss classification methods, that is, methods to predict a qualitative or categorical response: Logistic Regression Canonical Discriminant Analysis Linear Discriminant Analysis Quadratic Discriminant Analysis Bayes’ Classifier "],
["logistic.html", "16 Logistic Regression 16.1 Motivation 16.2 Logistic Regression Model", " 16 Logistic Regression So far we’ve been dealing with predicting a quantitative response, using mostly linear models. But what about predicting a qualitative or categorical response? We now turn our attention to predicting a discrete (aka categorical) response. To have a gentle transition from regression models into classification models, we’ll start with the famous logistic regression. 16.1 Motivation Let’s consider a classic example: predicting heart attack. The data consists of a number of individuals (patients) with some medical variables. Consider the following (very famous) example, found in Hosmer and Lemeshow (edition from 2000) in which we wish to predict coronary heart disease (chd). The data is relatively small (\\(n = 100\\) patients), and we start with considering only one predictor: \\(age\\). #&gt; age chd #&gt; 1 20 0 #&gt; 2 23 0 #&gt; 3 24 0 #&gt; 4 25 0 #&gt; 5 25 1 #&gt; 6 26 0 For illustration purposes, let’s consider the response \\(\\mathbf{y}\\) and only one predictor \\(\\mathbf{x}\\). If we graph a scatterplot we get: With respect to the x-axis, we have values ranging from small x’s to large x’s. In contrast, the response is a binary response, so there are only 0’s or 1’s. As you can tell, the distribution of points does not seem to uniform along the x-axis. Moreover, for a given response value, say \\(y_i = 0\\) there are more small values \\(x\\) than large ones. And viceversa, for \\(y_i = 1\\), there are more large values \\(x\\) than small ones. In other words, we can see that (in general) younger people are less likely to have CHD than older people. Hence, there seems to be some information about chd in age. The goal is to fit a model that predicts chd from age: \\[ \\text{chd} = f(\\text{age}) + \\varepsilon \\] 16.1.1 First Approach: Fitting a Line As a naive first approach, we could try to fit a linear model: $ = b_0 + b_1 = $ where: \\[ \\mathbf{X} = \\begin{pmatrix} 1 &amp; x_1 \\\\ 1 &amp; x_2 \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; x_n \\\\ \\end{pmatrix}; \\hspace{5mm} \\mathbf{b} = \\begin{pmatrix} b_0 \\\\ b_1 \\\\ \\end{pmatrix} \\] So let’s see what happens if we use least squares to fit such line. # fit line reg = lm(chd ~ age, data = dat) summary(reg) #&gt; #&gt; Call: #&gt; lm(formula = chd ~ age, data = dat) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -0.85793 -0.33992 -0.07274 0.31656 0.99269 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) -0.537960 0.168809 -3.187 0.00193 ** #&gt; age 0.021811 0.003679 5.929 4.57e-08 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 0.429 on 98 degrees of freedom #&gt; Multiple R-squared: 0.264, Adjusted R-squared: 0.2565 #&gt; F-statistic: 35.15 on 1 and 98 DF, p-value: 4.575e-08 There seems to be some sort of positive relation between age and chd. To better see this, let’s take a look at the scatterplot, and see how the fitted line looks like: This model yields a very awkward fit, with a couple of issues going on. For one thing, the line, and consequently the predicted values \\(\\hat{y}_i\\), extend beyond the range \\([0,1]\\). Think about it: we could obtain fitted values \\(\\hat{y}_i\\) taking any number between 0 and 1 (which, in this context, makes no sense). We could also get negative predicted values, or even predicted values greater than 1! On the other hand, if we examine the residuals, then things don’t look great for the linear regression. So, we need a way to fix these problems. 16.1.2 Secodn Approach: Harsh Thresholding One idea to try to solve the issues from the regression line would be to set some threshold \\(c\\) (for example, \\(c = 0.5\\)) and look at how the signal compares to it, which will allow us to create a decision rule. That is, for a given age value \\(x_0\\), compare its predicted value to the threshold. If \\(\\hat{y}_0 \\geq 0.5\\), classify it as “1,” otherwise classify it as “0.” \\[ \\hat{y}_i = \\begin{cases} 1 \\quad \\text{if} &amp; b_0 + b_1 x_i \\geq c \\\\ 0 \\quad \\text{if} &amp; b_0 + b_1 x_i &lt; c \\end{cases} \\] We can further arrange the terms above: \\(\\hat{f}(x_i) = b_0 + b_1 x_i - c\\) \\[\\begin{align*} \\hat{f}(x_i) &amp;= b_0 + b_1 x_i - c \\\\ &amp;= b_{0}^{&#39;} + b_1 x_i, \\qquad b_{0}^{&#39;} = b_0 - c \\end{align*}\\] By paying attention to the sign of the signal, we can transform our fitted model into: \\(\\text{sign}(\\mathbf{b^\\mathsf{T}x})\\). \\[ \\hat{y}_i = \\begin{cases} 1 \\quad \\text{if} &amp; \\text{sign}(b_{0}^{&#39;} + b_1 x_i) \\geq 0 \\\\ 0 \\quad \\text{if} &amp; \\text{sign}(b_{0}^{&#39;} + b_1 x_i) &lt; 0 \\end{cases} \\] This transformation imposes a harsh threshold on the signal. Notice that the signal is still linear but we apply a non-linear transformation \\(\\phi(x) = \\text{sign}(x)\\). Insert image of harsh threshold I like to think about this transformation is a quick fix. It’s definitely not something to be proud of, but you could use it to get the job done—although in a quick-dirty fashion. 16.1.3 Third Approach: Conditional Means Using a sign-transformation allows us to overcome some of the limitations of the linear regression model, but it’s far from ideal. An alternative approach involves calculating conditional means. How does that work? The idea is very simple and clever. Say you are looking at patients \\(x = 24\\) years old, and you count the relative frequency of chd cases. In other words, you count the proportion of chd cases among individuals 24 years old. This is nothing else than computing the conditional mean: \\(avg(y_i | x_i = 24)\\). Following this idea, we could compute all conditional means for all age values: \\((\\bar{y}|x_i = 25), (\\bar{y}|x_i = 26), \\dots, (\\bar{y}|x_i = 69)\\). #&gt; age_group #&gt; 20-29 30-34 35-39 40-44 45-49 50-54 55-59 60-69 #&gt; 10 15 12 15 13 8 17 10 Now that we have age by groups, we can get the proportion of coronary heart disease cases in each age group #&gt; # A tibble: 8 x 3 #&gt; age_group count_chd prop_chd #&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 20-29 10 0.1 #&gt; 2 30-34 15 0.133 #&gt; 3 35-39 12 0.25 #&gt; 4 40-44 15 0.333 #&gt; 5 45-49 13 0.462 #&gt; 6 50-54 8 0.625 #&gt; 7 55-59 17 0.765 #&gt; 8 60-69 10 0.8 And then graph these averages on the scatterplot, connecting the dots with a line: Sometimes, however, you may not have data points for a specific \\(x\\)-value. So instead we can use groups of ages. For example, say we define a first group of ages to be 24 - 29 years. The corresponding average will be: \\(avg(y_i | x_i = \\text{group}_1)\\) Theoretically, we are modeling the conditional expectations: \\(\\mathbb{E}(y|x)\\). Which is exactly the regression function. By connecting the averages, we get an interesting sigmoid pattern This pattern can be approximated by some mathematical functions, the most popular being the so-called logistic function: \\[ \\text{logistic function:} \\qquad f(s) = \\frac{e^{s}}{1 + e^{s}} \\] Sometimes you may also find the logistic equation in an alternative form: \\[\\begin{align*} f(x) &amp;= \\frac{e^{\\beta_0 + \\beta_1 x}}{1 + e^{\\beta_0 + \\beta_1 x}} \\\\ &amp;= \\frac{1}{\\frac{1 + e^{\\beta_0 + \\beta_1 x}}{e^{\\beta_0 + \\beta_1 x}}} \\\\ &amp;= \\frac{1}{\\frac{1}{e^{\\beta_0 + \\beta_1 x}} + 1} \\\\ &amp;= \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x})} \\end{align*}\\] Since probability values range inside \\([0,1]\\), instead of using a line to try to approximate these values, we should use a more adequate curve. This is the reason why sigmoid-like curves, such as the logistic function, are preferred for this purpose. 16.2 Logistic Regression Model We consider the following model: \\[ Prob(y \\mid x; \\mathbf{b}) = f(x) \\] We don’t get to observe the true probability; rather, we observe the noisy target \\(y_i\\), generated/affected by the probability \\(f(x)\\). How will we model the probability? We would ideally like a mathematical function that looks like the sigmoid shape from the toy example above; that is, use a sigmoid function. The most famous function—and the function we will use—is the logistic function defined in the previous section \\[ \\text{logistic function:} \\qquad f(s) = \\frac{e^{s}}{1 + e^{s}} \\] In other words, we have the following model: \\[ Prob(y _i \\mid \\mathbf{X} ; \\boldsymbol{\\beta} ) = \\frac{e^{\\mathbf{b^\\mathsf{T} x_i}}}{1 + e^{\\mathbf{b^\\mathsf{T} x_i}}} \\] where here \\(\\mathbf{x_i}\\) represents the vector of observations for individual \\(i\\). Figure 16.1: Regression Model Diagram Hence we have: \\[ Prob(y_i \\mid \\mathbf{x_i}, \\mathbf{b}) = \\begin{cases} h(\\mathbf{x_i})&amp; \\to y_i = 1 \\\\ 1 - h(\\mathbf{x_i}) &amp; \\to y_i = 0 \\\\ \\end{cases} \\] where \\(h\\) denotes the logistic function. 16.2.1 The Criterion Being Optimized We will not be using the MSE as our error measure (since doing so would make no sense in this particular context). Instead, we will use an “error” based on Maximum Likelihood Estimation. In order to do so, we must assume that our model is true; that is, that \\(h(x) = f(x)\\); given that assumption, we ask “how likely is it that we observe the data we already observed (i.e. \\(y_i\\))?” We start with the likelihood function \\(L(\\mathbf{b})\\). Note that \\(L\\) implicitly also depends on \\(\\mathbf{X}\\), and also compute the log-likelihood \\(\\ell(\\mathbf{b})\\): \\[\\begin{align*} Prob(\\mathbf{y} \\mid x_1, x_2, \\dots, x_p; \\mathbf{b}) &amp; = \\prod_{i=1}^{n} P(y_i \\mid \\mathbf{b^\\mathsf{T} x_i} ) \\\\ &amp; = \\prod_{i=1}^{n} h(\\mathbf{b^\\mathsf{T} x_i})^{y_i} \\left[ 1 - h(\\mathbf{b^\\mathsf{T} x_i} ) \\right]^{1 - y_i} \\\\ \\ell(\\mathbf{b}) := \\ln[L(\\mathbf{b}) ]&amp; = \\sum_{i=1}^{n} \\ln\\left[ P(y_i \\mid \\mathbf{b^\\mathsf{T} x_i} ) \\right] \\\\ &amp; = \\sum_{i=1}^{n} \\left\\{ y_i \\ln[h(\\mathbf{b^\\mathsf{T} x_i})] + (1 - y_i) \\ln[ 1 - h(\\mathbf{b^\\mathsf{T} x_i} ) ] \\right\\} \\\\ &amp; = \\sum_{i=1}^{n} \\left[ y_i \\ln\\left( \\frac{e^{\\mathbf{b^\\mathsf{T} x_i}} }{1 + e^{\\mathbf{b^\\mathsf{T} x_i} } } \\right) + (1 - y_i) \\ln \\left( 1 - \\frac{e^{\\mathbf{b^\\mathsf{T} x_i}} }{1 + e^{\\mathbf{b^\\mathsf{T} x_i} } } \\right) \\right] \\\\ &amp; = \\sum_{i=1}^{n} \\left[ y_i \\mathbf{b^\\mathsf{T} x_i} - \\ln \\left( 1 + e ^{\\mathbf{b^\\mathsf{T} x_i} } \\right) \\right] \\end{align*}\\] Now, here is the bad news: differentiating and setting equal to 0 yields and equation for which no closed-form solution exists. To find the maximum likelihood estimate of \\(\\mathbf{b}\\), we require an iterative method: for example, the Newton-Raphson method, or gradient ascent. \\[\\begin{align*} \\nabla \\ell(\\mathbf{b}) &amp; = \\sum_{i=1}^{n} \\left[ y_i \\mathbf{x_i} - \\left( \\frac{e^{\\mathbf{b^\\mathsf{T} x_i}} }{1 + e^{\\mathbf{b^\\mathsf{T} x_i} } } \\right) \\mathbf{x_i} \\right] \\\\ &amp; = \\sum_{i=1}^{n} \\left[ y_i \\mathbf{x_i} - \\phi(\\mathbf{b^\\mathsf{T} x_i} ) \\mathbf{x_i} \\right] \\\\ &amp; = \\sum_{i=1}^{n} \\left[ y_i - \\phi(\\mathbf{b^\\mathsf{T} x_i} ) \\right] \\mathbf{x_i} \\end{align*}\\] Hence, in gradient ascent, we would use: \\[ \\mathbf{b}^{(s + 1)} = \\mathbf{b}^{(s)} + \\alpha \\nabla \\ell(\\mathbf{b}^{(s)}) \\] 16.2.2 Another Way to Solve Logistic Regression The overall picture stills stands: Now, however, we change our probability expression a bit: \\[ Prob(y_i \\mid \\mathbf{x_i}, \\mathbf{b}) = \\begin{cases} h(\\mathbf{x_i})&amp; \\to y_i = 1 \\\\ 1 - h(\\mathbf{x_i}) &amp; \\to y_i = -1 \\\\ \\end{cases} \\] We also need a new way of combining these expression (like we did inside the likelihood expression). They key observation is to note the following: \\(h(-s) = 1 - h(s)\\). With this, we begin computing the likelihood: \\[\\begin{align*} Prob(\\mathbf{y} \\mid x_1, x_2, \\dots, x_p; \\mathbf{b}) &amp; = \\prod_{i=1}^{n} P(y_i \\mid \\mathbf{b^\\mathsf{T} x_i} ) \\\\ &amp; = \\prod_{i=1}^{n} \\phi\\left( y_i \\mathbf{b^\\mathsf{T} x_i} \\right) \\\\ \\ell(\\mathbf{b}) : = \\ln[L(\\mathbf{\\beta})] &amp; = \\sum_{i=1}^{n} \\ln\\left[ \\phi\\left( y_i \\mathbf{b^\\mathsf{T} x_i} \\right) \\right] \\\\ &amp; \\Rightarrow \\frac{1}{n} \\sum_{i=1}^{n} \\underbrace{ \\ln\\left[ \\phi\\left( y_i \\mathbf{b^\\mathsf{T} x_i} \\right) \\right] }_{\\text{logit}} \\end{align*}\\] In other words, we can perform the following minimization problem: \\[\\begin{align*} &amp; \\min_{\\mathbf{b}} \\left\\{ - \\frac{1}{n} \\sum_{i=1}^{n} \\ln\\left[ \\phi\\left( y_i \\mathbf{b^\\mathsf{T} x_i} \\right) \\right] \\right\\} \\ \\Leftrightarrow \\ \\boxed{ \\min_{\\mathbf{b}} \\left\\{ \\underbrace{ \\frac{1}{n} \\sum_{i=1}^{n} \\underbrace{ \\ln\\left( 1 + e^{-y_i \\mathbf{b^\\mathsf{T} x_i} } \\right) }_{\\text{pointwise error}} }_{E_{in}(\\mathbf{b})} \\right\\} } \\end{align*}\\] The product \\(y_i \\mathbf{b^\\mathsf{T} x_i} =: y_i \\mathbf{s}\\) (where \\(\\mathbf{s}\\) represents “signal”) can be understood using the following table: We can think of the term \\(\\mathbf{b^\\mathsf{T} x_i}\\) as a numeric value ranging from “very small” to “very large”. A small signal means that the probability \\(\\phi(\\mathbf{b^\\mathsf{T} x_i})\\) will be small. Conversely, a large signal means that \\(\\phi(\\mathbf{b^\\mathsf{T} x_i})\\) will be large. In turn, the term \\(y_i\\) can be either \\(-1\\) or \\(+1\\). If we have a correct prediction, we would expect a small probability \\(\\phi(\\mathbf{b^\\mathsf{T} x_i})\\) to agree with \\(y_i = -1\\). Likewise, we would also expect a large probability \\(\\phi(\\mathbf{b^\\mathsf{T} x_i})\\) to agree with \\(y_i = +1\\). In turn, if we have an incorrect prediction, we would expect a small probability \\(\\phi(\\mathbf{b^\\mathsf{T} x_i})\\) to mismatch \\(y_i = +1\\). Likewise, we would also expect a large probability \\(\\phi(\\mathbf{b^\\mathsf{T} x_i})\\) to disagree with an observed \\(y_i = -1\\). With agreements, \\(e^{-y_i \\mathbf{b^\\mathsf{T} x_i} }\\) will be small, and will consequently give you a small error. With disagreements, \\(e^{-y_i \\mathbf{b^\\mathsf{T} x_i} }\\) will be large, and will consequently give you a large error. We will also refer to our \\(E_{in}(\\mathbf{b})\\) quantity as the mean cross-entropy error. Technically it isn’t a “cross-entropy” measure in the classification sense, however we will ignore that technicality for now. We find: \\[ \\nabla E_{in}(\\mathbf{b}) = - \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\frac{1}{1 + e^{-y_i \\mathbf{b^\\mathsf{T} x_i} } } \\right) y_i \\mathbf{x_i} \\] and we would use gradient descent to compute the minimum. "]
]
