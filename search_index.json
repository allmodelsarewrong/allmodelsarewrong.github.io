[
["learning.html", "7 Theoretical Framework 7.1 Mental Map 7.2 Two Types of Predictions 7.3 Two Types of Data 7.4 Types of Errors 7.5 Noisy Targets", " 7 Theoretical Framework Finally we have arrived to the part of the book in which we provide a framework for the theory of learning. Well, to be more precise, the framework is really about the theory of supervised learning. At the end on the day, in supervised learning, we want a “good” model. What does “good” model mean? It means that we want to estimate a model \\(\\hat{f}\\) that gives “good” predictions. What do we mean by “good” predictions? Loosely speaking, it means that we want to obtain “accurate” predictions. Before clarifying the notion of accurate predictions, let’s discuss first the concept of predictions. Keep in mind that most of what will be covered in this chapter is highly theoretical. It has to do with the concepts and principles that ideally we expect to have in a perfect world. Having said, the real world, more often than not, is far from being ideal. So we will also need to discuss what to do in practice to overcome the idealistic limitations of our theoretical assumptions. 7.1 Mental Map So far, we have seen an example of Unsupervised Learning (PCA), as well as one method of Supervised Learning (regression). Now, we begin discussing learning ideas at an abstract level. Let’s return to our example of predicting NBA players’ salaries. We have a series of inputs: height, weight, 2PTS, 3PTS, fouls, etc. From these inputs, we obtain an output: the salary of a player. We also have a target function \\(f : \\mathcal{X} \\to \\mathcal{Y}\\). (i.e. a function mapping from the column spaces of \\(\\mathbf{X}\\) to the output space \\(\\mathbf{y}\\)). Keep in mind that this function is an ideal, and remains unknown throughout our computations. Here’s a metaphor that we like to use. Pretend that the target function is some sort of mythical creature, like a unicorn (or your preferred creature). We are trying to find this elusive guy. More generally, we have a dataset \\(\\mathcal{D}: \\{ (\\mathbf{x_1}, y_1), ( \\mathbf{x_2}, y_2), \\dots, (\\mathbf{x_n}, y_n) \\}\\) (where \\(\\mathbf{x_i}\\) represents the vector of observations for the \\(i\\)-th player/individual). From this data, we wish to obtain a hypothesis model \\(\\widehat{f}: \\mathcal{X} \\to \\mathcal{Y}\\) that is an approximation to the unknown function \\(f\\). We can sketch these basic ideas in a sort of “mental map” We will refer to this picture as the “diagram for unsupervised learning”. Figure 7.1: Supervised Learning Diagram (ver 1) The use of orange clouds around certain concepts is intentional; those concepts appearing in orange are more intangible than those appearing in black. For instance, we never really “discover” the function \\(f\\) in its entirety; rather, we just find a good enough approximation to it. As we modify our map, we will encounter more orange concepts as well of highly theoretical nature. The goal of supervised learning is to find a “good” model \\(\\widehat{f}\\). What is a “good” model? Well, a “good” model is a model that gives “good” (or accurate) predictions. What do we mean by “accurate?” Well, let’s not get too bogged down with details here. The magic word here is predictions. There are two kinds of predictions: (1) predictions \\(\\hat{y}_i\\) of observed/seen values \\(x_i\\), and (2) predicitons \\(\\hat{y}_0\\) of unobserved/unseed values \\(x_0\\). 7.2 Two Types of Predictions Think of a simple linear regression model (e.g. with one predictor). Having a fitted model \\(\\hat{f}(x)\\), we can use it to make two types of predictions. On one hand, for an observed point \\(x_i\\), we can compute \\(\\hat{y}_i = \\hat{f}(x_i)\\). By observed point we mean that \\(x_i\\) was part of the data used to find \\(\\hat{f}()\\). On the other hand, we can also compute \\(\\hat{y}_0 = \\hat{f}(x_0)\\) for a point \\(x_0\\) what was not part of the data used when deriving \\(\\hat{f}()\\). The two types of predictions refer to: (1) predictions \\(\\hat{y}_i\\) of observed/seen values \\(x_i\\), and (2) predicitons \\(\\hat{y}_0\\) of unobserved/unseed values \\(x_0\\). Each type of prediction is associated with a certain behavioral feature of a model. The predictions of observed data, \\(\\hat{y}_i\\), have to do with the memorizing aspect (apparent error, resubstitution error). The predictions of unobserved data, \\(\\hat{y}_0\\), have to do with the generalization aspect (generalization error, prediction error). Both kinds of predictions are important, and each of them is interesting in its own right. However, from the supervised learning standpoint, it is the second type of predictions that we are ultimately interested in. That is, we want to find models that are able to give predictions \\(\\hat{y}_0\\) as accurate as possible for the real value \\(y_0\\). Don’t get me wrong. Having good predictions \\(\\hat{y}_i\\) is important and desirable. And to a large extent, it is a necessary condition for a good model. However, it is not a sufficient condition. It is not enough to fit the observed data well, in order to get a good predictive model. Sometimes, you can have perfect fit of the observed data, but a terrible performance for unobserved values \\(x_0\\). Simply put, in supervised learning we want models with a good generalization ability. 7.3 Two Types of Data As you can tell from the previous section, we care about two distinct types of predictions, which in turn involve two slightly different kinds of data. The data points \\(x_i\\) that we use to fit a model is what we call training or learning data. The data points \\(x_0\\) that we use to assess the performance of a model are points NOT supposed to be part of the training set. This implies that, at least in theory, we need two finds of data sets: In-sample data, denoted \\(\\mathcal{D}_{in}\\), used to fit a model Out-of-sample data, denoted \\(\\mathcal{D}_{out}\\), used to measure the predictive quality of a model 7.4 Types of Errors Given that we’ll have, at least theoretically, two types of data, it shouldn’t be a surprise that we will also need separate types of errors for each kind of data. Correspondingly, we will have (at least in theory) two kinds of errors: In-sample Error, denoted \\(E_{in}\\) Out-of-sample Error, denoted \\(E_{out}\\) 7.4.1 Overall Errors Both errors are overall measures of error. This means that we need a function that summarizes, somehow, the total amount of error. Most overall error measures are based on the sum of individual errors: \\[ E(\\hat{f},f) = \\text{measure} \\left( \\sum err_i (\\hat{y}, y) \\right) \\] Unless otherwise said, in this book we will use the mean sum of errors are the default overall error measure. The in-sample error is the average of pointwise errors: \\[ E_{in} (\\hat{f}, f) = \\frac{1}{n} \\sum_{i} err_i \\] The out-of-sample error is the theoretical mean, or expected value, of the pointwise errors: \\[ E_{out} (\\hat{f}, f) = \\mathbb{E}_{\\mathcal{X}} \\left[ err \\left( \\hat{f}(x), f(x) \\right) \\right] \\] The expectation is taken over the input space \\(\\mathcal{X}\\). The point \\(x\\) denotes a general data point in such space \\(\\mathcal{X}\\). Notice the theoretical nature of \\(E_{out}\\). In practice, you will never, never, be able to compute this quantity. In the machine learning literature, these overall measures of error are formally known as cost functions or risks. 7.4.2 Individual Errors What form does the individual error function, \\(err()\\), take? In theory, they can take any form you want. This means that you can invent your own individual error function. However, the most common ones are: squared error: \\(\\quad err(\\hat{f}, f) = \\left( \\hat{y}_i - y_i \\right)^2\\) absolute error: \\(\\quad err(\\hat{f}, f) = \\left| \\hat{y}_i - y_i \\right|\\) misclassification error: \\(\\quad err(\\hat{f}, f) = [\\![ \\hat{y}_i \\neq y_i ]\\!]\\) In the machine learning literature, these individual errors are formally known as loss functions. Let’s update our Machine Learning Map to include error measures: Figure 7.2: Supervised Learning Diagram (ver 2) 7.4.3 Auxiliary Technicality We need to assume some probability distribution \\(P\\) on \\(\\mathcal{X}\\). That is, we assume our vectors \\(\\mathbf{x_1}, \\dots, \\mathbf{x_n}\\) are independent identically distributed (i.i.d.) samples from this distribution \\(P\\). (Exactly what distribution you pick - normal, chi-squared, \\(t\\), etc. - is, for the moment, irrelevant). Recall that out-of-sample data is highly theoretical; we will never be able to obtain it in its entirety. The best we can do is obtain a subset of the out-of-sample data (the test data), and estimate the rest of the data. Our imposition of a distributional structure on \\(\\mathcal{X}\\) enables us to link the in-sample error with the out-of-sample data. Recall that our ultimate goal is to get a good function \\(\\widehat{f} \\approx f\\). What do we mean by the symbol “\\(\\approx\\)”? Technically speaking, we want \\(E_{\\mathrm{out}}(\\widehat{f}) \\approx 0\\). If this is the case, we can safely say that our model has been successfully trained. However, we can never check if this is the case, since we don’t have access to \\(E_{\\mathrm{out}}\\). To solve this, we break our goal into two sub-goals: \\[ E_{\\mathrm{out}} (\\widehat{f}) \\approx 0 \\ \\Rightarrow \\begin{cases} E_{\\mathrm{in}}(\\widehat{f}) \\approx 0 &amp; \\text{practical result} \\\\ E_{\\mathrm{out}}(\\widehat{f}) \\approx E_{\\mathrm{in}}(\\widehat{f}) &amp; \\text{technical/theoretical result} \\\\ \\end{cases} \\] The first condition is easy to check. How do we check the second? We check the second condition by invoking our distributional assumption \\(P\\) on \\(\\mathcal{X}\\). Using our assumption, we can cite various theorems to assert that the second result indeed holds true. We will later find ways to estimate \\(E_{\\mathrm{out}}(\\widehat{f})\\). Figure 7.3: Supervised Learning Diagram (ver 3) 7.5 Noisy Targets In practice, our function won’t necessarily be a nice (or smooth) function. Rather, there will be some noise. Hence, instead of saying \\(y = f(x)\\) where \\(f : \\mathcal{X} \\to \\mathcal{Y}\\), a better statement might be something like \\(y = f(x) + \\varepsilon\\). But even this notation has some flaws; for example, we could have multiple inputs mapping to the same output (which cannot happen if \\(f\\) is a proper “function”). That is, we may have two individuals with the exact same inputs \\(\\mathbf{x_A} = \\mathbf{x_B}\\) but with different response variables \\(y_A \\neq y_B\\). Instead, it makes more sense to consider some target distribution \\(Prob(y \\mid x)\\). In this way, we can think of our data as forming a joint probability distribution \\(Prob(\\mathbf{x}, y)\\). That is because \\(P(\\mathbf{x}, y) = P(\\mathbf{x}) P(y \\mid \\mathbf{x})\\). Figure 7.4: Supervised Learning Diagram (ver 4) In machine learning, we want to learn the conditional distribution \\(P(y \\mid \\mathbf{x})\\). Again, we can think of this probability as \\(y = f() + \\text{noise}\\). Also, sometimes the Hypothesis Sets and Learning Algorithm boxes are combined into one, called the Learning Model. "]
]
