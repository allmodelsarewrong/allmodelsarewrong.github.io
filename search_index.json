[
["index.html", "All Models Are Wrong Concepts of Statistical Learning (CSL) Preface", " All Models Are Wrong Concepts of Statistical Learning (CSL) by Gaston Sanchez, and Ethan Marzban 2019-11-07 Preface This is a work in progress for an introductory text about concepts of Statistical Learning, covering common supervised as well as unsupervised methods. How to cite this book: Sanchez, G., Marzban E. (2019) All Models Are Wrong: Concepts of Statistical Learning. https://allmodelsarewrong.github.io © 2019 Sanchez, Marzban. All Rights Reserved. "],
["about.html", "1 About this book 1.1 Prerequisites 1.2 Acknowledgements", " 1 About this book Knowing that the field(s) of Machine Learning, Statisticial Learning, and any other name about learning from data, is a very broad subject, we should warn you that this book is not intended to be the ultimate compilation of every single SL technique ever devised. Instead, we focus on the concepts that we consider the building blocks that any user or practitioner needs to make sense of most common SL techniques. A big shortcoming of the book: we don’t cover neural networks. At least not in this first round of iterations. Sorry! On the plus side: We’ve tried hard to keep the notation as simple and consistent as possible. And we’ve also made a serious effort to make it very visual (lots of diagrams, pictures, plots, graphs, figures, …, you name it). 1.1 Prerequisites We are assuming that you already have some knowledge under your belt. You will better understand (and hopefully enjoy) the book if you’ve taken one or more courses on the following subjects: linear or matrix algebra multivariable calculus statistics probability programming or scripting 1.2 Acknowledgements Many thanks to the UC Berkeley students of Stat 154 Modern Statistical Prediction and Machine Learning (Fall 2017, Spring 2018, Fall 2019). "],
["intro.html", "2 Introduction 2.1 Basic Notation 2.2 About Statistical Learning", " 2 Introduction Picture a data set containing scores of several course for college students. For example, courses like matrix algebra, multivariable calculus, statistics, and probability. And say we also have historical data about a course in Statistical Learning. In particular we have final scores measured on a scale from 0 to 100, we also have final grades (letter grade scale), as well as a third interesting variable “Pass - Non-Pass” indicating whether the student passed statistical learning. Some data like that fits perfectly well in a tabular format. The rows contain the records for a bunch of students, and the columns refer to the variables. Math 54 Math 55 Stat 135 Stat 134 Stat 154 Grade P/NP \\(x_{11}\\) \\(x_{12}\\) \\(x_{13}\\) \\(x_{14}\\) \\(x_{15}\\) \\(y_{15}\\) \\(y_{16}\\) \\(x_{21}\\) \\(x_{22}\\) \\(x_{23}\\) \\(x_{24}\\) \\(x_{25}\\) \\(y_{15}\\) \\(y_{16}\\) … … … … … … … \\(x_{n1}\\) \\(x_{n2}\\) \\(x_{n3}\\) \\(x_{n4}\\) \\(x_{n5}\\) \\(y_{n5}\\) \\(y_{n6}\\) Suppose that, based on this historical data, we wish to predict the score of a new student (whose Math 54, Math 55, and Stat 135 grades are known) in Stat 154. To do so, we would fit some sort of model to our data; i.e. we would perform regression. This is a form of supervised learning, since our model is trained using known inputs (i.e. Math 54, Math 55, and Stat 135 grades) as well as known responses (i.e. the Stat 154 grades of the previous students). Unsupervised Learning: where we have inputs, but not response variables. 2.1 Basic Notation In this book we are going to use a fair amount of math notation. Becoming familiar with the meaning of all the different symbols as soon as possible, should allow you to keep the learning curve a little bit less steep. The starting point is always the data, which we will assume to be in a tabular format, that can be translated into a mathematical matrix object. Here’s an example of a data matrix \\(\\mathbf{X}\\) of size \\(n \\times p\\) \\[ \\mathbf{X} = \\ \\begin{bmatrix} x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1p} \\\\ x_{21} &amp; x_{22} &amp; \\cdots &amp; x_{2p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_{n1} &amp; x_{n2} &amp; \\cdots &amp; x_{np} \\\\ \\end{bmatrix} \\] By default, we will assume that the rows of a data matrix correspond to the individuals or objects. Likewise, we will also assume that the columns of a data matrix correspond to the variables or features observed on the individuals. In this sense, the symbol \\(x_{ij}\\) represents the value observed for the \\(j\\)-th variable on the \\(i\\)-th individual. Throughout this book, every time you see the letter \\(i\\), either alone or as an index associated with any other symbol (superscript or subscript), it means that such term corresponds to an individual or a row of some data matrix. For instance, symbols like \\(x_i\\), \\(\\mathbf{x_i}\\), and \\(\\alpha_i\\) are all examples that refer to—or denote a connection with—individuals. In turn, we will always use the letter \\(j\\) to conveyed association with variables. For instance, \\(x_j\\), \\(\\mathbf{x_j}\\), and \\(\\alpha_j\\) are examples that refer to—or denote a connection with—variables. For better or for worse, we’ve made the decision to represent row vectors and column vectors with the same notation: as bold lower case letters such as \\(\\mathbf{x_i}\\) and \\(\\mathbf{x_j}\\). Because of the risk of confusing a row vector with a column vector, sometimes we will use the arrow notation for row vectors: \\(\\mathbf{\\vec{x}_i}\\). So, going back to the above data matrix \\(\\mathbf{X}\\), we can represent the first variable as a vector \\(\\mathbf{x_1} = (x_{11}, x_{21}, \\dots, x_{n1})\\). Likewise, we can represent the first individual with the vector \\(\\mathbf{\\vec{x}_1} = (x_{11}, x_{12}, \\dots, x_{1n})\\). Here’s a reference table with the notation and symbols used throughout the book Symbol Description \\(n\\) number of objects \\(p\\) number of variables \\(i\\) running index for rows \\(j\\) running index for columns \\(k\\) running index for sets of variables (i.e. blocks) \\(l, m, q\\) auxiliar index \\(f()\\), \\(g()\\), \\(h()\\) functions \\(\\lambda, \\mu, \\gamma, \\alpha\\) greek letters represent scalars \\(\\varepsilon\\) decimal tolerance threshold (e.g. 0.00001) \\(\\mathbf{x}\\), \\(\\mathbf{y}\\) variables, size determined by context \\(\\mathbf{w}\\), \\(\\mathbf{a}\\), \\(\\mathbf{b}\\) vectors of weight coefficients \\(\\mathbf{z}\\), \\(\\mathbf{t}\\), \\(\\mathbf{u}\\) components or latent variables \\(\\mathbf{X} : n \\times p\\) data matrix with \\(n\\) rows and \\(p\\) columns \\(x_{ij}\\) element of a matrix in \\(i\\)-th row and \\(j\\)-th column \\(\\mathbf{1}\\) vector of ones, size determined by context \\(\\mathbf{I}\\) identity matrix, size determined by context By the way, there are many more symbols that will appear in later chapters. But for now these are the fundamental ones. Common operators Symbol Description \\(\\mathbb{E}[X]\\) expected value of a random variable \\(X\\) \\(\\|\\mathbf{a}\\|\\) euclidean norm of a vector \\(\\mathbf{a}^{\\mathsf{T}}\\) transpose of a vector (or matrix) \\(\\mathbf{a^{\\mathsf{T}}b}\\) inner product of two vectors \\(\\langle \\mathbf{a}, \\mathbf{b} \\rangle\\) inner product of two vectors \\(det(\\mathbf{A})\\) determinant of a square matrix \\(tr(\\mathbf{A})\\) trace of a square matrix \\(\\mathbf{A}^{-1}\\) inverse of a square matrix \\(diag(\\mathbf{A})\\) diagonal of a square matrix \\(var()\\) variance \\(cov()\\) covariance 2.2 About Statistical Learning We will focus on supervised learning as well as unsupervised learning. We won’t discuss more recent fields of deep learning and reinforcement learning. To visualize the different types of learning, the different types of variables, and the methodology associated with each combination of learning/data types, we can use the following graphic: Figure 2.1: Supervised and Unsupervised "],
["duality.html", "3 Geometric Duality 3.1 Rows Space 3.2 Columns Space 3.3 Cloud of Individuals 3.4 Cloud of Variables", " 3 Geometric Duality The way we like to introduce you to the statistical learning world, is by talking and thinking about data in a geometric sense. Let’s suppose we have some data in the form of a data matrix. For convenience purposes, let’s also suppose that all variables are measured in a real-value scale. Obviously not all data is expressed or even encoded numerically. You may have categorical or symbolic data. But for this illustration, let’s assume that any categorical and symbolic data has already been transformed into a numeric scale. It’s very enlightening to think of a data matrix as viewed from the glass of Geometry. The key idea is to think of the data in a matrix as elements living in a multidimensional space. Actually, we can regard a data matrix from two apparently different perspectives that, in reality, are intimately connected: the rows perspective and the columns perspective. In order to explain these perspectives, let me use the following diagram of a data matrix \\(\\mathbf{X}\\) with \\(n\\) rows and \\(p\\) columns, with \\(x_{ij}\\) representing the element in the \\(i\\)-th row and \\(j\\)-th column. Figure 3.1: Duality of a data matrix When we look at a data matrix from the columns perpective what we are doing is focusing on the \\(p\\) variables. In a similar way, when looking at a data matrix from its rows perspective, we are focusing on the \\(n\\) individuals. Like a coin, though, this matrix has two sides: a rows side, and a columns side. That is, we could look at the data from the rows point of view, or the columns point of view. These two views are (of course) not completely independent. This double perspective or duality for short, is like the two sides of the same coin. 3.1 Rows Space We know that human vision is limited to three-dimensions, but pretend that you had superpowers that let you visualize a space with any number of dimensions. Because each row of the data matrix has \\(p\\) elements, we can regard individuals as objects that live in a \\(p\\)-dimensional space. For visualization purposes, think of each variable as playing the role of a dimension associated to a given axis in this space; likewise, consider each of the \\(n\\) individuals as being depicted as a point (or particle) in such space, like in the following diagram: Figure 3.2: Rows space In the figure above, even though I’m showing only three axes, you should pretend that you are visualizing a \\(p\\)-dimensional space (imaging that there are \\(p\\) axes). Each point in this space corresponds to a single individual, and they all form what you can call a cloud of points. 3.2 Columns Space We can do the same visual exercise with the columns of a data matrix. Since each variable has \\(n\\) elements, we can regard the set of \\(p\\) variables as objects that live in an \\(n\\)-dimensional space. However, instead of representing each variable with a dot, it’s better to graphically represent them with an arrow (or vector). Why? Because of two reasons: one is to distinguish them from the individuals (dots). But more important, because the esential thing with a variable is not really its magnitude (and therefore its position) but its direction. Often, as part of the preprocessing steps we apply transformations on variables that change their scales (e.g. shrinking them, or stretching them) without modifying their directions. Figure 3.3: Columns space Analogously to the rows space and its cloud of individuals, you should also pretend that the image above is displaying an \\(n\\)-dimensional space with a bunch of blue arrows pointing in various directions. What’s next? Now that we know how to think of data from a geometric perspective, the next step is to discuss a handful of common operations that can be performed with points and vectors that live in some geometric space. 3.3 Cloud of Individuals In the previous chapter we introduce the powerful idea of looking at the rows and columns of a data matrix from the lens of geometry. We are assuming in general that the rows have to do with \\(n\\) individuals that lie in a \\(p\\)-dimensional space. Figure 3.4: Cloud of points Let’s start describing a set of common operations that we can apply on the individuals (living in a \\(p\\)-dimensional space). 3.3.1 Average Individual We can ask about the typical or average individual. If you only have one variable, then all the individual points lie in a one-dimensional space, which is basically a line: Figure 3.5: Points in one dimension In this case, the average individual is simply the average of the values, which geometrically corresponds to the balancing point: Figure 3.6: Average individual Algebraically we have: individuals \\(x_1, x_2, \\dots, x_n\\), and the average is: \\[ \\bar{x} = \\frac{x_1 + \\dots + x_n}{n} = \\frac{1}{n} \\sum_{i=1}^{n} x_i \\] In vector notation, the average can be calculated with an inner product between \\(\\mathbf{x} = (x_1, x_2, \\dots, x_n)\\), and a constant vector of \\(n\\)-ones \\(\\mathbf{1}\\): \\[ \\bar{x} = \\mathbf{x^\\mathsf{T}1} \\] What about the multivariate case? It turns out that we can also ask about the average individual of a cloud of points, like in the following figure: Figure 3.7: Cloud of points with centroid (i.e. average individual) The average individual, in a \\(p\\)-dimensional space is the point \\(\\mathbf{\\vec{g}}\\) containing as coordiantes the averages of all the variables: \\[ \\mathbf{\\vec{g}} = (\\bar{x}_1, \\bar{x}_2, \\dots, \\bar{x}_j) \\] where \\(\\bar{x}_j\\) is the average of the \\(j\\)-th variable. This average individual \\(\\mathbf{\\vec{g}}\\) is also known as the centroid, barycenter, or center of gravity of the cloud of points. 3.3.2 Centered Data Often, it is convenient to transform the data in such a way that the centroid of a data set becomes the origin of the cloud of points. Geometrically, this type of transformation involves a shif of the axes in the \\(p\\)-dimensional space. Algebraically, this transformation corresponds to expresing the values of each variable in terms of deviations from their means. Figure 3.8: Cloud of points of mean-centered data 3.3.3 Distance between individuals Another common operation that we may be interested in is the distance between two individuals. Obviously the notion of distance is not unique, since you can choose different types of distance measures. Perhaps the most comon type of distance is the (squared) Euclidean distance. Unless otherwise mentioned, this will be the default distance used in this book. Figure 3.9: Distance between two individuals If you have one variable \\(X\\), then the squared distance \\(d^2(i,l)\\) between two individuals \\(x_i\\) and \\(x_l\\) is: \\[ d^2(i,l) = (x_i - x_l)^2 \\] In general, with \\(p\\) variables, the squared distance between the \\(i\\)-th individual and the \\(l\\)-th individual is: \\[\\begin{align*} d^2(i,l) &amp;= (x_{i1} - x_{l1})^2 + (x_{i2} - x_{l2})^2 + \\dots + (x_{ip} - x_{lp})^2 \\\\ &amp;= (\\mathbf{\\vec{x}_i} - \\mathbf{\\vec{x}_l})^\\mathsf{T} (\\mathbf{\\vec{x}_i} - \\mathbf{\\vec{x}_l}) \\end{align*}\\] 3.3.4 Distance to the centroid A special case is the distance between any individual \\(i\\) and the average individual: \\[\\begin{align*} d^2(i,g) &amp;= (x_{i1} - \\bar{x}_1)^2 + (x_{i2} - \\bar{x}_2)^2 + \\dots + (x_{ip} - \\bar{x}_p)^2 \\\\ &amp;= (\\mathbf{\\vec{x}_i} - \\mathbf{\\vec{g}})^\\mathsf{T} (\\mathbf{\\vec{x}_i} - \\mathbf{\\vec{g}}) \\end{align*}\\] 3.3.5 Measures of Dispersion What else can we calculate with the individuals? Think about it. So far we’ve seen how to calculate the average individual, as well as distances between individuals. The average individual or centroid plays the role of a measure of center. And everytime you get a measure of center, it makes sense to get a measure of spread. Overall Dispersion One way to compute a measure of scatter among individuals is to consider all the squared distances between pairs of individuals. For instance, say you have three individuals \\(a\\), \\(b\\), and \\(c\\). We can calculate all pairwise distances and add them up: \\[ d^2(a,b) + d^2(b,a) + d^2(a,c) + d^2(c,a) + d^2(b,c) + d^2(c,b) \\] In general, when you have \\(n\\) individuals, you can obtain up to \\(n^2\\) squared distances. We will give the generic name of Overall Dispersion to the sum of all squared pairwise distances: \\[ \\text{overall dispersion} = \\sum_{i=1}^{n} \\sum_{l=1}^{n} d^2(i,l) \\] Inertia Another measure of scatter among individuals can be computed by adding the distances between all individuals and the centroid. Figure 3.10: Inertia The sum of squared distances from each point to the centroid then becomes \\[ \\frac{1}{n} \\sum_{i=1}^{n} d^2(i,g) = \\frac{1}{n} \\sum_{i=1}^{n} (\\mathbf{\\vec{x}_i} - \\mathbf{\\vec{g}})^\\mathsf{T} (\\mathbf{\\vec{x}_i} - \\mathbf{\\vec{g}}) \\] We will name this measure Inertia, borrowing this term from the concept of inertia used in mechanics (in physics). \\[ \\text{Inertia} = \\frac{1}{n} \\sum_{i=1}^{n} d^2(i,g) \\] What is the motivation behind this measure? Consider the \\(p = 1\\) case; i.e. when \\(\\mathbf{X}\\) is simply a column vector \\[ \\mathbf{X} = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\\\ \\end{pmatrix} \\] The centroid will simply be the mean of these points: i.e. \\(\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\\). The sum of squared-distances from each point to the centroid then becomes: \\[ (x_1 - \\bar{x})^2 + (x_2 - \\bar{x})^2 + \\dots + (x_n - \\bar{x})^2 = \\sum_{i=1}^{n} (x_i - \\bar{x})^2 \\] Does the above formula look familiar? What if we take the average of the squared distances: \\[ \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})^2 = \\frac{(x_1 - \\bar{x})^2 + (x_2 - \\bar{x})^2 + \\dots + (x_n - \\bar{x})^2}{n} \\] Same question: Do you recognize this formula? You better do… This is nothing else than the formula of the variance of \\(X\\). And yes, we are dividing by \\(n\\) (not by \\(n-1\\)). Hence, you can think of inertia as a multidimensional extension of variance, which gives the typical squared distance around the centroid. Overall Dispersion and Inertia Interestingly, the overall dispersion and the inertia are connected through the following relation: \\[\\begin{align*} \\text{overall dispersion} &amp;= \\sum_{i=1}^{n} \\sum_{l=1}^{n} d^2(i,l) \\\\ &amp;= 2n \\sum_{i=1}^{n} d^2(i,g) \\\\ &amp;= (2n^2) \\text{Inertia} \\end{align*}\\] The proof of this relation is left as a homework exercise. 3.4 Cloud of Variables The starting point when analyzing variables involves computing various summary measures—such as means, and variances—to get an idea of the common or central values, and the amount of variability of each variable. In this chapter we will review how concepts like the mean of a variable, the variance, covariance, and correlation, can be interpreted in a geometric sense, as well as their expressions in terms of vector-matrix operations. 3.4.1 Mean of a Variable To measure variation, we usually begin by calculating a “typical” value. The idea is to summarize the values of a variable with one or two representative values. You will find this notion under several terms like measures of center, location, central tendency, or centrality. The prototypical summary value of center is the mean, sometimes referred to as average. The mean of an \\(n-\\)element variable \\(X = (x_1, x_2, \\dots, x_n)\\), represented by \\(\\bar{x}\\), is obtained by adding all the \\(x_i\\) values and then dividing by their total number \\(n\\): \\[ \\bar{x} = \\frac{x_1 + x_2 + \\dots + x_n}{n} \\] Using summation notation we can express \\(\\bar{x}\\) in a very compact way as: \\[ \\bar{x} = \\frac{1}{n} \\sum_{i = 1}^{n} x_i \\] If you associate a constant weight of \\(1/n\\) to each observation \\(x_i\\), you can look at the formula of the mean as a weighted sum: \\[ \\bar{x} = \\frac{1}{n} x_1 + \\frac{1}{n} x_2 + \\dots + \\frac{1}{n} x_n \\] This is a slightly different way of looking at the mean that will allow you to generalize the concept of an “average” as a weighted aggregation of information. For example, if we denote the weight of the \\(i\\)-th individual as \\(w_i\\), then the average can be expressed as: \\[ \\bar{x} = w_1 x_1 + w_2 x_2 + \\dots + w_n x_n = \\sum_{i=1}^{n} w_i x_i \\] 3.4.2 Variance of a Variable A measure of center such as the mean is not enoguh to summarize the information of a variable. We also need a measure of the amount of variability. Synonym terms are variation, spread, scatter, and dispersion. Because of its relevance and importance for statistical learning methods, we will focus on one particular measure of spread: the variance (and its square root the standard deviation). Simply put, the variance is a measure of spread around the mean. The main idea behind the calculation of the variance is to quantify the typical concentration of values around the mean. The way this is done is by averaging the squared deviations from the mean. \\[ var(X) = \\frac{(x_1 - \\bar{x})^2 + \\dots + (x_n - \\bar{x})^2}{n} = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})^2 \\] Let’s disect the terms and operations involved in the formula of the variance. the main terms are the deviations from the mean \\((x_i - \\bar{x})\\), that is, the difference between each observation \\(x_i\\) and the mean \\(\\bar{x}\\). conceptually speaking, we want to know what is the average size of the deviations around the mean. simply averaging the deviations won’t work because their sum is zero (i.e. the sum of deviations around the mean will cancel out because the mean is the balancing point). this is why we square each deviation: \\((x_i - \\bar{x})^2\\), which literally means getting the squared distance from \\(x_i\\) to \\(\\bar{x}\\). having squared all the deviations, then we average them to get the variance. Because the variance has squared units, we need to take the square root to “recover” the original units in which \\(X\\) is expressed. This gives us the standard deviation \\[ sd(X) = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})^2} \\] In this sense, you can say that the standard deviation is roughly the average distance that the data points vary from the mean. Sample Variance In practice, you will often find two versions of the formula for the variance: one in which the sum of squared deviations is divided by \\(n\\), and another one in which the division is done by \\(n-1\\). Each version is associated to the statistical inference view of variance in terms of whether the data comes from the population or from a sample of the population. The population variance is obtained dividing by \\(n\\): \\[ \\textsf{population variance:} \\quad \\frac{1}{(n)} \\sum_{i=1}^{n} (x_i - \\bar{x})^2 \\] The sample variance is obtained dividing by \\(n - 1\\) instead of dividing by \\(n\\). The reason for doing this is to get an unbiased estimor of the population variance: \\[ \\textsf{sample variance:} \\quad \\frac{1}{(n-1)} \\sum_{i=1}^{n} (x_i - \\bar{x})^2 \\] It is important to note that most statistical software compute the variance with the unbiased version. If you implement your own functions and are planning to compare them against other software, then it is crucial to known what other programmers are using for computing the variance. Otherwise, your results might be a bit different from the ones with other people’s code. In this book, unless indicated otherwise, we will use the factor \\(\\frac{1}{n}\\) when introducing concepts of variance, and related measures. If needed, we will let you know when a formula needs to use the factor \\(\\frac{1}{n-1}\\). 3.4.3 Variance with Vector Notation In a similar way to expressing the mean with vector notation, you can also formulate the variance in terms of vector-matrix notation. First, notice that the formula of the variance consists of the addition of squared terms. Second, recall that a sum of numbers can be expressed with an inner product by using the unit vector (or summation operator). If we denote a vector of ones of size \\(n\\) as \\(\\mathbf{1}_{n}\\), then the variance of a vector \\(\\mathbf{x}\\) can be obtained with the following inner product: \\[ var(\\mathbf{x}) = \\frac{1}{n} (\\mathbf{x} - \\mathbf{\\bar{x}})^\\mathsf{T} (\\mathbf{x} - \\mathbf{\\bar{x}}) \\] where \\(\\mathbf{\\bar{x}}\\) is an \\(n\\)-element vector of mean values \\(\\bar{x}\\). Assuming that \\(\\mathbf{x}\\) is already mean-centered, then the variance is proportional to the squared norm of \\(\\mathbf{x}\\) \\[ var(\\mathbf{x}) = \\frac{1}{n} \\hspace{1mm} \\mathbf{x}^{\\mathsf{T}} \\mathbf{x} = \\frac{1}{n} \\| \\mathbf{x} \\|^2 \\] This means that we can formulate the variance with the general notion of an inner product: \\[ var(\\mathbf{x}) = \\frac{1}{n} \\langle \\mathbf{x}, \\mathbf{x} \\rangle \\] 3.4.4 Standard Deviation as a Norm If we use a metric matrix \\(\\mathbf{D} = diag(1/n)\\) then we have that the variance is given by a special type of inner product: \\[ var(\\mathbf{x}) = \\langle \\mathbf{x}, \\mathbf{x} \\rangle_{D} = \\mathbf{x}^{\\mathsf{T}} \\mathbf{D x} \\] From this point of view, we can say that the variance of \\(\\mathbf{x}\\) is equivalent to its squared norm when the vector space is endowed with a metric \\(\\mathbf{D}\\). Consequently, the standard deviation is simply the length of \\(\\mathbf{x}\\) in this particular geometric space. \\[ sd(\\mathbf{x}) = \\| \\mathbf{x} \\|_{D} \\] When looking at the standard deviation from this perspective, you can actually say that the amount of spread of a vector \\(\\mathbf{x}\\) is actually its length (under the metric \\(\\mathbf{D}\\)). 3.4.5 Covariance The covariance generalizes the concept of variance for two variables. Recall that the formula for the covariance between \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) is: \\[ cov(\\mathbf{x, y}) = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x}) (y_i - \\bar{y}) \\] where \\(\\bar{x}\\) is the mean value of \\(\\mathbf{x}\\) obtained as: \\[ \\bar{x} = \\frac{1}{n} (x_1 + x_2 + \\dots + x_n) = \\frac{1}{n} \\sum_{i = 1}^{n} x_i \\] and \\(\\bar{y}\\) is the mean value of \\(\\mathbf{y}\\): \\[ \\bar{y} = \\frac{1}{n} (y_1 + y_2 + \\dots + y_n) = \\frac{1}{n} \\sum_{i = 1}^{n} y_i \\] Basically, the covariance is a statistical summary that is used to assess the linear association between pairs of variables. Assuming that the variables are mean-centered, we can get a more compact expression of the covariance in vector notation: \\[ cov(\\mathbf{x, y}) = \\frac{1}{n} (\\mathbf{x^{\\mathsf{T}} y}) \\] Properties of covariance: the covariance is a symmetric index: \\(cov(X,Y) = cov(Y,X)\\) the covariance can take any real value (negative, null, positive) the covariance is linked to variances under the name of the Cauchy-Schwarz inequality: \\[cov(X,Y)^2 \\leq var(X) var(Y) \\] 3.4.6 Correlation Although the covariance indicates the direction—positive or negative—of a possible linear relation, it does not tell us how big or small the relation might be. To have a more interpretable index, we must transform the convariance into a unit-free measure. To do this we must consider the standard deviations of the variables so we can normalize the covariance. The result of this normalization is the coefficient of linear correlation defined as: \\[ cor(X, Y) = \\frac{cov(X, Y)}{\\sqrt{var(X)} \\sqrt{var(Y)}} \\] Representing \\(X\\) and \\(Y\\) as vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\), we can express the correlation as: \\[ cor(\\mathbf{x}, \\mathbf{y}) = \\frac{cov(\\mathbf{x}, \\mathbf{y})}{\\sqrt{var(\\mathbf{x})} \\sqrt{var(\\mathbf{y})}} \\] Assuming that \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are mean-centered, we can express the correlation as: \\[ cor(\\mathbf{x, y}) = \\frac{\\mathbf{x^{\\mathsf{T}} y}}{\\|\\mathbf{x}\\| \\|\\mathbf{y}\\|} \\] As it turns out, the norm of a mean-centered variable \\(\\mathbf{x}\\) is proportional to the square root of its variance (or standard deviation): \\[ \\| \\mathbf{x} \\| = \\sqrt{\\mathbf{x^{\\mathsf{T}} x}} = \\sqrt{n} \\sqrt{var(\\mathbf{x})} \\] Consequently, we can also express the correlation with inner products as: \\[ cor(\\mathbf{x, y}) = \\frac{\\mathbf{x^{\\mathsf{T}} y}}{\\sqrt{(\\mathbf{x^{\\mathsf{T}} x})} \\sqrt{(\\mathbf{y^{\\mathsf{T}} y})}} \\] or equivalently: \\[ cor(\\mathbf{x, y}) = \\frac{\\mathbf{x^{\\mathsf{T}} y}}{\\| \\mathbf{x} \\| \\hspace{1mm} \\| \\mathbf{y} \\|} \\] In the case that both \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are standardized (mean zero and unit variance), that is: \\[ \\mathbf{x} = \\begin{bmatrix} \\frac{x_1 - \\bar{x}}{\\sigma_{x}} \\\\ \\frac{x_2 - \\bar{x}}{\\sigma_{x}} \\\\ \\vdots \\\\ \\frac{x_n - \\bar{x}}{\\sigma_{x}} \\end{bmatrix}, \\hspace{5mm} \\mathbf{y} = \\begin{bmatrix} \\frac{y_1 - \\bar{y}}{\\sigma_{y}} \\\\ \\frac{y_2 - \\bar{y}}{\\sigma_{y}} \\\\ \\vdots \\\\ \\frac{y_n - \\bar{y}}{\\sigma_{y}} \\end{bmatrix} \\] the correlation is simply the inner product: \\[ cor(\\mathbf{x, y}) = \\mathbf{x^{\\mathsf{T}} y} \\hspace{5mm} \\textsf{(standardized variables)} \\] 3.4.7 Geometry of Correlation Let’s look at two variables (i.e. vectors) from a geometric perspective. Figure 3.11: Two vectors in a 2-dimensional space The inner product ot two mean-centered vectors \\(\\langle \\mathbf{x}, \\mathbf{y} \\rangle\\) is obtained with the following equation: \\[ \\mathbf{x^{\\mathsf{T}} y} = \\|\\mathbf{x}\\| \\hspace{1mm} \\|\\mathbf{y}\\| \\hspace{1mm} cos(\\theta_{x,y}) \\] where \\(cos(\\theta_{x,y})\\) is the angle between \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\). Rearranging the terms in the previous equation we get that: \\[ cos(\\theta_{x,y}) = \\frac{\\mathbf{x^\\mathsf{T} y}}{\\|\\mathbf{x}\\| \\|\\mathbf{y}\\|} = cor(\\mathbf{x, y}) \\] which means that the correlation between mean-centered vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) turns out to be the cosine of the angle between \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\). 3.4.8 Orthogonal Projections Last but not least, we finish this chapter with a discussion of projections. To be more specific, the statistical interpretation of orthogonal projections. Let’s motivate this discussion with the following question: Consider two variables \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\). Can we approximate one of the variables in terms of the other? This is an asymmetric type of association since we seek to say something about the variability of one variable, say \\(\\mathbf{y}\\), in terms of the variability of \\(\\mathbf{x}\\). Figure 3.12: Two vectors in n-dimensional space We can think of several ways to approximate \\(\\mathbf{y}\\) in terms of \\(\\mathbf{x}\\). The approximation of \\(\\mathbf{y}\\), denoted by \\(\\mathbf{\\hat{y}}\\), means finding a scalar \\(b\\) such that: \\[ \\mathbf{\\hat{y}} = b \\mathbf{x} \\] The common approach to get \\(\\mathbf{\\hat{y}}\\) in some optimal way is by minimizing the square difference between \\(\\mathbf{y}\\) and \\(\\mathbf{\\hat{y}}\\). Figure 3.13: Orthogonal projection of y onto x The answer to this question comes in the form of a projection. More precisely, we orthogonally project \\(\\mathbf{y}\\) onto \\(\\mathbf{x}\\): \\[ \\mathbf{\\hat{y}} = \\mathbf{x} \\left( \\frac{\\mathbf{y^\\mathsf{T} x}}{\\mathbf{x^\\mathsf{T} x}} \\right) \\] or equivalently: \\[ \\mathbf{\\hat{y}} = \\mathbf{x} \\left( \\frac{\\mathbf{y^\\mathsf{T} x}}{\\| \\mathbf{x} \\|^2} \\right) \\] For convenience purposes, we can rewrite the above equation in a slightly different format: \\[ \\mathbf{\\hat{y}} = \\mathbf{x} (\\mathbf{x^\\mathsf{T}x})^{-1} \\mathbf{x^\\mathsf{T}y} \\] If you are familiar with linear regression, you should be able to recognize this equation. We’ll come back to this when we get to the chapter about Linear regression. 3.4.9 The mean as an orthogonal projection Let’s go back to the concept of mean of a variable. As we previously mention, a variable \\(X = (x_1, \\dots, x_n)\\), can be thought of a vector \\(\\mathbf{x}\\) in an \\(n\\)-dimensional space. Furthermore, let’s also consider the constant vector \\(\\mathbf{1}\\) of size \\(n\\). Here’s a conceptual diagram for this situation: Figure 3.14: Two vectors in n-dimensional space Out of curiosity, what happens when we ask about the orthogonal projection of \\(\\mathbf{x}\\) onto \\(\\mathbf{1}\\)? Something like in the following picture: Figure 3.15: Orthogonal projection of vector x onto constant vector 1 This projection is expressed in vector notation as: \\[ \\mathbf{\\hat{x}} = \\mathbf{1} \\left( \\frac{\\mathbf{x^\\mathsf{T} 1}}{\\mathbf{1^\\mathsf{T} 1}} \\right) \\] or equivalently: \\[ \\mathbf{\\hat{x}} = \\mathbf{1} \\left( \\frac{\\mathbf{x^\\mathsf{T} 1}}{\\| \\mathbf{1} \\|^2} \\right) \\] Note that the term in parenthesis is just a scalar, so we can actually express \\(\\mathbf{\\hat{x}}\\) as \\(b \\mathbf{1}\\). This means that a projection implies multiplying \\(\\mathbf{1}\\) by some number \\(b\\), such that \\(\\mathbf{\\hat{x}} = b \\mathbf{1}\\) is a stretched or shrinked version of \\(\\mathbf{1}\\). So, what is the scalar \\(b\\)? It is simply the mean of \\(\\mathbf{x}\\): \\[ \\mathbf{\\hat{x}} = \\mathbf{1} \\left( \\frac{\\mathbf{x^\\mathsf{T} 1}}{\\| \\mathbf{1} \\|^2} \\right) = \\bar{x} \\mathbf{1} \\] This is better appreciated in the following figure. Figure 3.16: Mean of x as length of its projection onto constant vector 1 What this tells us is that the mean of the variable \\(X\\), denoted by \\(\\bar{x}\\), has a very interesting geometric interpretation. As you can tell, \\(\\bar{x}\\) is the length of the projected vector \\(\\mathbf{\\hat{x}}\\). Or in more formal terms, \\(\\bar{x}\\) is the scalar projection of \\(\\mathbf{x}\\) onto \\(\\mathbf{1}\\). "],
["pca.html", "4 Principal Components Analysis 4.1 Low-dimensional Representations 4.2 Projections 4.3 Maximization Problem 4.4 Another Perspective of PCA 4.5 Data Decomposition Model", " 4 Principal Components Analysis Our first unsupervised method of the book is Principal Components Analysis, commonly referred to as PCA. Principal Components Analysis (PCA) is the workhorse method of multivariate data analysis. Simply put, PCA helps us study and explore a data set of quantitative variables measured on a set of objects. One way to look at the purpose of principal components analysis is to get the best low-dimensional representation of the variation in data. Among the various appealing features of PCA is that it allows us to obtain a visualization of the objects in order to see their proximities. Likewise, it also provides us results to get a graphic representation of the variables in terms of their correlations. Overall, PCA is a multivariate technique that allows us to summarize the systematic patterns of variations in a data set. The classic reference for PCA is the work by the eminent British biostatistician Karl Pearson “On Lines and Planes of Closest Fit to Systems of Points in Space,” from 1901. This publication presents the PCA problem under a purely geometric standpoint, describing how to find low-dimensional subspaces that best fit—in the least squares sense—a cloud of points. The other seminal work of PCA is the one by the American mathematician and economic theorist Harold Hotelling with “Analysis of a Complex of Statistical Variables into Principal Components,” from 1933. Unlike Pearson, Hotelling finds the principal components as orthogonal linear combinations of the variables of maximum variance. PCA is one of those methods that can be approached from multiple, seemingly unrelated, perspectives. The way we are going to introduce PCA is not the typical way in which PCA is discussed in most books published in English. However, our introduction is actually based on the ideas and concepts originally published in Karl Pearson’s 1901 paper On lines and planes of closest fit to systems of points in space. This is what can be considered to be the first paper on PCA, although keep in mind that Karl Pearson never used the term principal components analysis. That term was coined by Harold Hotelling, who formalized the method by giving it a more mature statistical perspective. 4.1 Low-dimensional Representations Let’s play the following game. Imagine for a minute that you have the superpower to see any type of multidimensional space (not just three-dimensions). As we mentioned before, we think of the individuals as forming a cloud of points in a \\(p\\)-dim space, and the variables forming a cloud of arrows in an \\(n\\)-dim space. Pretend that you have some data in which its cloud of points has the shape of a mug, like in the following diagram: Figure 4.1: Cloud of points in the form of a mug This mug is supposed to be high-dimensional, and something that you are not supposed to ever see in real life. So the question is: Is there a way in which we can get a low-dimensional representation of this data? Luckily, the answer is: YES, we can! How? Well, the name of the game is projections: we can look for projections of the data into sub-spaces of lower dimension, like in the diagram below. Figure 4.2: Various projections onto subspaces Think of projections as taking photographs or x-rays of the mug. You can take a photo of the mug from different angles. For instance, you can take a picture in which the lens of the camera is on the top of the mug, or another picture in which the lens is below the mug (from the bottom), and so on. If you have to take the “best” photograph of the mug, from what angle would you take such picture? To answer this question we need to be more precise about what do we mean by “best”. Here, we are talking about getting a picture in which the image of the mug is as similar as possible to the original object. As you can tell from the above figure, we have three candidate subspaces: \\(\\mathbb{H}_A\\), \\(\\mathbb{H}_B\\), and \\(\\mathbb{H}_C\\). Among the three possible projections, subspace \\(\\mathbb{H}_C\\) is the one that provides the best low dimensional representation, in the sense that the projected silhouette is the most similar to the original mug shape. Figure 4.3: The shape of the projection is similar to the original mug shape. We can say that the “photo” from projecting onto subspace \\(\\mathbb{H}_C\\) is the one that most resembles the original object. Now, keep in mind that the resulting image in the low-dimensional space is not capturing the whole pattern. In other words, there is some loss of information. However, by chosing the right projection, we hope to minimize such loss. 4.2 Projections Following the idea of projections, let’s now discuss with more detail this concept, and its implications. Pretend that we zoom in to see some of the individuals of the cloud of points that form the mug (see figure below). Keep in mind that these data points are in a \\(p\\)-dimensional space, which will also have its centroid (i.e. average individual). Figure 4.4: Set of individuals in p-dim space. Because our goal is to look for a low-dimensional represention, we can start by considering a one-dimensional space, that is, some axis. In the above diagram (as well as the one below) this dimension is depicted with the yellow line, labeled as \\(dim_{\\mathbf{v}}\\). Figure 4.5: Set of individuals in p-dim space. We should note that we don’t really manipulate \\(dim_{\\mathbf{v}}\\) directly. Instead, what we manipulate is a vector \\(\\mathbf{v}\\) along this dimension. Figure 4.6: Dimension spanned by vector v At the end of the day, we want to project the individuals onto this dimension. In particular, the type of projections that we are interested in are orthogonal projections. Figure 4.7: Projections onto one dimension 4.2.1 Vector and Scalar Projections Let’s consider a specific individual, for example the \\(i\\)-th individual. And let’s take the centroid as the origin of the cloud of points. In this way, the dimension that we are looking for has to pass thorugh the centroid of this cloud. Obtaining the orthogonal projection of the \\(i\\)-th individual onto the dimension \\(dim_{\\mathbf{v}}\\) involves projecting \\(\\mathbf{x_i}\\) onto \\(\\mathbf{v}\\). Figure 4.8: Projection of an individual onto one dimension Recall that an orthogonal projection can be split into two types of projections: (1) the vector projection, and (2) the scalar projection. The vector projection of \\(\\mathbf{x_i}\\) onto the \\(\\mathbf{v}\\) is defined as: \\[ proj_{\\mathbf{v}} (\\mathbf{x_i}) = \\frac{\\mathbf{x_{i}^\\mathsf{T} v}}{\\mathbf{v^\\mathsf{T}v}} \\hspace{1mm} \\mathbf{v} = \\mathbf{\\hat{v}} \\] The scalar projection of \\(\\mathbf{x_i}\\) onto \\(\\mathbf{v}\\) is defined as: \\[ comp_{\\mathbf{v}} (\\mathbf{x_i}) = \\frac{\\mathbf{x_{i}^\\mathsf{T}v}}{\\|\\mathbf{v}\\|} = z_{ik} \\] The following diagram displays both types of projections: Figure 4.9: Scalar and vector projections of i-th individual onto vector v We are not really interested in obtaining the vector projection \\(proj_{\\mathbf{v}} (\\mathbf{x_i})\\). Instead, what we care about is the scalar projection \\(comp_{\\mathbf{v}} (\\mathbf{x_i})\\). In other words, we just want to obtain the coordinate of the \\(i\\)-th individual along this axis. 4.2.2 Projected Inertia As we said before, our goal is to find the angle that gives us the “best” photo of the mug (i.e. cloud of points). This can be translated as finding a subspace in which the distances between the points is most similar to those on the actual mug. Figure 4.10: Projection Goal Now, we need to define what we mean by “similar” distances in \\(\\mathbb{H}\\). Consider the overall dispersion of our original system (the one with all \\(p\\) dimensions): \\(\\sum_{j} \\sum_{\\ell} d^2 (i, \\ell)\\). This is a fixed number; and we cannot change it. However, we can try fine-tune our subset \\(\\mathbb{H}\\) such that \\(\\sum_{j} \\sum_{\\ell} d^2 (i, \\ell) \\approx \\sum_{i} \\sum_{\\ell} d_H^2(i, \\ell)\\). \\[ \\sum_{j} \\sum_{\\ell} d^2 (i, \\ell) = \\ 2n \\sum_i d^2 (i, g) \\] However, it will be easier to simply consider the inertia of the system (as opposed to the overall dispersion). \\[ 2n \\sum_i d^2 (i, g) \\ \\to \\ \\frac{1}{n} \\sum_{i=1}^{n} d^2 (i, g) \\] In mathematical terms, finding the subspace \\(\\mathbb{H}\\) that gives us “similar” distances to those of the original space corresponds to maximizing the projected inertia: \\[ \\max_{\\mathbb{H}} \\left\\{ \\frac{1}{n} \\sum_{i=1}^{n} d_{\\mathbb{H}}^2 (i, g) \\right\\} \\] Now, as we discuss before, the simplest subspace will be one dimension, that is, we consider the case where \\(\\mathbb{H} \\subseteq \\mathbb{R}^1\\). So the first approach is to project our data points onto a vector \\(\\mathbf{v}\\). Hence, the projected inertia becomes: \\[ \\frac{1}{n} \\sum_{i=1}^{n} d_\\mathbb{H}^2 (i, g) = \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\mathbf{x_i}^\\mathsf{T} \\mathbf{v} \\right)^2 = \\frac{1}{n} \\sum_{i=1}^{n} z_i^2 \\] and our maximization problem becomes \\[ \\max_{\\mathbf{v}} \\left\\{ \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\mathbf{x_i}^\\mathsf{T} \\mathbf{v} \\right)^2 \\right\\} \\quad \\mathrm{s.t.} \\quad \\mathbf{v}^\\mathsf{T} \\mathbf{v} = 1 \\] For convenience, we add the restriction that \\(\\mathbf{v}\\) is a unit-vector (vector of unit norm). Why do we need the constraint? If we didn’t have the constraint, we could always find a \\(\\mathbf{v}\\) that obtains higher inertia, and things would explode. Furthermore, as we will see in the next sectoin, this helps in the algebra we will use when we actually perform the maximization. 4.3 Maximization Problem We now need to compute the projected Inertia. Since we have projected onto a line spanned by \\(\\mathbf{v}\\), the projected inertia \\(I_{\\mathbb{H}}\\) will simply be the variance of the projected data points: \\[ I_{\\mathbb{H}} = \\frac{1}{n} \\sum_{i=1}^{n} d_\\mathbb{H}^2(i, 0) = \\frac{1}{n} \\sum_{i=1}^{n} z_i^2 \\] (note that \\(g = 0\\) since we are assuming our data is mean-centered). We can simplify notation as follows: \\[ \\mathbf{z} := \\begin{pmatrix} z_1 \\\\ z_2 \\\\ \\vdots \\\\ z_n \\\\ \\end{pmatrix} \\ \\Longrightarrow \\ I_\\mathbb{H} = \\frac{1}{n} \\mathbf{z}^\\mathsf{T} \\mathbf{z} = \\frac{1}{n} \\mathbf{v}^\\mathsf{T} \\mathbf{X}^\\mathsf{T} \\mathbf{X} \\mathbf{v} \\] Hence, the maximization problem becomes \\[ \\max_{\\mathbf{v}} \\left\\{ \\frac{1}{n} \\mathbf{v}^\\mathsf{T} \\mathbf{X}^\\mathsf{T} \\mathbf{X} \\mathbf{v} \\right\\} \\qquad \\text{s.t.} \\qquad \\mathbf{v}^\\mathsf{T} \\mathbf{v} = 1 \\] To solve this maximization problem, we utilize the method of Lagrange Multipliers: \\[ \\mathcal{L} = \\frac{1}{n} \\mathbf{v}^\\mathsf{T} \\mathbf{X}^\\mathsf{T} \\mathbf{X} \\mathbf{v} - \\lambda \\left( \\mathbf{v}^\\mathsf{T} \\mathbf{v} - 1 \\right) \\] Calculating the derivative of \\(mathcal{L}\\) with respect to \\(mathbf{v}\\): \\[ \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{v}} = \\frac{2}{n} \\mathbf{X}^\\mathsf{T} \\mathbf{X} \\mathbf{v} - 2 \\lambda \\mathbf{v} = 0 \\ \\Rightarrow \\ \\underbrace{ \\frac{1}{n} \\mathbf{X}^\\mathsf{T} \\mathbf{X} \\mathbf{v} }_{:= \\mathbf{S} } = \\lambda \\mathbf{v} \\ \\Rightarrow \\ \\mathbf{S} \\mathbf{v} = \\lambda \\mathbf{v} \\] That is, we obtain the equation \\(\\mathbf{S} \\mathbf{v} = \\lambda \\mathbf{v}\\) where \\(\\mathbf{S} \\in \\mathbb{R}^{p \\times p}\\) is symmetric. This means that \\(\\mathbf{v}\\) is an eigenvector (with eigenvalue \\(\\lambda\\)) of \\(\\mathbf{S}\\). 4.3.1 Eigenvectors of \\(\\mathbf{S}\\) Assume that \\(\\mathbf{X}\\) has full rank (i.e. \\(\\mathrm{rank}(\\mathbf{X}) = p\\)). We then obtain \\(p\\) eigenvectors, which together form the matrix \\(\\mathbf{V}\\): \\[ \\mathbf{V} = \\begin{bmatrix} \\mathbf{v_1} &amp; \\mathbf{v_2} &amp; \\dots &amp; \\mathbf{v_k} &amp; \\dots &amp; \\mathbf{v_p} \\\\ \\end{bmatrix} \\] We can also obtain \\(\\mathbf{\\Lambda}_{p \\times p} := \\mathrm{diag}\\left\\{ \\lambda_i \\right\\}_{i=1}^{n}\\) (i.e. the matrix of eigenvalues): that is, \\[ \\mathbf{\\Lambda} = \\begin{bmatrix} \\lambda_1 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; \\lambda_2 &amp; \\dots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; \\lambda_p \\\\ \\end{bmatrix} \\] Finally, we can also obtain the matrix \\(\\mathbf{Z}_{n \\times p}\\) which is the matrix consisting of the vectors \\(\\mathbf{z_i}\\): \\[ \\mathbf{Z} = \\begin{bmatrix} \\mathbf{z_1} &amp; \\mathbf{z_2} &amp; \\dots &amp; \\mathbf{z_k} &amp; \\dots &amp; \\mathbf{z_p} \\\\ \\end{bmatrix} \\] The matrix \\(\\mathbf{V}\\) consists of the loadings The matrix \\(\\mathbf{Z}\\) contains the principal components (PC’s), also known as scores. Let us examine the \\(k\\)th principal component \\(\\mathbf{z_k}\\): \\[ \\mathbf{z_k} = \\mathbf{X v_k} = v_{k1} \\mathbf{x_1} + v_{k2} \\mathbf{x_2} + \\dots + v_{kp} \\mathbf{x_p} \\] (where \\(\\mathbf{x_k}\\) denotes columns of \\(\\mathbf{X}\\)). Note that \\(\\mathrm{mean}(\\mathbf{z_k}) = 0\\); since we are assuming that the data is mean-centered, we have that \\(\\mathrm{mean}(\\mathbf{x_1}) = 0\\). What about variance? \\[\\begin{align*} Var(\\mathbf{z_k}) &amp; = \\frac{1}{n} \\mathbf{z_{k}^\\mathsf{T}} \\mathbf{z_k} \\\\ &amp; = \\frac{1}{n}\\left( \\mathbf{X} \\mathbf{v_k} \\right)^{\\mathsf{T}} \\left( \\mathbf{X} \\mathbf{v_k} \\right) \\\\ &amp; = \\frac{1}{n} \\mathbf{v_{k}^\\mathsf{T}} \\mathbf{X}^\\mathsf{T} \\mathbf{X} \\mathbf{v_k} \\\\ &amp; = \\mathbf{v_k}^\\mathsf{T} \\mathbf{S} \\mathbf{v_k} \\\\ &amp; = \\mathbf{v_k}^\\mathsf{T} \\left( \\lambda_k \\mathbf{v_k} \\right) = \\lambda_k \\left( \\mathbf{v_k}^\\mathsf{T} \\mathbf{v_k} \\right) \\\\ &amp; = \\lambda_k \\end{align*}\\] Hence, we obtain the following interesting result: the \\(k\\)-th eigenvalue of \\(\\mathbf{S}\\) is simply the variance of the \\(k\\)-th principal component. If \\(\\mathbf{X}\\) is mean centered, then \\(\\mathbf{S} = \\frac{1}{n} \\mathbf{X^\\mathsf{T} X}\\) is nothing but the variance-covariance matrix of our data. If \\(\\mathbf{X}\\) is standardized (i.e. mean-centered and scaled by the variance), then \\(\\mathbf{S}\\) becomes the correlation matrix. In summary: \\(\\mathrm{mean}(\\mathbf{z_k}) = 0\\) \\(Var(\\mathbf{z_k}) = \\lambda_k\\) \\(\\mathrm{sd}(\\mathbf{z_k}) = \\sqrt{\\lambda_k}\\) We have the following fact (the proof is omitted, and may be assigned as homework or as a test question): \\[ I = \\frac{1}{n} \\sum_{i=1}^{n} d^2(i, g) = \\sum_{k} \\lambda_k = \\mathrm{tr}\\left( \\frac{1}{n} \\mathbf{X^\\mathsf{T} X} \\right) \\] The dimensions that we find in our analysis (through \\(\\mathbf{v_k}\\)) relates directly to \\(\\mathbf{z_k}\\). \\(\\sum_{k=1}^{p} \\lambda_k\\) relates directly to the total amount of variability of our data. Remark: The principal components capture different parts of the variability of the full data. 4.4 Another Perspective of PCA The overall idea behind PCA is the following. Given a set of \\(p\\) variables \\(\\mathbf{x_1}, \\mathbf{x_2}, \\dots, \\mathbf{x_p}\\), we want to obtain new \\(k\\) variables \\(\\mathbf{z_1}, \\mathbf{z_2}, \\dots, \\mathbf{z_k}\\), called the Principal Components (PCs). A principal component is a linear combination: \\(\\mathbf{z} = \\mathbf{Xv}\\). The first PC is a linear mix: Figure 4.11: PCs as linear combinations of X-variables The second PC is another linear mix: Figure 4.12: PCs as linear combinations of X-variables We want to compute the PCs as linear combinations of the original variables. \\[ \\begin{array}{c} \\mathbf{z_1} = v_{11} \\mathbf{x_1} + \\dots + v_{p1} \\mathbf{x_p} \\\\ \\mathbf{z_2} = v_{12} \\mathbf{x_1} + \\dots + v_{p2} \\mathbf{x_p} \\\\ \\vdots \\\\ \\mathbf{z_k} = v_{1k} \\mathbf{x_1} + \\dots + v_{pk} \\mathbf{x_p} \\\\ \\end{array} \\] Or in matrix notation: \\[ \\mathbf{Z} = \\mathbf{X V} \\] where \\(\\mathbf{Z}\\) is an \\(n \\times k\\) matrix of principal components, and \\(\\mathbf{V}\\) is a \\(p \\times k\\) matrix of weights, also known as directional vectors of the principal axes. The following figure shows a graphical representation of a PCA problem in diagram notation: Figure 4.13: PCs as linear combinations of X-variables We look to transform the original variables into a smaller set of new variables, the Principal Components (PCs), that summarize the variation in data. The PCs are obtained as linear combinations (i.e. weighted sums) of the original variables. We look for PCs is in such a way that they have maximum variance, and being mutually orthogonal. 4.4.1 Finding Principal Components The way to find principal components is to construct them as weighted sums of the original variables, looking to optimize some criterion and following some constraints. One way in which we can express the criterion is to require components \\(\\mathbf{z_1}, \\mathbf{z_2}, \\dots, \\mathbf{z_k}\\) that capture most of the variation in the data \\(\\mathbf{X}\\). “Capturing most of the variation,” implies looking for a vector \\(\\mathbf{v_h}\\) such that a component \\(\\mathbf{z_h} = \\mathbf{X v_h}\\) has maximum variance: \\[ \\max_{\\mathbf{v_h}} \\; var(\\mathbf{z_h}) \\quad \\Rightarrow \\quad \\max_{\\mathbf{v_h}} \\; var(\\mathbf{X v_h}) \\] that is \\[ \\max_{\\mathbf{v_h}} \\; \\frac{1}{n} \\mathbf{v_{h}^\\mathsf{T} X^\\mathsf{T} X v_h} \\] As you can tell, this is a maximization problem. Without any constraints, this problem is unbounded, not to mention useless. We could take \\(\\mathbf{v_h}\\) as bigger as we want without being able to reach any maximum. To get a feasible solution we need to impose some kind of restriction. The standard adopted constraint is to require \\(\\mathbf{v_h}\\) to be of unit norm: \\[ \\| \\mathbf{v_h} \\| = 1 \\; \\hspace{1mm} \\Rightarrow \\; \\hspace{1mm} \\mathbf{v_{h}^\\mathsf{T} v_h} = 1 \\] Note that \\((1/n) \\mathbf{X^\\mathsf{T} X}\\) is the variance-covariance matrix. If we denote \\(\\mathbf{S} = (1/n) \\mathbf{X^\\mathsf{T} X}\\) then the criterion to be maximized is: \\[ \\max_{\\mathbf{v_h}} \\; \\mathbf{v_{h}^\\mathsf{T} S v_h} \\] subject to \\(\\mathbf{v_{h}^\\mathsf{T} v_h} = 1\\) To avoid a PC \\(\\mathbf{z_h}\\) from capturing the same variation as other PCs \\(\\mathbf{z_l}\\) (i.e. avoiding redundant information), we also require them to be mutually orthogonal so they are uncorrelated with each other. Formally, we impose the restriction \\(\\mathbf{z_h}\\) to be perpendicular to other components: \\(\\mathbf{z_{h}^\\mathsf{T} z_l} = 0; (h \\neq l)\\). 4.4.2 Finding the first PC In order to get the first principal component \\(\\mathbf{z_1} = \\mathbf{X v_1}\\), we need to find \\(\\mathbf{v_1}\\) such that: \\[ \\max_{\\mathbf{v_1}} \\; \\mathbf{v_{1}^\\mathsf{T} S v_1} \\] subject to \\(\\mathbf{v_{1}^\\mathsf{T} v_1} = 1\\) Being a maximization problem, the typical procedure to find the solution is by using the Lagrangian multiplier method. Using Lagrange multipliers we get: \\[ \\mathbf{v_{1}^\\mathsf{T} S v_1} - \\lambda (\\mathbf{v_{1}^\\mathsf{T} v_1} - 1) = 0 \\] Differentiation with respect to \\(\\mathbf{v_1}\\), and equating to zero gives: \\[ \\mathbf{S v_1} - \\lambda_1 \\mathbf{v_1} = \\mathbf{0} \\] Rearranging some terms we get: \\[ \\mathbf{S v_1} = \\lambda_1 \\mathbf{v_1} \\] What does this mean? It means that \\(\\lambda_1\\) is an eigenvalue of \\(\\mathbf{S}\\), and \\(\\mathbf{v_1}\\) is the corresponding eigenvector. 4.4.3 Finding the second PC In order to find the second principal component \\(\\mathbf{z_2} = \\mathbf{X v_2}\\), we need to find \\(\\mathbf{v_2}\\) such that \\[ \\max_{\\mathbf{v_2}} \\; \\mathbf{v_{2}^\\mathsf{T} S v_2} \\] subject to \\(\\| \\mathbf{v_2} \\| = 1\\) and \\(\\mathbf{z_{1}^\\mathsf{T} z_2} = 0\\). Remember that \\(\\mathbf{z_2}\\) must be uncorrelated to \\(\\mathbf{z_1}\\). Applying the Lagrange multipliers, it can be shown that the desired \\(\\mathbf{v_2}\\) is such that \\[ \\mathbf{S v_2} = \\lambda_2 \\mathbf{v_2} \\] In other words. \\(\\lambda_2\\) is an eigenvalue of \\(\\mathbf{S}\\) and \\(\\mathbf{v_2}\\) is the corresponding eigenvector. 4.4.4 Finding all PCs All PCs can be found simultaneously by diagonalizing \\(\\mathbf{S}\\). Diagonalizing \\(\\mathbf{S}\\) involves expressing it as the product: \\[ \\mathbf{S} = \\mathbf{V \\Lambda V^\\mathsf{T}} \\] where: \\(\\mathbf{D}\\) is a diagonal matrix the elements in the diagonal of \\(\\mathbf{D}\\) are the eigenvalues of \\(\\mathbf{S}\\) the columns of \\(\\mathbf{V}\\) are orthonormal: \\(\\mathbf{V^\\mathsf{T} V= I}\\) the columns of \\(\\mathbf{V}\\) are the eigenvectors of \\(\\mathbf{S}\\) \\(\\mathbf{V^\\mathsf{T}} = \\mathbf{V^{-1}}\\) Diagonalizing a symmetric matrix is nothing more than obtaining its eigenvalue decomposition (a.k.a. spectral decomposition). A \\(p \\times p\\) symmetric matrix \\(\\mathbf{S}\\) has the following properties: \\(\\mathbf{S}\\) has \\(p\\) real eigenvalues (counting multiplicites) the eigenvectors corresponding to different eigenvalues are orthogonal \\(\\mathbf{S}\\) is orthogonally diagonalizable (\\(\\mathbf{S} = \\mathbf{V \\Lambda V^\\mathsf{T}}\\)) the set of eigenvalues of \\(\\mathbf{S}\\) is called the spectrum of \\(\\mathbf{S}\\) In summary: The PCA solution can be obtained with an Eigenvalue Decomposition of the matrix \\(\\mathbf{S} = (1/n) \\mathbf{X^\\mathsf{T}X}\\) 4.5 Data Decomposition Model Formally, PCA involves finding scores and loadings such that the data can be expressed as a product of two matrices: \\[ \\underset{n \\times p}{\\mathbf{X}} = \\underset{n \\times r}{\\mathbf{Z}} \\underset{r \\times p}{\\mathbf{V^\\mathsf{T}}} \\] where \\(\\mathbf{Z}\\) is the matrix of PCs or scores, and \\(\\mathbf{V}\\) is the matrix of loadings. We can obtain as many different eigenvalues as the rank of \\(\\mathbf{S}\\) denoted by \\(r\\). Ideally, we expect \\(r\\) to be smaller than \\(p\\) so we get a convenient data reduction. But usually we will only retain just a few PCs (i.e. \\(k \\ll p\\)) expecting not to lose too much information: \\[ \\underset{n \\times p}{\\mathbf{X}} \\approx \\underset{n \\times k}{\\mathbf{Z}} \\hspace{1mm} \\underset{k \\times p}{\\mathbf{V^\\mathsf{T}}} + \\text{Residual} \\] The previous expression means that just a few PCs will optimally summarize the main structure of the data 4.5.1 Alternative Approaches Finding \\(\\mathbf{z_h} = \\mathbf{X v_h}\\) with maximum variance has another important property that it is not always mentioned in multivariate textbooks but that we find worth mentioning. \\(\\mathbf{z_h}\\) is such that \\[ \\max \\sum_{j = 1}^{p} cor^2(\\mathbf{z_h, x_j}) \\] What this expression implies is that principal components \\(\\mathbf{z_h}\\) are computed to be the best representants in terms of maximizing the sum of squared correlations with the variables \\(\\mathbf{x_1}, \\mathbf{x_2}, \\dots, \\mathbf{x_j}\\). Interestingly, you can think of PCs as predictors of the variables in \\(\\mathbf{X}\\). Under this perspective, we can reverse the relations and see PCA from a regression-like model perspective: \\[ \\mathbf{x_j} = v_{jh} \\mathbf{z_h} + \\mathbf{e_h} \\] Notice that the regression coefficient is the \\(j\\)-th element of the \\(h\\)-th eigenvector. "],
["ols.html", "5 Linear Regression 5.1 Motivation 5.2 The Idea/Intuition of Regression 5.3 The Linear Regression Model 5.4 The Error Measure 5.5 The Least Squares Algorithm 5.6 Geometries of OLS", " 5 Linear Regression Before entering supervised learning territory, we want to discuss the general framework of linear regression. We will introduce this topic from a pure model-fitting point of view. In other words, we will postpone the learning aspect (the prediction aspect) after the chapter of theory of learning. The reason to cover linear regression in this way is for us to have something to work with when we start talking about the theory of supervised learning. 5.1 Motivation Consider, again, the NBA dataset example from previous chapters. Suppose we want to use this data to predict the salary of NBA players, in terms of certain variables like player’s team, player’s height, player’s weight, player’s position, player’s years of professional experience, player’s number of 2pts, player’ number of 3 points, number of blocks, etc. Of course, we need information on the salaries of some current NBA player’s: Player Height Weight Yrs Expr 2 Points 3 Points 1 \\(\\bigcirc\\) \\(\\bigcirc\\) \\(\\bigcirc\\) \\(\\bigcirc\\) \\(\\bigcirc\\) 2 \\(\\bigcirc\\) \\(\\bigcirc\\) \\(\\bigcirc\\) \\(\\bigcirc\\) \\(\\bigcirc\\) 3 \\(\\bigcirc\\) \\(\\bigcirc\\) \\(\\bigcirc\\) \\(\\bigcirc\\) \\(\\bigcirc\\) … … … … … … As usual, we use the symbol \\(\\mathbf{x_i}\\) to denote the vector of measurements of player \\(i\\)’s statistics (e.g. height, weight, etc.); in turn, the salary of the \\(i\\)-th player is represented with \\(y_i\\). Ideally, there exists some function \\(f : \\mathcal{X} \\to \\mathcal{y}\\) (i.e. a function that takes values from \\(\\mathcal{X}\\) space and maps them to a single value \\(y\\)). We will refer to this function the ideal “formula” for salary. Here we are using the word formula in a very lose sense, and not necessarily using the word “formula” in the mathematical sense. We now seek a hypothesized model (which we call \\(\\widehat{f} : \\mathcal{X} \\to y\\)), which we select from some set of candidate functions \\(h_1, h_2, \\dots h_m\\). Our task is to obtain \\(\\widehat{f}\\) in a way that we can claim that it is a good approximation of the (unknown) function \\(f\\). 5.2 The Idea/Intuition of Regression Consider again the NBA dataset, and again let \\(y_i\\) denote the salary for the \\(i\\)-th player (ignoring inflation). Say we now have a new prospective player from Europe; and we are tasked with predicting their salary. Scenario 1. Suppose we now have no information on this new player. How would we compute \\(y_{0}\\) (i.e. the salary of this new player)? One possibility is to guestimate \\(y_0\\) using the historical average salary \\(\\bar{y}\\) of NBA players. In other words, we would simply calculate: \\(\\hat{y}_0 = \\bar{y}\\). In this case we are using \\(\\bar{y}\\) as the typical score (e.g. a measure of center) as a plausible guestimate for \\(y_0\\). We could also look at the median of the existing salaries, if we are concerned about outliers. Scenario 2. Now, suppose we know that this new player will sign on to the LA Lakers. We could then use \\(\\hat{y}_0 = \\text{avg}(\\text{Laker&#39;s Salaries})\\): that is, the average salary of all Laker’s players. Figure 5.1: Average salary by team Scenario 3. Similarly, if we know this new player’s years of experience (e.g. 6 years), we would look at the average of salaries corresponding to players with the same level of experience. Figure 5.2: Average salary by years of experience What do the three previous scenarios correspond to? In all of these examples, the prediction is basically a conditional mean: \\(\\hat{y}_0 = \\text{ave}(y_i|x_i = x_0)\\). Of course, the previous strategy only makes sense when we have data points \\(x_i\\) that are equal to \\(x_0\\). But what if none of the available \\(x_i\\) values are equal to \\(x_0\\)? We’ll talk about this later. The previous hypothetical scenarios illustrate the core idea of regression: we predict using quantities of the form: \\[ \\mathbb{E}(y_i \\mid x_{i1}^{*} , x_{i2}^{*}, \\dots, x_{ip}^{*}) \\] where \\(x_{ij}^{*}\\) represents the \\(i\\)-th measurement of the \\(j\\)-th variable. The above equation is what we call the regression function; note that the regression function is nothing more than a conditional expectation! 5.3 The Linear Regression Model A regression model: use one or more features \\(X\\) to say something about the reponse \\(Y\\). A linear regression model tells us to combine our features in a linear way in order to approximate the response: \\[ \\hat{Y} = b_0 + b_1 X \\] In pointwise format, that is for a given individual \\(i\\), we have: \\[ \\hat{y}_i = b_0 + b_1 x_i \\] In vector notation: \\[ \\mathbf{\\hat{y}} = b_0 + b_1 \\mathbf{x} \\] To simplify notation, sometimes we prefer to add an auxiliary constant feature in the form of a vector of 1’s with \\(n\\) elements. \\[ \\mathbf{X} = \\ \\begin{bmatrix} 1 &amp; x_{1} \\\\ 1 &amp; x_{2} \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; x_{n} \\\\ \\end{bmatrix}, \\qquad \\mathbf{\\hat{y}} = \\begin{bmatrix} \\hat{y}_{1} \\\\ \\hat{y}_{2} \\\\ \\vdots \\\\ \\hat{y}_{n} \\\\ \\end{bmatrix}, \\qquad \\mathbf{b} = \\begin{bmatrix} b_{0} \\\\ b_{1} \\\\ \\end{bmatrix} \\] In the multidimensional case when we have \\(p&gt;1\\) predictors: \\[ \\mathbf{X} = \\ \\begin{bmatrix} 1 &amp; x_{11} &amp; \\dots &amp; x_{1n} \\\\ 1 &amp; x_{12} &amp; \\dots &amp; x_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; x_{1n} &amp; \\dots &amp; x_{nn} \\\\ \\end{bmatrix}, \\qquad \\mathbf{\\hat{y}} = \\begin{bmatrix} \\hat{y}_{1} \\\\ \\hat{y}_{2} \\\\ \\vdots \\\\ \\hat{y}_{n} \\\\ \\end{bmatrix}, \\qquad \\mathbf{b} = \\begin{bmatrix} b_{0} \\\\ b_{1} \\\\ \\vdots \\\\ b_{p} \\\\ \\end{bmatrix} \\] With the matrix of features, the response, and the coefficients we have: \\[ \\mathbf{\\hat{y}} = \\mathbf{Xb} \\] In path diagram form, the linear model looks like this: Figure 5.3: Linear combination with constant term If we assume centered predictors then we don’t have to worry about the constant term: Figure 5.4: Linear combination without constant term Obviously the question becomes: how do we obtain the vector of coefficients \\(\\mathbf{b}\\)? 5.4 The Error Measure We would like to get \\(\\hat{y}_i\\) to be “as close as” possible to \\(y_i\\). This requires to come up with some type of measure of closeness. Among the various functions that we can use to measure how close \\(\\hat{y}_i\\) and \\(y_i\\) are, the most common option is to use the squared distance between such values: \\[ d^2(y_i, \\hat{y}_i) = (y_i - \\hat{y}_i)^2 = (\\hat{y}_i - y_i)^2 \\] Replacing \\(\\hat{y}_i\\) with \\(\\mathbf{b^\\mathsf{T}\\vec{x}_i}\\) we have: \\[ d^2(y_i, \\hat{y}_i) = (\\mathbf{b^\\mathsf{T}\\vec{x}_i} - y_i)^2 \\] Notice that \\(d^2(y_i, \\hat{y}_i)\\) is a pointwise error measure. But we need to define a global measure of error. This is typically done by adding all the pointwise error measures \\(e_{i}^{2} = (\\hat{y}_i - y_i)^2\\). There are two flavors of overall error measures based on squared pointwise differences: (1) the sum of squared errors or \\(\\text{SSE}\\), and (2) the mean squared error or \\(\\text{MSE}\\). The sum of squared errors, \\(\\text{SSE}\\), is defined as: \\[ \\text{overall error} = \\sum_{i=1}^{n} \\text{err}_i = \\text{SSE} \\] The mean squared error, \\(\\text{MSE}\\), is defined as: \\[ \\text{overall error} = \\frac{1}{n} \\sum_{i=1}^{n} \\text{err}_i = \\text{MSE} \\] As you can tell, \\(\\text{SSE} = n \\text{MSE}\\), and viceversa, \\(\\text{MSE} = \\text{SSE} / n\\) Throughout this book, unless mentioned otherwise, when dealing with regression problems, we will consider the \\(\\text{MSE}\\) as the default overall error function to be minimized (you could also take \\(\\text{SSE}\\) instead). \\[ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (\\mathbf{b^\\mathsf{T}\\vec{x}_i} - y_i)^2 = (\\mathbf{Xb} - \\mathbf{y})^\\mathsf{T} (\\mathbf{Xb} - \\mathbf{y}) \\] 5.5 The Least Squares Algorithm We want to minimize the mean of squared errors. So we compute the derivative of \\(\\text{MSE}\\) with respect to \\(\\mathbf{b}\\): \\[ \\frac{\\partial }{\\partial \\mathbf{b}}\\text{MSE}(\\mathbf{b}) = \\frac{\\partial }{\\partial \\mathbf{b}} (\\mathbf{b^\\mathsf{T}X^\\mathsf{T}Xb - 2 b^\\mathsf{T}X^\\mathsf{T}y + \\mathbf{y^\\mathsf{T}y}}) \\] which becomes: \\[ 2 \\mathbf{X^\\mathsf{T}Xb} - 2 \\mathbf{X^\\mathsf{T}y} \\] Equating to zero we get that: \\[ \\mathbf{X^\\mathsf{T}Xb} = \\mathbf{X^\\mathsf{T}y} \\quad (\\text{normal equations}) \\] These are the so-called Normal equations. It is a system of \\(n\\) equations with \\(p+1\\) unknowns. If the cross-product matrix \\(\\mathbf{X^\\mathsf{T}X}\\) is invertible, which is not a minor assumption, then the vector of regression coefficients \\(\\mathbf{b}\\) that we are looking for is given by: \\[ \\mathbf{b} = (\\mathbf{X^\\mathsf{T}X})^{-1} \\mathbf{X^\\mathsf{T}y} \\] Having obtained \\(\\mathbf{b}\\), we can easily compute the response vector: \\[\\begin{align*} \\mathbf{\\hat{y}} &amp;= \\mathbf{Xb} \\\\ &amp;= \\mathbf{X} (\\mathbf{X^\\mathsf{T}X})^{-1} \\mathbf{X^\\mathsf{T} y} \\end{align*}\\] If we denote \\(\\mathbf{H} = \\mathbf{X} (\\mathbf{X^\\mathsf{T}X})^{-1} \\mathbf{X^\\mathsf{T}}\\), then the predicted response is: \\(\\mathbf{\\hat{y}} = \\mathbf{Hy}\\). This matrix is better known as the hat matrix, because it puts the hat on the response. More importantly, the \\(\\mathbf{H}\\) is an orthogonal projector. From linear algebra, orthogonal projectors have very interesting properties: they are symmetric they are idempotent their eigenvalues are either 0 or 1 5.6 Geometries of OLS Now that we’ve seen the algebra, it’s time to look at the geometric interpretation of all the action that is going on within linear regression via OLS. We will discuss three geometric perspectives: OLS from the individuals point of view (i.e. rows of the data matrix). OLS from the variables point of view (i.e. columns of the data matrix). OLS from the parameters point of view, and the error surface. 5.6.1 Rows Perspective This is probably the most popular perspective covered in most textbooks. Consider a \\((p + 1)\\)-dimensional space. For illustration purposes let’s assume that our data has just \\(p=1\\) predictor. In other words, we have the response \\(Y\\) and one predictor \\(X\\). We can depict individuals as points in this space: Figure 5.5: Scatterplot of individuals In linear regression, we want to predict \\(y_i\\) by linearly mixing the inputs \\(\\hat{y}_{i} = b_0 + b_1 x_i\\). In two dimensions, the fitted model corresponds to a line. In three dimensions it would correspond to a plane. And in higher dimensions this would corresponds to a hyperplane. Figure 5.6: Scatterplot with regression line With a fitted line, we obtain predicted values \\(\\hat{y}_i\\). Some predicted values may be equal to the observed values. Other predicted values will be greater than the observed values. And some predicted values will be smaller than the observed values. Figure 5.7: Observed values and predicted values As you can imagine, given a set of data points, you can fit an infinite number of lines (in general). So which line are we looking for? We want to obtain the line that minimizes the square of the errors \\(e_i = \\hat{y}_{i} - y_{i}\\). In the figure below, these errors are represented by the vertical difference between the observed values \\(y_i\\) and the predicted values \\(\\hat{y}_i\\). Figure 5.8: OLS focuses on minimizing the squared errors Combining all residuals, we want to obtain parameters \\(b_0, \\dots, b_p\\) that minimize the squared norm of the residuals: \\[ \\sum_{i=1}^{n} e_{i}^{2} = \\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2 = \\sum_{i=1}^{n} (b_0 + b_1 x_i - y_i)^2 \\] In vector-matrix form we have: \\[\\begin{align*} \\| \\mathbf{e} \\|^2 &amp;= \\| \\mathbf{\\hat{y}} - \\mathbf{y} \\|^2 \\\\ &amp;= \\| \\mathbf{Xb} - \\mathbf{y} \\|^2 \\\\ &amp;= (\\mathbf{Xb} - \\mathbf{y})^\\mathsf{T} (\\mathbf{Xb} - \\mathbf{y}) \\end{align*}\\] 5.6.2 Columns Perspective This is less common than the rows perspective. Imagine variables in an \\(n\\)-dimensional space, both the response and the predictors. In this space, the \\(X\\) variables will span some subspace \\(\\mathbb{S}_{X}\\). This subspace is not supposed to contain the response—unless \\(Y\\) happens to be a linear combination of \\(X_1, \\dots, X_p\\). Figure 5.9: Features and Response view What are we looking for? We’re looking for a linear combination \\(\\mathbf{Xb}\\). As you can tell, there’s an infinite number of linear combinations that can be formed with \\(X_1, \\dots, X_p\\). Figure 5.10: Linear combination of features The mix of features that we are interested in, \\(\\mathbf{\\hat{y}} = \\mathbf{Xb}\\), is the one that gives us the closest approximation to \\(\\mathbf{y}\\). Figure 5.11: Linear combination to be as close as possible to response Now, what do we mean by closest approximation? How do we determine the closeness between \\(\\mathbf{\\hat{y}}\\) and \\(\\mathbf{y}\\)? By looking at the difference, which results in a vector \\(\\mathbf{e} = \\mathbf{\\hat{y}} - \\mathbf{y}\\). And then measuring the size or norm of this vector. Well, the squared norm to be precise. In other words, we want to obtain \\(\\mathbf{\\hat{y}}\\) such that the squared norm \\(\\| \\mathbf{e} \\|^2\\) is as small as possible. \\[ \\text{Minimize} \\quad \\| \\mathbf{e} \\|^2 = \\| \\mathbf{\\hat{y}} - \\mathbf{y} \\|^2 \\] 5.6.3 Parameters Perspective We could also visualize the regression problem from the point of view of the parameters \\(\\mathbf{b}\\) and the error surface. This is the least common perspective discussed in the literature that has to do with linear regression in general. However, it is not that uncommon within the Statistical Learning literature. For illustration purposes, assume that we have only two predictors \\(X_1\\) and \\(X_2\\). Recall that the Mean Squared Error (MSE) is: \\[ E(\\mathbf{y}, \\mathbf{\\hat{y}}) = \\frac{1}{n} \\left( \\mathbf{b^\\mathsf{T} X^\\mathsf{T}} \\mathbf{X b} - 2 \\mathbf{b^\\mathsf{T} X^\\mathsf{T} y} + \\mathbf{y^\\mathsf{T}y} \\right) \\] Now, from the point of view of \\(\\mathbf{b} = (b_1, b_2)\\), we can classify the order of each term: \\[ E(\\mathbf{y}, \\mathbf{\\hat{y}}) = \\frac{1}{n} ( \\underbrace{\\mathbf{b^\\mathsf{T} X^\\mathsf{T}} \\mathbf{X b}}_{\\text{Quadratic Form}} - \\underbrace{ 2 \\mathbf{b^\\mathsf{T} X^\\mathsf{T} y}}_{\\text{Linear}} + \\underbrace{ \\mathbf{y^\\mathsf{T}y}}_{\\text{Constant}} ) \\] Since \\(\\mathbf{X^\\mathsf{T} X}\\) is positive semidefinite, we know that \\(\\mathbf{b^\\mathsf{T} X^\\mathsf{T} Xb} \\geq 0\\). Furthermore, we know that (from vector calculus) it will be a paraboloid (bowl-shaped surface) in the \\((E, b_1, b_2)\\) space. The following diagram depicts this situation. Figure 5.12: Error Surface Imagine that we get horizontal slices of the error surface. For any of those slices, we can project them onto the plane spanned by the parameters \\(b_1\\) and \\(b_2\\). The resulting projections will be like a topographic map, with error contours on this plane. In general, those contours will be ellipses. Figure 5.13: Error Surface with slices, and their projections With quadratic error surfaces like this, they have a minimum value, and we are guaranteed the existence of \\(\\mathbf{b}^* = (b_1^{*}, b_2^{*})\\) s.t. \\(\\mathbf{b^\\mathsf{T} X^\\mathsf{T} X b}\\) is minimized. This is a powerful result! Consider, for example, a parabolic cylinder. Such a shape has no unique minimum; rather, it has an infinite number of points (all lying on a line) that minimize the function. The point being; with positive semi-definite matrices, we never have this latter case. Figure 5.14: Error Surface with contour errors and the minimum The minimum of the error surface occurs at the point \\((b_{1}^{*}, b_{2}^{*})\\). This is the precisely the OLS solution. "],
["gradient.html", "6 Gradient Descent 6.1 Error Surface 6.2 Idea of Gradient Descent 6.3 Gradient Descent and our Model", " 6 Gradient Descent Before moving to the next part of the book that deals with the theory of learning, we want to introduce a very popular optimization technique that is commonly used in many statistical learning methods: the famous gradient descent algorithm. 6.1 Error Surface Consider again the error surface: Figure 6.1: Error Surface We previously saw that the vector \\(\\mathbf{w}^{*}\\) that minimizes the error surface is exactly our OLS estimate for \\(\\mathbf{w}\\), our vector of regression coefficients. In the previous chapter, we used direct methods (i.e. computing the derivative of the error function and setting it equal to \\(\\mathbf{0}\\)) to find this minimum point. Interestingly, we can also use iterative methods to compute this minimum. The iterative method that we will discuss in this chapter is called gradient descent. 6.2 Idea of Gradient Descent The main idea of gradient descent is as follows: we start with an arbitrary point \\(\\mathbf{w}^{(0)} = ( w_0^{(0)}, w_1^{(0)}, \\dots, w_p^{(0)} )\\) somewhere on the error surface. We then “move down” the surface to obtain a new vector \\(\\mathbf{w}^{(1)}\\), closer to the minimizing point \\(\\mathbf{w}^{*}\\). For illustrative purposes, consider a \\(1-\\)dimensional error “surface”: Figure 6.2: One dimensional error surface What do we mean by “moving down?” Well, mathematically, this means we generate the new vector \\(\\mathbf{w}^{(1)}\\) using \\[ \\mathbf{w}^{(1)} = \\mathbf{w}^{(0)} + \\alpha \\mathbf{v} \\] We call \\(\\alpha\\) the step size (intuitively, \\(\\alpha\\) tells us how far down the surface we are moving) and we set \\(\\mathbf{v}\\) to be a unit vector, in some direction. We will discuss how to find the direction of \\(\\mathbf{v}\\) in a little bit. Right now let’s just focus on generating new vectors in this manner: \\[\\begin{align*} \\mathbf{w}^{(2)} &amp; = \\mathbf{w}^{(1)} + \\alpha \\mathbf{v}^{(1)} \\\\ \\mathbf{w}^{(3)} &amp; = \\mathbf{w}^{(2)} + \\alpha \\mathbf{v}^{(2)} \\\\ \\vdots &amp; \\hspace{10mm} \\vdots \\\\ \\mathbf{w}^{(s+1)} &amp; = \\mathbf{w}^{(s)} + \\alpha \\mathbf{v}^{(s)} \\end{align*}\\] Note that we are assuming a constant step size; that is, note that \\(\\alpha\\) remains the same at each iteration. There are more sophisticated versions of gradient descent that allow a variable step size, however we will not consider that case. As we can see in the \\(1-\\)D example above, the direction in which we travel will change at each step of the process. We will also see this mathematically in the next subsection. 6.2.1 The direction of \\(\\mathbf{v}\\) How do we find the direction of \\(\\mathbf{v}\\)? Consider the gradient of our error function. The gradient always points in the direction of steepest ascent (i.e. largest positive change). Hence, we want to travel in the exact opposite direction of the gradient. Let’s “prove” this mathematically. In terms of the error function itself, what does it mean for our vectors \\(\\mathbf{w}^{(s+1)}\\) to be “getting closer” to the minimizing point? Well, it means that the error at point \\(\\mathbf{w}^{(s+1)}\\) is less than the error at point \\(\\mathbf{w}^{(s)}\\). Hence, we examine the difference between the errors at these two points: \\[\\begin{align*} \\Delta E_{\\mathbf{w}} &amp; := E\\left( \\mathbf{w}^{(s + 1)} \\right) - E\\left( \\mathbf{w}^{(s)} \\right) \\\\ &amp; = E \\left( \\mathbf{w}^{(s)} + \\alpha \\mathbf{v} \\right) - E\\left( \\mathbf{w}^{(s)} \\right) \\end{align*}\\] In order for our vector \\(\\mathbf{w}^{(s+1)}\\) to be closer to the minimizing point that \\(\\mathbf{w}^{(s)}\\), we want this quantity to be as negative as possible. To find the \\(\\mathbf{v}\\) that makes this true, we need to use a trick: Taylor Expand \\(E(\\mathbf{w}^{(s)} + \\alpha \\mathbf{v})\\). Doing so, we obtain: \\[\\begin{align*} \\Delta E_{\\mathbf{w}} &amp; = E\\left( \\mathbf{w}^{(s)} \\right) + \\nabla E\\left( \\mathbf{w}^{(s)} \\right)^{\\mathsf{T}} \\left(\\alpha \\mathbf{v} \\right) + O \\left( \\alpha^2 \\right) - E\\left( \\mathbf{w}^{(s)} \\right) \\\\ &amp; = \\alpha \\nabla E \\left( \\mathbf{w}^{(s)} \\right)^{\\mathsf{T}} \\mathbf{v} + O(\\alpha^2) \\end{align*}\\] Hence, we must examine \\([ \\nabla E(\\mathbf{w}^{(s)})]^{\\mathsf{T}} \\mathbf{v}\\). For notational convenience, let us (temporarily) define \\(\\mathbf{u} : = \\nabla E ( \\mathbf{w}^{(s)} )\\) With respect to the orientation of \\(\\mathbf{v}\\) and \\(\\mathbf{u}\\), we can consider three prototypical cases: \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are either antiparallel, parallel, or orthogonal. Figure 6.3: Three prototypical cases When \\(\\mathbf{u} \\parallel \\mathbf{v}\\) (i.e. the second case above), then \\(\\mathbf{u}^\\mathsf{T}\\mathbf{v} = \\| \\mathbf{u} \\|\\). When \\(\\mathbf{u} \\not\\parallel \\mathbf{v}\\) (i.e. the first case above), then \\(\\mathbf{u}^\\mathsf{T} = - \\| \\mathbf{u} \\|\\). And, of course, if \\(\\mathbf{u} \\perp \\mathbf{v}\\) then \\(\\mathbf{u}^\\mathsf{T} \\mathbf{v} = 0\\). Therefore, in any of the three cases, we have that \\[ \\mathbf{u}^\\mathsf{T} \\mathbf{v} \\geq - \\left\\| \\mathbf{u} \\right\\| \\] Hence, recalling that \\(\\mathbf{u} := \\nabla E(\\mathbf{w}^{(s)})\\), we can plug this result into our error computation to obtain: \\[\\begin{align*} \\Delta E_{\\mathbf{w}} &amp; = \\alpha \\nabla E \\left( \\mathbf{w}^{(s)} \\right)^{\\mathsf{T}} \\mathbf{v} + O(\\alpha^2) \\\\ &amp; \\geq - \\alpha \\left\\| \\nabla E\\left( \\mathbf{w}^{(s)} \\right) \\right\\| \\mathbf{v} \\end{align*}\\] Thus, to make \\(\\Delta E_{\\mathbf{w}}\\) as negative as possible, we should take \\(\\mathbf{v}\\) parallel to \\(( - \\| \\nabla E(\\mathbf{w}^{(s)}) \\| )\\). The Moral: We want the following: \\[ \\mathbf{v} = - \\frac{\\nabla E(\\mathbf{w}^{(s)}) }{\\left\\| \\nabla E(\\mathbf{w}^{(s)}) \\right\\| } \\] which means we want to move in the direction opposite to that of the gradient. Keep in mind that we divided by the norm because we previously defined \\(\\mathbf{v}\\) to be a unit} vector. This also reveals the meaning behind the name gradient descent; we are descending in the direction opposite to the gradient of the error function. 6.3 Gradient Descent and our Model Before we posit the full algorithm for gradient descent in the context of regression, let us investigate the actual gradient further. Since we have a formula for \\(E_{\\mathbf{w}}\\), we can find a closed form for its gradient: \\[\\begin{align*} E(\\mathbf{w}) &amp; = \\frac{1}{n} \\left( \\mathbf{y} - \\mathbf{X} \\mathbf{w} \\right)^{\\mathsf{T}} \\left( \\mathbf{y} - \\mathbf{X} \\mathbf{w} \\right) \\\\ \\\\ &amp; = \\frac{1}{n} \\left( \\mathbf{w}^\\mathsf{T} \\mathbf{X}^\\mathsf{T} \\mathbf{X} \\mathbf{w} - 2 \\mathbf{w}^\\mathsf{T} \\mathbf{X}^\\mathsf{T} \\mathbf{y} + \\mathbf{y}^\\mathsf{T} \\mathbf{y} \\right) \\\\ \\\\ \\nabla E \\left( \\mathbf{w} \\right) &amp; = \\frac{1}{n} \\left( 2 \\mathbf{X}^\\mathsf{T} \\mathbf{X} \\mathbf{w} - 2 \\mathbf{X}^\\mathsf{T} \\mathbf{y} \\right) \\end{align*}\\] This was from the columns point of view; we can also find a different formula from the row’s perspective: \\[\\begin{align*} E(\\mathbf{w}) &amp; = \\frac{1}{n} \\sum_{i = 1}^{n} \\left( y_i - \\mathbf{w}^\\mathsf{T} \\mathbf{x_i} \\right)^2 = \\frac{1}{n} \\sum_{i=1}^{n} \\left( y_i - \\hat{y}_i \\right) \\\\ &amp; = \\frac{1}{n} \\sum_{i=1}^{n} \\left( y_i - w_0 x_{i0} - w_1 x_{i1} - \\dots - w_{j} x_{ij} - \\dots - w_{p} x_{ip} \\right)^2 \\\\ \\frac{\\partial}{\\partial w_j} E(\\mathbf{w}) &amp; = - \\frac{2}{n} \\sum_{i=1}^{n} \\left( y_i - w_0 x_{i0} - w_1 x_{i1} - \\dots - w_{j} x_{ij} - \\dots - w_{p} x_{ip} \\right) x_{ij} \\\\ \\frac{\\partial}{\\partial w_j} E(\\mathbf{w}^{(0)}) &amp; = - \\frac{2}{n} \\sum_{i=1}^{n} \\left( y_i - w_0 x_{i0} - w_1 x_{i1} - \\dots - w_{j} x_{ij} - \\dots - w_{p} x_{ip} \\right) x_{ij} \\\\ &amp; = - \\frac{2}{n} \\sum_{i=1}^{n} \\left( y_i - \\left[ \\mathbf{w}^{(0)} \\right]^{\\mathsf{T}} \\mathbf{x_i} \\right) x_{ij} \\end{align*}\\] This will be a better formula to use in our iterative algorithm, posited below. Algorithm Initialize a vector \\(\\mathbf{w}^{(0)} = \\left( w_{0}^{(0)}, w_{1}^{(0)}, \\dots, w_{p}^{(0)} \\right)\\) Repeat the following over \\(s\\) (starting with \\(s = 0\\)), until convergence: \\(w_{j}^{(s+1)}:= w_{j}^{(s)} + \\alpha \\cdot \\frac{2}{n} \\sum_{i=1}^{n} \\left( y_i - \\left[\\mathbf{w}^{(s)}\\right]^{\\mathsf{T}} \\mathbf{x_i} \\right) x_{ij}\\) for all \\(j = 0, \\dots, p\\) simultaneously. In more compact notation, \\[ w_{j}^{(s+1)} = w_{j}^{(s)} + \\alpha \\cdot \\frac{\\partial }{\\partial w_j} E(\\mathbf{w}^{(s)}) \\] Store these elements in the vector \\(\\mathbf{w}^{(s+1)} = \\left( w_{0}^{(s+1)} , w_{1}^{(s+1)} , \\dots, w_{p}^{(s+1)} \\right)\\) When there is little change between \\(\\mathbf{w}^{(k + 1)}\\) and \\(\\mathbf{w}^{(k)}\\) (for some integer \\(k\\)), the algorithm will have converged and \\(\\mathbf{w}^{*} = \\mathbf{w}^{(k+1)}\\). Note that in the algorithm, we have dropped the condition that \\(\\mathbf{v}\\) is a unit vector. Indeed, if you look on the Wikipedia page for “Gradient Descent,” the algorithm listed there also omits the unit-length condition. "],
["learning.html", "7 Theoretical Framework 7.1 Mental Map 7.2 Two Types of Predictions 7.3 Two Types of Data 7.4 Types of Errors 7.5 Noisy Targets", " 7 Theoretical Framework Finally we have arrived to the part of the book in which we provide a framework for the theory of learning. Well, to be more precise, the framework is really about the theory of supervised learning. At the end on the day, in supervised learning, we want a “good” model. What does “good” model mean? It means that we want to estimate a model \\(\\hat{f}\\) that gives “good” predictions. What do we mean by “good” predictions? Loosely speaking, it means that we want to obtain “accurate” predictions. Before clarifying the notion of accurate predictions, let’s discuss first the concept of predictions. Keep in mind that most of what will be covered in this chapter is highly theoretical. It has to do with the concepts and principles that ideally we expect to have in a perfect world. Having said, the real world, more often than not, is far from being ideal. So we will also need to discuss what to do in practice to overcome the idealistic limitations of our theoretical assumptions. 7.1 Mental Map So far, we have seen an example of Unsupervised Learning (PCA), as well as one method of Supervised Learning (regression). Now, we begin discussing learning ideas at an abstract level. Let’s return to our example of predicting NBA players’ salaries. We have a series of inputs: height, weight, 2PTS, 3PTS, fouls, etc. From these inputs, we obtain an output: the salary of a player. We also have a target function \\(f : \\mathcal{X} \\to \\mathcal{Y}\\). (i.e. a function mapping from the column spaces of \\(\\mathbf{X}\\) to the output space \\(\\mathbf{y}\\)). Keep in mind that this function is an ideal, and remains unknown throughout our computations. Here’s a metaphor that we like to use. Pretend that the target function is some sort of mythical creature, like a unicorn (or your preferred creature). We are trying to find this elusive guy. More generally, we have a dataset \\(\\mathcal{D}: \\{ (\\mathbf{x_1}, y_1), ( \\mathbf{x_2}, y_2), \\dots, (\\mathbf{x_n}, y_n) \\}\\) (where \\(\\mathbf{x_i}\\) represents the vector of observations for the \\(i\\)-th player/individual). From this data, we wish to obtain a hypothesis model \\(\\widehat{f}: \\mathcal{X} \\to \\mathcal{Y}\\) that is an approximation to the unknown function \\(f\\). We can sketch these basic ideas in a sort of “mental map” We will refer to this picture as the “diagram for unsupervised learning”. Figure 7.1: Supervised Learning Diagram (ver 1) The use of orange clouds around certain concepts is intentional; those concepts appearing in orange are more intangible than those appearing in black. For instance, we never really “discover” the function \\(f\\) in its entirety; rather, we just find a good enough approximation to it. As we modify our map, we will encounter more orange concepts as well of highly theoretical nature. The goal of supervised learning is to find a “good” model \\(\\widehat{f}\\). What is a “good” model? Well, a “good” model is a model that gives “good” (or accurate) predictions. What do we mean by “accurate?” Well, let’s not get too bogged down with details here. The magic word here is predictions. There are two kinds of predictions: (1) predictions \\(\\hat{y}_i\\) of observed/seen values \\(x_i\\), and (2) predicitons \\(\\hat{y}_0\\) of unobserved/unseed values \\(x_0\\). 7.2 Two Types of Predictions Think of a simple linear regression model (e.g. with one predictor). Having a fitted model \\(\\hat{f}(x)\\), we can use it to make two types of predictions. On one hand, for an observed point \\(x_i\\), we can compute \\(\\hat{y}_i = \\hat{f}(x_i)\\). By observed point we mean that \\(x_i\\) was part of the data used to find \\(\\hat{f}()\\). On the other hand, we can also compute \\(\\hat{y}_0 = \\hat{f}(x_0)\\) for a point \\(x_0\\) what was not part of the data used when deriving \\(\\hat{f}()\\). The two types of predictions refer to: (1) predictions \\(\\hat{y}_i\\) of observed/seen values \\(x_i\\), and (2) predicitons \\(\\hat{y}_0\\) of unobserved/unseed values \\(x_0\\). Each type of prediction is associated with a certain behavioral feature of a model. The predictions of observed data, \\(\\hat{y}_i\\), have to do with the memorizing aspect (apparent error, resubstitution error). The predictions of unobserved data, \\(\\hat{y}_0\\), have to do with the generalization aspect (generalization error, prediction error). Both kinds of predictions are important, and each of them is interesting in its own right. However, from the supervised learning standpoint, it is the second type of predictions that we are ultimately interested in. That is, we want to find models that are able to give predictions \\(\\hat{y}_0\\) as accurate as possible for the real value \\(y_0\\). Don’t get me wrong. Having good predictions \\(\\hat{y}_i\\) is important and desirable. And to a large extent, it is a necessary condition for a good model. However, it is not a sufficient condition. It is not enough to fit the observed data well, in order to get a good predictive model. Sometimes, you can have perfect fit of the observed data, but a terrible performance for unobserved values \\(x_0\\). Simply put, in supervised learning we want models with a good generalization ability. 7.3 Two Types of Data As you can tell from the previous section, we care about two distinct types of predictions, which in turn involve two slightly different kinds of data. The data points \\(x_i\\) that we use to fit a model is what we call training or learning data. The data points \\(x_0\\) that we use to assess the performance of a model are points NOT supposed to be part of the training set. This implies that, at least in theory, we need two finds of data sets: In-sample data, denoted \\(\\mathcal{D}_{in}\\), used to fit a model Out-of-sample data, denoted \\(\\mathcal{D}_{out}\\), used to measure the predictive quality of a model 7.4 Types of Errors Given that we’ll have, at least theoretically, two types of data, it shouldn’t be a surprise that we will also need separate types of errors for each kind of data. Correspondingly, we will have (at least in theory) two kinds of errors: In-sample Error, denoted \\(E_{in}\\) Out-of-sample Error, denoted \\(E_{out}\\) 7.4.1 Overall Errors Both errors are overall measures of error. This means that we need a function that summarizes, somehow, the total amount of error. Most overall error measures are based on the sum of individual errors: \\[ E(\\hat{f},f) = \\text{measure} \\left( \\sum err_i (\\hat{y}, y) \\right) \\] Unless otherwise said, in this book we will use the mean sum of errors are the default overall error measure. The in-sample error is the average of pointwise errors: \\[ E_{in} (\\hat{f}, f) = \\frac{1}{n} \\sum_{i} err_i \\] The out-of-sample error is the theoretical mean, or expected value, of the pointwise errors: \\[ E_{out} (\\hat{f}, f) = \\mathbb{E}_{\\mathcal{X}} \\left[ err \\left( \\hat{f}(x), f(x) \\right) \\right] \\] The expectation is taken over the input space \\(\\mathcal{X}\\). The point \\(x\\) denotes a general data point in such space \\(\\mathcal{X}\\). Notice the theoretical nature of \\(E_{out}\\). In practice, you will never, never, be able to compute this quantity. In the machine learning literature, these overall measures of error are formally known as cost functions or risks. 7.4.2 Individual Errors What form does the individual error function, \\(err()\\), take? In theory, they can take any form you want. This means that you can invent your own individual error function. However, the most common ones are: squared error: \\(\\quad err(\\hat{f}, f) = \\left( \\hat{y}_i - y_i \\right)^2\\) absolute error: \\(\\quad err(\\hat{f}, f) = \\left| \\hat{y}_i - y_i \\right|\\) misclassification error: \\(\\quad err(\\hat{f}, f) = [\\![ \\hat{y}_i \\neq y_i ]\\!]\\) In the machine learning literature, these individual errors are formally known as loss functions. Let’s update our Machine Learning Map to include error measures: Figure 7.2: Supervised Learning Diagram (ver 2) 7.4.3 Auxiliary Technicality We need to assume some probability distribution \\(P\\) on \\(\\mathcal{X}\\). That is, we assume our vectors \\(\\mathbf{x_1}, \\dots, \\mathbf{x_n}\\) are independent identically distributed (i.i.d.) samples from this distribution \\(P\\). (Exactly what distribution you pick - normal, chi-squared, \\(t\\), etc. - is, for the moment, irrelevant). Recall that out-of-sample data is highly theoretical; we will never be able to obtain it in its entirety. The best we can do is obtain a subset of the out-of-sample data (the test data), and estimate the rest of the data. Our imposition of a distributional structure on \\(\\mathcal{X}\\) enables us to link the in-sample error with the out-of-sample data. Recall that our ultimate goal is to get a good function \\(\\widehat{f} \\approx f\\). What do we mean by the symbol “\\(\\approx\\)”? Technically speaking, we want \\(E_{\\mathrm{out}}(\\widehat{f}) \\approx 0\\). If this is the case, we can safely say that our model has been successfully trained. However, we can never check if this is the case, since we don’t have access to \\(E_{\\mathrm{out}}\\). To solve this, we break our goal into two sub-goals: \\[ E_{\\mathrm{out}} (\\widehat{f}) \\approx 0 \\ \\Rightarrow \\begin{cases} E_{\\mathrm{in}}(\\widehat{f}) \\approx 0 &amp; \\text{practical result} \\\\ E_{\\mathrm{out}}(\\widehat{f}) \\approx E_{\\mathrm{in}}(\\widehat{f}) &amp; \\text{technical/theoretical result} \\\\ \\end{cases} \\] The first condition is easy to check. How do we check the second? We check the second condition by invoking our distributional assumption \\(P\\) on \\(\\mathcal{X}\\). Using our assumption, we can cite various theorems to assert that the second result indeed holds true. We will later find ways to estimate \\(E_{\\mathrm{out}}(\\widehat{f})\\). Figure 7.3: Supervised Learning Diagram (ver 3) 7.5 Noisy Targets In practice, our function won’t necessarily be a nice (or smooth) function. Rather, there will be some noise. Hence, instead of saying \\(y = f(x)\\) where \\(f : \\mathcal{X} \\to \\mathcal{Y}\\), a better statement might be something like \\(y = f(x) + \\varepsilon\\). But even this notation has some flaws; for example, we could have multiple inputs mapping to the same output (which cannot happen if \\(f\\) is a proper “function”). That is, we may have two individuals with the exact same inputs \\(\\mathbf{x_A} = \\mathbf{x_B}\\) but with different response variables \\(y_A \\neq y_B\\). Instead, it makes more sense to consider some target distribution \\(Prob(y \\mid x)\\). In this way, we can think of our data as forming a joint probability distribution \\(Prob(\\mathbf{x}, y)\\). That is because \\(P(\\mathbf{x}, y) = P(\\mathbf{x}) P(y \\mid \\mathbf{x})\\). Figure 7.4: Supervised Learning Diagram (ver 4) In machine learning, we want to learn the conditional distribution \\(P(y \\mid \\mathbf{x})\\). Again, we can think of this probability as \\(y = f() + \\text{noise}\\). Also, sometimes the Hypothesis Sets and Learning Algorithm boxes are combined into one, called the Learning Model. "],
["mse.html", "8 MSE of Estimator 8.1 MSE of an Estimator", " 8 MSE of Estimator In this chapter we provide a preliminary review of the Mean Squared Error (MSE) of an estimator. This will allow us to have a more gentle introduction to the next chapter about the famous Bias-Variance tradeoff. 8.1 MSE of an Estimator In order to discuss the bias-variance decomposition of a regression function and its expected MSE, we would like to first review the concept of the mean squared error of an estimator. Recall that estimation consists of providing an approximate value to the parameter of a population, using a ( random) sample of observations drawn from such population. Say we have a population of \\(n\\) objects and we are interested in describing them with some numeric characteristic \\(\\theta\\). For example, our population may be formed by all students in some college, and we want to know their average height. We call this (theoretical) average the parameter. Figure 8.1: Population described by some parameter of interest. To estimate the value of the parameter, we may draw a random sample of \\(m &lt; n\\) students from the population and compute a statistic \\(\\hat{\\theta}\\). Ideally, we would use some statistic \\(\\hat{\\theta}\\) that approximates well the parameter \\(\\theta\\). Figure 8.2: Random sample from a population In practice, this is the typical process that you would carry out: Get a random sample from a population. Use the limited amount of data in the sample to estimate \\(\\theta\\) using some formula to compute \\(\\hat{\\theta}\\). Make a statement about how reliable of an estimator \\(\\hat{\\theta}\\) is. Now, for illustration purposes, let’s do the following mental experiment. Pretend that you can draw multiple random samples, all of the same size \\(m\\), from the population. In fact, you should pretend that you can get an infinite number of samples. And suppose that for each sample you will compute a statistic \\(\\hat{\\theta}\\). A first random sample of size \\(m\\) would result in \\(\\hat{\\theta}_1\\). A second random sample of size \\(m\\) would result in \\(\\hat{\\theta}_2\\). And so on. Figure 8.3: Various random samples of equal size and their statistics A couple of important things to notice: An estimator is a random variable\" A first sample will result in \\(\\hat{\\theta}_1\\) A second sample will result in \\(\\hat{\\theta}_2\\) A third sample will result in \\(\\hat{\\theta}_3\\) and so on … Some samples will yield a \\(\\hat{\\theta}_k\\) that overestimates \\(\\theta\\) Other samples will yield a \\(\\hat{\\theta}_k\\) that underestimates \\(\\theta\\) Some samples will yield a \\(\\hat{\\theta}_k\\) matching \\(\\theta\\) In theory, we could get a very large number of samples, and visualize the distribution of \\(\\hat{\\theta}\\), like in the figure below: Figure 8.4: Distribution of an estimator As you would expect, some estimators will be close to the parameter \\(\\theta\\), while others not so much. Under general assumptions, we can also assume that the estimator has expected value \\(\\mathbb{E}(\\hat{\\theta})\\), with finite variance \\(var(\\hat{\\theta})\\). Figure 8.5: Distribution of an estimator An interesting question to consider is: In general, how much different—or similar—is \\(\\hat{\\theta}\\) from \\(\\theta\\)? To be more concrete: on average, how close we expect the estimator to be from the parameter? To answer this question we can look for a measure to assess the typical distance of estimators from the parameter. This involves looking at the difference: \\(\\hat{\\theta} - \\theta\\), which is commonly referred to as the estimation error: \\[ \\text{estimation error} = \\hat{\\theta} - \\theta \\] We would like to measure the “size” of such difference. Notice that the estimation error is also a random variable: A first sample will result in an error \\(\\hat{\\theta}_1 - \\theta\\) A second sample will result in an error \\(\\hat{\\theta}_2 - \\theta\\) A third sample will result in an error \\(\\hat{\\theta}_3 - \\theta\\) and so on … So how do we measure the “size” of the estimation errors? The typical way to quantify the amount of estimation error is by calculating the squared errors, and then averaging over all the possible values of the estimators. This is known as the Mean Squared Error (MSE) of \\(\\hat{\\theta}\\): \\[ \\text{MSE}(\\hat{\\theta}) = \\mathbb{E} [(\\hat{\\theta} - \\theta)^2] \\] MSE is the squared distance from our estimator \\(\\hat{\\theta}\\) to the true value \\(\\theta\\), averaged over all possible samples. It is convenient to regard the estimation error, \\(\\hat{\\theta} - \\theta\\), with respect to \\(\\mathbb{E}(\\hat{\\theta})\\). In other words, the distance between \\(\\hat{\\theta}\\) and \\(\\theta\\) can be expressed with respect to the expected value \\(\\mathbb{E}(\\hat{\\theta})\\): Figure 8.6: Estimator, its mean, and the parameter Let’s rewrite \\((\\hat{\\theta} - \\theta)^2\\) as \\(( \\hat{\\theta} - \\mathbb{E}(\\hat{\\theta}) + \\mathbb{E}(\\hat{\\theta}) - \\theta)^2\\), and let \\(\\mathbb{E}(\\hat{\\theta}) = \\mu_{\\hat{\\theta}}\\). Then: \\[\\begin{align*} (\\hat{\\theta} - \\theta)^2 &amp;= \\left ( \\hat{\\theta} - \\mathbb{E}(\\hat{\\theta}) + \\mathbb{E}(\\hat{\\theta}) - \\theta \\right )^2 \\\\ &amp;= ( \\hat{\\theta} - \\mu_{\\hat{\\theta}} + \\mu_{\\hat{\\theta}} - \\theta )^2 \\\\ &amp;= (\\underbrace{\\hat{\\theta} - \\mu_{\\hat{\\theta}}}_{a} + \\underbrace{\\mu_{\\hat{\\theta}} - \\theta}_{b})^2 \\\\ &amp;= a^2 + b^2 + 2ab \\\\ \\Longrightarrow \\mathbb{E} \\left [ (\\hat{\\theta} - \\theta)^2 \\right ] &amp;= \\mathbb{E}[a^2 + b^2 + 2ab] \\end{align*}\\] We have that \\(\\text{MSE}(\\hat{\\theta}) = \\mathbb{E} [(\\hat{\\theta} - \\theta)^2]\\) can be decomposed as: \\[\\begin{align*} \\mathbb{E} \\left [ (\\hat{\\theta} - \\theta)^2 \\right ] &amp;= \\mathbb{E}[a^2 + b^2 + 2ab] \\\\ &amp;= \\mathbb{E}(a^2) + \\mathbb{E}(b^2) + 2\\mathbb{E}(ab) \\\\ &amp;= \\mathbb{E} [ (\\hat{\\theta} - \\mu_{\\hat{\\theta}})^2 ] + \\mathbb{E} [ (\\mu_{\\hat{\\theta}} - \\theta)^2 ] + 2\\mathbb{E}(ab) \\end{align*}\\] Notice that \\(\\mathbb{E}(ab)\\): \\[ \\mathbb{E}(ab) = \\mathbb{E}[ (\\hat{\\theta} - \\mu_{\\hat{\\theta}}) (\\mu_{\\hat{\\theta}} - \\theta) ] = 0 \\] Consequently \\[\\begin{align*} \\text{MSE}(\\hat{\\theta}) &amp;= \\mathbb{E} \\left [ (\\hat{\\theta} - \\theta)^2 \\right ] \\\\ &amp; \\\\ &amp;= \\mathbb{E} [ (\\hat{\\theta} - \\mu_{\\hat{\\theta}})^2 ] + \\mathbb{E} [ (\\mu_{\\hat{\\theta}} - \\theta)^2 ] \\\\ &amp; \\\\ &amp;= \\mathbb{E} [(\\hat{\\theta} - \\mu_{\\hat{\\theta}})^2] + \\mathbb{E} [ (\\mu_{\\hat{\\theta}} - \\theta) ]^2 \\\\ &amp; \\\\ &amp;= \\underbrace{\\mathbb{E} [(\\hat{\\theta} - \\mu_{\\hat{\\theta}})^2]}_{\\text{Variance}} + (\\underbrace{\\mu_{\\hat{\\theta}} - \\theta}_{\\text{Bias}})^2 \\\\ &amp; \\\\ &amp;= \\text{Var}(\\hat{\\theta}) + \\text{Bias}^{2} (\\hat{\\theta}) \\end{align*}\\] The MSE of an estimator can be decomposed in terms of Bias and Variance. Bias, \\(\\mu_{\\hat{\\theta}} - \\theta\\), is the tendency of \\(\\hat{\\theta}\\) to overestimate or underestimate \\(\\theta\\) over all possible samples. Variance, \\(\\text{Var}(\\hat{\\theta})\\), simply measures the average variability of the estimators around their mean \\(\\mathbb{E}(\\hat{\\theta})\\). 8.1.1 Prototypical Cases of Bias and Variance Depending on the type of estimator \\(\\hat{\\theta}\\), and the sample size \\(m\\), we can get statistics having different behaviors. The following diagram illustrates four classic scenarios contrasting low and high values for both the bias and the variance. Figure 8.7: Prototypical scenarios for Bias-Variance "],
["biasvar.html", "9 Bias-Variance Tradeoff 9.1 Bias-Variance Tradeoff 9.2 Motivation Example 9.3 Learning from two points 9.4 Bias-Variance Derivation", " 9 Bias-Variance Tradeoff In this chapter we discuss one of the theoretical dogmas of statistical learning: the famous Bias-Variance tradeoff. 9.1 Bias-Variance Tradeoff In the previous chapter we reviewed the concept of Mean Squared Error of a statistic (or estimator) \\(\\hat{\\theta}\\). As we saw, we can decompose \\(\\text{MSE}(\\hat{\\theta})\\) as the sum of two components: Bias-squared and Variance. \\[ \\text{MSE}(\\hat{\\theta}) = \\text{Var}(\\hat{\\theta}) + \\text{Bias}^{2} (\\hat{\\theta}) \\] In this chapter we extend this notion to the MSE of a regression model. 9.2 Motivation Example Let’s start with a toy example. Consider a noiseless target function \\(f(x) = sin(\\pi x)\\), with the input variable \\(x\\) in the interval \\([-1,1]\\), like in the following picture: (#fig:plot_target)Target function 9.2.1 Two Hypotheses Let’s assume a learning scenario in which, given a data set of \\(n\\) points, we fit the data using one of two models (see the idealized figure shown below): \\(\\mathcal{H}_0\\): Set of all lines of the form \\(h(x) = b\\) \\(\\mathcal{H}_1\\): Set of all lines of the form \\(h(x) = b_0 + b_1 x\\) (#fig:plot_two_hypotheses)Two Learning hypothesis models 9.3 Learning from two points In this case study, we will assume a data set of size \\(n = 2\\). That is, we sample \\(x\\) uniformly in \\([-1,1]\\) to generate a data set of two points \\((x_1, y_1), (x_2, y_2)\\); and fit the data using the two models \\(\\mathcal{H}_0\\) and \\(\\mathcal{H}_1\\). For \\(\\mathcal{H}_0\\), we choose the constant hypothesis that best fits the data (the horizontal line at the midpoint \\(b = \\frac{y_1 + y_2}{2}\\)). For \\(\\mathcal{H}_1\\), we choose the line that passes through the two data points \\((x_1, y_1)\\) and \\((x_2, y_2)\\). Here’s an example in R of two \\(x\\)-points randomly sampled from a uniform distribution in the interval \\([-1,1]\\), and their corresponding \\(y\\)-points: \\(p_1(x_1, y_1) = (0.0949158, 0.2937874)\\) \\(p_2(x_2, y_2) = (0.4880941, 0.9993006)\\) With the given points above, the two fitted models are: \\(h_0(x) = 0.646544\\) \\(h_1(x) = 0.123472 + 1.794385 \\hspace{1mm} x\\) 9.4 Bias-Variance Derivation Given a data set \\(\\mathcal{D}\\) of \\(n\\) points, and a hypothesis \\(g(x)\\), the expectation of the Squared Error for a given out-of-sample point \\(x_o\\), over all possible training sets, is expressed as (assuming a noiseless target): \\[ \\mathbb{E}_{\\mathcal{D}} \\left [ \\left( g^{(\\mathcal{D})}(x_0) - f(x_0) \\right)^2 \\right ] = \\underbrace{\\mathbb{E}_{\\mathcal{D}} \\left [ \\left (g^{(\\mathcal{D})}(x_0) - \\bar{g}(x_0) \\right)^2 \\right ]}_{\\text{variance}} + \\underbrace{\\left [ (\\bar{g}(x_0) - f(x_0))^2 \\right ] }_{\\text{bias}^2} \\] The target function is represented by \\(f(x)\\), and the average hypothesis is represented by \\(\\bar{g}(x) = \\mathbb{E}_{\\mathcal{D}}[g^{\\mathcal{D}} (x)]\\). Now, when there is noise in the data we have that: \\(y = f(x) + \\epsilon\\). If \\(\\epsilon\\) is a zero-mean noise random variable with variance \\(\\sigma^2\\), the bias-variance decomposition becomes: \\[ \\mathbb{E}_{\\mathcal{D}} \\left [ \\left (g^{(\\mathcal{D})}(x_o) - y_o \\right)^2 \\right ] = \\text{bias}^2 + \\text{var} + \\sigma^2 \\] Notice that the above equation assumes that the squared error corresponds to just one out-of-sample (i.e. test) point \\((x_0, y_0) = (x_0, f(x_0))\\). "]
]
