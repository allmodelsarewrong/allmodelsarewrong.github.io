[
["index.html", "All Models Are Wrong: Concepts of Statistical Learning Preface", " All Models Are Wrong: Concepts of Statistical Learning Gaston SanchezEthan Marzban Preface This is a work in progress for an introductory text about concepts of Statistical Learning, covering some of the common supervised as well as unsupervised methods. How to cite this book: Sanchez, G., Marzban, E. (2020) All Models Are Wrong: Concepts of Statistical Learning. https://allmodelsarewrong.github.io © 2020 Sanchez, Marzban. All Rights Reserved. "],
["about.html", "1 About this book", " 1 About this book This is a book about concepts of Statistical Learning. The book has an overarching take-home message that we use as the title of this work: All Models Are Wrong. This comes from a famous quote credited to great British statistician George Edward Pelham Box: “All models are wrong, but some are useful.” Because Statistical Learning is inherently related to model building, we must never forget that every proposed model will be wrong. This is why we need to measure the generalization error of a model. We may never be able to come up with a perfect model that gives zero error. And that is okay. It doesn’t have to be perfect. The important—and challenging—thing is to find useful models (yes, we know, easier said than done). Knowing that the field(s) of Machine Learning, Statisticial Learning (SL), and any other name about learning from data, is a very broad subject, we should warn you that this book is not intended to be the ultimate compilation of every single SL technique ever devised. Instead, we focus on the concepts that we consider the building blocks that any user or practitioner needs to make sense of most common SL techniques. A big shortcoming of the book: we don’t cover neural networks. At least not in this first round of iterations. Sorry. On the plus side: We’ve tried hard to keep the notation as simple and consistent as possible. And we’ve also made a titanic effort to make it extremely visual (lots of diagrams, pictures, plots, graphs, figures, …, you name it). Prerequisites We are assuming that you already have some knowledge under your belt. You will better understand (and hopefully enjoy) the book if you’ve taken one or more courses on the following subjects: linear or matrix algebra multivariable calculus statistics probability programming or scripting Acknowledgements Many thanks to the UC Berkeley students, and teaching staff, of Stat 154 Modern Statistical Prediction and Machine Learning (Fall 2017, Spring 2018, Fall 2019, Spring 2020). In particular, thank you to Jin Kweon, and Jiyoon Jeong for catching many errors in the first iteration of the course slides. Likewise, thank you to Sharon Hui for spotting dozens of typos in the first drafts of the book. Also, thanks to Anita Silver, Joyce Yip, Jingwei Guan, Skylar Liang, Raymond Chang, Houyu Jiang, Valeria Garcia, and Bing Li for being amazing and committed note takers. Likewise, thanks to Johnny Hong, Omid Shams Solari, Ryan Theisen, Frank Qiu, and Billy Fang for their collaboration as TAs. "],
["intro.html", "2 Introduction 2.1 Basic Notation", " 2 Introduction Picture a data set containing scores of several courses for college students. For example, courses like matrix algebra, multivariable calculus, statistics, and probability. And say we also have historical data about a course in Statistical Learning. In particular we have final scores measured on a scale from 0 to 100, we also have final grades (in letter grade scale), as well as a third interesting variable “Pass - Non-Pass” indicating whether the student passed statistical learning. Some data like that fits perfectly well in a tabular format. The rows contain the records for a bunch of students, and the columns refer to the variables. Student LinAlg Calculus Statistics StatLearn Grade P/NP 1 \\(\\bigcirc\\) \\(\\bigcirc\\) \\(\\bigcirc\\) 92 A P 2 \\(\\bigcirc\\) \\(\\bigcirc\\) \\(\\bigcirc\\) 85 B P 3 \\(\\bigcirc\\) \\(\\bigcirc\\) \\(\\bigcirc\\) 40 F NP New \\(\\bigcirc\\) \\(\\bigcirc\\) \\(\\bigcirc\\) ? ? ? Suppose that, based on this historical data, we wish to predict the score of a new student (whose Linear Algebra, Calculus, and Statistics grades are known) in Statistical Learning. To do so, we would fit some sort of model to our data; i.e. we would perform regression. This is a form of supervised learning, since our model is trained using known inputs (i.e. LinAlg, Calculus, and Statistics grades) as well as known responses (i.e. the Statistical Learning grades of the previous students). Likewise, we may be interested in studying the data not from a prediction oriented perspective but from a purely exploratory perspective. For example, maybe we want to investigate what is the relationship between the courses Linear Algebra, Calculus, and Statistics; that is: explore the relationship between the features. Or maybe we want to study the resemblance among individuals and see what kind of students have similar scores, or if there are “natural” groups of individuals based on their features. Both of these tasks are examples of unsupervised learning. We use the information in the data to discover patterns, without focusing on any single variable as a target response. In summary, we will focus on two types of learning paradigms: Supervised Learning: where we have inputs, and one (or more) response variable(s). Unsupervised Learning: where we have inputs, but not response variables. Figure 2.1: Inputs and outputs in supervised and unsupervised learning By the way, there are other types of Learning paradigms (e.g. deep learning, reinforcement learning), but we won’t discuss them in this book. To visualize the different types of learning, the different types of variables, and the methodology associated with each combination of learning/data types, we can use the following graphic: Figure 2.2: Supervised and Unsupervised Corners 2.1 Basic Notation In this book we are going to use a fair amount of math notation. Becoming familiar with the meaning of all the different symbols as soon as possible, should allow you to keep the learning curve a little bit less steep. The starting point is always the data, which we will assume to be in a tabular format, that can be translated into a mathematical matrix object. Here’s an example of a data matrix \\(\\mathbf{X}\\) of size \\(n \\times p\\) \\[ \\mathbf{X} = \\ \\begin{bmatrix} x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1j} &amp; \\cdots &amp; x_{1p} \\\\ x_{21} &amp; x_{22} &amp; \\cdots &amp; x_{2j} &amp; \\cdots &amp; x_{2p} \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots &amp; &amp; \\vdots \\\\ x_{i1} &amp; x_{i2} &amp; \\cdots &amp; x_{ij} &amp; \\cdots &amp; x_{ip} \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots &amp; &amp; \\vdots \\\\ x_{n1} &amp; x_{n2} &amp; \\cdots &amp; x_{nj} &amp; \\cdots &amp; x_{np} \\\\ \\end{bmatrix} \\tag{2.1} \\] By default, we will assume that the rows of a data matrix correspond to the individuals or objects. Likewise, we will also assume that the columns of a data matrix correspond to the variables or features observed on the individuals. In this sense, the symbol \\(x_{ij}\\) represents the value observed for the \\(j\\)-th variable on the \\(i\\)-th individual. Throughout this book, every time you see the letter \\(i\\), either alone or as an index associated with any other symbol (superscript or subscript), it means that such term corresponds to an individual or a row of some data matrix. For instance, symbols like \\(x_i\\), \\(\\mathbf{x_i}\\), and \\(\\alpha_i\\) are all examples that refer to—or denote a connection with—individuals. In turn, we will always use the letter \\(j\\) to convey association with variables or columns of some data matrix. For instance, \\(x_j\\), \\(\\mathbf{x_j}\\), and \\(\\alpha_j\\) are examples that refer to—or denote a connection with—variables. For better or for worse, we’ve made the decision to represent both the rows and the columns of a matrix as vectors using the same notation: as bold lower case letters such as \\(\\mathbf{x_i}\\) and \\(\\mathbf{x_j}\\). Because we know that there’s a risk of confusing a vector that corresponds to a row with a vector that corresponds to a column, sometimes we will use the arrow notation for vectors associated to the row of a data matrix: \\(\\mathbf{\\vec{x}_i}\\). So, going back to the above data matrix \\(\\mathbf{X}\\), we can represent the first variable as a vector \\(\\mathbf{x_1} = (x_{11}, x_{21}, \\dots, x_{n1})\\). Likewise, we can represent the first individual with the vector \\(\\mathbf{\\vec{x}_1} = (x_{11}, x_{12}, \\dots, x_{1p})\\). Here’s a reference table with the most common symbols and notation used throughout the book. Symbol Description \\(n\\) number of objects or individuals \\(p\\) number of variables or features \\(i\\) running index for rows or individuals \\(j\\) running index for columns or variables \\(k\\) running index determined by context \\(\\ell, m, q\\) other auxiliary indexes \\(f()\\), \\(h()\\), \\(d()\\) functions \\(\\lambda, \\mu, \\gamma, \\alpha\\) greek letters represent scalars \\(\\mathbf{x}\\), \\(\\mathbf{y}\\) variables, size determined by context \\(\\mathbf{w}\\), \\(\\mathbf{a}\\), \\(\\mathbf{b}\\) vectors of weight coefficients \\(\\mathbf{z}\\), \\(\\mathbf{t}\\), \\(\\mathbf{u}\\) components or latent variables \\(\\mathbf{X} : n \\times p\\) data matrix with \\(n\\) rows and \\(p\\) columns \\(x_{ij}\\) element of a matrix in \\(i\\)-th row and \\(j\\)-th column \\(\\mathbf{1}\\) vector of ones, size determined by context \\(\\mathbf{I}\\) identity matrix, size determined by context By the way, there are many more symbols that will appear in later chapters. But for now these are the fundamental ones. Likewise, the table below contains some of the most common operators that we will use in subsequent chapters: Symbol Description \\(\\mathbb{E}[X]\\) expected value of a random variable \\(X\\) \\(\\|\\mathbf{a}\\|\\) euclidean norm of a vector \\(\\mathbf{a}^{\\mathsf{T}}\\) transpose of a vector (or matrix) \\(\\mathbf{a^{\\mathsf{T}}b}\\) inner product of two vectors \\(\\langle \\mathbf{a}, \\mathbf{b} \\rangle\\) inner product of two vectors \\(det(\\mathbf{A})\\) determinant of a square matrix \\(tr(\\mathbf{A})\\) trace of a square matrix \\(\\mathbf{A}^{-1}\\) inverse of a square matrix \\(diag(\\mathbf{A})\\) diagonal of a square matrix \\(E()\\) overall error function \\(err()\\) pointwise error function \\(sign()\\) sign function \\(var()\\) variance function \\(cov()\\) covariance function \\(cor()\\) correlation function "],
["duality.html", "3 Geometric Duality 3.1 Rows Space 3.2 Columns Space 3.3 Cloud of Individuals 3.4 Cloud of Variables", " 3 Geometric Duality Before discussing unsupervised as well as supervised learning methods, we prefer to give you a prelude by talking and thinking about data in a geometric sense. This chapter will set the stage for most of the topics covered in later chapters. Let’s suppose we have some data in the form of a data matrix. For convenience purposes, let’s also suppose that all variables are measured in a real-value scale. Obviously not all data is expressed or even encoded numerically. You may have categorical or symbolic data. But for this illustration, let’s assume that any categorical and symbolic data has already been transformed into a numeric scale (e.g. dummy indicators, optimal scaling). It’s very enlightening to think of a data matrix as viewed from the glass of Geometry. The key idea is to think of the data in a matrix as elements living in a multidimensional space. Actually, we can regard a data matrix from two apparently different perspectives that, in reality, are intimately connected: the rows perspective and the columns perspective. In order to explain these perspectives, let’s use the following diagram of a data matrix \\(\\mathbf{X}\\) with \\(n\\) rows and \\(p\\) columns, with \\(x_{ij}\\) representing the element in the \\(i\\)-th row and \\(j\\)-th column. Figure 3.1: Duality of a data matrix When we look at a data matrix from the columns perpective what we are doing is focusing on the \\(p\\) variables. In a similar way, when looking at a data matrix from its rows perspective, we are focusing on the \\(n\\) individuals. Like a coin, though, this matrix has two sides: a rows side, and a columns side. That is, we could look at the data from the rows point of view, or the columns point of view. These two views are (of course) not completely independent. This double perspective or duality for short, is like the two sides of the same coin. 3.1 Rows Space We know that human vision is limited to three-dimensions, but pretend that you had superpowers that let you visualize a space with any number of dimensions. Because each row of the data matrix has \\(p\\) elements, we can regard individuals as objects that live in a \\(p\\)-dimensional space. For visualization purposes, think of each variable as playing the role of a dimension associated to a given axis in this space; likewise, consider each of the \\(n\\) individuals as being depicted as a point (or particle) in such space, like in the following diagram: Figure 3.2: Rows space In the figure above, even though we are showing only three axes, you should pretend that you are visualizing a \\(p\\)-dimensional space (imaging that there are \\(p\\) axes). Each point in this space corresponds to a single individual, and they all form what we can call a cloud of points. 3.2 Columns Space We can do the same visual exercise with the columns of a data matrix. Since each variable has \\(n\\) elements, we can regard the set of \\(p\\) variables as objects that live in an \\(n\\)-dimensional space. However, instead of representing each variable with a dot, it’s better to graphically represent them with an arrow (or vector). Why? Because of two reasons: one is to distinguish them from the individuals (dots). But more important, because the esential thing with a variable is not really its magnitude (and therefore its position) but its direction. Often, as part of the data preprocessing steps, we apply transformations on variables that change their scales (e.g. shrinking them, or stretching them) without modifying their directions. So it’s more convenient to focus primarily on their directions. Figure 3.3: Columns space Analogously to the rows space and its cloud of individuals, you should also pretend that the image above is displaying an \\(n\\)-dimensional space with a bunch of blue arrows pointing in various directions. What’s next? Now that we know how to think of data from a geometric perspective, the next step is to discuss a handful of common operations that can be performed with points and vectors that live in some geometric space. 3.3 Cloud of Individuals In the previous sections, we introduced the powerful idea of looking at the rows and columns of a data matrix from the lens of geometry. We are assuming in general that the rows have to do with \\(n\\) individuals that lie in a \\(p\\)-dimensional space. Figure 3.4: Cloud of points Let’s start describing a set of common operations that we can apply on the individuals (living in a \\(p\\)-dimensional space). 3.3.1 Average Individual We can ask about the typical or average individual. If you only have one variable, then all the individual points lie in a one-dimensional space, which is basically a line. Here’s a simple example with five individuals described by one variable: Figure 3.5: Points in one dimension The most common way to think about the typical or average individual is in terms of the arihmetic average of the values, which geometrically corresponds to the “balancing point”. The diagram below shows three possible locations for a fulcrum (represented as a red triangle). Only the average value 5 results in the balancing point which keeps the values on the number line in equilibrium: Figure 3.6: Average individual Algebraically we have: individuals \\(x_1, x_2, \\dots, x_n\\), and the average is: \\[ \\bar{x} = \\frac{x_1 + \\dots + x_n}{n} = \\frac{1}{n} \\sum_{i=1}^{n} x_i \\] In vector notation, the average can be calculated with an inner product between \\(\\mathbf{x} = (x_1, x_2, \\dots, x_n)\\), and a constant vector of \\(n\\)-elements \\((1/n)\\mathbf{1}\\): \\[ \\bar{x} = \\frac{1}{n} \\mathbf{x^\\mathsf{T}1} \\] What about the multivariate case? It turns out that we can also ask about the average individual of a cloud of points, like in the following figure: Figure 3.7: Cloud of points with centroid (i.e. average individual) The average individual, in a \\(p\\)-dimensional space is the point \\(\\mathbf{\\vec{g}}\\) containing as coordiantes the averages of all the variables: \\[ \\mathbf{\\vec{g}} = (\\bar{x}_1, \\bar{x}_2, \\dots, \\bar{x}_j) \\] where \\(\\bar{x}_j\\) is the average of the \\(j\\)-th variable. This average individual \\(\\mathbf{\\vec{g}}\\) is also known as the centroid, barycenter, or center of gravity of the cloud of points. 3.3.2 Centered Data Often, it is convenient to transform the data in such a way that the centroid of a data set becomes the origin of the cloud of points. Geometrically, this type of transformation involves a shif of the axes in the \\(p\\)-dimensional space. Algebraically, this transformation corresponds to expresing the values of each variable in terms of deviations from their means. Figure 3.8: Cloud of points of mean-centered data In the unidimensional case, say we have \\(n\\) individuals \\(\\mathbf{x} = (x_1, x_2, \\dots, x_n)\\) with a mean of \\(\\bar{x} = \\sum_i^n x_i\\). The vector of centered values are: \\[ \\mathbf{\\bar{x}} = (x_1 - \\bar{x}, x_2 - \\bar{x}, \\dots, x_n - \\bar{x}) \\] In the multidimensional case, the set of centered data values are: \\[ \\mathbf{x_1} - \\mathbf{g}, \\mathbf{x_2} - \\mathbf{g}, \\dots, \\mathbf{x_n} - \\mathbf{g} \\] 3.3.3 Distance between individuals Another common operation that we may be interested in is the distance between two individuals. Obviously the notion of distance is not unique, since you can choose different types of distance measures. Perhaps the most comon type of distance is the (squared) Euclidean distance. Unless otherwise mentioned, this will be the default distance used in this book. Figure 3.9: Distance between two individuals If you have one variable \\(X\\), then the squared distance \\(d^2(i,\\ell)\\) between two individuals \\(x_i\\) and \\(x_\\ell\\) is: \\[ d^2(i,\\ell) = (x_i - x_\\ell)^2 \\] In general, with \\(p\\) variables, the squared distance between the \\(i\\)-th individual and the \\(\\ell\\)-th individual is: \\[\\begin{align*} d^2(i,\\ell) &amp;= (x_{i1} - x_{\\ell 1})^2 + (x_{i2} - x_{\\ell 2})^2 + \\dots + (x_{ip} - x_{\\ell p})^2 \\\\ &amp;= (\\mathbf{\\vec{x}_i} - \\mathbf{\\vec{x}_\\ell})^\\mathsf{T} (\\mathbf{\\vec{x}_i} - \\mathbf{\\vec{x}_\\ell}) \\end{align*}\\] 3.3.4 Distance to the centroid A special case is the distance between any individual \\(i\\) and the average individual: \\[\\begin{align*} d^2(i,g) &amp;= (x_{i1} - \\bar{x}_1)^2 + (x_{i2} - \\bar{x}_2)^2 + \\dots + (x_{ip} - \\bar{x}_p)^2 \\\\ &amp;= (\\mathbf{\\vec{x}_i} - \\mathbf{\\vec{g}})^\\mathsf{T} (\\mathbf{\\vec{x}_i} - \\mathbf{\\vec{g}}) \\end{align*}\\] 3.3.5 Measures of Dispersion What else can we calculate with the individuals? Think about it. So far we’ve seen how to calculate the average individual, as well as distances between individuals. The average individual or centroid plays the role of a measure of center. And everytime you get a measure of center, it makes sense to get a measure of spread. Overall Dispersion One way to compute a measure of scatter among individuals is to consider all the squared distances between pairs of individuals. For instance, say you have three individuals \\(a\\), \\(b\\), and \\(c\\). We can calculate all pairwise distances and add them up: \\[ d^2(a,a) + d^2(b,b) + d^2(c,c) + \\\\ d^2(a,b) + d^2(b,a) + \\\\ d^2(a,c) + d^2(c,a) + \\\\ d^2(b,c) + d^2(c,b) \\] In general, when you have \\(n\\) individuals, you can obtain up to \\(n^2\\) squared distances. We will give the generic name of Overall Dispersion to the sum of all squared pairwise distances: \\[ \\text{overall dispersion} = \\sum_{i=1}^{n} \\sum_{\\ell=1}^{n} d^2(i,\\ell) \\] Inertia Another measure of scatter among individuals can be computed by averaging the distances between all individuals and the centroid. Figure 3.10: Inertia as a measure of spread around the centroid The average sum of squared distances from each point to the centroid then becomes \\[ \\frac{1}{n} \\sum_{i=1}^{n} d^2(i,g) = \\frac{1}{n} \\sum_{i=1}^{n} (\\mathbf{\\vec{x}_i} - \\mathbf{\\vec{g}})^\\mathsf{T} (\\mathbf{\\vec{x}_i} - \\mathbf{\\vec{g}}) \\] We will name this measure Inertia, borrowing this term from the concept of inertia used in mechanics (in physics). \\[ \\text{Inertia} = \\frac{1}{n} \\sum_{i=1}^{n} d^2(i,g) \\] What is the motivation behind this measure? Consider the \\(p = 1\\) case; i.e. when \\(\\mathbf{X}\\) is simply a column vector \\[ \\mathbf{X} = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\\\ \\end{pmatrix} \\] The centroid will simply be the mean of these points: i.e. \\(\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\\). The sum of squared-distances from each point to the centroid then becomes: \\[ (x_1 - \\bar{x})^2 + (x_2 - \\bar{x})^2 + \\dots + (x_n - \\bar{x})^2 = \\sum_{i=1}^{n} (x_i - \\bar{x})^2 \\] Does the above formula look familiar? What if we take the average of the squared distances to the centroid? \\[ \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})^2 = \\frac{(x_1 - \\bar{x})^2 + (x_2 - \\bar{x})^2 + \\dots + (x_n - \\bar{x})^2}{n} \\] Same question: Do you recognize this formula? You better do… This is nothing else than the formula of the variance of \\(X\\). And yes, we are dividing by \\(n\\) (not by \\(n-1\\)). Hence, you can think of inertia as a multidimensional extension of variance, which gives the typical squared distance around the centroid. Overall Dispersion and Inertia Interestingly, the overall dispersion and the inertia are connected through the following relation: \\[\\begin{align*} \\text{overall dispersion} &amp;= \\sum_{i=1}^{n} \\sum_{\\ell=1}^{n} d^2(i,\\ell) \\\\ &amp;= 2n \\sum_{i=1}^{n} d^2(i,g) \\\\ &amp;= (2n^2) \\text{Inertia} \\end{align*}\\] The proof of this relation is left as a homework exercise. 3.4 Cloud of Variables The starting point when analyzing variables involves computing various summary measures—such as means, and variances—to get an idea of the common or central values, and the amount of variability of each variable. In this section we will review how concepts like the mean of a variable, the variance, covariance, and correlation, can be interpreted in a geometric sense, as well as their expressions in terms of vector-matrix operations. 3.4.1 Mean of a Variable To measure variation of one variable, we usually begin by calculating a “typical” value. The idea is to summarize the values of a variable with one or two representative values. You will find this notion under several terms like measures of center, location, central tendency, or centrality. As mentioned in the previous section, the prototypical summary value of center is the mean, sometimes referred to as average. The mean of an \\(n-\\)element variable \\(X = (x_1, x_2, \\dots, x_n)\\), represented by \\(\\bar{x}\\), is obtained by adding all the \\(x_i\\) values and then dividing by their total number \\(n\\): \\[ \\bar{x} = \\frac{x_1 + x_2 + \\dots + x_n}{n} \\] Using summation notation we can express \\(\\bar{x}\\) in a very compact way as: \\[ \\bar{x} = \\frac{1}{n} \\sum_{i = 1}^{n} x_i \\] If you associate a constant weight of \\(1/n\\) to each observation \\(x_i\\), you can look at the formula of the mean as a weighted sum: \\[ \\bar{x} = \\frac{1}{n} x_1 + \\frac{1}{n} x_2 + \\dots + \\frac{1}{n} x_n \\] This is a slightly different way of looking at the mean that will allow you to generalize the concept of an “average” as a weighted aggregation of information. For example, if we denote the weight of the \\(i\\)-th individual as \\(w_i\\), then the average can be expressed as: \\[ \\bar{x} = w_1 x_1 + w_2 x_2 + \\dots + w_n x_n = \\sum_{i=1}^{n} w_i x_i = \\mathbf{w^\\mathsf{T} x} \\] 3.4.2 Variance of a Variable A measure of center such as the mean is not enoguh to summarize the information of a variable. We also need a measure of the amount of variability. Synonym terms are variation, spread, scatter, and dispersion. Because of its relevance and importance for statistical learning methods, we will focus on one particular measure of spread: the variance (and its square root the standard deviation). Simply put, the variance is a measure of spread around the mean. The main idea behind the calculation of the variance is to quantify the typical concentration of values around the mean. The way this is done is by averaging the squared deviations from the mean. \\[ var(X) = \\frac{(x_1 - \\bar{x})^2 + \\dots + (x_n - \\bar{x})^2}{n} = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})^2 \\] Let’s disect the terms and operations involved in the formula of the variance. the main terms are the deviations from the mean \\((x_i - \\bar{x})\\), that is, the difference between each observation \\(x_i\\) and the mean \\(\\bar{x}\\). conceptually speaking, we want to know what is the average size of the deviations around the mean. simply averaging the deviations won’t work because their sum is zero (i.e. the sum of deviations around the mean will cancel out because the mean is the balancing point). this is why we square each deviation: \\((x_i - \\bar{x})^2\\), which literally means getting the squared distance from \\(x_i\\) to \\(\\bar{x}\\). having squared all the deviations, then we average them to get the variance. Because the variance has squared units, we need to take the square root to “recover” the original units in which \\(X\\) is expressed. This gives us the standard deviation \\[ sd(X) = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})^2} \\] In this sense, you can say that the standard deviation is roughly the average distance that the data points vary from the mean. Sample Variance In practice, you will often find two versions of the formula for the variance: one in which the sum of squared deviations is divided by \\(n\\), and another one in which the division is done by \\(n-1\\). Each version is associated to the statistical inference view of variance in terms of whether the data comes from the population or from a sample of the population. The population variance is obtained dividing by \\(n\\): \\[ \\textsf{population variance:} \\quad \\frac{1}{(n)} \\sum_{i=1}^{n} (x_i - \\bar{x})^2 \\] The sample variance is obtained dividing by \\(n - 1\\) instead of dividing by \\(n\\). The reason for doing this is to get an unbiased estimor of the population variance: \\[ \\textsf{sample variance:} \\quad \\frac{1}{(n-1)} \\sum_{i=1}^{n} (x_i - \\bar{x})^2 \\] It is important to note that most statistical software compute the variance with the unbiased version. If you implement your own functions and are planning to compare them against other software, then it is crucial to known what other programmers are using for computing the variance. Otherwise, your results might be a bit different from the ones with other people’s code. In this book, unless indicated otherwise, we will use the factor \\(\\frac{1}{n}\\) when introducing concepts of variance, and related measures. If needed, we will let you know when a formula needs to use the factor \\(\\frac{1}{n-1}\\). 3.4.3 Variance with Vector Notation In a similar way to expressing the mean with vector notation, you can also formulate the variance in terms of vector-matrix notation. First, notice that the formula of the variance consists of the addition of squared terms. Second, recall that a sum of numbers can be expressed with an inner product by using the unit vector (or summation operator). If we denote the mean vector as \\(\\mathbf{\\bar{x}}\\), then the variance of a vector \\(\\mathbf{x}\\) can be obtained with the following inner product: \\[ var(\\mathbf{x}) = \\frac{1}{n} (\\mathbf{x} - \\mathbf{\\bar{x}})^\\mathsf{T} (\\mathbf{x} - \\mathbf{\\bar{x}}) \\] where \\(\\mathbf{\\bar{x}}\\) is an \\(n\\)-element vector of mean values \\(\\bar{x}\\). Assuming that \\(\\mathbf{x}\\) is already mean-centered, then the variance is proportional to the squared norm of \\(\\mathbf{x}\\) \\[ var(\\mathbf{x}) = \\frac{1}{n} \\hspace{1mm} \\mathbf{x}^{\\mathsf{T}} \\mathbf{x} = \\frac{1}{n} \\| \\mathbf{x} \\|^2 \\] This means that we can formulate the variance with the general notion of an inner product: \\[ var(\\mathbf{x}) = \\frac{1}{n} \\langle \\mathbf{x}, \\mathbf{x} \\rangle \\] 3.4.4 Standard Deviation as a Norm If we use a metric matrix \\(\\mathbf{D} = diag(1/n)\\) then we have that the variance is given by a special type of inner product: \\[ var(\\mathbf{x}) = \\langle \\mathbf{x}, \\mathbf{x} \\rangle_{D} = \\mathbf{x}^{\\mathsf{T}} \\mathbf{D x} \\] From this point of view, we can say that the variance of \\(\\mathbf{x}\\) is equivalent to its squared norm when the vector space is endowed with a metric \\(\\mathbf{D}\\). Consequently, the standard deviation is simply the length of \\(\\mathbf{x}\\) in this particular geometric space. \\[ sd(\\mathbf{x}) = \\| \\mathbf{x} \\|_{D} \\] When looking at the standard deviation from this perspective, you can actually say that the amount of spread of a vector \\(\\mathbf{x}\\) is actually its length (under the metric \\(\\mathbf{D}\\)). 3.4.5 Covariance The covariance generalizes the concept of variance for two variables. Recall that the formula for the covariance between \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) is: \\[ cov(\\mathbf{x, y}) = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x}) (y_i - \\bar{y}) \\] where \\(\\bar{x}\\) is the mean value of \\(\\mathbf{x}\\) obtained as: \\[ \\bar{x} = \\frac{1}{n} (x_1 + x_2 + \\dots + x_n) = \\frac{1}{n} \\sum_{i = 1}^{n} x_i \\] and \\(\\bar{y}\\) is the mean value of \\(\\mathbf{y}\\): \\[ \\bar{y} = \\frac{1}{n} (y_1 + y_2 + \\dots + y_n) = \\frac{1}{n} \\sum_{i = 1}^{n} y_i \\] Basically, the covariance is a statistical summary that is used to assess the linear association between pairs of variables. Assuming that the variables are mean-centered, we can get a more compact expression of the covariance in vector notation: \\[ cov(\\mathbf{x, y}) = \\frac{1}{n} (\\mathbf{x^{\\mathsf{T}} y}) \\] Properties of covariance: the covariance is a symmetric index: \\(cov(X,Y) = cov(Y,X)\\) the covariance can take any real value (negative, null, positive) the covariance is linked to variances under the name of the Cauchy-Schwarz inequality: \\[cov(X,Y)^2 \\leq var(X) var(Y) \\] 3.4.6 Correlation Although the covariance indicates the direction—positive or negative—of a possible linear relation, it does not tell us how big or small the relation might be. To have a more interpretable index, we must transform the convariance into a unit-free measure. To do this we must consider the standard deviations of the variables so we can normalize the covariance. The result of this normalization is the coefficient of linear correlation defined as: \\[ cor(X, Y) = \\frac{cov(X, Y)}{\\sqrt{var(X)} \\sqrt{var(Y)}} \\] Representing \\(X\\) and \\(Y\\) as vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\), we can express the correlation as: \\[ cor(\\mathbf{x}, \\mathbf{y}) = \\frac{cov(\\mathbf{x}, \\mathbf{y})}{\\sqrt{var(\\mathbf{x})} \\sqrt{var(\\mathbf{y})}} \\] Assuming that \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are mean-centered, we can express the correlation as: \\[ cor(\\mathbf{x, y}) = \\frac{\\mathbf{x^{\\mathsf{T}} y}}{\\|\\mathbf{x}\\| \\hspace{1mm} \\|\\mathbf{y}\\|} \\] As it turns out, the norm of a mean-centered variable \\(\\mathbf{x}\\) is proportional to the square root of its variance (or standard deviation): \\[ \\| \\mathbf{x} \\| = \\sqrt{\\mathbf{x^{\\mathsf{T}} x}} = \\frac{1}{\\sqrt{n}} \\sqrt{var(\\mathbf{x})} \\] Consequently, we can also express the correlation with inner products as: \\[ cor(\\mathbf{x, y}) = \\frac{\\mathbf{x^{\\mathsf{T}} y}}{\\sqrt{(\\mathbf{x^{\\mathsf{T}} x})} \\sqrt{(\\mathbf{y^{\\mathsf{T}} y})}} \\] or equivalently: \\[ cor(\\mathbf{x, y}) = \\frac{\\mathbf{x^{\\mathsf{T}} y}}{\\| \\mathbf{x} \\| \\hspace{1mm} \\| \\mathbf{y} \\|} \\] In the case that both \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are standardized (mean zero and unit variance), that is: \\[ \\mathbf{x} = \\begin{bmatrix} \\frac{x_1 - \\bar{x}}{\\sigma_{x}} \\\\ \\frac{x_2 - \\bar{x}}{\\sigma_{x}} \\\\ \\vdots \\\\ \\frac{x_n - \\bar{x}}{\\sigma_{x}} \\end{bmatrix}, \\hspace{5mm} \\mathbf{y} = \\begin{bmatrix} \\frac{y_1 - \\bar{y}}{\\sigma_{y}} \\\\ \\frac{y_2 - \\bar{y}}{\\sigma_{y}} \\\\ \\vdots \\\\ \\frac{y_n - \\bar{y}}{\\sigma_{y}} \\end{bmatrix} \\] the correlation is simply the inner product: \\[ cor(\\mathbf{x, y}) = \\mathbf{x^{\\mathsf{T}} y} \\hspace{5mm} \\textsf{(standardized variables)} \\] 3.4.7 Geometry of Correlation Let’s look at two variables (i.e. vectors) from a geometric perspective. Figure 3.11: Two vectors in a 2-dimensional space The inner product ot two mean-centered vectors \\(\\langle \\mathbf{x}, \\mathbf{y} \\rangle\\) is obtained with the following equation: \\[ \\mathbf{x^{\\mathsf{T}} y} = \\|\\mathbf{x}\\| \\hspace{1mm} \\|\\mathbf{y}\\| \\hspace{1mm} cos(\\theta_{x,y}) \\] where \\(cos(\\theta_{x,y})\\) is the angle between \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\). Rearranging the terms in the previous equation we get that: \\[ cos(\\theta_{x,y}) = \\frac{\\mathbf{x^\\mathsf{T} y}}{\\|\\mathbf{x}\\| \\hspace{1mm} \\|\\mathbf{y}\\|} = cor(\\mathbf{x, y}) \\] which means that the correlation between mean-centered vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) turns out to be the cosine of the angle between \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\). 3.4.8 Orthogonal Projections Last but not least, we finish this chapter with a discussion of projections. To be more specific, the statistical interpretation of orthogonal projections. Let’s motivate this discussion with the following question: Consider two variables \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\). Can we approximate one of the variables in terms of the other? This is an asymmetric type of association since we seek to say something about the variability of one variable, say \\(\\mathbf{y}\\), in terms of the variability of \\(\\mathbf{x}\\). Figure 3.12: Two vectors in n-dimensional space We can think of several ways to approximate \\(\\mathbf{y}\\) in terms of \\(\\mathbf{x}\\). The approximation of \\(\\mathbf{y}\\), denoted by \\(\\mathbf{\\hat{y}}\\), means finding a scalar \\(b\\) such that: \\[ \\mathbf{\\hat{y}} = b \\mathbf{x} \\] The common approach to get \\(\\mathbf{\\hat{y}}\\) in some optimal way is by minimizing the square difference between \\(\\mathbf{y}\\) and \\(\\mathbf{\\hat{y}}\\). Figure 3.13: Orthogonal projection of y onto x The answer to this question comes in the form of a projection. More precisely, we orthogonally project \\(\\mathbf{y}\\) onto \\(\\mathbf{x}\\): \\[ \\mathbf{\\hat{y}} = \\mathbf{x} \\left( \\frac{\\mathbf{y^\\mathsf{T} x}}{\\mathbf{x^\\mathsf{T} x}} \\right) \\] or equivalently: \\[ \\mathbf{\\hat{y}} = \\mathbf{x} \\left( \\frac{\\mathbf{y^\\mathsf{T} x}}{\\| \\mathbf{x} \\|^2} \\right) \\] For convenience purposes, we can rewrite the above equation in a slightly different format: \\[ \\mathbf{\\hat{y}} = \\mathbf{x} (\\mathbf{x^\\mathsf{T}x})^{-1} \\mathbf{x^\\mathsf{T}y} \\] If you are familiar with linear regression, you should be able to recognize this equation. We’ll come back to this when we get to the chapter about Linear regression. 3.4.9 The mean as an orthogonal projection Let’s go back to the concept of mean of a variable. As we previously mention, a variable \\(X = (x_1, \\dots, x_n)\\), can be thought of a vector \\(\\mathbf{x}\\) in an \\(n\\)-dimensional space. Furthermore, let’s also consider the constant vector \\(\\mathbf{1}\\) of size \\(n\\). Here’s a conceptual diagram for this situation: Figure 3.14: Two vectors in n-dimensional space Out of curiosity, what happens when we ask about the orthogonal projection of \\(\\mathbf{x}\\) onto \\(\\mathbf{1}\\)? Something like in the following picture: Figure 3.15: Orthogonal projection of vector x onto constant vector 1 This projection is expressed in vector notation as: \\[ \\mathbf{\\hat{x}} = \\mathbf{1} \\left( \\frac{\\mathbf{x^\\mathsf{T} 1}}{\\mathbf{1^\\mathsf{T} 1}} \\right) \\] or equivalently: \\[ \\mathbf{\\hat{x}} = \\mathbf{1} \\left( \\frac{\\mathbf{x^\\mathsf{T} 1}}{\\| \\mathbf{1} \\|^2} \\right) \\] Note that the term in parenthesis is just a scalar, so we can actually express \\(\\mathbf{\\hat{x}}\\) as \\(b \\mathbf{1}\\). This means that a projection implies multiplying \\(\\mathbf{1}\\) by some number \\(b\\), such that \\(\\mathbf{\\hat{x}} = b \\mathbf{1}\\) is a stretched or shrinked version of \\(\\mathbf{1}\\). So, what is the scalar \\(b\\)? It is simply the mean of \\(\\mathbf{x}\\): \\[ \\mathbf{\\hat{x}} = \\mathbf{1} \\left( \\frac{\\mathbf{x^\\mathsf{T} 1}}{\\| \\mathbf{1} \\|^2} \\right) = \\bar{x} \\mathbf{1} \\] This is better appreciated in the following figure. Figure 3.16: Mean of x and its relation with the projection onto constant vector 1 What this tells us is that the mean of the variable \\(X\\), denoted by \\(\\bar{x}\\), has a very interesting geometric interpretation. As you can tell, \\(\\bar{x}\\) is the scalar by which you would multiply \\(\\mathbf{1}\\) in order to obtain the vector projection \\(\\mathbf{\\hat{x}}\\). "],
["pca.html", "4 Principal Components Analysis 4.1 Low-dimensional Representations 4.2 Projections 4.3 Maximization Problem 4.4 Another Perspective of PCA 4.5 Data Decomposition Model", " 4 Principal Components Analysis Our first unsupervised method of the book is Principal Components Analysis, commonly referred to as PCA. Principal Components Analysis (PCA) is the workhorse method of multivariate data analysis. Simply put, PCA helps us study and explore a data set of quantitative variables measured on a set of objects. One way to look at the purpose of principal components analysis is to get the best low-dimensional representation of the variation in data. Among the various appealing features of PCA is that it allows us to obtain a visualization of the objects in order to see their proximities. Likewise, it also provides us results to get a graphic representation of the variables in terms of their correlations. Overall, PCA is a multivariate technique that allows us to summarize the systematic patterns of variations in a data set. The classic reference for PCA is the work by the eminent British biostatistician &lt;a href=“https://en.wikipedia.org/wiki/Karl_Pearson” target=\"_blank&gt;Karl Pearson’s “On Lines and Planes of Closest Fit to Systems of Points in Space,” from 1901. This publication presents the underlying problem of PCA from a purely geometric standpoint, describing how to find low-dimensional subspaces that best fit—in the least squares sense—a cloud of points. Keep in mind that Pearson never used the term principal components analysis. The other seminal work of PCA is the one by American mathematician and economic theorist Harold Hotelling with “Analysis of a Complex of Statistical Variables into Principal Components,” from 1933. It is Hotelling who coined the term principal components analysis and gave the method a more mature statistical perspective. Unlike Pearson, Hotelling finds the principal components as orthogonal linear combinations of the input variables in a way that such combinations have maximum variance. PCA is one of those methods that can be approached from multiple, seemingly unrelated, perspectives. The way we are going to introduce PCA is not the typical way in which PCA is discussed in most books published in English. However, our introduction is actually based on the ideas and concepts originally published by Karl Pearson. 4.1 Low-dimensional Representations Let’s play the following game. Imagine for a minute that you have the superpower to see any type of multidimensional space (not just three-dimensions). As we mentioned before, we think of the individuals as forming a cloud of points in a \\(p\\)-dim space, and the variables forming a cloud of arrows in an \\(n\\)-dim space. Pretend that you have some data in which its cloud of points has the shape of a mug, like in the following diagram: Figure 4.1: Cloud of points in the form of a mug This mug is supposed to be high-dimensional, and something that you are not supposed to ever see in real life. So the question is: Is there a way in which we can get a low-dimensional representation of this data? Luckily, the answer is: YES, we can! How? Well, the name of the game is projections: we can look for projections of the data into sub-spaces of lower dimension, like in the diagram below. Figure 4.2: Various projections onto subspaces Think of projections as taking photographs or x-rays of the mug. You can take a photo of the mug from different angles. For instance, you can take a picture in which the lens of the camera is on the top of the mug, or another picture in which the lens is below the mug (from the bottom), and so on. If you have to take the “best” photograph of the mug, from what angle would you take such picture? To answer this question we need to be more precise about what do we mean by “best”. Here, we are talking about getting a picture in which the image of the mug is as similar as possible to the original object. As you can tell from the above figure, we have three candidate subspaces: \\(\\mathbb{H}_A\\), \\(\\mathbb{H}_B\\), and \\(\\mathbb{H}_C\\). Among the three possible projections, subspace \\(\\mathbb{H}_C\\) is the one that provides the best low dimensional representation, in the sense that the projected silhouette is the most similar to the original mug shape. Figure 4.3: The shape of the projection is similar to the original mug shape. We can say that the “photo” from projecting onto subspace \\(\\mathbb{H}_C\\) is the one that most resembles the original object. Now, keep in mind that the resulting image in the low-dimensional space is not capturing the whole pattern. In other words, there is some loss of information. However, by chosing the right projection, we hope to minimize such loss. 4.2 Projections Following the idea of projections, let’s now discuss with more detail this concept, and its implications. Pretend that we zoom in to see some of the individuals of the cloud of points that form the mug (see figure below). Keep in mind that these data points are in a \\(p\\)-dimensional space, and the cloud will have its centroid \\(\\mathbf{g}\\) (i.e. average individual). Figure 4.4: Set of individuals in p-dim space. Because our goal is to look for a low-dimensional represention, we can start by considering the simplest type of low-dimensional representations: a one-dimensional space. This in turn can be graphically displayed as one axis. In the above diagram (as well as the one below) this dimension is depicted with the yellow line, labeled as \\(dim_{\\mathbf{v}}\\). Figure 4.5: Set of individuals in p-dim space. We should note that we don’t really manipulate dimension \\(dim_{\\mathbf{v}}\\) directly. Instead, what we manipulate is a vector \\(\\mathbf{v}\\) along this dimension. Figure 4.6: Dimension spanned by vector v At the end of the day, we want to project the individuals onto this dimension. In particular, the type of projections that we are interested in are orthogonal projections. Figure 4.7: Projections onto one dimension 4.2.1 Vector and Scalar Projections Let’s consider a specific individual, for example the \\(i\\)-th individual. And let’s take the centroid \\(\\mathbf{g}\\) as the origin of the cloud of points. In this way, the dimension that we are looking for has to pass thorugh the centroid of this cloud. Obtaining the orthogonal projection of the \\(i\\)-th individual onto the dimension \\(dim_{\\mathbf{v}}\\) involves projecting \\(\\mathbf{x_i}\\) onto any vector \\(\\mathbf{v}\\) along this dimension. Figure 4.8: Projection of an individual onto one dimension Recall that an orthogonal projection can be split into two types of projections: (1) the vector projection, and (2) the scalar projection. The vector projection of \\(\\mathbf{x_i}\\) onto the \\(\\mathbf{v}\\) is defined as: \\[ proj_{\\mathbf{v}} (\\mathbf{x_i}) = \\frac{\\mathbf{x_{i}^\\mathsf{T} v}}{\\mathbf{v^\\mathsf{T}v}} \\hspace{1mm} \\mathbf{v} = \\mathbf{\\hat{v}} \\] The scalar projection of \\(\\mathbf{x_i}\\) onto \\(\\mathbf{v}\\) is defined as: \\[ comp_{\\mathbf{v}} (\\mathbf{x_i}) = \\frac{\\mathbf{x_{i}^\\mathsf{T}v}}{\\|\\mathbf{v}\\|} = z_{ik} \\] The following diagram displays both types of projections: Figure 4.9: Scalar and vector projections of i-th individual onto vector v We are not really interested in obtaining the vector projection \\(proj_{\\mathbf{v}} (\\mathbf{x_i})\\). Instead, what we care about is the scalar projection \\(comp_{\\mathbf{v}} (\\mathbf{x_i})\\). In other words, we just want to obtain the coordinate of the \\(i\\)-th individual along this axis. 4.2.2 Projected Inertia As we said before, our goal is to find the angle that gives us the “best” photo of the mug (i.e. cloud of points). This can be translated as finding a subspace in which the distances between the points are most similar to those on the actual mug. Figure 4.10: Projection Goal Now, we need to define what we mean by “similar” distances in \\(\\mathbb{H}\\). Consider the overall dispersion of our original system (the one with all \\(p\\) dimensions): \\(\\sum_{j} \\sum_{\\ell} d^2 (i, \\ell)\\). This is a fixed number; and we cannot change it. However, we can try fine-tune our subset \\(\\mathbb{H}\\) such that: \\[ \\sum_{j} \\sum_{\\ell} d^2 (i, \\ell) \\approx \\sum_{i} \\sum_{\\ell} d_H^2(i, \\ell) \\] In the previous chapter, we learned about the so-called measure of “Overall Dispersion” which is basically the sum of all pairwise squared distances, which can also be expressed in terms of a sum of squared distances to the centroid: \\[ \\sum_{j} \\sum_{\\ell} d^2 (i, \\ell) = \\ 2n \\sum_i d^2 (i, g) \\] However, it will be easier to simply consider the inertia of the system (as opposed to the overall dispersion). \\[ 2n \\sum_i d^2 (i, g) \\ \\longrightarrow \\ \\frac{1}{n} \\sum_{i=1}^{n} d^2 (i, g) = \\text{Inertia} \\] In mathematical terms, finding the subspace \\(\\mathbb{H}\\) that gives us “similar” distances to those of the original space corresponds to maximizing the projected inertia: \\[ \\max_{\\mathbb{H}} \\left\\{ \\frac{1}{n} \\sum_{i=1}^{n} d_{\\mathbb{H}}^2 (i, g) \\right\\} \\] Now, as we discuss before, the simplest subspace will be one dimension, that is, we consider the case where \\(\\mathbb{H} \\subseteq \\mathbb{R}^1\\). So the first approach is to project our data points onto a vector \\(\\mathbf{v}\\). Hence, the projected inertia becomes: \\[ \\frac{1}{n} \\sum_{i=1}^{n} d_\\mathbb{H}^2 (i, g) = \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\mathbf{x_i}^\\mathsf{T} \\mathbf{v} \\right)^2 = \\frac{1}{n} \\sum_{i=1}^{n} z_i^2 \\] and our maximization problem becomes \\[ \\max_{\\mathbf{v}} \\left\\{ \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\mathbf{x_i}^\\mathsf{T} \\mathbf{v} \\right)^2 \\right\\} \\quad \\mathrm{s.t.} \\quad \\mathbf{v}^\\mathsf{T} \\mathbf{v} = 1 \\] For convenience, we add the restriction that \\(\\mathbf{v}\\) is a unit-vector (vector of unit norm). Why do we need the constraint? If we didn’t have the constraint, we could always find a \\(\\mathbf{v}\\) that obtains higher inertia, and things would explode. Furthermore, as we will see in the next section, this helps in the algebra we will use when we actually perform the maximization. 4.3 Maximization Problem We now need to compute the projected Inertia. Without loss of generality, we will assume that we have mean-centered data. This implies that the centroid \\(g\\) of the cloud of points is now the origin \\(g = 0\\). Since we are projecting onto a line spanned by a unit-vector \\(\\mathbf{v}\\), the projected inertia \\(I_{\\mathbb{H}}\\) will simply be the variance of the projected data points: \\[ I_{\\mathbb{H}} = \\frac{1}{n} \\sum_{i=1}^{n} d_\\mathbb{H}^2(i, 0) = \\frac{1}{n} \\sum_{i=1}^{n} z_i^2 \\] In vector-matrix notation we have the following expressions: \\[ \\mathbf{z} := \\begin{pmatrix} z_1 \\\\ z_2 \\\\ \\vdots \\\\ z_n \\\\ \\end{pmatrix} \\ \\Longrightarrow \\ I_\\mathbb{H} = \\frac{1}{n} \\mathbf{z}^\\mathsf{T} \\mathbf{z} = \\frac{1}{n} \\mathbf{v}^\\mathsf{T} \\mathbf{X}^\\mathsf{T} \\mathbf{X} \\mathbf{v} \\] Hence, the maximization problem becomes \\[ \\max_{\\mathbf{v}} \\left\\{ \\frac{1}{n} \\mathbf{v}^\\mathsf{T} \\mathbf{X}^\\mathsf{T} \\mathbf{X} \\mathbf{v} \\right\\} \\qquad \\text{s.t.} \\qquad \\mathbf{v}^\\mathsf{T} \\mathbf{v} = 1 \\] To solve this maximization problem, we utilize the method of Lagrange Multipliers. The lagrangian \\(\\mathcal{L}\\) is given by: \\[ \\mathcal{L} = \\frac{1}{n} \\mathbf{v}^\\mathsf{T} \\mathbf{X}^\\mathsf{T} \\mathbf{X} \\mathbf{v} - \\lambda \\left( \\mathbf{v}^\\mathsf{T} \\mathbf{v} - 1 \\right) \\] Calculating the derivative of the lagrangian \\(\\mathcal{L}\\) with respect to \\(\\mathbf{v}\\), and equating to zero we get: \\[ \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{v}} = \\frac{2}{n} \\mathbf{X}^\\mathsf{T} \\mathbf{X} \\mathbf{v} - 2 \\lambda \\mathbf{v} = \\mathbf{0} \\] Doing some algebra, we get: \\[ \\underbrace{ \\frac{1}{n} \\mathbf{X}^\\mathsf{T} \\mathbf{X} \\mathbf{v} }_{:= \\mathbf{S} } = \\lambda \\mathbf{v} \\quad \\Rightarrow \\quad \\mathbf{S} \\mathbf{v} = \\lambda \\mathbf{v} \\] That is, we obtain the equation \\(\\mathbf{S} \\mathbf{v} = \\lambda \\mathbf{v}\\) where \\(\\mathbf{S} \\in \\mathbb{R}^{p \\times p}\\) is symmetric. This means that \\(\\mathbf{v}\\) is an eigenvector (with eigenvalue \\(\\lambda\\)) of \\(\\mathbf{S}\\). Moreover, \\(\\lambda\\) is the value of the projected inertia \\(I_\\mathbb{H}\\)—which is the thing that we want to maximize. 4.3.1 Eigenvectors of \\(\\mathbf{S}\\) Assume that \\(\\mathbf{X}\\) has full rank (i.e. \\(\\mathrm{rank}(\\mathbf{X}) = p\\)). We then obtain \\(p\\) eigenvectors, which together form the matrix \\(\\mathbf{V}\\): \\[ \\mathbf{V} = \\begin{bmatrix} \\mathbf{v_1} &amp; \\mathbf{v_2} &amp; \\dots &amp; \\mathbf{v_k} &amp; \\dots &amp; \\mathbf{v_p} \\\\ \\end{bmatrix} \\] We can also obtain \\(\\mathbf{\\Lambda}_{p \\times p} := \\mathrm{diag}\\left\\{ \\lambda_i \\right\\}_{i=1}^{n}\\) (i.e. the matrix of eigenvalues): that is, \\[ \\mathbf{\\Lambda} = \\begin{bmatrix} \\lambda_1 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; \\lambda_2 &amp; \\dots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; \\lambda_p \\\\ \\end{bmatrix} \\] Finally, we can also obtain the matrix \\(\\mathbf{Z}_{n \\times p}\\) which is the matrix consisting of the vectors \\(\\mathbf{z_i}\\): \\[ \\mathbf{Z} = \\begin{bmatrix} \\mathbf{z_1} &amp; \\mathbf{z_2} &amp; \\dots &amp; \\mathbf{z_k} &amp; \\dots &amp; \\mathbf{z_p} \\\\ \\end{bmatrix} \\] The matrix of eigenvectors \\(\\mathbf{V}\\) is known as the matrix of loadings (in PCA jargon) The matrix of projected points \\(\\mathbf{Z}\\) is known as the matrix of principal components (PC’s), also known as scores (in PCA jargon). Let us examine the \\(k\\)-th principal component \\(\\mathbf{z_k}\\): \\[ \\mathbf{z_k} = \\mathbf{X v_k} = v_{1k} \\mathbf{x_1} + v_{2k} \\mathbf{x_2} + \\dots + v_{pk} \\mathbf{x_p} \\] (where \\(\\mathbf{x_k}\\) denotes columns of \\(\\mathbf{X}\\)). Note that \\(\\mathrm{mean}(\\mathbf{z_k}) = 0\\); since we are assuming that the data is mean-centered, we have that \\(\\mathrm{mean}(\\mathbf{x_1}) = 0\\). What about variance of \\(\\mathbf{z_k}\\)? \\[\\begin{align*} Var(\\mathbf{z_k}) &amp; = \\frac{1}{n} \\mathbf{z_{k}^\\mathsf{T}} \\mathbf{z_k} \\\\ &amp; = \\frac{1}{n}\\left( \\mathbf{X} \\mathbf{v_k} \\right)^{\\mathsf{T}} \\left( \\mathbf{X} \\mathbf{v_k} \\right) \\\\ &amp; = \\frac{1}{n} \\mathbf{v_{k}^\\mathsf{T}} \\mathbf{X}^\\mathsf{T} \\mathbf{X} \\mathbf{v_k} \\\\ &amp; = \\mathbf{v_k}^\\mathsf{T} \\mathbf{S} \\mathbf{v_k} \\\\ &amp; = \\mathbf{v_k}^\\mathsf{T} \\left( \\lambda_k \\mathbf{v_k} \\right) = \\lambda_k \\left( \\mathbf{v_k}^\\mathsf{T} \\mathbf{v_k} \\right) \\\\ &amp; = \\lambda_k \\end{align*}\\] Hence, we obtain the following interesting result: the \\(k\\)-th eigenvalue of \\(\\mathbf{S}\\) is simply the variance of the \\(k\\)-th principal component. If \\(\\mathbf{X}\\) is mean centered, then \\(\\mathbf{S} = \\frac{1}{n} \\mathbf{X^\\mathsf{T} X}\\) is nothing but the variance-covariance matrix of our data. If \\(\\mathbf{X}\\) is standardized (i.e. mean-centered and scaled by the variance), then \\(\\mathbf{S}\\) becomes the correlation matrix. In summary: \\(\\mathrm{mean}(\\mathbf{z_k}) = 0\\) \\(Var(\\mathbf{z_k}) = \\lambda_k\\) \\(\\mathrm{sd}(\\mathbf{z_k}) = \\sqrt{\\lambda_k}\\) We have the following fact (the proof is omitted, and may be assigned as homework or as a test question): \\[ \\text{Inertia} = \\frac{1}{n} \\sum_{i=1}^{n} d^2(i, g) = \\sum_{k} \\lambda_k = \\mathrm{tr}\\left( \\frac{1}{n} \\mathbf{X^\\mathsf{T} X} \\right) \\] The dimensions that we find in our analysis (through \\(\\mathbf{v_k}\\)) relates directly to \\(\\mathbf{z_k}\\). \\(\\sum_{k=1}^{p} \\lambda_k\\) relates directly to the total amount of variability of our data. Remark: The principal components capture different parts of the variability in the data. 4.4 Another Perspective of PCA Having seen how to approach PCA from a geometric point of view, let us present another way to approach PCA from a variance maximazation standpoint, which is the most common way in which PCA is introduced within the Anglo-Saxon literature. Given a set of \\(p\\) variables \\(\\mathbf{x_1}, \\mathbf{x_2}, \\dots, \\mathbf{x_p}\\), we want to obtain new \\(k\\) variables \\(\\mathbf{z_1}, \\mathbf{z_2}, \\dots, \\mathbf{z_k}\\), called the Principal Components (PCs). A principal component is a linear combination of the \\(p\\) variables: \\(\\mathbf{z} = \\mathbf{Xv}\\). The first PC is a linear mix which we graphically depict like this: Figure 4.11: PCs as linear combinations of X-variables The second PC is another linear mix: Figure 4.12: PCs as linear combinations of X-variables We want to compute the PCs as linear combinations of the original variables. \\[ \\begin{array}{c} \\mathbf{z_1} = v_{11} \\mathbf{x_1} + \\dots + v_{p1} \\mathbf{x_p} \\\\ \\mathbf{z_2} = v_{12} \\mathbf{x_1} + \\dots + v_{p2} \\mathbf{x_p} \\\\ \\vdots \\\\ \\mathbf{z_k} = v_{1k} \\mathbf{x_1} + \\dots + v_{pk} \\mathbf{x_p} \\\\ \\end{array} \\] Or in matrix notation: \\[ \\mathbf{Z} = \\mathbf{X V} \\] where \\(\\mathbf{Z}\\) is an \\(n \\times k\\) matrix of principal components, and \\(\\mathbf{V}\\) is a \\(p \\times k\\) matrix of weights, also known as directional vectors of the principal axes. The following figure shows a graphical representation of a PCA problem in diagram notation: Figure 4.13: PCs as linear combinations of X-variables We look to transform the original variables into a smaller set of new variables, the Principal Components (PCs), that summarize the variation in data. The PCs are obtained as linear combinations (i.e. weighted sums) of the original variables. We look for PCs is in such a way that they have maximum variance, and being mutually orthogonal. 4.4.1 Finding Principal Components The way to find principal components is to construct them as weighted sums of the original variables, looking to optimize some criterion and following some constraints. One way in which we can express the criterion is to require components \\(\\mathbf{z_1}, \\mathbf{z_2}, \\dots, \\mathbf{z_k}\\) that capture most of the variation in the data \\(\\mathbf{X}\\). “Capturing most of the variation,” implies looking for a vector \\(\\mathbf{v_h}\\) such that a component \\(\\mathbf{z_h} = \\mathbf{X v_h}\\) has maximum variance: \\[ \\max_{\\mathbf{v_h}} \\; var(\\mathbf{z_h}) \\quad \\Rightarrow \\quad \\max_{\\mathbf{v_h}} \\; var(\\mathbf{X v_h}) \\] that is \\[ \\max_{\\mathbf{v_h}} \\; \\frac{1}{n} \\mathbf{v_{h}^\\mathsf{T} X^\\mathsf{T} X v_h} \\] As you can tell, this is a maximization problem. Without any constraints, this problem is unbounded, not to mention useless. We could take \\(\\mathbf{v_h}\\) as bigger as we want without being able to reach any maximum. To get a feasible solution we need to impose some kind of restriction. The standard adopted constraint is to require \\(\\mathbf{v_h}\\) to be of unit norm: \\[ \\| \\mathbf{v_h} \\| = 1 \\; \\hspace{1mm} \\Rightarrow \\; \\hspace{1mm} \\mathbf{v_{h}^\\mathsf{T} v_h} = 1 \\] Note that \\((1/n) \\mathbf{X^\\mathsf{T} X}\\) is the variance-covariance matrix. If we denote \\(\\mathbf{S} = (1/n) \\mathbf{X^\\mathsf{T} X}\\) then the criterion to be maximized is: \\[ \\max_{\\mathbf{v_h}} \\; \\mathbf{v_{h}^\\mathsf{T} S v_h} \\] subject to \\(\\mathbf{v_{h}^\\mathsf{T} v_h} = 1\\) To avoid a PC \\(\\mathbf{z_h}\\) from capturing the same variation as other PCs \\(\\mathbf{z_l}\\) (i.e. avoiding redundant information), we also require them to be mutually orthogonal so they are uncorrelated with each other. Formally, we impose the restriction \\(\\mathbf{z_h}\\) to be perpendicular to other components: \\(\\mathbf{z_{h}^\\mathsf{T} z_l} = 0; (h \\neq l)\\). 4.4.2 Finding the first PC In order to get the first principal component \\(\\mathbf{z_1} = \\mathbf{X v_1}\\), we need to find \\(\\mathbf{v_1}\\) such that: \\[ \\max_{\\mathbf{v_1}} \\; \\mathbf{v_{1}^\\mathsf{T} S v_1} \\] subject to \\(\\mathbf{v_{1}^\\mathsf{T} v_1} = 1\\) Being a maximization problem, the typical procedure to find the solution is by using the Lagrangian multiplier method. Using Lagrange multipliers we get: \\[ \\mathbf{v_{1}^\\mathsf{T} S v_1} - \\lambda (\\mathbf{v_{1}^\\mathsf{T} v_1} - 1) = 0 \\] Differentiation with respect to \\(\\mathbf{v_1}\\), and equating to zero gives: \\[ \\mathbf{S v_1} - \\lambda_1 \\mathbf{v_1} = \\mathbf{0} \\] Rearranging some terms we get: \\[ \\mathbf{S v_1} = \\lambda_1 \\mathbf{v_1} \\] What does this mean? It means that \\(\\lambda_1\\) is an eigenvalue of \\(\\mathbf{S}\\), and \\(\\mathbf{v_1}\\) is the corresponding eigenvector. 4.4.3 Finding the second PC In order to find the second principal component \\(\\mathbf{z_2} = \\mathbf{X v_2}\\), we need to find \\(\\mathbf{v_2}\\) such that \\[ \\max_{\\mathbf{v_2}} \\; \\mathbf{v_{2}^\\mathsf{T} S v_2} \\] subject to \\(\\| \\mathbf{v_2} \\| = 1\\) and \\(\\mathbf{z_{1}^\\mathsf{T} z_2} = 0\\). Remember that \\(\\mathbf{z_2}\\) must be uncorrelated to \\(\\mathbf{z_1}\\). Applying the Lagrange multipliers, it can be shown that the desired \\(\\mathbf{v_2}\\) is such that \\[ \\mathbf{S v_2} = \\lambda_2 \\mathbf{v_2} \\] In other words. \\(\\lambda_2\\) is an eigenvalue of \\(\\mathbf{S}\\) and \\(\\mathbf{v_2}\\) is the corresponding eigenvector. 4.4.4 Finding all PCs All PCs can be found simultaneously by diagonalizing \\(\\mathbf{S}\\). Diagonalizing \\(\\mathbf{S}\\) involves expressing it as the product: \\[ \\mathbf{S} = \\mathbf{V \\Lambda V^\\mathsf{T}} \\] where: \\(\\mathbf{D}\\) is a diagonal matrix the elements in the diagonal of \\(\\mathbf{D}\\) are the eigenvalues of \\(\\mathbf{S}\\) the columns of \\(\\mathbf{V}\\) are orthonormal: \\(\\mathbf{V^\\mathsf{T} V= I}\\) the columns of \\(\\mathbf{V}\\) are the eigenvectors of \\(\\mathbf{S}\\) \\(\\mathbf{V^\\mathsf{T}} = \\mathbf{V^{-1}}\\) Diagonalizing a symmetric matrix is nothing more than obtaining its eigenvalue decomposition (a.k.a. spectral decomposition). A \\(p \\times p\\) symmetric matrix \\(\\mathbf{S}\\) has the following properties: \\(\\mathbf{S}\\) has \\(p\\) real eigenvalues (counting multiplicites) the eigenvectors corresponding to different eigenvalues are orthogonal \\(\\mathbf{S}\\) is orthogonally diagonalizable (\\(\\mathbf{S} = \\mathbf{V \\Lambda V^\\mathsf{T}}\\)) the set of eigenvalues of \\(\\mathbf{S}\\) is called the spectrum of \\(\\mathbf{S}\\) In summary: The PCA solution can be obtained with an Eigenvalue Decomposition of the matrix \\(\\mathbf{S} = (1/n) \\mathbf{X^\\mathsf{T}X}\\) 4.5 Data Decomposition Model Formally, PCA involves finding scores and loadings such that the data can be expressed as a product of two matrices: \\[ \\underset{n \\times p}{\\mathbf{X}} = \\underset{n \\times r}{\\mathbf{Z}} \\underset{r \\times p}{\\mathbf{V^\\mathsf{T}}} \\] where \\(\\mathbf{Z}\\) is the matrix of PCs or scores, and \\(\\mathbf{V}\\) is the matrix of loadings. We can obtain as many different eigenvalues as the rank of \\(\\mathbf{S}\\) denoted by \\(r\\). Ideally, we expect \\(r\\) to be smaller than \\(p\\) so we get a convenient data reduction. But usually we will only retain just a few PCs (i.e. \\(k \\ll p\\)) expecting not to lose too much information: \\[ \\underset{n \\times p}{\\mathbf{X}} \\approx \\underset{n \\times k}{\\mathbf{Z}} \\hspace{1mm} \\underset{k \\times p}{\\mathbf{V^\\mathsf{T}}} + \\text{Residual} \\] The previous expression means that just a few PCs will optimally summarize the main structure of the data 4.5.1 Alternative Approaches Finding \\(\\mathbf{z_h} = \\mathbf{X v_h}\\) with maximum variance has another important property that it is not always mentioned in multivariate textbooks but that we find worth mentioning. \\(\\mathbf{z_h}\\) is such that \\[ \\max \\sum_{j = 1}^{p} cor^2(\\mathbf{z_h, x_j}) \\] What this expression implies is that principal components \\(\\mathbf{z_h}\\) are computed to be the best representants in terms of maximizing the sum of squared correlations with the variables \\(\\mathbf{x_1}, \\mathbf{x_2}, \\dots, \\mathbf{x_p}\\). Interestingly, you can think of PCs as predictors of the variables in \\(\\mathbf{X}\\). Under this perspective, we can reverse the relations and see PCA from a regression-like model perspective: \\[ \\mathbf{x_j} = v_{jh} \\mathbf{z_h} + \\mathbf{e_h} \\] Notice that the regression coefficient is the \\(j\\)-th element of the \\(h\\)-th eigenvector. "],
["ols.html", "5 Linear Regression 5.1 Motivation 5.2 The Idea/Intuition of Regression 5.3 The Linear Regression Model 5.4 The Error Measure 5.5 The Least Squares Algorithm 5.6 Geometries of OLS", " 5 Linear Regression Before entering supervised learning territory, we want to discuss the general framework of linear regression. We will introduce this topic from a pure model-fitting point of view. In other words, we will postpone the learning aspect (the prediction of new data) after the chapter of theory of learning. The reason to cover linear regression in this way is for us to have something to work with when we start talking about the theory of supervised learning. 5.1 Motivation Consider, again, the NBA dataset example from previous chapters. Suppose we want to use this data to predict the salary of NBA players, in terms of certain variables like player’s team, player’s height, player’s weight, player’s position, player’s years of professional experience, player’s number of 2pts, player’s number of 3 points, number of blocks, etc. Of course, we need information on the salaries of some current NBA player’s: Player Height Weight Yrs Expr 2 Points 3 Points 1 \\(\\bigcirc\\) \\(\\bigcirc\\) \\(\\bigcirc\\) \\(\\bigcirc\\) \\(\\bigcirc\\) 2 \\(\\bigcirc\\) \\(\\bigcirc\\) \\(\\bigcirc\\) \\(\\bigcirc\\) \\(\\bigcirc\\) 3 \\(\\bigcirc\\) \\(\\bigcirc\\) \\(\\bigcirc\\) \\(\\bigcirc\\) \\(\\bigcirc\\) … … … … … … As usual, we use the symbol \\(\\mathbf{x_i}\\) to denote the vector of measurements of player \\(i\\)’s statistics (e.g. height, weight, etc.); in turn, the salary of the \\(i\\)-th player is represented with \\(y_i\\). Ideally, we assume the existance of some function \\(f : \\mathcal{X} \\to \\mathcal{y}\\) (i.e. a function that takes values from \\(\\mathcal{X}\\) space and maps them to a single value \\(y\\)). We will refer to this function the ideal “formula” for salary. Here we are using the word formula in a very lose sense, and not necessarily using the word “formula” in the mathematical sense. We now seek a hypothesized model (which we call \\(\\widehat{f} : \\mathcal{X} \\to y\\)), which we select from some set of candidate functions \\(h_1, h_2, \\dots, h_m\\). Our task is to obtain \\(\\widehat{f}\\) in a way that we can claim that it is a good approximation of the (unknown) function \\(f\\). 5.2 The Idea/Intuition of Regression Let’s go back to our example with NBA players. Recall that \\(y_i\\) denotes the salary for the \\(i\\)-th player. For simplicity’s sake let’s not worry about inflation. Say we now have a new prospective player from Europe; and we are tasked with predicting their salary denoted by \\(y_0\\). Let’s review a couple of scenarios to get a high-level intuition for this task. Scenario 1. Suppose we have no information on this new player. How would we compute \\(y_{0}\\) (i.e. the salary of this new player)? One possibility is to guesstimate \\(y_0\\) using the historical average salary \\(\\bar{y}\\) of NBA players. In other words, we would simply calculate: \\(\\hat{y}_0 = \\bar{y}\\). In this case we are using \\(\\bar{y}\\) as the typical score (e.g. a measure of center) as a plausible guesstimate for \\(y_0\\). We could also look at the median of the existing salaries, if we are concerned about outliers or some skewed distribution of salaries. Scenario 2. Now, suppose we know that this new player will sign on to the LA Lakers. Compared to scenario 1, we now have a new bit of information since we know which team will hire this player. Therefore, we can use this fact to have a more educated guess for \\(y_0\\). How? Instead of using the salaries of all playes, we can focus on the salaries of Laker’s players. We could then use \\(\\hat{y}_0 = \\text{avg}(\\text{Laker&#39;s Salaries})\\): that is, the average salary of all Laker’s players. It is reasonable that \\(\\hat{y}_0\\) is “closer” to the average salary of Laker’s than to the overall average salary of all NBA players. Figure 5.1: Average salary by team Scenario 3. Similarly, if we know this new player’s years of experience (e.g. 6 years), we would look at the average of salaries corresponding to players with the same level of experience. Figure 5.2: Average salary by years of experience What do the three previous scenarios correspond to? In all of these examples, the prediction is basically a conditional mean: \\[ \\hat{y}_0 = \\text{ave}(y_i|x_i = x_0) \\] Of course, the previous strategy only makes sense when we have data points \\(x_i\\) that are equal to the qeury point \\(x_0\\). But what if none of the available \\(x_i\\) values are equal to \\(x_0\\)? We’ll talk about this later. The previous hypothetical scenarios illustrate the core idea of regression: we obtain predictions \\(\\hat{y}_0\\) using quantities of the form \\(\\text{ave}(y_i|x_i = x_0)\\) which can be formalized—under some assumptions—into the notion of conditional expectations of the form: \\[ \\mathbb{E}(y_i \\mid x_{i1}^{*} , x_{i2}^{*}, \\dots, x_{ip}^{*}) \\] where \\(x_{ij}^{*}\\) represents the \\(i\\)-th measurement of the \\(j\\)-th variable. The above equation is what we call the regression function; note that the regression function is nothing more than a conditional expectation! 5.3 The Linear Regression Model In a regression model we use one or more features \\(X\\) to say something about the reponse \\(Y\\). In turn, a linear regression model tells us to combine our features in a linear way in order to approximate the response, In the univarite case, we have a linear equation: \\[ \\hat{Y} = b_0 + b_1 X \\tag{5.1} \\] In pointwise format, that is for a given individual \\(i\\), we have: \\[ \\hat{y}_i = b_0 + b_1 x_i \\tag{5.2} \\] In vector notation: \\[ \\mathbf{\\hat{y}} = b_0 + b_1 \\mathbf{x} \\tag{5.3} \\] To simplify notation, sometimes we prefer to add an auxiliary constant feature in the form of a vector of 1’s with \\(n\\) elements, and then use matrix notation with the following elements: \\[ \\mathbf{X} = \\ \\begin{bmatrix} 1 &amp; x_{1} \\\\ 1 &amp; x_{2} \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; x_{n} \\\\ \\end{bmatrix}, \\qquad \\mathbf{\\hat{y}} = \\begin{bmatrix} \\hat{y}_{1} \\\\ \\hat{y}_{2} \\\\ \\vdots \\\\ \\hat{y}_{n} \\\\ \\end{bmatrix}, \\qquad \\mathbf{b} = \\begin{bmatrix} b_{0} \\\\ b_{1} \\\\ \\end{bmatrix} \\] In the multidimensional case when we have \\(p&gt;1\\) predictors: \\[ \\mathbf{X} = \\ \\begin{bmatrix} 1 &amp; x_{11} &amp; \\dots &amp; x_{1p} \\\\ 1 &amp; x_{21} &amp; \\dots &amp; x_{2p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; x_{n1} &amp; \\dots &amp; x_{np} \\\\ \\end{bmatrix}, \\qquad \\mathbf{\\hat{y}} = \\begin{bmatrix} \\hat{y}_{1} \\\\ \\hat{y}_{2} \\\\ \\vdots \\\\ \\hat{y}_{n} \\\\ \\end{bmatrix}, \\qquad \\mathbf{b} = \\begin{bmatrix} b_{0} \\\\ b_{1} \\\\ \\vdots \\\\ b_{p} \\\\ \\end{bmatrix} \\] With the matrix of features, the response, and the coefficients we have a compact expression for the predicted outcomes: \\[ \\mathbf{\\hat{y}} = \\mathbf{Xb} \\] In path diagram form, the linear model looks like this: Figure 5.3: Linear combination with constant term If we assume that the predictors and the response are mean-centered, then we don’t have to worry about the constant term \\(\\mathbf{x_0}\\): Figure 5.4: Linear combination without constant term Obviously the question becomes: how do we obtain the vector of coefficients \\(\\mathbf{b}\\)? 5.4 The Error Measure We would like to get \\(\\hat{y}_i\\) to be “as close as” possible to \\(y_i\\). This requires to come up with some type of measure of closeness. Among the various functions that we can use to measure how close \\(\\hat{y}_i\\) and \\(y_i\\) are, the most common option is to use the squared distance between such values: \\[ d^2(y_i, \\hat{y}_i) = (y_i - \\hat{y}_i)^2 = (\\hat{y}_i - y_i)^2 \\tag{5.4} \\] Replacing \\(\\hat{y}_i\\) with \\(\\mathbf{b^\\mathsf{T}\\vec{x}_i}\\) we have: \\[ d^2(y_i, \\hat{y}_i) = (\\mathbf{b^\\mathsf{T}\\vec{x}_i} - y_i)^2 \\tag{5.5} \\] Notice that \\(d^2(y_i, \\hat{y}_i)\\) is a pointwise error measure that we can generally denote as \\(\\text{err}_i\\). But we also need to define a global measure of error. This is typically done by adding all the pointwise error measures \\(\\text{err}_{i}\\). There are two flavors of overall error measures based on squared pointwise differences: the sum of squared errors or \\(\\text{SSE}\\), and the mean squared error or \\(\\text{MSE}\\). The sum of squared errors, \\(\\text{SSE}\\), is defined as: \\[ \\text{SSE} = \\sum_{i=1}^{n} \\text{err}_i \\tag{5.6} \\] The mean squared error, \\(\\text{MSE}\\), is defined as: \\[ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} \\text{err}_i \\tag{5.7} \\] As you can tell, \\(\\text{SSE} = n \\text{MSE}\\), and viceversa, \\(\\text{MSE} = \\text{SSE} / n\\) Throughout this book, unless mentioned otherwise, when dealing with regression problems, we will consider the \\(\\text{MSE}\\) as the default overall error function to be minimized (you could also take \\(\\text{SSE}\\) instead). Let \\(e_i = (y_i - \\hat{y}_i) \\to e_i^2 = (y_i - \\hat{y}_i)^2 = \\text{err}_i\\). Doing some algebra, it’s easy to see that: \\[\\begin{align} \\text{MSE} &amp;= \\frac{1}{n} \\sum_{i=1}^{n} e_{i}^{2} \\\\ &amp;= \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2 \\\\ &amp;= \\frac{1}{n} \\sum_{i=1}^{n} (\\mathbf{b^\\mathsf{T}\\vec{x}_i} - y_i)^2 \\\\ &amp;= \\frac{1}{n} (\\mathbf{Xb} - \\mathbf{y})^\\mathsf{T} (\\mathbf{Xb} - \\mathbf{y}) \\\\ &amp;= \\frac{1}{n} \\| \\mathbf{Xb} - \\mathbf{y} \\|^2 \\\\ &amp;= \\frac{1}{n} \\| \\mathbf{\\hat{y}} - \\mathbf{y} \\|^2 \\tag{5.8} \\end{align}\\] As you can tell, the Mean Squared Error \\(\\text{MSE}\\) is proportional to the squared norm of the difference vector \\(\\mathbf{e} = \\mathbf{\\hat{y}} - \\mathbf{y}\\). \\[ \\text{MSE} = \\frac{1}{n} \\| \\mathbf{e} \\|^2 = \\frac{1}{n} (\\mathbf{\\hat{y}} - \\mathbf{y})^\\mathsf{T} (\\mathbf{\\hat{y}} - \\mathbf{y}) \\] 5.5 The Least Squares Algorithm In (ordinary) least squares regression, we want to minimize the mean of squared errors (\\(\\text{MSE}\\)). This minimization problem involves computing the derivative of \\(\\text{MSE}\\) with respect to \\(\\mathbf{b}\\). In other words, we compute the gradient of \\(\\text{MSE}\\), denoted \\(\\nabla \\text{MSE}(\\mathbf{b})\\), which is the vector of partial derivatives of \\(\\text{MSE}\\) with respecto to each parameter \\(b_0, b_1, \\dots, b_p\\): () &amp;= () \\ &amp;= ( - + ) \\tag{5.9} \\end{aligned*} which becomes: \\[ \\nabla \\text{MSE}(\\mathbf{b}) = \\frac{2}{n} \\mathbf{X^\\mathsf{T}Xb} - \\frac{2}{n} \\mathbf{X^\\mathsf{T}y} \\tag{5.10} \\] Equating to zero we get that: \\[ \\mathbf{X^\\mathsf{T}Xb} = \\mathbf{X^\\mathsf{T}y} \\quad (\\text{normal equations}) \\tag{5.11} \\] The above equation defines a system of equations that most authors refer to as the so-called Normal equations. It is a system of \\(n\\) equations with \\(p+1\\) unknowns. If the cross-product matrix \\(\\mathbf{X^\\mathsf{T}X}\\) is invertible, which is not a minor assumption, then the vector of regression coefficients \\(\\mathbf{b}\\) that we are looking for is given by: \\[ \\mathbf{b} = (\\mathbf{X^\\mathsf{T}X})^{-1} \\mathbf{X^\\mathsf{T}y} \\tag{5.12} \\] Having obtained \\(\\mathbf{b}\\), we can easily compute the response vector: \\[\\begin{align*} \\mathbf{\\hat{y}} &amp;= \\mathbf{Xb} \\\\ &amp;= \\mathbf{X} (\\mathbf{X^\\mathsf{T}X})^{-1} \\mathbf{X^\\mathsf{T} y} \\tag{5.13} \\end{align*}\\] If we denote \\(\\mathbf{H} = \\mathbf{X} (\\mathbf{X^\\mathsf{T}X})^{-1} \\mathbf{X^\\mathsf{T}}\\), then the predicted response is: \\[ \\mathbf{\\hat{y}} = \\mathbf{Hy} \\] This matrix \\(\\mathbf{H}\\) is better known as the hat matrix, because it puts the hat on the response. More importantly, the matrix \\(\\mathbf{H}\\) is an orthogonal projector. From linear algebra, orthogonal projectors have very interesting properties: they are symmetric they are idempotent their eigenvalues are either 0 or 1 5.6 Geometries of OLS Now that we’ve seen the algebra, it’s time to look at the geometric interpretation of all the action that is going on within linear regression via OLS. We will discuss three geometric perspectives: OLS from the individuals point of view (i.e. rows of the data matrix). OLS from the variables point of view (i.e. columns of the data matrix). OLS from the parameters point of view, and the error surface. 5.6.1 Rows Perspective This is probably the most popular perspective covered in most textbooks. For illustration purposes let’s assume that our data has just \\(p=1\\) predictor. In other words, we have the response \\(Y\\) and one predictor \\(X\\). We can depict individuals as points in this space: Figure 5.5: Scatterplot of individuals In linear regression, we want to predict \\(y_i\\) by linearly mixing the inputs \\(\\hat{y}_{i} = b_0 + b_1 x_i\\). In two dimensions, the fitted model corresponds to a line. In three dimensions it would correspond to a plane. And in higher dimensions this would correspond to a hyperplane. Figure 5.6: Scatterplot with regression line With a fitted line, we obtain predicted values \\(\\hat{y}_i\\). Some predicted values may be equal to the observed values. Other predicted values will be greater than the observed values. And some predicted values will be smaller than the observed values. Figure 5.7: Observed values and predicted values As you can imagine, given a set of data points, you can fit an infinite number of lines (in general). So which line are we looking for? We want to obtain the line that minimizes the square of the errors \\(e_i = \\hat{y}_{i} - y_{i}\\). In the figure below, these errors (also known as _residuals) are represented by the vertical difference between the observed values \\(y_i\\) and the predicted values \\(\\hat{y}_i\\). Figure 5.8: OLS focuses on minimizing the squared errors Combining all residuals, we want to obtain parameters \\(b_0, \\dots, b_p\\) that minimize the squared norm of the vector of residuals: \\[ \\sum_{i=1}^{n} e_{i}^{2} = \\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2 = \\sum_{i=1}^{n} (b_0 + b_1 x_i - y_i)^2 \\tag{5.14} \\] In vector-matrix form we have: \\[\\begin{align*} \\| \\mathbf{e} \\|^2 &amp;= \\| \\mathbf{\\hat{y}} - \\mathbf{y} \\|^2 \\\\ &amp;= \\| \\mathbf{Xb} - \\mathbf{y} \\|^2 \\\\ &amp;= (\\mathbf{Xb} - \\mathbf{y})^\\mathsf{T} (\\mathbf{Xb} - \\mathbf{y}) \\\\ &amp;\\propto \\text{MSE} \\tag{5.15} \\end{align*}\\] As you can tell, minimizing the squared norm of the vector of residuals, is equivalent to minimizing the mean squared error. 5.6.2 Columns Perspective We can also look at the geometry of OLS from the columns perspective. This is less common than the rows perspective, but still very enlightening. Imagine variables in an \\(n\\)-dimensional space, both the response and the predictors. In this space, the \\(X\\) variables will span some subspace \\(\\mathbb{S}_{X}\\). This subspace is not supposed to contain the response—unless \\(Y\\) happens to be a linear combination of \\(X_1, \\dots, X_p\\). Figure 5.9: Features and Response view What are we looking for? We’re looking for a linear combination \\(\\mathbf{Xb}\\) that gives us a good approximation to \\(\\mathbf{y}\\). As you can tell, there’s an infinite number of linear combinations that can be formed with \\(X_1, \\dots, X_p\\). Figure 5.10: Linear combination of features The mix of features that we are interested in, \\(\\mathbf{\\hat{y}} = \\mathbf{Xb}\\), is the one that gives us the closest approximation to \\(\\mathbf{y}\\). Figure 5.11: Linear combination to be as close as possible to response Now, what do we mean by closest approximation? How do we determine the closeness between \\(\\mathbf{\\hat{y}}\\) and \\(\\mathbf{y}\\)? By looking at the difference, which results in a vector \\(\\mathbf{e} = \\mathbf{\\hat{y}} - \\mathbf{y}\\). And then measuring the size or norm of this vector. Well, the squared norm to be precise. In other words, we want to obtain \\(\\mathbf{\\hat{y}}\\) such that the squared norm \\(\\| \\mathbf{e} \\|^2\\) is as small as possible. \\[ \\text{Minimize} \\quad \\| \\mathbf{e} \\|^2 = \\| \\mathbf{\\hat{y}} - \\mathbf{y} \\|^2 \\tag{5.16} \\] Minimizing the squared norm of \\(\\mathbf{e}\\) involves minimizing the mean squared error. 5.6.3 Parameters Perspective In addition to the two previously discussed perspectives (rows and columns), we could also visualize the regression problem from the point of view of the parameters \\(\\mathbf{b}\\) and the error surface from \\(\\text{MSE}\\). This is the least common perspective discussed in the literature that has to do with linear regression in general. However, it is not that uncommon within the Statistical Learning literature. For illustration purposes, assume that we have only two predictors \\(X_1\\) and \\(X_2\\). Recall that the Mean Squared Error (MSE) is: \\[ E(\\mathbf{y}, \\mathbf{\\hat{y}}) = \\frac{1}{n} \\left( \\mathbf{b^\\mathsf{T} X^\\mathsf{T}} \\mathbf{X b} - 2 \\mathbf{b^\\mathsf{T} X^\\mathsf{T} y} + \\mathbf{y^\\mathsf{T}y} \\right) \\tag{5.17} \\] Now, from the point of view of \\(\\mathbf{b} = (b_1, b_2)\\), we can classify the order of each term: \\[ E(\\mathbf{y}, \\mathbf{\\hat{y}}) = \\frac{1}{n} ( \\underbrace{\\mathbf{b^\\mathsf{T} X^\\mathsf{T}} \\mathbf{X b}}_{\\text{Quadratic Form}} - \\underbrace{ 2 \\mathbf{b^\\mathsf{T} X^\\mathsf{T} y}}_{\\text{Linear}} + \\underbrace{ \\mathbf{y^\\mathsf{T}y}}_{\\text{Constant}} ) \\tag{5.18} \\] Since \\(\\mathbf{X^\\mathsf{T} X}\\) is positive semidefinite, we know that \\(\\mathbf{b^\\mathsf{T} X^\\mathsf{T} Xb} \\geq 0\\). Furthermore, we know that (from vector calculus) it will be a paraboloid (bowl-shaped surface) in the \\((E, b_1, b_2)\\) space. The following diagram depicts this situation. Figure 5.12: Error Surface Imagine that we get horizontal slices of the error surface. For any of those slices, we can project them onto the plane spanned by the parameters \\(b_1\\) and \\(b_2\\). The resulting projections will be like a topographic map, with error contours on this plane. In general, those contours will be ellipses. Figure 5.13: Error Surface with slices, and their projections With quadratic error surfaces like this, they have a minimum value, and we are guaranteed the existence of \\(\\mathbf{b}^* = (b_1^{*}, b_2^{*})\\) s.t. \\(\\mathbf{b^\\mathsf{T} X^\\mathsf{T} X b}\\) is minimized. This is a powerful result! Consider, for example, a parabolic cylinder. Such a shape has no unique minimum; rather, it has an infinite number of points (all lying on a line) that minimize the function. The point being; with positive semi-definite matrices, we never have this latter case. Figure 5.14: Error Surface with contour errors and the minimum The minimum of the error surface occurs at the point \\((b_{1}^{*}, b_{2}^{*})\\). This is the precisely the OLS solution. "],
["gradient.html", "6 Gradient Descent 6.1 Error Surface 6.2 Idea of Gradient Descent 6.3 Moving Down an Error Surface 6.4 Gradient Descent and our Model", " 6 Gradient Descent Before moving to the next part of the book which deals with the theory of learning, we want to introduce a very popular optimization technique that is commonly used in many statistical learning methods: the famous gradient descent algorithm. 6.1 Error Surface Consider the overall error measure of a linear regression problem, for example the mean squared error (\\(\\text{MSE}\\))—or if you prefer the sum of squared errors, which is simply \\(\\text{SSE} = n \\text{MSE}\\)). \\[ E(\\mathbf{y}, \\mathbf{\\hat{y}}) = \\frac{1}{n} \\left( \\mathbf{b^\\mathsf{T} X^\\mathsf{T}} \\mathbf{X b} - 2 \\mathbf{b^\\mathsf{T} X^\\mathsf{T} y} + \\mathbf{y^\\mathsf{T}y} \\right) \\tag{6.1} \\] As we saw in the previous chapter, we can look at such error measure from the perspective of the parameters (i.e. the regression coefficients). From this perspective, we denote this error function as \\(E(\\mathbf{b})\\), making explicit its dependency on the vector of coefficients \\(\\mathbf{b}\\). \\[ E(\\mathbf{b}) = \\frac{1}{n} ( \\underbrace{\\mathbf{b^\\mathsf{T} X^\\mathsf{T}} \\mathbf{X b}}_{\\text{Quadratic Form}} - \\underbrace{ 2 \\mathbf{b^\\mathsf{T} X^\\mathsf{T} y}}_{\\text{Linear}} + \\underbrace{ \\mathbf{y^\\mathsf{T}y}}_{\\text{Constant}} ) \\tag{6.2} \\] As you can tell, \\(E(\\mathbf{b})\\) is a quadratic function with respect to \\(\\mathbf{b}\\). Moreover, \\(E(\\mathbf{b})\\) is a positive semidefinite quadratic form which implies that it is a convex function. What does this all mean? For illustration purposes, let’s consider again a linear regression with two inputs \\(X_1\\) and \\(X_2\\), and assume that there is no constant term. In this case, the error function \\(E(\\mathbf{b})\\) will generate a convex error surface with the shape of a bowl: Figure 6.1: Error Surface In general, with a convex error function, we know that there is a minimum, and the challenge is to find such value. Figure 6.2: Error Surface With OLS we can use direct methods to obtain the minimum. All we need to do is to compute the derivative of the error function \\(E(\\mathbf{b}\\)), set it equal to zero, and find this minimum point \\(\\mathbf{\\overset{*}{b}}\\). As you know, assuming that the matrix \\(\\mathbf{X^\\mathsf{T} X}\\) is invertible, the OLS minimum is easily calculated as: \\(\\mathbf{b} = (\\mathbf{X^\\mathsf{T}X})^{-1} \\mathbf{X^\\mathsf{T} y}\\). Interestingly, we can also use iterative methods to compute this minimum. The iterative method that we will discuss here is called gradient descent. 6.2 Idea of Gradient Descent The main idea of gradient descent is as follows: we start with an arbitrary point \\(\\mathbf{b}^{(0)} = ( b_1^{(0)}, b_2^{(0)} )\\) of model parameters, and we evaluate the error function at this point: \\(E(\\mathbf{b}^{(0)})\\). This gives us a location somewhere on the error surface. See figure below. Figure 6.3: Starting position on error surface We then get a new vector \\(\\mathbf{b}^{(1)}\\) in a way that we “move down” the surface to obtain a new position \\(E(\\mathbf{b}^{(1)})\\): Figure 6.4: Begin to move down on error surface We keep “moving down” the surface, obtaining at each step \\(s\\) subsequent new vectors \\(\\mathbf{b}^{(s)}\\) that result in an error \\(E(\\mathbf{b}^{(s)})\\) that is closer to the minimum of \\(E(\\mathbf{b})\\). Figure 6.5: Keep moving down on error surface Eventually, we should get very, very close to the minimizing point, and ideally, arrive at the minimum \\(\\mathbf{\\overset{*}{b}}\\). One important thing to always keep in mind when dealing with minimization (and maximization) problems is that, in practice, we don’t get to see the surface. Instead, we only have local information at the current point we evaluate the error function. Here’s a useful metaphor: Imagine that you are on the top a mountain (or some similar landscape) and your goal is to get to the bottom of the valley. The issue is that it is a foggy day (or night), and the visibility conditions are so poor that you only get to see/feel what is very near you (a couple of inches around you). What would you do to get to the bottom of the valley? Well, you start touching your surroundings trying to feel in which direction the slope of the terrain goes down. The key is to identify the direction in which the slope gives you the steepest descent. This is the conceptual idea behind optimization algorithms like gradient descent. 6.3 Moving Down an Error Surface What do we mean by “moving down the error surface”? Well, mathematically, this means we generate the new vector \\(\\mathbf{b}^{(1)}\\) from the initial point \\(\\mathbf{b}^{(0)}\\), using the following formula: \\[ \\mathbf{b}^{(1)} = \\mathbf{b}^{(0)} + \\alpha \\mathbf{v} \\tag{6.3} \\] As you can tell, in addition to the values of parameter vectors \\(\\mathbf{b}^{(0)}\\) and \\(\\mathbf{b}^{(1)}\\), we have two extra ingredients: \\(\\alpha\\) and \\(\\mathbf{v}\\). We call \\(\\alpha\\) the step size. Intuitively, \\(\\alpha\\) tells us how far down the surface we are moving. In turn, \\(\\mathbf{v}\\) is the vector indicating the direction in which we need to move. Because we are interested in this direction, we can simply consider \\(\\mathbf{v}\\) to be a unit vector. We will discuss how to find the direction of \\(\\mathbf{v}\\) in a little bit. Right now let’s just focus on generating new vectors in this manner: \\[\\begin{align*} \\tag{6.4} \\mathbf{b}^{(2)} &amp; = \\mathbf{b}^{(1)} + \\alpha \\mathbf{v}^{(1)} \\\\ \\mathbf{b}^{(3)} &amp; = \\mathbf{b}^{(2)} + \\alpha \\mathbf{v}^{(2)} \\\\ \\vdots &amp; \\hspace{10mm} \\vdots \\\\ \\mathbf{b}^{(s+1)} &amp; = \\mathbf{b}^{(s)} + \\alpha \\mathbf{v}^{(s)} \\end{align*}\\] Note that we are assuming a constant step size \\(\\alpha\\); that is, note that \\(\\alpha\\) remains the same at each iteration. We should say that there are more sophisticated versions of gradient descent that allow a variable step size, however we will not consider that case. As we can see in the series of figures above, the direction in which we travel will change at each step of the process. We will also see this mathematically in the next subsection. 6.3.1 The direction of \\(\\mathbf{v}\\) How do we find the direction of \\(\\mathbf{v}\\)? Consider the gradient of our error function. The gradient always points in the direction of steepest ascent (i.e. largest positive change). Hence, we want to travel in the exact opposite direction of the gradient. Let’s “prove” this mathematically. In terms of the error function itself, what does it mean for our vectors \\(\\mathbf{b}^{(s+1)}\\) to be “getting closer” to the minimizing point? Well, it means that the error at point \\(\\mathbf{b}^{(s+1)}\\) is less than the error at point \\(\\mathbf{b}^{(s)}\\). Hence, we examine \\(\\Delta E_{\\mathbf{b}}\\), the difference between the errors at these two points: \\[\\begin{align*} \\Delta E_{\\mathbf{b}} &amp; := E\\big( \\mathbf{b}^{(s + 1)} \\big) - E\\big( \\mathbf{b}^{(s)} \\big) \\\\ &amp; = E \\big( \\mathbf{b}^{(s)} + \\alpha \\mathbf{v} \\big) - E\\big( \\mathbf{b}^{(s)} \\big) \\tag{6.5} \\end{align*}\\] In order for our vector \\(\\mathbf{b}^{(s+1)}\\) to be closer to the minimizing point that \\(\\mathbf{b}^{(s)}\\), we want this quantity to be as negative as possible. To find the \\(\\mathbf{v}\\) that makes this true, we need to use a trick: get the Taylor series expansion of \\(E(\\mathbf{b}^{(s)} + \\alpha \\mathbf{v})\\). Doing so, we obtain: \\[\\begin{align*} \\Delta E_{\\mathbf{b}} &amp;= E\\big( \\mathbf{b}^{(s)} \\big) + \\nabla E\\big( \\mathbf{b}^{(s)} \\big)^{\\mathsf{T}} \\big(\\alpha \\mathbf{v} \\big) + O \\big( \\alpha^2 \\big) - E\\big( \\mathbf{b}^{(s)} \\big) \\\\ &amp; = \\alpha \\hspace{1mm} \\nabla E \\big( \\mathbf{b}^{(s)} \\big)^{\\mathsf{T}} \\mathbf{v} + O(\\alpha^2) \\tag{6.6} \\end{align*}\\] where \\(O(\\alpha^2)\\) denotes those terms of order 2 or greater in the series expansion. We will focus our attention on the series expansion of just one term, and ignore the higher order terms comprised by \\(O(\\alpha^2)\\). In other words, we will focus on the term: \\(\\alpha \\hspace{1mm} \\nabla E \\big( \\mathbf{b}^{(s)} \\big)^{\\mathsf{T}} \\mathbf{v}\\). Notice that this term involves the inner product between the gradient and the unit vector \\(\\mathbf{v}\\), that is: \\([ \\nabla E(\\mathbf{b}^{(s)})]^{\\mathsf{T}} \\mathbf{v}\\). For notational convenience, let us (temporarily) define \\(\\mathbf{u} : = \\nabla E ( \\mathbf{b}^{(s)} )\\). In this way, we need to examine the inner product: \\(\\mathbf{u^\\mathsf{T}v}\\). We can consider three prototypical cases with respect to the orientation of \\(\\mathbf{v}\\) and \\(\\mathbf{u}\\): either antiparallel, parallel, or orthogonal. Figure 6.6: Three prototypical cases When both vectors are parallel, \\(\\mathbf{u} \\parallel \\mathbf{v}\\) (i.e. the second case above), then \\(\\mathbf{u}^\\mathsf{T}\\mathbf{v} = \\| \\mathbf{u} \\|\\). Recall that we are assuming \\(\\mathbf{v}\\) to be a unit vector. When both vectors are antiparallel, \\(\\mathbf{u} \\not\\parallel \\mathbf{v}\\) (i.e. the first case above), then \\(\\mathbf{u}^\\mathsf{T} \\mathbf{v} = - \\| \\mathbf{u} \\|\\). When both vectors are orthogonal, \\(\\mathbf{u} \\perp \\mathbf{v}\\), then \\(\\mathbf{u}^\\mathsf{T} \\mathbf{v} = 0\\). Therefore, in any of the three cases, we have that \\[ \\mathbf{u}^\\mathsf{T} \\mathbf{v} \\geq - \\left\\| \\mathbf{u} \\right\\| \\tag{6.7} \\] This means that the least we can get is \\(- \\left\\| \\mathbf{u} \\right\\|\\), which occurs when \\(\\mathbf{v}\\) is in the opposite direction of \\(\\mathbf{u}\\). Hence, recalling that \\(\\mathbf{u} := \\nabla E(\\mathbf{b}^{(s)})\\), we can plug this result into our error computation to obtain: \\[\\begin{align*} \\Delta E_{\\mathbf{b}} &amp; := E\\big( \\mathbf{b}^{(s + 1)} \\big) - E\\big( \\mathbf{b}^{(s)} \\big) \\\\ &amp; = E \\big( \\mathbf{b}^{(s)} + \\alpha \\mathbf{v} \\big) - E\\big( \\mathbf{b}^{(s)} \\big) \\\\ &amp; = \\alpha \\hspace{1mm} \\nabla E \\big( \\mathbf{b}^{(s)} \\big)^{\\mathsf{T}} \\mathbf{v} + O(\\alpha^2) \\\\ &amp; \\geq - \\alpha \\left\\| \\nabla E\\big( \\mathbf{b}^{(s)} \\big) \\right\\| \\tag{6.8} \\end{align*}\\] Thus, to make \\(\\Delta E_{\\mathbf{b}}\\) as negative as possible, we should take \\(\\mathbf{v}\\) parallel to the opposite direction of the gradient. The Moral: We want the following: \\[ \\mathbf{v} = - \\frac{\\nabla E(\\mathbf{b}^{(s)}) }{\\left\\| \\nabla E(\\mathbf{b}^{(s)}) \\right\\| } \\tag{6.9} \\] which means we want to move in the direction opposite to that of the gradient. Keep in mind that we divided by the norm because we previously defined \\(\\mathbf{v}\\) to be a unit vector. This also reveals the meaning behind the name gradient descent; we are descending in the direction opposite to the gradient of the error function. 6.4 Gradient Descent and our Model Before we present the full algorithm for gradient descent in the context of regression, let us investigate the actual gradient further. Since we have a formula for \\(E(\\mathbf{b})\\), we can find a closed form for its gradient \\(\\nabla E(\\mathbf{b})\\): \\[\\begin{align*} E(\\mathbf{b}) &amp; = \\frac{1}{n} \\left( \\mathbf{y} - \\mathbf{X} \\mathbf{b} \\right)^{\\mathsf{T}} \\left( \\mathbf{y} - \\mathbf{X} \\mathbf{b} \\right) \\\\ \\\\ &amp; = \\frac{1}{n} \\left( \\mathbf{b}^\\mathsf{T} \\mathbf{X}^\\mathsf{T} \\mathbf{X} \\mathbf{b} - 2 \\mathbf{b}^\\mathsf{T} \\mathbf{X}^\\mathsf{T} \\mathbf{y} + \\mathbf{y}^\\mathsf{T} \\mathbf{y} \\right) \\\\ \\\\ \\nabla E(\\mathbf{b}) &amp; = \\frac{1}{n} \\left( 2 \\mathbf{X}^\\mathsf{T} \\mathbf{X} \\mathbf{b} - 2 \\mathbf{X}^\\mathsf{T} \\mathbf{y} \\right) \\tag{6.10} \\end{align*}\\] This was from the columns point of view; we can also find a different formula from the row’s perspective in pointwise notation: \\[\\begin{align*} E(\\mathbf{b}) &amp; = \\frac{1}{n} \\sum_{i = 1}^{n} \\left( y_i - \\mathbf{b}^\\mathsf{T} \\mathbf{x_i} \\right)^2 = \\frac{1}{n} \\sum_{i=1}^{n} \\left( y_i - \\hat{y}_i \\right) \\\\ &amp; = \\frac{1}{n} \\sum_{i=1}^{n} \\left( y_i - b_0 x_{i0} - b_1 x_{i1} - \\dots - b_{j} x_{ij} - \\dots - b_{p} x_{ip} \\right)^2 \\\\ \\frac{\\partial}{\\partial b_j} E(\\mathbf{b}) &amp; = - \\frac{2}{n} \\sum_{i=1}^{n} \\left( y_i - b_0 x_{i0} - b_1 x_{i1} - \\dots - b_{j} x_{ij} - \\dots - b_{p} x_{ip} \\right) x_{ij} \\\\ \\frac{\\partial}{\\partial b_j} E(\\mathbf{b}^{(0)}) &amp; = - \\frac{2}{n} \\sum_{i=1}^{n} \\left( y_i - b_0 x_{i0} - b_1 x_{i1} - \\dots - b_{j} x_{ij} - \\dots - b_{p} x_{ip} \\right) x_{ij} \\\\ &amp; = - \\frac{2}{n} \\sum_{i=1}^{n} \\left( y_i - \\left[ \\mathbf{b}^{(0)} \\right]^{\\mathsf{T}} \\mathbf{x_i} \\right) x_{ij} \\tag{6.11} \\end{align*}\\] This will be a better formula to use in our iterative algorithm, described below. Algorithm Initialize a vector \\(\\mathbf{b}^{(0)} = \\left( b_{0}^{(0)}, b_{1}^{(0)}, \\dots, b_{p}^{(0)} \\right)\\) Repeat the following over \\(s\\) (starting with \\(s = 0\\)), until convergence: \\(b_{j}^{(s+1)}:= b_{j}^{(s)} + \\alpha \\cdot \\frac{2}{n} \\sum_{i=1}^{n} \\left( y_i - \\left[\\mathbf{b}^{(s)}\\right]^{\\mathsf{T}} \\mathbf{x_i} \\right) x_{ij}\\) for all \\(j = 0, \\dots, p\\) simultaneously. In more compact notation, \\[ b_{j}^{(s+1)} = b_{j}^{(s)} + \\alpha \\cdot \\frac{\\partial }{\\partial b_j} E(\\mathbf{b}^{(s)}) \\] Store these elements in the vector \\(\\mathbf{b}^{(s+1)} = \\left( b_{0}^{(s+1)} , b_{1}^{(s+1)} , \\dots, b_{p}^{(s+1)} \\right)\\) When there is little change between \\(\\mathbf{b}^{(k + 1)}\\) and \\(\\mathbf{b}^{(k)}\\) (for some integer \\(k\\)), the algorithm will have converged and \\(\\mathbf{b}^{*} = \\mathbf{b}^{(k+1)}\\). Note that in the algorithm, we have dropped the condition that \\(\\mathbf{v}\\) is a unit vector. Indeed, if you look on the Wikipedia page for Gradient Descent, the algorithm listed there also omits the unit-length condition. "],
["learning.html", "7 Theoretical Framework 7.1 Mental Map 7.2 Kinds of Predictions 7.3 Two Types of Errors 7.4 Noisy Targets", " 7 Theoretical Framework Finally we have arrived to the part of the book in which we provide a framework for the theory of learning. Well, to be more precise, the framework is really about the theory of supervised learning. The purpose of this chapter is to give you a meantal map of the conceptual elements that are present in a supervised learning problem. Keep in mind that most of what is covered in this chapter is highly theoretical. It has to do with the concepts and principles that ideally we expect to find in a prediction task (e.g. regression, classification). Having said, we will also need to discuss what to do in practice in order to handle most of these theoretical elements (in the upcoming chapters). 7.1 Mental Map So far, we have seen an example of Unsupervised Learning (PCA), as well as one method of Supervised Learning (linear regression). Now, we begin discussing learning ideas at an abstract level. Let’s return to our example of predicting NBA players’ salaries. We have a series of inputs: height, weight, 2PTS, 3PTS, fouls, etc. From these inputs, we obtain an output: the salary of a player. We also have a target function \\(f : \\mathcal{X} \\to \\mathcal{Y}\\). (i.e. a function mapping from the column spaces of \\(\\mathbf{X}\\) to the output space \\(\\mathbf{y}\\)). Keep in mind that this function is an ideal, and remains unknown throughout our computations. Here’s a metaphor that we like to use. Pretend that the target function is some sort of mythical creature, like a unicorn (or your preferred creature). We are trying to find this elusive guy. More generally, we have a dataset \\(\\mathcal{D}: (\\mathbf{x_1}, y_1), ( \\mathbf{x_2}, y_2), \\dots, (\\mathbf{x_n}, y_n)\\), where \\(\\mathbf{x_i}\\) represents the vector of features for the \\(i\\)-th player/individual. From this data, we wish to obtain a hypothesis model \\(\\widehat{f}: \\mathcal{X} \\to \\mathcal{Y}\\) that is an approximation to the unknown function \\(f\\). We can sketch these basic ideas in a sort of “mental map”. We will refer to this picture as the “diagram for supervised learning”. Figure 7.1: Supervised Learning Diagram (ver 1) Here’s how to read this diagram. We are using orange clouds around those concepts that are more intangible than those appearing within rectangular or oval shapes. One of these clouds is the unknown target function \\(f\\), which as its name indicates it is unknown. This implies that we never really “discover” the function \\(f\\) in its entirety; rather, we just find a good enough approximation to it by estimating \\(\\widehat{f}\\). Now, as you can tell from the mental map, the other orange clound has to do with precisely this idea of good approximation: \\(\\widehat{f} \\approx f\\). It is also theoretical because of what we just said: that we don’t know \\(f\\). We should let you know that as we modify our diagram, we will encounter more orange concepts as well of highly theoretical nature. Now let’s turn our attention to the blue rectangular elements. One of them is the data set \\(\\mathcal{D}\\) which is influenced by the unknown target function. The other blue rectangle has to do with a set of candidate models \\(\\mathcal{H}\\), which is sometimes referred to as the hypothesis set. Both the data set and the set of models are tangible ingredients. Moreover, the set of candidate models is totally under out control. We get to decide what type of models we want try out (e.g. linear model, polynomial models, non-parametric models). Then we go to the yellow oval shape which right now is simply labeled as the learning algorithm \\(\\mathcal{A}\\). This corresponds to the set of instructions and steps to be carried out when learning from data. It is also the stage of the diagram in which most computations take place. Finally, we arrive at the yellow rectangle containing the final model \\(\\widehat{f}\\). This is supposed to be the selected model by the learning algorithm from the set of hypothesis models. Ideally, this model is the one that provides a good approximation for the target function \\(f\\). Going back to the holy grail of supervised learning, our goal is to find a model \\(\\widehat{f}\\) that gives “good” or accurate predictions. Before discussing what exactly do we mean by “accurate”, we first need to talk about predictions. 7.2 Kinds of Predictions What is the ultimate goal in supervised learning? Quick answer, we want a “good” model. What does “good” model mean? Simply put, it means that we want to estimate an unknown model \\(f\\) with some model \\(\\widehat{f}\\) that gives “good” predictions. What do we mean by “good” predictions? Loosely speaking, it means that we want to obtain “accurate” predictions. Before clarifying the notion of accurate predictions, let’s discuss first the concept of predictions. Think of a simple linear regression model (e.g. with one predictor). Having a fitted model \\(\\widehat{f}(x)\\), we can use it to make two types of predictions. On one hand, for an observed point \\(x_i\\), we can compute \\(\\hat{y}_i = \\widehat{f}(x_i)\\). By observed point we mean that \\(x_i\\) was part of the data used to find \\(\\widehat{f}\\). On the other hand, we can also compute \\(\\hat{y}_0 = \\widehat{f}(x_0)\\) for a point \\(x_0\\) what was not part of the data used when deriving \\(\\widehat{f}\\). 7.2.1 Two Types of Data The two distinct types of predictions involve two slightly different kinds of data. The data points \\(x_i\\) that we use to fit a model is what we call training or learning data. The data points \\(x_0\\) that we use to assess the performance of a model are points NOT supposed to be part of the training set. This implies that, at least in theory, we need two kinds of data sets: In-sample data, denoted \\(\\mathcal{D}_{in}\\), used to fit a model Out-of-sample data, denoted \\(\\mathcal{D}_{out}\\), used to measure the predictive quality of a model 7.2.2 Two Types of Predictions Given the two kinds of data points, we have two types of predictions: predictions \\(\\hat{y}_i\\) of observed/seen values \\(x_i\\) predicitons \\(\\hat{y}_0\\) of unobserved/unseed values \\(x_0\\) Each type of prediction is associated with a certain behavioral feature of a model. The predictions of observed data, \\(\\hat{y}_i\\), have to do with the memorizing aspect (apparent error, resubstitution error). The predictions of unobserved data, \\(\\hat{y}_0\\), have to do with the generalization aspect (generalization error, prediction error). Both kinds of predictions are important, and each of them is interesting in its own right. However, from the supervised learning standpoint, it is the second type of predictions that we are ultimately interested in. That is, we want to find models that are able to give predictions \\(\\hat{y}_0\\) as accurate as possible for the real value \\(y_0\\). Don’t get us wrong. Having good predictions \\(\\hat{y}_i\\) of observed values is important and desirable. And to a large extent, it is a necessary condition for a good model. However, it is not a sufficient condition. It is not enough to fit the observed data well, in order to get a good predictive model. Sometimes, you can perfectly fit the observed data, but have a terrible performance for unobserved values \\(x_0\\). 7.3 Two Types of Errors In theory, we are dealing with two types of predictions, each of which is associated to certain types of data points. Because we are interested in obtaining models that give accurate predictions, we need a way to measure the accuracy of such predictions. At the conceptual level we need some mechanism to quantify how different the fitted model is from the target function \\(f\\): \\[ \\widehat{f} \\text{ -vs- } f \\] It would be nice to have some measure of how much discrepancy exists between the estimated model and the target model. This means that we need a function that summarizes, somehow, the total amount of error. We will denote such term as an Overall Measure of Error: \\[ \\text{Overall Measure of Error:} \\quad E(\\widehat{f},f) \\] The typical way in which an overall measure of error is defined is in terms of individual or pointwise errors \\(err_i(\\hat{y}_i, y_i)\\) that quantify the difference between an observed value \\(y_i\\) and its predicted value \\(\\hat{y}_i\\). As a matter of fact, most overall errors focus on the addition of the pointwise errors: \\[ E(\\widehat{f},f) = \\text{measure} \\left( \\sum err_i(\\hat{y}_i, y_i) \\right ) \\] Unless otherwise said, in this book we will use the mean sum of errors as the default overall error measure: \\[ E(\\widehat{f},f) = \\frac{1}{n} \\left( \\sum_i err_i (\\hat{y}_i, y_i) \\right) \\] 7.3.1 Individual Errors What form does the individual error function, \\(err()\\), take? In theory, they can take any form you want. This means that you can invent your own individual error function. However, the most common ones are: squared error: \\(\\quad err(\\widehat{f}, f) = \\left( \\hat{y}_i - y_i \\right)^2\\) absolute error: \\(\\quad err(\\widehat{f}, f) = \\left| \\hat{y}_i - y_i \\right|\\) misclassification error: \\(\\quad err(\\widehat{f}, f) = [\\![ \\hat{y}_i \\neq y_i ]\\!]\\) In the machine learning literature, these individual errors are formally known as loss functions. 7.3.2 Overall Errors As you can imagine, there are actually two types of overall error measures, based on the type of data that is used to assess the individual errors: In-sample Error, denoted \\(E_{in}\\) Out-of-sample Error, denoted \\(E_{out}\\) The in-sample error is the average of pointwise errors from data points of the in-sample data \\(\\mathcal{D}_{in}\\): \\[ E_{in} (\\widehat{f}, f) = \\frac{1}{n} \\sum_{i} err_i \\] The out-of-sample error is the theoretical mean, or expected value, of the pointwise errors over the entire input space: \\[ E_{out} (\\widehat{f}, f) = \\mathbb{E}_{\\mathcal{X}} \\left[ err \\left( \\widehat{f}(x), f(x) \\right) \\right] \\] The point \\(x\\) denotes a general data point in the input space \\(\\mathcal{X}\\). And as we said, the expectation is taken over the input space \\(\\mathcal{X}\\). Which means that the nature of \\(E_{out}\\) is highly theoretical. In practice, you will never, never, be able to compute this quantity. In the machine learning literature, these overall measures of error tend to be formally known as cost functions or risks. Let’s update our supervised learning diagram to include error measures (see figure below). We add a new box (in blue) involving an overall error measure as well as some pointwise error function. Figure 7.2: Supervised Learning Diagram (ver 2) Notice the connections of the error elements to both the learning algortihm \\(\\mathcal{A}\\), and the final model \\(\\hat{f}\\). Why is this? As we will learn in the upcoming chapters, learning algorithms use—implicitly or explicitly—a pointwise error function. In turn, in order to determine which candidate model \\(h()\\) is the best approximation to the target model \\(f()\\), we need to use an overall measure of error. 7.3.3 Auxiliary Technicality We need to assume some probability distribution \\(P\\) on \\(\\mathcal{X}\\). That is, we assume our vectors \\(\\mathbf{x_1}, \\dots, \\mathbf{x_n}\\) are independent identically distributed (i.i.d.) samples from this distribution \\(P\\). (Exactly what distribution you pick - normal, chi-squared, \\(t\\), etc. - is, for the moment, irrelevant). Recall that out-of-sample data is highly theoretical; we will never be able to obtain it in its entirety. The best we can do is obtain a subset of the out-of-sample data (the test data), and estimate the rest of the data. Our imposition of a distributional structure on \\(\\mathcal{X}\\) enables us to link the in-sample error with the out-of-sample data. Recall that our ultimate goal is to get a good function \\(\\widehat{f} \\approx f\\). What do we mean by the symbol “\\(\\approx\\)”? Technically speaking, we want \\(E_{\\mathrm{out}}(\\widehat{f}) \\approx 0\\). If this is the case, we can safely say that our model has been successfully trained. However, we can never check if this is the case, since we don’t have access to \\(E_{\\mathrm{out}}\\). To solve this, we break our goal into two sub-goals: \\[ E_{\\mathrm{out}} (\\widehat{f}) \\approx 0 \\ \\Rightarrow \\begin{cases} E_{\\mathrm{in}}(\\widehat{f}) \\approx 0 &amp; \\text{practical result} \\\\ E_{\\mathrm{out}}(\\widehat{f}) \\approx E_{\\mathrm{in}}(\\widehat{f}) &amp; \\text{technical/theoretical result} \\\\ \\end{cases} \\] The first condition is easy to check. How do we check the second? We check the second condition by invoking our distributional assumption \\(P\\) on \\(\\mathcal{X}\\). Using our assumption, we can cite various theorems to assert that the second result indeed holds true. We will later find ways to estimate \\(E_{\\mathrm{out}}(\\widehat{f})\\). Figure 7.3: Supervised Learning Diagram (ver 3) 7.4 Noisy Targets In practice, our function won’t necessarily be a nice (or smooth) function. Rather, there will be some noise. Hence, instead of saying \\(y = f(x)\\) where \\(f : \\mathcal{X} \\to \\mathcal{Y}\\), a better statement might be something like \\(y = f(x) + \\varepsilon\\). But even this notation has some flaws; for example, we could have multiple inputs mapping to the same output (which cannot happen if \\(f\\) is a proper “function”). That is, we may have two individuals with the exact same inputs \\(\\mathbf{x_A} = \\mathbf{x_B}\\) but with different response variables \\(y_A \\neq y_B\\). Instead, it makes more sense to consider some target distribution \\(P(y \\mid x)\\). In this way, we can think of our data as forming a joint probability distribution \\(P(\\mathbf{x}, y)\\). That is because \\(P(\\mathbf{x}, y) = P(\\mathbf{x}) P(y \\mid \\mathbf{x})\\). Figure 7.4: Supervised Learning Diagram (ver 4) In supervised learning, we want to learn the conditional distribution \\(P(y \\mid \\mathbf{x})\\). Again, we can think of this probability as \\(y = f() + \\text{noise}\\). Also, sometimes the Hypothesis Sets and Learning Algorithm boxes are combined into one, called the Learning Model. "],
["mse.html", "8 MSE of Estimator 8.1 MSE of an Estimator", " 8 MSE of Estimator In this chapter we provide a preliminary review of the Mean Squared Error (MSE) of an estimator. This will allow us to have a more gentle introduction to the next chapter about the famous Bias-Variance tradeoff. 8.1 MSE of an Estimator In order to discuss the bias-variance decomposition of a regression function and its expected MSE, we would like to first review the concept of the mean squared error of an estimator. Recall that estimation consists of providing an approximate value to the parameter of a population, using a (random) sample of observations drawn from such population. Say we have a population of \\(n\\) objects and we are interested in describing them with some numeric characteristic \\(\\theta\\). For example, our population may be formed by all students in some college, and we want to know their average height. We call this (theoretical) average the parameter. Figure 8.1: Population described by some parameter of interest. To estimate the value of the parameter, we may draw a random sample of \\(m &lt; n\\) students from the population and compute a statistic \\(\\hat{\\theta}\\). Ideally, we would use some statistic \\(\\hat{\\theta}\\) that approximates well the parameter \\(\\theta\\). Figure 8.2: Random sample from a population In practice, this is the typical process that you would carry out: Get a random sample from a population. Use the limited amount of data in the sample to estimate \\(\\theta\\) using some formula to compute \\(\\hat{\\theta}\\). Make a statement about how reliable of an estimator \\(\\hat{\\theta}\\) is. Now, for illustration purposes, let’s do the following mental experiment. Pretend that you can draw multiple random samples, all of the same size \\(m\\), from the population. In fact, you should pretend that you can get an infinite number of samples. And suppose that for each sample you will compute a statistic \\(\\hat{\\theta}\\). A first random sample of size \\(m\\) would result in \\(\\hat{\\theta}_1\\). A second random sample of size \\(m\\) would result in \\(\\hat{\\theta}_2\\). And so on. Figure 8.3: Various random samples of equal size and their statistics A couple of important things to notice: An estimator is a random variable A first sample will result in \\(\\hat{\\theta}_1\\) A second sample will result in \\(\\hat{\\theta}_2\\) A third sample will result in \\(\\hat{\\theta}_3\\) and so on … Some samples will yield a \\(\\hat{\\theta}_k\\) that overestimates \\(\\theta\\) Other samples will yield a \\(\\hat{\\theta}_k\\) that underestimates \\(\\theta\\) Some samples will yield a \\(\\hat{\\theta}_k\\) matching \\(\\theta\\) In theory, we could get a very large number of samples, and visualize the distribution of \\(\\hat{\\theta}\\), like in the figure below: Figure 8.4: Distribution of an estimator As you would expect, some estimators will be close to the parameter \\(\\theta\\), while others not so much. Under general assumptions, we can also assume that the estimator has expected value \\(\\mathbb{E}(\\hat{\\theta})\\), with finite variance \\(var(\\hat{\\theta})\\). Figure 8.5: Distribution of an estimator An interesting question to consider is: In general, how much different—or similar—is \\(\\hat{\\theta}\\) from \\(\\theta\\)? To be more concrete: on average, how close we expect the estimator to be from the parameter? To answer this question we can look for a measure to assess the typical distance of estimators from the parameter. This involves looking at the difference: \\(\\hat{\\theta} - \\theta\\), which is commonly referred to as the estimation error: \\[ \\text{estimation error} = \\hat{\\theta} - \\theta \\] We would like to measure the “size” of such difference. Notice that the estimation error is also a random variable: A first sample will result in an error \\(\\hat{\\theta}_1 - \\theta\\) A second sample will result in an error \\(\\hat{\\theta}_2 - \\theta\\) A third sample will result in an error \\(\\hat{\\theta}_3 - \\theta\\) and so on … So how do we measure the “size” of the estimation errors? The typical way to quantify the amount of estimation error is by calculating the squared errors, and then averaging over all the possible values of the estimators. This is known as the Mean Squared Error (MSE) of \\(\\hat{\\theta}\\): \\[ \\text{MSE}(\\hat{\\theta}) = \\mathbb{E} [(\\hat{\\theta} - \\theta)^2] \\] MSE is the squared distance from our estimator \\(\\hat{\\theta}\\) to the true value \\(\\theta\\), averaged over all possible samples. It is convenient to regard the estimation error, \\(\\hat{\\theta} - \\theta\\), with respect to \\(\\mathbb{E}(\\hat{\\theta})\\). In other words, the distance between \\(\\hat{\\theta}\\) and \\(\\theta\\) can be expressed with respect to the expected value \\(\\mathbb{E}(\\hat{\\theta})\\): Figure 8.6: Estimator, its mean, and the parameter Let’s rewrite \\((\\hat{\\theta} - \\theta)^2\\) as \\(( \\hat{\\theta} - \\mathbb{E}(\\hat{\\theta}) + \\mathbb{E}(\\hat{\\theta}) - \\theta)^2\\), and let \\(\\mathbb{E}(\\hat{\\theta}) = \\mu_{\\hat{\\theta}}\\). Then: \\[\\begin{align*} (\\hat{\\theta} - \\theta)^2 &amp;= \\left ( \\hat{\\theta} - \\mathbb{E}(\\hat{\\theta}) + \\mathbb{E}(\\hat{\\theta}) - \\theta \\right )^2 \\\\ &amp;= ( \\hat{\\theta} - \\mu_{\\hat{\\theta}} + \\mu_{\\hat{\\theta}} - \\theta )^2 \\\\ &amp;= (\\underbrace{\\hat{\\theta} - \\mu_{\\hat{\\theta}}}_{a} + \\underbrace{\\mu_{\\hat{\\theta}} - \\theta}_{b})^2 \\\\ &amp;= a^2 + b^2 + 2ab \\\\ \\Longrightarrow \\mathbb{E} \\left [ (\\hat{\\theta} - \\theta)^2 \\right ] &amp;= \\mathbb{E}[a^2 + b^2 + 2ab] \\end{align*}\\] We have that \\(\\text{MSE}(\\hat{\\theta}) = \\mathbb{E} [(\\hat{\\theta} - \\theta)^2]\\) can be decomposed as: \\[\\begin{align*} \\mathbb{E} \\left [ (\\hat{\\theta} - \\theta)^2 \\right ] &amp;= \\mathbb{E}[a^2 + b^2 + 2ab] \\\\ &amp;= \\mathbb{E}(a^2) + \\mathbb{E}(b^2) + 2\\mathbb{E}(ab) \\\\ &amp;= \\mathbb{E} [ (\\hat{\\theta} - \\mu_{\\hat{\\theta}})^2 ] + \\mathbb{E} [ (\\mu_{\\hat{\\theta}} - \\theta)^2 ] + 2\\mathbb{E}(ab) \\end{align*}\\] Notice that \\(\\mathbb{E}(ab)\\): \\[ \\mathbb{E}(ab) = \\mathbb{E}[ (\\hat{\\theta} - \\mu_{\\hat{\\theta}}) (\\mu_{\\hat{\\theta}} - \\theta) ] = 0 \\] Consequently \\[\\begin{align*} \\text{MSE}(\\hat{\\theta}) &amp;= \\mathbb{E} \\left [ (\\hat{\\theta} - \\theta)^2 \\right ] \\\\ &amp; \\\\ &amp;= \\mathbb{E} [ (\\hat{\\theta} - \\mu_{\\hat{\\theta}})^2 ] + \\mathbb{E} [ (\\mu_{\\hat{\\theta}} - \\theta)^2 ] \\\\ &amp; \\\\ &amp;= \\mathbb{E} [(\\hat{\\theta} - \\mu_{\\hat{\\theta}})^2] + \\mathbb{E} [ (\\mu_{\\hat{\\theta}} - \\theta) ]^2 \\\\ &amp; \\\\ &amp;= \\underbrace{\\mathbb{E} [(\\hat{\\theta} - \\mu_{\\hat{\\theta}})^2]}_{\\text{Variance}} + (\\underbrace{\\mu_{\\hat{\\theta}} - \\theta}_{\\text{Bias}})^2 \\\\ &amp; \\\\ &amp;= \\text{Var}(\\hat{\\theta}) + \\text{Bias}^{2} (\\hat{\\theta}) \\end{align*}\\] The MSE of an estimator can be decomposed in terms of Bias and Variance. Bias, \\(\\mu_{\\hat{\\theta}} - \\theta\\), is the tendency of \\(\\hat{\\theta}\\) to overestimate or underestimate \\(\\theta\\) over all possible samples. Variance, \\(\\text{Var}(\\hat{\\theta})\\), simply measures the average variability of the estimators around their mean \\(\\mathbb{E}(\\hat{\\theta})\\). In summary, the MSE of an estimator can be simply described as a sum of a term measuring how far off the estimator is “on average” from its expected value, and a term measuring the variability of the estimator. 8.1.1 Prototypical Cases of Bias and Variance Depending on the type of estimator \\(\\hat{\\theta}\\), and the sample size \\(m\\), we can get statistics having different behaviors. The following diagram illustrates four classic scenarios contrasting low and high values for both the bias and the variance. Figure 8.7: Prototypical scenarios for Bias-Variance "],
["biasvar.html", "9 Bias-Variance Tradeoff 9.1 Introduction 9.2 Motivation Example 9.3 Learning from two points 9.4 Bias-Variance Derivation 9.5 The Tradeoff", " 9 Bias-Variance Tradeoff In this chapter we discuss one of the theoretical dogmas of statistical learning: the famous Bias-Variance tradeoff. Because Bias-Variance (BV) depends on Mean Squared Error (MSE), it is a topic that is typically discussed within the confines of regression, although a generalization is possible to classification problems (but the math is not as clean and straightforward as in regression). Even though we only discuss Bias-Variance from a regression perspective, keep in mind that the practical implications of the bias-variance tradeoff are applicable to all supervised learning contexts. 9.1 Introduction From the theoretical framework of supervised learning, the main goal is to find a model \\(\\hat{f}\\) that is a good approximation to the target model \\(f\\), or put it compactly, we want \\(\\hat{f} \\approx f\\). By good approximation we mean finding a model \\(\\hat{f}\\) that gives “good” predictions of both types of data points: predictions of in-sample data \\(\\hat{y}_i = \\hat{f}(x_i)\\), and predictions of out-of-sample data \\(\\hat{y}_0 = \\hat{f}(x_0)\\). In turn, these two types of predictions involve two types of errors: in-sample error: \\(E_{in}(\\hat{f})\\) out-of-sample error: \\(E_{out}(\\hat{f})\\) Consequently, our desire to have \\(\\hat{f} \\approx f\\) can be broken down into two separate wishes that are supposed to be fulfilled simultaneously: small in-sample error: \\(E_{in}(\\hat{f}) \\approx 0\\) out-of-sample error similar to in-sample error: \\(E_{out}(\\hat{f}) \\approx E_{in}(\\hat{f})\\) Achieving both wishes essentially means that \\(\\hat{f}\\) is a good model. Now, before discussing how to get \\(E_{out}(\\hat{f}) \\approx E_{in}(\\hat{f})\\), we are going to first study some aspects about \\(E_{out}(\\hat{f})\\). More specifically, we are going to study the theoretical behavior of \\(E_{out}(\\hat{f})\\) from a regression perspective, and taking the form of Mean Squared Error (MSE). 9.2 Motivation Example Let’s start with a toy example. Consider a noiseless target function given by \\(f(x) = sin(\\pi x)\\), with the input variable \\(x\\) in the interval \\([-1,1]\\), like in the following picture: Figure 9.1: Target function 9.2.1 Two Hypotheses Let’s assume a learning scenario in which, given a data set of \\(n\\) points, we fit the data using one of two models \\(\\mathcal{H}_0\\) and \\(\\mathcal{H}_1\\) (see the idealized figure shown below): \\(\\mathcal{H}_0\\): Set of all lines of the form \\(h(x) = b\\) \\(\\mathcal{H}_1\\): Set of all lines of the form \\(h(x) = b_0 + b_1 x\\) Figure 9.2: Two Learning hypothesis models 9.3 Learning from two points In this case study, we will assume a tiny data set of size \\(n = 2\\). That is, we sample \\(x\\) uniformly in \\([-1,1]\\) to generate a data set of two points \\((x_1, y_1), (x_2, y_2)\\); and fit the data using the two models \\(\\mathcal{H}_0\\) and \\(\\mathcal{H}_1\\). For \\(\\mathcal{H}_0\\), we choose the constant hypothesis that best fits the data: the horizontal line at the midpoint \\(b = \\frac{y_1 + y_2}{2}\\). For \\(\\mathcal{H}_1\\), we choose the line that passes through the two data points \\((x_1, y_1)\\) and \\((x_2, y_2)\\). Here’s an example in R of two \\(x\\)-points randomly sampled from a uniform distribution in the interval \\([-1,1]\\), and their corresponding \\(y\\)-points: \\(p_1(x_1, y_1) = (0.0949158, 0.2937874)\\) \\(p_2(x_2, y_2) = (0.4880941, 0.9993006)\\) With the given points above, the two fitted models are: \\(h_0(x) = 0.646544\\) \\(h_1(x) = 0.123472 + 1.794385 \\hspace{1mm} x\\) As you can tell, the fitted lines are very different. As expected, \\(h_0\\) is a constant line (with zero slope), while \\(h_1\\) has a steep positive slope. Obviously the fitted lines depend on which two points are sampled. As you can imagine, a new sample of two points \\((x_1, y_1)\\) and \\((x_2, y_2)\\) would provide different fitted models \\(h_0\\) and \\(h_1\\). So let’s see what could happen if we actually repeat this sampling process multiple times. Small Simulation Here’s a simulation of the sampling process described above (e.g. repeat it 500 times): we randomly sample two points in the interval \\([-1,1]\\), and fit both models \\(h_0\\) and \\(h_1\\). The figures which follow show the resulting 500 fits on the same (random) data sets for both methods. We are also displaying the average hypotheses \\(\\bar{h}_0\\) and \\(\\bar{h}_1\\) (colored in red), in each case. Look at the plot of \\(\\mathcal{H}_0\\) models (the constant lines). If we average all 500 fitted models, we get \\(\\bar{h}_0\\) which roughly corresponds to the horizontal line at \\(y = 0\\) (i.e. the red horizontal line). Generally speaking, all the individual fitted lines have the same slope of the average hypothesis, but different \\(y\\)-intercept values. We say that the class of \\(\\mathcal{H}_0\\) models have “low” variance, and “high” bias. What about the plot of \\(\\mathcal{H}_1\\) models? Averaging all 500 fitted models, we get \\(\\bar{h}_1\\) which roughly corresponds to the red line with positive slope. Generally speaking, the individual fitted lines have all sorts of slopes from extremely negative, to zero or close to zero, to extremely positive. This reflects the fact that there is a substantial amount of variability between the average profile \\(\\bar{h}_1\\) and the form of any single fit \\(h_1\\). However, the fact that the average hypothesis has positive slope, tells us that the majority of fitted lines also have positive slope. Moreover, the average hypothesis somewhat matches the overall trend of the target function \\(f()\\) around its middle section (range of \\(x \\in [-0.5, 0.5]\\)). We can summarize all of this by saying that the class of \\(\\mathcal{H}_1\\) models have “high” variance, and “low” bias. We hope that this simulation example gives you a good starting point to motivate the discussion of the bias-variance tradeoff. But we know it is not enough, and it does not prove anything mathematically. So let’s go ahead and fully disect this mythical bias-variance concept. 9.4 Bias-Variance Derivation From the previous chapter, we saw that the mean squared error (\\(\\text{MSE}\\)) of an estimator \\(\\hat{\\theta}\\) can be decomposed in terms of bias and variance as: \\[ \\text{MSE}(\\hat{\\theta}) = \\underbrace{\\mathbb{E} [(\\hat{\\theta} - \\mu_{\\hat{\\theta}})^2]}_{\\text{Variance}} + (\\underbrace{\\mu_{\\hat{\\theta}} - \\theta}_{\\text{Bias}})^2 \\] with \\(\\mu_{\\hat{\\theta}} = \\mathbb{E}(\\hat{\\theta})\\). In words: the bias, \\(\\mu_{\\hat{\\theta}} - \\theta\\), is the tendency of \\(\\hat{\\theta}\\) to overestimate or underestimate \\(\\theta\\) over all possible samples. the variance, \\(\\text{Var}(\\hat{\\theta})\\), simply measures the average variability of the estimators around their mean \\(\\mathbb{E}(\\hat{\\theta})\\). In this chapter, though, we are not talking about any generic estimator \\(\\hat{\\theta}\\). In this chapter we are discussing things within the confines of regression. More specifically, the estimator we are dealing with is \\(\\hat{f}()\\), our approximation of a target function \\(f()\\). Because of this very particular point of view, we need to study the BV decomposition in more detail. 9.4.1 Out-of-Sample Predictions In order to talk about the mean squared error \\(\\text{MSE}\\) as a theoretical expected value—as opposed to an empirical average—we have to suppose the existance of an out-of-sample data point \\(x_0\\). Given a learning data set \\(\\mathcal{D}\\) of \\(n\\) points, and a hypothesis \\(h(x)\\), the expectation of the Squared Error for a given out-of-sample point \\(x_0\\), over all possible learning sets, is expressed as: \\[ \\mathbb{E}_{\\mathcal{D}} \\left [ \\left( h^{(\\mathcal{D})}(x_0) - f(x_0) \\right)^2 \\right ] \\] For the sake of simplicity, assume the target \\(f(x)\\) is noiseless. Of course this is an idealistic assumption but it will help us to simplify notation. Here, \\(h^{(\\mathcal{D})}(x_0)\\) denotes the value predicted by model \\(h()\\) fitted on a specific learning data set \\(\\mathcal{D}\\). This is nothing but an estimator, and we can treat \\(h^{(\\mathcal{D})}(x_0)\\) as playing the role of \\(\\hat{\\theta}\\). Likewise, \\(f(x_0)\\) plays the role of \\(\\theta\\). What about the term that corresponds to \\(\\mu_{\\hat{\\theta}} = \\mathbb{E}(\\hat{\\theta})\\)? To answer this question we need to introduce \\(\\bar{h} := \\mathbb{E}_{\\mathcal{D}} \\left[ h^{(\\mathcal{D})}(x_0) \\right]\\); simply put, think of this term as the “average hypothesis”. Now that we have the names and symbols for all the ingredients, let’s do the algebra to find out what the expected squared error for a given out-of-sample point \\(x_0\\), over all possible learning sets, turns out to be: \\[\\begin{align*} \\mathbb{E}_{\\mathcal{D}} \\left [ \\left( h^{(\\mathcal{D})}(x_0) - f(x_0) \\right)^2 \\right ] &amp;= \\mathbb{E}_{\\mathcal{D}} \\left [ \\left (h^{(\\mathcal{D})}(x_0) - \\bar{h} + \\bar{h} - f(x_0) \\right)^2 \\right ] \\\\ &amp;= \\mathbb{E}_{\\mathcal{D}} \\Big [ \\Big (\\underbrace{h^{(\\mathcal{D})}(x_0) - \\bar{h}}_{a} + \\underbrace{\\bar{h} - f(x_0)}_{b} \\Big)^2 \\Big ] \\\\ &amp;= \\mathbb{E}_{\\mathcal{D}} \\left [ (a + b)^2 \\right ] \\\\ &amp;= \\mathbb{E}_{\\mathcal{D}} \\left [ a^2 + 2ab + b^2 \\right ] \\\\ &amp;= \\mathbb{E}_{\\mathcal{D}} [a^2] + \\mathbb{E}_{\\mathcal{D}} [b^2] + \\mathbb{E}_{\\mathcal{D}} [2ab] \\\\ \\end{align*}\\] Let’s examine the first two terms: \\(\\mathbb{E}_{\\mathcal{D}} [a^2] = \\mathbb{E}_{\\mathcal{D}} \\left [ \\left (h^{(\\mathcal{D})}(x_0) - \\bar{h} \\right)^2 \\right ] = \\text{variance} \\left( h(x_0) \\right)\\) \\(\\mathbb{E}_{\\mathcal{D}} [b^2] = \\mathbb{E}_{\\mathcal{D}} \\left [ \\left (\\bar{h} - f(x_0) \\right)^2 \\right ] = \\text{Bias}^2 \\left( h(x_0) \\right)\\) Now, what about the cross-term: \\(\\mathbb{E}_{\\mathcal{D}} [2ab]\\)? \\[\\begin{align*} \\mathbb{E}_{\\mathcal{D}} \\left[ 2 \\left( h^{(\\mathcal{D})}(x_0) - \\bar{h} \\right) \\left( \\bar{h} - f(x_0) \\right) \\right] &amp; \\propto \\mathbb{E}_{\\mathcal{D}} \\left[ h^{(\\mathcal{D})}(x_0) \\right] - \\bar{h} \\\\ &amp; = \\bar{g} - \\bar{g} = 0 \\end{align*}\\] Hence, under the assumption of a noiseless target function \\(f\\), we have that the expectation of the Squared Error for a given out-of-sample point \\(x_0\\), over all possible learning sets, is expressed as: \\[ \\mathbb{E}_{\\mathcal{D}} \\left [ \\left( h^{(\\mathcal{D})}(x_0) - f(x_0) \\right)^2 \\right ] = \\underbrace{\\mathbb{E}_{\\mathcal{D}} \\left [ \\left (h^{(\\mathcal{D})}(x_0) - \\bar{h}(x_0) \\right)^2 \\right ]}_{\\text{variance}} + \\underbrace{\\left [ (\\bar{h}(x_0) - f(x_0))^2 \\right ] }_{\\text{bias}^2} \\] 9.4.2 Noisy Target Now, when there is noise in the data we have that: \\(y = f(x) + \\epsilon\\). If \\(\\epsilon\\) is a zero-mean noise random variable with variance \\(\\sigma^2\\), the bias-variance decomposition becomes: \\[ \\mathbb{E}_{\\mathcal{D}} \\left [ \\left (h^{(\\mathcal{D})}(x_0) - y_0 \\right)^2 \\right ] = \\text{bias}^2 + \\text{var} + \\sigma^2 \\] Notice that the above equation assumes that the squared error corresponds to just one out-of-sample (i.e. test) point \\((x_0, y_0) = (x_0, f(x_0))\\). 9.4.3 Types of Theoretical MSEs At this point we need to make an important confession: for better or for worse, there is not just one type of \\(\\text{MSE}\\). The truth is that there are several flavors of theoretical \\(\\text{MSE}&#39;s\\), listed below in no particular order of importance: \\(\\mathbb{E}_\\mathcal{D} \\left[ \\left( h^{\\mathcal{D}}(x_0) - f(x_0) \\right)^2 \\right]\\) \\(\\mathbb{E}_{\\mathcal{X}} \\left[ \\left( h(x_0) - f(x_0) \\right)^2 \\right]\\) \\(\\mathbb{E}_{\\mathcal{X}} \\left[ \\mathbb{E}_\\mathcal{D} \\left\\{ \\left( h^{\\mathcal{D}}(x_0) - f(x_0) \\right)^2 \\right\\} \\right]\\) The first MSE involves a single out-of-sample point \\(x_0\\), measuring the performance of a single type of hypothesis \\(h()\\) over multiple learning data sets \\(\\mathcal{D}\\). This is what ISL calls expected test MSE (page 34). The second MSE involves a single hypothesis \\(h()\\), measuring its performance over all out-of-sample points \\(x_0\\). Notice that \\(h()\\) has been fitted on just one learning set \\(\\mathcal{D}\\). As for the third MSE, this is a combination of the two previous MSEs. Notice how this MSE involves a double expectation. Simply put, it measures the performance of a class of hypothesis \\(h()\\), over multiple learning data sets \\(\\mathcal{D}\\), over all out-of-sample points \\(x_0\\). This is what ISL calls overall expected test MSE (page 34). Unfortunately, in the Machine Learning literature, most authors simply mention an MSE without really specifying which type of MSE. Naturally, this may cause some confusion to the unaware reader. The good news is that, regardless of which MSE flavor you are faced with, they all admit a decomposition into bias-squared, variance, and noise. We cannot emphasize enough the theoretical nature of any type of \\(\\text{MSE}\\). This means that in practice we will never be able to compute the mean squared error. Why? First, we don’t know the target function \\(f\\). Second, we don’t have access to all out-of-sample points. And third, we cannot have infinite learning sets in order to compute the average hypothesis. We can, however, try to compute an approximation—an estimate—of an \\(\\text{MSE}\\) using a test data set denoted \\(\\mathcal{D}_{test}\\). This data set will be assumed to be a representative subset (i.e. an unbiased sample) of the out-of-sample data \\(\\mathcal{D}_{out}\\). 9.5 The Tradeoff We finish this chapter with a brief discussion of the so-called bias-variance tradeoff. Keep in mind that BV is a good theoretical device; i.e. we can never compute it in practice (if we could, we would have access to \\(f\\), in which case we wouldn’t really need Statistical Learning in the first place!) 9.5.1 Bias-Variance Tradeoff Picture Consider several classes of hypothesis, for example a class \\(\\mathcal{H}_1\\) of cubic polynomials; a class \\(\\mathcal{H}_2\\) of quadratic models; and a class \\(\\mathcal{H}_3\\) of linear models. For visualization purposes suppose that we can look at the (abstract) space that contains these types of models, like in the diagram below. Figure 9.3: Space of hypotheses. Each hollow (non-filled) point on the diagram above represents a fit based on some particular dataset, and each filled point represents the average model of a particular class of hypotheses. For example, if \\(\\mathcal{H}_1\\) represents fits based on linear models, each circle represents some linear polynomial \\(ax + b\\) with coefficients \\(a, b\\) that change depending on the sample. We can slightly modify the diagram by grouping all models of the same class, like in the following figure. Figure 9.4: Space of hypotheses. Let’s make our diagram more interesting by taking into account the variability in each class of models (see diagram below). In this way, the dashed lines represent the deviations of each fitted model against their average hypothesis. That is, the set of all dashed lines conveys the idea of variance in each model class: i.e. how spread out the models within each class are. Figure 9.5: Each class of hypothesis has a certain variability. Finally, suppose that we can also locate the true model in this space, indicated by a solid diammond (see diagram below). Notice that this true model \\(f()\\) is assumed to be of class \\(\\mathcal{H}_1\\). The solid lines between each average hypothesis and the target function represent the bias of each class of model. In practice, of course, we won’t have access to either the average models (shown in solid colors; i.e. the solid square, triangle, and circle) or the true model (shown in gray). Figure 9.6: Each class has a certain bias w.r.t. the target function. Bias Let’s first focus on the bias, \\(\\bar{h} - f(x)\\). The average \\(\\bar{h}\\) comes from a class of hypotheses (e.g. constant models, linear models, etc.) In other words, \\(\\bar{h}\\) is a prototypical example of a certain class of hypotheses. The bias term thus can be interpreted as a measure of how well a particular type of hypothesis \\(\\mathcal{H}\\) (e.g. constant model, linear model, quadratic model, etc.) approximates the target function \\(f\\). Another way to think about bias is as a deterministic noise. \\[ \\text{MSE} = \\mathrm{Variance} \\quad + \\underbrace{ \\mathrm{Bias} }_{\\text{deterministic noise}}+ \\underbrace{ \\text{Noise} }_{\\text{random noise}} \\] Variance Let’s focus on the variance, \\(\\mathbb{E}_\\mathcal{D} \\left[ (h^{(\\mathcal{D})}(x) - \\bar{h} )^2 \\right ]\\). This is a measure of how close a particular \\(h^{(\\mathcal{D})}(x)\\) can get to the average functional form \\(\\bar{h}\\). Put it in other terms, how precise is our particular function compared to the average function. Tradeoff Ideally, a model should have both small variance and small bias. But, of course, there is a tradeoff between these two (hence the Bias-Variance tradeoff). Decreasing bias comes at the cost of increasing variance, and vice-versa. Also, to understand the bias-variance decomposition, you need access to \\(\\bar{h}\\). In practice, however, we will never have access to \\(\\bar{h}\\) as computing \\(\\bar{h}\\) requires first computing every model of a particular type (e.g. linear, quadratic, etc.). More complex/flexible models tend to have less bias, and therefore a better opportunity to approximate \\(f(x)\\). They also tend to produce small in-sample error: \\(E_{in} \\approx 0\\). On the downside, more complex models tend to have higher variance. They also have a higher risk of producing large out-of-sample error: \\(E_{out}(h) \\neq 0\\). You will also need more resources (training data, as well as computational resources) to produce a more complicated model. Less complex/flexible models (i.e. simpler models) tend to have less variance, but more bias (i.e. less opportunity to estimate \\(f\\), but a higher chance to approximate out of sample error): \\(E_{in} \\approx E_{out}\\), but \\(E_{in} \\approx E_{out} \\neq 0\\). Simpler models also tend to require less data/computational resources. To decrease bias, a model has to be more flexible/complex. We should note that notions of “flexibility” and “complexity” are difficult to define precisely. For example, in the context of classification, complexity could be related to various hyperparameters (e.g. number of child nodes in our tree, etc.). In theory, to decrease bias, one needs “insider” information. That is, to truly decrease bias, we need some information on the form of \\(f\\). Hence, it is nearly impossible to have zero bias. We therefore put our efforts towards decreasing variance when working with more complex/flexible models. One way to decrease variance would be to add more training data. However, there is a price to pay: we will have less test data. We could reduce the dimensions of our data (i.e. play with lower-rank data matrices through, for example, PCA). We could also apply regularization. At its core, this notion relates to making the size of a model’s parameters small. Consequently, there is a reduction in the variance of a model. Summarizing: in general, decreasing bias comes at the cost of increasing variance, and vice-versa. "],
["phases.html", "10 Learning Phases 10.1 Introduction 10.2 Model Assessment 10.3 Model Selection 10.4 Model Training", " 10 Learning Phases In this chapter we further discuss more theoretical elements of the supervised learning framework. In particular, we take a deep dive into some of the activities to be performed in every learning process, namely, model training, model selection, and model assessment. A word of caution needs to be said about the terminology that we use here. If you look at other books or resources about statistical/machine learning, you will find that there is no consistent use of words such as training, testing, validation, evaluation, assessment, and other similar terms. We have decided to use specific words—for certain concepts—that other authors or practitioners may handle differently. Also, bear in mind that many of the notions and ideas described in this chapter tend to have a decisive theoretical flavor. As we move on, we will provide more details and explanations on how to make such ideas more concrete, and how to execute them in practice. 10.1 Introduction Let’s bring back a simplified version of the supervised learning diagram depicted in the figure below. We know that there are more elements in the full-fledged diagram, but we want to focus on three main stages that are present in all supervised learning contexts. Figure 10.1: Simplified Supervised Learning Diagram In every learning situation, there is a handful of tasks we need to carry out. First, we need to fit several models, typically involving working with different classes of hypothesis. For example, we can consider four types of regression methods: principal components regression (\\(\\mathcal{H_1}\\)), partial least squares regression (\\(\\mathcal{H_2}\\)), ridge regression (\\(\\mathcal{H_3}\\)), and lasso (\\(\\mathcal{H_4}\\)). All of these types of regression models have tuning parameters that cannot be derived analytically from the data, but have to be determined through trial-error steps. For each hypothesis family, we need to find the optimal tuning parameter, which involves choosing a finalist model: the best principal components regression, the best partial least regression, etc. Then, we need to select the best model among the finalist models. This will be the final model to be delivered. And finally, we need to measure the predicting performance of the final model: measure how the model will behave with out-of-sample points. We can formalize these tasks into three major phases encountered in every supervised learning system: 1) model training, 2) model selection, and 3) model assessment. Training: Given some data, and a certain modeling hypothesis \\(\\mathcal{H}_m\\), how can we fit/estimate a model \\(h_m\\)? Often, we are also interested in telling something about its predicting performance (in a limited sense). Selection: Choosing a model from a set of candidate models. Typically, this has to do with two types of selection: Pre-Selection: choosing a finalist model from a set of models belonging to a certain class of hypothesis. Final-Selection: choosing a final model among a set of finalist models; that is, choosing the very final model. Assessment: Given a final \\(\\widehat{f}\\), how can we provide an estimate \\(\\widehat{E}_{out}(\\hat{f})\\) of the out-of-sample error? In other words, how to measure the performance of the final model in order to provide an estimation of its predictive behavior out-of-sample. To expand our mental map, let’s place the learning phases in the supervised learning diagram (see image below). As you can tell, now we are taking a peek inside the learning algorithm section. This is where the model training, the pre-selection, and the final-selection tasks occur. In turn, the model assessment part has to do with estimating the out-of-sample performance of the final model. Figure 10.2: Schematic of Learning Phases Having identified the main learning tasks, and knowing how they show up inside the supervised larning diagram, the next thing to consider is: What data should we use to perform each task? We will answer this question in the following sections, starting first with the model assessment phase, then the selection of the final model, then the pre-selection of finalist, and finally the training of candidate models. 10.2 Model Assessment Let us consider a situation in which we have a final model \\(h\\). The next logical step should involve measuring its prediction quality. In other words, we want to see how good (or how bad) the predictions of \\(h\\) are. The general idea is fairly simple—at least conceptually. Given an input data \\(\\mathbf{x_i}\\), we need to assess the discrepancy between the predicted value \\(\\hat{y}_i = h(\\mathbf{x_i})\\) and the observed value \\(y_i\\). Simple, right? … Well, not really. As you may recall from the chapter about the theoretical framework of supervised learning, there are two major types of predictions. On one hand, we have predictions \\(\\hat{y}_i = h(\\mathbf{x_i})\\) of in-sample points \\((\\mathbf{x_i}, y_i) \\in \\mathcal{D}_{in}\\). In-sample points are those data points that we used to fit a given model. On the other hand, we have predictions \\(\\hat{y}_0 = h(\\mathbf{x_0})\\) of out-of-sample points \\((\\mathbf{x_0}, y_0) \\in \\mathcal{D}_{out}\\). Out-of-sample points are data points not used to fit a model. Because of this distinction between in-sample data \\(\\mathcal{D}_{in}\\), and out-of-sample data \\(\\mathcal{D}_{out}\\), we can measure the predictive quality of a model from these two perspectives. This obviously implies having predictions, and errors, of two different natures. Aggregating all the discrepancies between the predicted in-sample points \\(\\hat{y}_i = h(\\mathbf{x_i})\\) and their observed values \\(y_i\\), allows us to quantify the in-sample error \\(E_{in}\\), which inform us about the resubstitution power of the model \\(h\\) (i.e. how well the model fits the learning data). The more interesting part comes with the out-of-sample points. If we had access to all points in \\(\\mathcal{D}_{out}\\), measuring the discrepancies between the predicted out-of-sample points \\(\\hat{y}_0 = h(\\mathbf{x_0})\\) and their observed values \\(y_0\\), would allow us to quantify the out-of-sample error \\(E_{out}\\), truly measuring the generalization power of the model \\(h\\). In practice, unfortunately, we won’t have access to the entire out-of-sample data \\(\\mathcal{D}_{out}\\). Knowing that the whole out-of-sample data set is not within our reach, the second best thing that we can do is to find a proxy set for \\(\\mathcal{D}_{out}\\). If we can obtain a data set \\(\\mathcal{D}_{proxy}\\) that is a representative subset of \\(\\mathcal{D}_{out}\\), then we can compute \\(E_{proxy}\\) and use it to approximate \\(E_{out}\\), thus having a fair estimation of the generalization power of \\(h\\). This is precisely the idea of the so-called term Model Assessment: How can we estimate \\(E_{out}(\\widehat{f})\\) of a final model? Which basically reduces to: how can we find an unbiased sample \\(\\mathcal{D}_{proxy} \\subset \\mathcal{D}_{out}\\) in order to get \\(\\hat{E}_{out}\\)? 10.2.1 Holdout Test Set In order to answer the question “How can we estimate \\(E_{out}\\)?”, let us discuss the theoretical rationale behind the so-called Holdout Method. In practice, we always have some available data \\(\\mathcal{D} = (\\mathbf{x_1}, y_1), \\dots, (\\mathbf{x_n}, y_n)\\). If we used all the \\(n\\) data points to train/fit a model, we could perfectly measure \\(E_{in}\\) but then we wouldn’t have any out-of-sample points to get an honest approximation \\(\\widehat{E}_{out}\\) of \\(E_{out}\\). Perhaps you are thinking: “Why not use \\(E_{in}\\) as an estimate \\(\\widehat{E}_{out}\\) of \\(E_{out}\\)?” Because it won’t be a realiable estimate. We may have a model \\(h\\) that produces a very small in-sample error, \\(E_{in} \\approx 0\\), but that doesn’t necessarily mean that it has good generalization performance out-of-sample. Given that the only available data set is \\(\\mathcal{D}\\), the Holdout Method proposes to split \\(\\mathcal{D}\\) into two subsets: (1) a subset \\(\\mathcal{D}_{train}\\), called the training set, used to train/fit the model, and (2) another subset \\(\\mathcal{D}_{test}\\), called the test set, to be used as a proxy of \\(\\mathcal{D}_{out}\\) for testing/assessment purposes. Figure 10.3: Data split into training and test sets How do we actually obtain these two sets? Usually by taking random samples of size \\(a\\), without replacement, from \\(\\mathcal{D}\\) (although there are exceptions). \\[ \\mathcal{D} \\to \\begin{cases} \\text{training } \\mathcal{D}_{train} &amp; \\to \\text{size } n - a \\\\ \\text{test } \\mathcal{D}_{test} &amp; \\to \\text{size } a \\\\ \\end{cases} \\] We then fit a particular model using \\(\\mathcal{D}_{train}\\); obtaining a model that we’ll call \\(h^{-}(x)\\), “\\(h\\)-minus”, because it is a model fitted with the training set \\(\\mathcal{D}_{train}\\), which is a subset of the available data \\(\\mathcal{D}\\). With the remainder points in \\(\\mathcal{D}_{test}\\), we can measure the performance of the model \\(h^{-}(x)\\) as: \\[ E_{test}(h^{-}) = \\frac{1}{a} \\sum_{\\ell=1}^{a} err\\left( h^{-}(\\mathbf{x_\\ell}) , y_\\ell \\right) ; \\hspace{5mm} (\\mathbf{x_\\ell}, y_\\ell) \\in \\mathcal{D}_{test} \\] As long as \\(\\mathcal{D}_{test}\\) is a representative sample of \\(\\mathcal{D}_{out}\\), \\(E_{test}\\) should give a good estimate of \\(E_{out}\\). Let’s see why? 10.2.2 Why does a test set work? Consider an out-of-sample point \\((\\mathbf{x_0}, y_0)\\) that is part of the test set: \\((\\mathbf{x_0}, y_0) \\in \\mathcal{D}_{test}\\). Given a pointwise error function \\(err()\\), we can measure the error: \\(err(h(\\mathbf{x_0}), y_0)\\). Moreover, we can treat it as a point estimate of \\(E_{out}(h)\\). Here’s a relevant question: Is \\(err(h(\\mathbf{x_0}), y_0)\\) an unbiased estimate of \\(E_{out}(h)\\)? To see whether the point estimate \\(err(h(\\mathbf{x_0}), y_0)\\) is an unbiased estimate of \\(E_{out}(h)\\), let’s determine its expectation over the input space: \\[ \\mathbb{E}_{\\mathcal{X}} [ err(h(\\mathbf{x_0}), y_0) ] \\] Remember what the above expression represent? Yes, it is precisely the out-of-sample error \\(E_{out}(h)\\)! Therefore, \\(err(h(\\mathbf{x_0}), y_0)\\) is an unbiased point estimate of the out-of-sample error. What about the variance: \\(Var[err(h(\\mathbf{x_0}), y_0)]\\)? For the sake of simplicity let’s assume that this variance is constant: \\[ Var[err(h(\\mathbf{x_0}), y_0)] = s^2 \\] Obviously this variance could be large (or small). So having just one test point \\((\\mathbf{x_0}, y_0)\\), even though is an unbiased estimate of \\(E_{out}\\), does not allow us to have an idea of how reliable that estimate is. Well, let’s consider a set \\(D_{test} = \\{ (\\mathbf{x_1}, y_1), \\dots, (\\mathbf{x_a}, y_a) \\}\\) containing \\(a &gt; 1\\) points. We can average their pointwise errors to get \\(E_{test}\\) \\[ E_{test}(h) = \\frac{1}{a} \\sum_{\\ell=1}^{a} err[ h^{-}(\\mathbf{x_\\ell}), y_\\ell ] \\] The question becomes: is \\(E_{test}(h)\\) an unbiased estimate of \\(E_{out}(h)\\)? Let’s find out: \\[\\begin{align*} \\mathbb{E}_{\\mathcal{X}} [E_{test}(h)] &amp;= \\mathbb{E}_{\\mathcal{X}} \\left [ \\frac{1}{a} \\sum_{\\ell=1}^{a} err[ h^{-}(\\mathbf{x_\\ell}), y_\\ell ] \\right ] \\\\ &amp;= \\frac{1}{a} \\sum_{k=1}^{a} \\mathbb{E}_{\\mathcal{X}} \\left [ err \\left ( h^{-}(\\mathbf{x_\\ell}), y_\\ell \\right) \\right ] \\\\ &amp;= \\frac{1}{a} \\sum_{\\ell=1}^{a} E_{out}(h) \\\\ &amp;= E_{out}(h) \\end{align*}\\] Yes, it turns out that \\(E_{test}(h)\\) is an unbiased estimate of \\(E_{out}(h)\\). But what about the variance? Let’s assume that the errors across points are independent (this may not be the case in practice, but we make this assumption to ease computation): It can be shown that the variance of \\(E_{test}(h)\\) is given by: \\[ Var[E_{test}(h)] = \\frac{1}{a^2} \\sum_{\\ell=1}^{a} Var[ err(h(\\mathbf{x_0}), y_0) ] = \\frac{s^2}{a} \\] The above equation tells us that, as we increase the number \\(a\\) of test points, the variance of the estimator \\(E_{test}(h)\\) will decrease. Simply put, the more test points we use, the more reliably \\(E_{test}(h)\\) estimates \\(E_{out}(h)\\). Of course, \\(a\\) is not freely selectable; the larger \\(a\\) is, the smaller our training dataset will be. In any case, the important thing is that reserving some points \\(\\mathcal{D}_{test}\\) from a learning set \\(\\mathcal{D}\\) to use them for testing purposes is a very wise idea. It definitely allows us to have a war for estimating the performance of a model \\(h\\), when applied to out-of-sample points. Again, the holdout method is simply a conceptual starting point. Also, depending on how you form your training and test sests, you may end up with a split that may not be truly representative of the studied phenomenon. So instead of using just one split, some authors propose to use several splits of training-test sets obtained through resampling methods. We talk about this topic with more detail in the next chapter. Holdout Algorithm Here’s the conceptual algorithm behind the holdout method. Compile the available data into a set \\(\\mathcal{D} = \\{(\\mathbf{x_1}, y_1), \\dots, (\\mathbf{x_n}, y_n) \\}\\). Choose \\(a \\in \\mathbb{Z}^{+}\\) elements from \\(\\mathcal{D}\\) to comprise a test set \\(\\mathcal{D}_{test}\\), and place the remaining \\(n - a\\) points into the training set \\(\\mathcal{D}_{train}\\). Use \\(\\mathcal{D}_{train}\\) to fit a particular model \\(h^{-}(x)\\). Measure the performance of \\(h^{-}\\) using \\(\\mathcal{D}_{test}\\); specifically, compute \\[ E_{test}(h^{-}) = \\frac{1}{a} \\sum_{\\ell=1}^{a} err_\\ell[ h^{-}(\\mathbf{x_\\ell}), y_\\ell ] \\] where \\((\\mathbf{x_\\ell}, y_\\ell) \\in \\mathcal{D}_{test}\\), and \\(err_\\ell\\) is some measure of pointwise error. Generate the final model \\(\\widehat{h}\\) by refitting \\(h^{-}\\) to the entire dataset \\(\\mathcal{D}\\). There are many different conventions as to how to pick \\(a\\): one common rule-of-thumb is to assign 80% of your data to the training set, and the remaining 20% to the test set. Also, keep in mind that the model that you ultimately deliver will not be \\(h^{-}\\); rather, you need to refit \\(h^{-}\\) using the entire data \\(\\mathcal{D}\\). This yields the final hypothesis model \\(\\widehat{h}\\). 10.3 Model Selection Now that we’ve talked about how to measure the generalization performance of a given final model, the next thing to discuss is how to compare different models, in order to select the best one. Say we have \\(M\\) models to choose from. Consider, for example, the following three cases (each with \\(M = 3\\)): We could have three different types of hypotheses: \\(h_1:\\) linear model \\(h_2:\\) neural network \\(h_3:\\) regression tree Or we could also have a particular type of model, e.g. polynomials, with three different degrees: \\(h_1:\\) quadratic model \\(h_2:\\) cubic model \\(h_3:\\) 15th-degree model Or maybe a principal components regression with three options for the number of components to retain: \\(h_1:\\) PCR \\((c = 1)\\) \\(h_2:\\) PCR \\((c = 2)\\) \\(h_3:\\) PCR \\((c = 3)\\) How do we select the best model from a set of candidate models? 10.3.1 Three-way Holdout Method We can extend the idea behind the holdout method, to go from one holdout set to two holdout sets. Namely, instead of splitting \\(\\mathcal{D}\\) into two sets (training and test), we split it into three sets that we will call: training \\(\\mathcal{D}_{train}\\), validation \\(\\mathcal{D}_{val}\\), and test \\(\\mathcal{D}_{test}\\). Figure 10.4: Data split into training, validation, and test sets The test set \\(\\mathcal{D}_{test}\\) is the set that will be used for assessing the performance of a final model. This means that once we have created \\(\\mathcal{D}_{test}\\), the only time we use this set is at end of the learning process. We only use it to quantify the generalization error of the final model. And that’s it. We don’t use this set to make any learning decision. What about the validation set \\(\\mathcal{D}_{val}\\)? What do we use it for? We recommend using the validation set for selecting the final model from a set of finalist models. However, keep in mind that other authors may recommend other uses for \\(\\mathcal{D}_{val}\\). Here’s our ideal suggestion on how to use the validation set. Figure 10.5: Using a validation set for final-selection \\(\\mathcal{H}_m\\) represents the \\(m\\)-th hypothesis, and \\(h^{-}_m\\) represents the best \\(m\\)-th fit to the \\(m\\)-th hypothesis using \\(\\mathcal{D}_{train}\\). In other words, \\(h^{-}_m\\) is the finalist model from class \\(\\mathcal{H}_m\\). After a finalist model \\(h^{-}_m\\) has been pre-selected for each class of hypothesis, then we use \\(\\mathcal{D}_{val}\\) to compute validation errors \\(E^{m}_{val}\\). The model with the smallest validation error is then selected as the final model. After the best model \\(h^{-}_{m}\\) has been selected (with the smallest \\(E_{val}\\)), a model \\(h_{\\overset{*}m}\\) is fitted using \\(\\mathcal{D}_{train} \\cap \\mathcal{D}_{val}\\). The performance of this model is assessed by using \\(\\mathcal{D}_{test}\\) to obtain \\(E_{test}(h_{m}^{*})\\). Finally, the “official” model is the model fitted on the entire data set \\(\\mathcal{D}\\), but the reported performance is \\(E^{m}_{test}\\). One important thing to notice is that \\(E^{m}_{test}\\) is an unbiased estimate of the out-of-sample performance of \\(h_{\\overset{*}m}\\), even though the actual model \\(h_m\\) that is delivered is the fitted on the entire data. Why are we calling \\(\\mathcal{D}_{val}\\) a “validation” set, when it appears to be serving the same role as a “test” set? Because choosing the “best” model (i.e. the model with the smallest \\(E_{val}\\)) is a learning decision. Had we stopped our procedure before making this choice (i.e. if we had stopped after considering \\(E_{val}^m\\) for \\(m = 1, 2, ..., M\\)), we could have plausibly called these errors “test errors.” However, we went one step further and as a result obtained a biased estimate of \\(E_{out}\\); namely, \\(E_{val}^{m}\\) for the chosen model \\(m\\). Of course, there is still a tradeoff when working with a 3-way split, which is precisely the fact that we need to split our data into three sets. The larger our test and validations sets are, the more reliable the estimates of the out-of-sample performance will be. At the same time, however, the mode data points for validation and testing, the smaller our training set will be. Which will very likely produce far from optimal finalist models, as well as the very final model. 10.4 Model Training In order to perform the pre-selection of finalist models, the selection of the final model, and its corresponding assessment, we need to be able to fit all candidate models belonging to different classes of hypotheses. The data set that we use to fit/train such models is, surprise-surprise, the training set \\(\\mathcal{D}_{train}\\). This will also be the set that we’ll use to pre-select the finalist models from each class. For example, say the class \\(\\mathcal{H}_{m}\\) is principal components regression (pcr), and we need to train several models \\(h_{1,m}, h_{2,m}, \\dots, h_{q,m}\\) with different number of principal components (i.e. the tuning parameter). Obviously, if we just simply use \\(\\mathcal{D}_{train}\\) to fit all possible pcr models, and also to choose the one with smallest error \\(E_{train}\\), then we run the risk of overfitting. Likewise, we know that \\(E_{train}\\) is an optimistic—unreliable—biased estimate of \\(E_{out}\\). One may ask, “Why don’t we use \\(\\mathcal{D}_{val}\\) to compute validation errors, and select the model with smallest validation error?” Quick answer: you could. But then you are going to run out of fresh points for the final-selection phase, also running the risk of choosing a model that overfits the validation data. It looks like we are heading into a dead-end road. On one hand, we cannot use all the training points for both model training, and pre-selection of finalists. On the other hand, if we use the validation points for the pre-selection phase, then we are left with no fresh points to choose the final model, unless we exhaust the points in the test set. In theory, it seems that we should use different data sets for each phase, something like a \\(\\mathcal{D}_{train}\\) for training candidate models, \\(\\mathcal{D}_{pre}\\) for pre-selecting finalist models, \\(\\mathcal{D}_{final}\\) for selecting the very final model, and \\(\\mathcal{D}_{assess}\\) for assessing the performance of the final model. We’ve seen what theory says about reserving some out-of-sample points for both 1) making learning decisions (e.g. choosing a finalist), and for 2) assessing the performance of a model. It is a wise idea to have fresh unseen points to be spent as we move on into a new learning phase. The problem is that theory doesn’t tell us exactly how many holdout sets we should have, or how big they should be. The main limitation is that we only have so much data. Can we really afford splitting our limited data resources into four different sets? Are we doomed …? Resampling methods to the rescue! Fortunately, we can use (re)sampling methods that will allow us to make the most out of—at least—the training set. Because of its practical relevance, we discuss this topic in the next chapter. "],
["resampling.html", "11 Resample Approaches 11.1 General Sampling Blueprint 11.2 Monte Carlo Cross Validation 11.3 Bootstrap Method 11.4 \\(K\\)-Fold Cross-Validation", " 11 Resample Approaches In this chapter we discuss how to make the most of a limitted data set. We describe several resampling techniques to generate multiple training sets, as well as multiple evaluation sets that we can use for both the training phase and the pre-selection phase. 11.1 General Sampling Blueprint The idea behind most common (re)sampling approaches is to have a way to generate multiple sets from just one data set. In statistical learning, the training set is the one on which we usually apply resampling methods. The figure below depicts the general sampling regime of the approaches that are described in the next sections. Suppose we are interested in fitting a \\(k\\)-Nearest-Neighbor (\\(k\\)-NN) model. In particular, we want to fit a \\(k\\)-NN model with \\(k=3\\) neighbors. So let’s denote this type of model as \\(\\mathcal{H}_1\\) hypothesis. The starting point is the training set \\(\\mathcal{D}_{train}\\). Using a sampling mechanism we split the training set into two subsets. One of the subsets, that we denote as \\(\\mathcal{D}_{train-b}\\), is used to train a model, denoted \\(h_{1,b}\\). The other subset, denoted \\(\\mathcal{D}_{eval-b}\\), is used to evaluate \\(h_{1,b}\\) by calculating \\(E_{eval-b}\\). The way the evaluation set is formed, is by taking those data points in \\(\\mathcal{D}_{train}\\), that are not included in \\(\\mathcal{D}_{train-b}\\), that is: \\(\\mathcal{D}_{eval-b} = \\mathcal{D}_{train} \\setminus \\mathcal{D}_{train-b}\\) This procedure is repeated \\(B\\) times. At the end, we average all the evaluation errors to get a single measure indicating the typical performance for the particular type of trained models. In this example, this would be the typical performance of 3-NN models. Figure 11.1: General scheme for sampling approaches What distinguishes each sampling approach is the size and way in which the random samples are generated to obtain the sets \\(\\mathcal{D}_{train-b}\\). They can be random samples without replacement, they can be random samples with replacement. 11.2 Monte Carlo Cross Validation The first sampling method that we discuss is Monte-Carlo Cross-Validation, sometimes known as Repeated Holdout Method. As the name indicates, this method repeatedly produces a holdout set, \\(\\mathcal{D}_{eval-b}\\), to be used for evaluating the performance of a model. At each iteration (or repetition) \\(b\\) we split \\(\\mathcal{D}_{train}\\) into \\(\\mathcal{D}_{train-b}\\) and \\(\\mathcal{D}_{eval-b}\\). We use the training subset to fit a particular model \\(h^{-}_{b}\\), and then validate it on \\(E_{eval-b}\\). Figure 11.2: Several splits into training and evaluation sets After several repetitions, we average all the test errors \\(E_{eval-b}\\) to obtain \\(E_{eval}(h^{-})\\), an estimate of \\(E_{out}(h^{-})\\). Repeated Holdout Algorithm Here’s the conceptual algorithm for the repeated holdout method: Compile the available training data into a set \\(\\mathcal{D}_{train} = \\{(\\mathbf{x_1}, y_1), \\dots, (\\mathbf{x_n}, y_n) \\}\\). Repeat the Holdout method \\(B\\) times, where \\(B\\) is a very large integer (for example, 500). At each iteration, obtain the following elements: Generate \\(\\mathcal{D}_{train-b}\\), by sampling \\(n-r\\) elements without replacement from \\(\\mathcal{D}_{train}\\). Generate \\(h^{-}_{b}\\), by fitting a model to \\(\\mathcal{D}_{train-b}\\) Compute \\(E_{eval-b} = \\frac{1}{r} \\sum_{i} err_i \\big( h^{-}_{b}(\\mathbf{x_i}), y_i \\big)\\) where \\((\\mathbf{x_i}, y_i) \\in \\mathcal{D}_{eval-b}\\) Obtain an overall value for \\(E_{eval}\\) by averaging the \\(E_{eval-b}\\) values: \\[ E_{eval} = \\frac{1}{B} \\sum_{b=1}^{B} E_{eval-b} \\] 11.3 Bootstrap Method Another interesting validation procedure is the “Bootstrap Method”. This method is very similar to the Repeated Holdout method. The main difference is in the way \\(\\mathcal{D}_{train}\\) is split in each repetition. In the bootstrap method, as the name says, the samples consists of bootstrap samples. Recall that a bootstrap sample is a random sample of the data taken with replacement. This means that the bootstrap sample is the same size as elements in \\(\\mathcal{D}_{train}\\). As a result, some samples will be represented multiple times in the bootstrap sample while other will not be selected at all. The samples not selected are usually referred to as the out-of-bag samples. Figure 11.3: Several splits with bootstrap samples into training and test sets Bootstrap Algorithm Here’s the conceptual algorithm for the bootstrap method: Compile the available data into a set \\(\\mathcal{D}_{train} = \\{(\\mathbf{x_1}, y_1), \\dots, (\\mathbf{x_n}, y_n) \\}\\). Repeat the following steps for \\(b = 1, 2, \\dots B\\) where \\(B\\) is a very large integer (for example, 500): Generate \\(\\mathcal{D}_{train-b}\\) by sampling \\(n\\) times with replacement from \\(\\mathcal{D}_{train}\\). (Note that \\(\\mathcal{D}_{train-b}\\) could contain repeated elements). Generate \\(\\mathcal{D}_{eval-b}\\) by combining all of the elements in \\(\\mathcal{D}_{train}\\) that were not captured in \\(\\mathcal{D}_{train-b}\\). Hence, the size of \\(\\mathcal{D}_{eval-b}\\) will change at each iteration. (Note that \\(\\mathcal{D}_{eval-b}\\) should not contain repeated values). Generate \\(h^{-}_b\\) by fitting a model to \\(\\mathcal{D}_{train-b}\\). Compute \\(E_{eval} = \\frac{1}{r_b} \\sum_{i} err_i \\big( h^{-}_b(\\mathbf{x_i}), y_i \\big)\\) where \\((\\mathbf{x_i}, y_i) \\in \\mathcal{D}_{eval-b}\\) and \\(r_b\\) is the size of \\(\\mathcal{D}_{eval-b}\\). Obtain an overall value for bootstrap \\(E_{eval}\\) by averaging the \\(E_{eval-b}\\) values: \\[ \\text{bootstrap } E_{eval} = \\frac{1}{B} \\sum_{b=1}^{B} E_{eval-b} \\] Bootstrapping enables us to construct confidence intervals for \\(E_{out}\\). We have that: \\[ \\mathrm{SE}_{boot} = \\sqrt{ \\frac{1}{B - 1} \\sum_{b=1}^{B} \\left(E_{eval-b} - E_{eval} \\right)^2 } \\] Empirical quantiles can be used. 11.4 \\(K\\)-Fold Cross-Validation The idea of the classic cross-validation method (not to confuse with monte carlo cross-validation) is to split the training data into \\(K\\) sets of equal (or almost equal) size. Each of the resulting subsets is referred to as a fold. Usually, the way in which the folds are formed is by randomly splitting the initial data. The diagram below illustrates a 3-fold cross validation sampling scheme: Figure 11.4: 3-fold splits As you can tell, the training data is (randomly) divided into three sets of similar size: the 3-folds. Then, at each iteration \\(b\\), one of the folds is held out for evaluation purposes, i.e. \\(\\mathcal{D}_{eval-b}\\); the reamining folds are merged into \\(\\mathcal{D}_{train-b}\\), and used for training purposes. As usual, at the end of the iterations (\\(b = K\\)), all evaluation errors \\(E_{eval-b}\\) are aggregated into an average \\(E_{eval}\\), commonly known as the cross-validation error \\(E_{cv}\\). 11.4.1 Leave-One-Out Cross Validation (LOOCV) One special case of \\(K\\)-fold cross-validation is when each fold consists of a single data point, that is \\(K = n\\). This particular scheme is known as leave-one-out cross-validation (loocv). Let’s see how to carry out loocv. Compile the available data into a set \\(\\mathcal{D}_{train} = \\{(\\mathbf{x_1}, y_1), \\dots, (\\mathbf{x_n}, y_n) \\}\\). Repeat the following for \\(i = 1, 2, \\dots, n\\) (where \\(n\\) is the number of data points) Generate the \\(i\\)-th training set \\(\\mathcal{D}_i\\) by removing the \\(i\\)-th element from \\(\\mathcal{D}_{train}\\); that is, \\(\\mathcal{D}_i = \\mathcal{D} \\backslash \\{ (\\mathbf{x_i}, y_i) \\}\\). Fit your model to \\(\\mathcal{D}_i\\) to obtain model \\(h_{i}^{-}\\) Use the point \\((\\mathbf{x_i}, y_i)\\) that is left-out to compute \\(E_{eval-i} = err \\big( h_{i}^{-}(\\mathbf{x_i}), y_i \\big)\\). Obtain the cross-validation error \\(E_{cv}\\) by averaging the individual errors: \\[ E_{cv} = \\frac{1}{n} \\sum_{i=1}^{n} err_i = \\frac{1}{n} \\sum_{i=1}^{n} E_{eval-i} \\] In this way, we are able to fit \\(n\\) models with \\(n - 1\\) points, while also generating \\(n\\) testing datasets. So, this looks like the best of both worlds: we have a large training data set, and a lot of test datasets! But, there is still a cost: the individual errors are no longer independent. For illustrative purposes, consider a data set withonly three points: \\(\\mathcal{D} = \\{p_1, p_2, p_3\\}\\) (pictured below). Figure 11.5: Illustration of loocv errors We would therefore repeat step (2) above \\(n = 3\\) times, to obtain \\(3\\) errors: \\(e_1, \\ e_2,\\) and \\(e_3\\) [note that in the last figure, the red dashed line extends to touch the blue line; unfortunately, the point of intersection was too low to feasibly show on the graph. In other words, use your imagination: the red line touches the blue line]. "],
["regular.html", "12 Regularization Techniques 12.1 Multicollinearity Issues 12.2 Irregular Coefficients 12.3 Connection to Regularization", " 12 Regularization Techniques In this part of the book we will talk about the notion of regularization (what is regularization, what is the purpose of regularization, what approaches are used for regularization) all of this within the context of linear models. However, keep in mind that you can also use regularization in non-linear contexts. So what is regularization? If you look at the dictionary definition of the term regularization you should find something like this: regularization; the act of bringing to uniformity; make (something) regular Simply put: regularization has to do with “making things regular”. But what about regularization in the context of supervised learning? What are the “irregular” things that need to be made “regular”? In order to answer this question, we will first motivate the discussion by looking at the effects of having predictors with high multicollinearity. We will discuss two main types of approaches to achieve regularization: Dimension Reduction: Principal Components Regression (PCR), Partial Least Squares regression (PLSR) Penalized Methods: Ridge Regression, Lasso regression. 12.1 Multicollinearity Issues One of the issues when fitting regression models is due to multicollinearity: the condition that arises when two or more predictors are highly correlated. How does this affect OLS regression? When one or more predictors are linear combinations of other predictors, then \\(\\mathbf{X^\\mathsf{T} X}\\) is singular. This is known as exact collinearity. When this happends, there is no unique least squares estimate \\(\\mathbf{b}\\). A more challenging problem arises when \\(\\mathbf{X^\\mathsf{T} X}\\) is close but not exactly singular. This is usually referred to as near perfect collinearity or, more commonly, as multicollinearity. Why is this a problem? Because multicollinearity leads to imprecise (unstable) estimates \\(\\mathbf{b}\\). What are the typical causes of multicollinearity? One or more predictors are linear combinations of other predictors One or more predictors are almost perfect linear combinations of other predictors There are more predictors than observations \\(p &gt; n\\) 12.1.1 Toy Example To illustrate the issues of dealing with multicollinearity, let’s play with mtcars, one of the built-in data sets in R. mtcars[1:10, ] #&gt; mpg cyl disp hp drat wt qsec vs am gear carb #&gt; Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 #&gt; Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 #&gt; Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 #&gt; Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 #&gt; Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 #&gt; Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 #&gt; Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 #&gt; Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 #&gt; Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 #&gt; Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 Let’s use mpg as response, and disp, hp, and wt as predictors. # response mpg &lt;- mtcars$mpg # predictors disp &lt;- mtcars$disp hp &lt;- mtcars$hp wt &lt;- mtcars$wt # standardized predictors X &lt;- scale(cbind(disp, hp, wt)) Let’s inspect the correlation matrix, assuming the data matrix of predictors \\(\\mathbf{X}\\) is mean-centered: \\[ \\mathbf{R} = \\frac{1}{n-1} \\mathbf{X^\\mathsf{T} X} \\] # correlation matrix cor(X) #&gt; disp hp wt #&gt; disp 1.0000000 0.7909486 0.8879799 #&gt; hp 0.7909486 1.0000000 0.6587479 #&gt; wt 0.8879799 0.6587479 1.0000000 If we look at the circle of correlations from a principal components analysis, we see the following pattern: hp, disp, and wt are positively correlated; indicated by the fact the arrows are pointing in a common general direction. Carrying out an ordinary least squares regression, that is, regressing mpg onto disp, hp, and wt we get the following output provided the linear model function lm(): #&gt; #&gt; Call: #&gt; lm(formula = mpg ~ disp + hp + wt) #&gt; #&gt; Coefficients: #&gt; (Intercept) disp hp wt #&gt; 37.105505 -0.000937 -0.031157 -3.800891 #&gt; #&gt; Call: #&gt; lm(formula = mpg ~ disp + hp + wt) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -3.891 -1.640 -0.172 1.061 5.861 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 37.105505 2.110815 17.579 &lt; 2e-16 *** #&gt; disp -0.000937 0.010350 -0.091 0.92851 #&gt; hp -0.031157 0.011436 -2.724 0.01097 * #&gt; wt -3.800891 1.066191 -3.565 0.00133 ** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 2.639 on 28 degrees of freedom #&gt; Multiple R-squared: 0.8268, Adjusted R-squared: 0.8083 #&gt; F-statistic: 44.57 on 3 and 28 DF, p-value: 8.65e-11 Analytically, the estimated vector of coefficients \\(\\mathbf{b}\\), and the predicted response \\(\\mathbf{\\hat{y}}\\) are obtained as: \\(\\mathbf{b} = (\\mathbf{X^\\mathsf{T} X})^{-1} \\mathbf{X^\\mathsf{T} y}\\) \\(\\mathbf{\\hat{y}} = \\mathbf{X} (\\mathbf{X^\\mathsf{T} X})^{-1} \\mathbf{X^\\mathsf{T} y}\\) So let’s take a look at the matrix \\((\\mathbf{X^\\mathsf{T} X})^{-1}\\) #&gt; disp hp wt #&gt; disp 0.23627475 -0.08598357 -0.15316573 #&gt; hp -0.08598357 0.08827847 0.01819843 #&gt; wt -0.15316573 0.01819843 0.15627798 Exact Collinearity Let’s introduce exact collinearity. For example, let’s create a fourth variable disp1 that is a multiple of disp, and then let’s add this variable to the data matrix X. What happens if we try to compute the inverse of \\(\\mathbf{X_{1}^\\mathsf{T} X_1}\\)? disp1 &lt;- 10 * disp X1 &lt;- scale(cbind(disp, disp1, hp, wt)) solve(t(X1) %*% X1) #&gt; Error in solve.default(t(X1) %*% X1): system is computationally singular: reciprocal condition number = 1.55757e-17 As you can tell, R detected perfect collinearity, and the computation of the inverse shows that the matrix \\(\\mathbf{X_{1}^\\mathsf{T} X_1}\\) is singular. Near-Exact Collinearity Let’s introduce near-exact collinearity by creating a new variable disp2 which is almost a clone of disp, and put everything together in a new matrix X2: set.seed(123) disp2 &lt;- disp + rnorm(length(disp)) X2 &lt;- scale(cbind(disp, disp2, hp, wt)) solve(t(X2) %*% X2) #&gt; disp disp2 hp wt #&gt; disp 588.167214 -590.721826 1.01055902 1.99383316 #&gt; disp2 -590.721826 593.525960 -1.10174784 -2.15719062 #&gt; hp 1.010559 -1.101748 0.09032362 0.02220277 #&gt; wt 1.993833 -2.157191 0.02220277 0.16411837 In this case, R was able to invert the matrix \\(\\mathbf{X_{2}^\\mathsf{T} X_2}\\). But if we compare \\((\\mathbf{X^\\mathsf{T} X})^{-1}\\) against \\((\\mathbf{X_{2}^\\mathsf{T} X_2})^{-1}\\), we’ll see that the diagonal entry of the matrices associated to disp are very different. More Extreme Near-Exact Collinearity Let’s make things even more extreme! What about increasing the amount of near-exact collinearity by creating a new variable disp3 which is almost a perfect clone of disp. This time we put everything in a new matrix X3: set.seed(123) disp3 &lt;- disp + rnorm(length(disp), mean = 0, sd = 0.1) X3 &lt;- scale(cbind(disp, disp3, hp, wt)) cor(disp, disp3) #&gt; [1] 0.9999997 And to make things more interesting, let’s create one more matrix, denoted X31, that is a copy of X3 but contains a tiny modification, that in theory, would hardly have any effect: # small changes may have a &quot;butterfly&quot; effect disp31 &lt;- disp3 # change just one observation disp31[1] &lt;- disp3[1] * 1.01 X31 &lt;- scale(cbind(disp, disp31, hp, wt)) cor(disp, disp31) #&gt; [1] 0.9999973 When we calculate the inverse of both \\(\\mathbf{X}_{3}^\\mathsf{T} \\mathbf{X}_{3}\\) and \\(\\mathbf{X}_{31}^\\mathsf{T} \\mathbf{X}_{31}\\), something weird happens: solve(t(X3) %*% X3) #&gt; disp disp3 hp wt #&gt; disp 59175.36325 -59202.97211 10.91501090 21.38646548 #&gt; disp3 -59202.97211 59230.83035 -11.00617104 -21.54976679 #&gt; hp 10.91501 -11.00617 0.09032362 0.02220277 #&gt; wt 21.38647 -21.54977 0.02220277 0.16411837 solve(t(X31) %*% X31) #&gt; disp disp31 hp wt #&gt; disp 5941.5946101 -5942.3977358 0.30661752 0.64947961 #&gt; disp31 -5942.3977358 5943.4373181 -0.39266978 -0.80278577 #&gt; hp 0.3066175 -0.3926698 0.08830442 0.01825147 #&gt; wt 0.6494796 -0.8027858 0.01825147 0.15638642 This is what we may call a “butterfly effect.” By modifying just one cell in \\(\\mathbf{X}_{31}\\) by adding a little amount of random noise, the inverses \\((\\mathbf{X}_{3}^\\mathsf{T} \\mathbf{X}_{3})^{-1}\\) and \\((\\mathbf{X}_{31}^\\mathsf{T} \\mathbf{X}_{31})^{-1}\\) have changed dramatically. Which illustrates some of the issues when dealing with matrices that are not technically singular, but have a large degree of multicollinearity. 12.2 Irregular Coefficients Another interesting thing to look at in linear regression—via OLS—, is the theoretical variance-covariance matrix of the regression coefficients, given by: \\[ Var(\\mathbf{\\boldsymbol{\\hat{\\beta}}}) = \\ \\begin{bmatrix} Var(\\hat{\\beta}_1) &amp; Cov(\\hat{\\beta}_1, \\hat{\\beta}_2) &amp; \\cdots &amp; Cov(\\hat{\\beta}_1, \\hat{\\beta}_p) \\\\ Cov(\\hat{\\beta}_2, \\hat{\\beta}_1) &amp; Var(\\hat{\\beta}_2) &amp; \\cdots &amp; Cov(\\hat{\\beta}_2, \\hat{\\beta}_p) \\\\ \\vdots &amp; &amp; \\ddots &amp; \\vdots \\\\ Cov(\\hat{\\beta}_p, \\hat{\\beta}_1) &amp; Cov(\\hat{\\beta}_p, \\hat{\\beta}_2) &amp; \\cdots &amp; Var(\\hat{\\beta}_p) \\\\ \\end{bmatrix} \\] Doing some algebra, the matrix \\(Var(\\boldsymbol{\\hat{\\beta}})\\) can be compactly expressed in matrix notation as: \\[ Var(\\boldsymbol{\\hat{\\beta}}) = \\sigma^2 (\\mathbf{X^\\mathsf{T} X})^{-1} \\] The variance of a particular coefficient \\(\\hat{\\beta}_j\\) is given by: \\[ Var(\\hat{\\beta}_j) = \\sigma^2 \\left [ (\\mathbf{X^\\mathsf{T} X})^{-1} \\right ]_{jj} \\] where \\(\\left [ (\\mathbf{X^\\mathsf{T} X})^{-1} \\right ]_{jj}\\) is the \\(j\\)-th diagonal element of \\((\\mathbf{X^\\mathsf{T} X})^{-1}\\) A couple of remarks: Recall again that we don’t know \\(\\sigma^2\\). How can we find an estimator \\(\\hat{\\sigma}^2\\)? We don’t observe the error terms \\(\\boldsymbol{\\varepsilon}\\) but we do have the residuals \\(\\mathbf{e = y - \\hat{y}}\\) We also have the Residual Sum of Squares (RSS) \\[ \\text{RSS} = \\sum_{i=1}^{n} e_{i}^{2} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 \\] Effect of Multicollinearity Assuming standardized variables, \\(\\mathbf{X^\\mathsf{T} X} = n \\mathbf{R}\\) It can be shown that: \\[ Var(\\boldsymbol{\\hat{\\beta}}) = \\sigma^2 \\left ( \\frac{\\mathbf{R}^{-1}}{n} \\right ) \\] and \\(Var(\\hat{\\beta}_j)\\) can then be expressed as: \\[ Var(\\hat{\\beta}_j) = \\frac{\\sigma^2}{n} [\\mathbf{R}^{-1}]_{jj} \\] It turns out that: \\[ [\\mathbf{R}^{-1}]_{jj} = \\frac{1}{1 - R_{j}^{2}} \\] is known as the Variance Inflation Factor or VIF. If \\(R_{j}^{2}\\) is close to 1, then VIF will be large, and so \\(Var(\\hat{\\beta})\\) will also be large. If we write the eigenvalue decomposition of \\(\\mathbf{R}\\) as: \\[ \\mathbf{R = V \\boldsymbol{\\Lambda} V^\\mathsf{T}} \\] Let us decompose this expression further. Consider the EVD of \\(\\mathbf{X}^\\mathsf{T} \\mathbf{X}\\): that is, write \\(\\mathbf{X}^\\mathsf{T} \\mathbf{X} = \\mathbf{V \\Lambda} \\mathbf{V}^\\mathsf{T}\\). Substituting this into the variance assumption yields: \\[\\begin{align} (\\mathbf{X}^\\mathsf{T} \\mathbf{X})^{-1} &amp;= (\\mathbf{V \\Lambda} \\mathbf{V}^\\mathsf{T})^{-1} \\\\ &amp;= \\mathbf{V} \\mathbf{\\Lambda}^{-1} \\mathbf{V}^\\mathsf{T} \\\\ &amp;= \\begin{bmatrix} \\mid &amp; \\mid &amp; &amp; \\mid \\\\ \\mathbf{v_1} &amp; \\mathbf{v_2} &amp; \\dots &amp; \\mathbf{v_p} \\\\ \\mid &amp; \\mid &amp; &amp; \\mid \\\\ \\end{bmatrix} \\begin{pmatrix} \\frac{1}{\\lambda_1} &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; \\frac{1}{\\lambda_2} &amp; \\dots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; \\frac{1}{\\lambda_p} \\\\ \\end{pmatrix} \\begin{bmatrix} - \\mathbf{v^\\mathsf{T}_1} - \\\\ - \\mathbf{v^\\mathsf{T}_2} - \\\\ \\vdots \\\\ - \\mathbf{v^\\mathsf{T}_p} - \\\\ \\end{bmatrix} \\end{align}\\] Therefore, the inverse of \\(\\mathbf{R}\\) becomes: \\[ \\mathbf{R^{-1} = V \\boldsymbol{\\Lambda}^{-1} V^\\mathsf{T}} \\] In this way, we obtain a formula for \\(V(\\hat{b}_j)\\) involving the eigenvectors and eigenvalues of \\(\\mathbf{X}^\\mathsf{T} \\mathbf{X}\\) (as well as \\(\\sigma^2\\)): \\[ V(\\hat{b}_j) = \\frac{1}{n} \\left ( \\frac{\\sigma^2}{\\lambda_1} v_{1j}^2 + \\frac{\\sigma^2}{\\lambda_2} v_{2j}^2 + \\dots + \\frac{\\sigma^2}{\\lambda_p} v_{pj}^2 \\right) = \\frac{\\sigma^2}{n} \\sum_{\\ell=1}^{p} \\frac{1}{\\lambda_i} v_{\\ell j}^2 \\] where \\(v_{ij}\\) denotes the \\(i\\)th eigenvalue, \\(j\\)th position. \\[ Var(\\hat{\\beta}_j) = \\left ( \\frac{\\sigma^2}{n} \\right ) \\sum_{l=1}^{p} \\frac{v^{2}_{jl}}{\\lambda_l} \\] The block matrix diagram above is designed to help visualize how we got these \\(v_{ij}^2\\) terms. As you can tell, the variance of the estimators depends on the inverses of the eigenvalues of \\(\\mathbf{R}\\). This formula is useful in interpreting collinearity. If we have perfect collinearity, some of the eigenvalues of \\((\\mathbf{X}^\\mathsf{T} \\mathbf{X})^{-1}\\) would be equal to 0. In the case of near-perfect collinearity, some eigenvalues will be very close to 0. That is, \\(\\frac{1}{\\lambda_i}\\) will be very large. Stated differently: if we have multicollinearity, our OLS coefficients will have large variance. In summary: the standard errors of \\(\\hat{\\beta}_j\\) are inflated. the fit is unstable, and becomes very sensitive to small perturbations. small changes in \\(Y\\) can lead to large changes in the coefficients. What would you do to overcome multicollinearity? Reduce number of predictors If \\(p &gt; n\\), then try to get more observations (increase \\(n\\)) Find an orthogonal basis for the predictors Impose constraints on the estimated coefficients A mix of some or all of the above? Other ideas? 12.3 Connection to Regularization So how all of the previous discussion connects with regularization? Generally speaking, we know that regularization has to do with “making things regular.” So, if you think about it, regularization implies that something is irregular. In the context of regression, what is it that is irregular? What are the things that we are interested in “making regular”? Within linear regression, regularization means reducing the variability—and therefore the size—of the vector of predicted coefficients. What we have seen is that if there is multicollinearity, some of the elements of \\(\\mathbf{b}_{ols}\\) will have high variance. In other words, \\(\\| \\mathbf{b}_{ols} \\|_{\\ell, p}\\) will be very large. When this is the case, we say that the coefficients are irregular. Consequently, regularization involves finding ways to mitigate the effects of these irregular coefficients. Regularization Metaphor Here’s a metaphor that we like to use when talking about regularization and the issue with “irregular” coefficients. As you know, in OLS regression, we want to find parameters that give us a linear combination of our response variables. Figure 12.1: Linear combination without constant term Think of finding the parameters \\(b_1, \\dots, b_p\\) as if you go “shopping” for parameters. We assume (initially) that we have a blank check, and can pick whatever parameters we can buy. No need to worry about about how much they cost. If we are not careful (if there’s high multicollinearity) we could end up spending quite a lot of money. To have some sort of budgeting control, we use regularization. This is like imposing some sort of restriction on what coefficients we can buy. We could use penalized methods or we could also use dimension reduction methods. In penalized methods, we explicitly impose a budget on how much money we can spend when buying parameters. In dimension reduction, there are no explicit penalties, but we still impose a “cost” on the regressors that are highly collinear. We reduce the number of parameters that we can buy. "],
["pcr.html", "13 Principal Components Regression 13.1 Motivation Example 13.2 The PCR Model 13.3 How does PCR work? 13.4 Selecting Number of PCs", " 13 Principal Components Regression The first dimension reduction method that we will describe to regularize a model is Principal Components Regression (PCR). 13.1 Motivation Example To introduce PCR we are going to use a subset of the “2004 New Car and Truck Data” curated by Roger W. Johnson using records from Kiplinger’s Personal Finance. You can find more information about this data in the following url: http://jse.amstat.org/datasets/04cars.txt The data file, cars2004.csv, is available in the following github repository: https://github.com/allmodelsarewrong/data The data set consists of 10 variables measured on 385 cars. Here’s what the first six rows (and ten columns) look like: #&gt; price engine cyl hp city_mpg #&gt; Acura 3.5 RL 4dr 43755 3.5 6 225 18 #&gt; Acura 3.5 RL w/Navigation 4dr 46100 3.5 6 225 18 #&gt; Acura MDX 36945 3.5 6 265 17 #&gt; Acura NSX coupe 2dr manual S 89765 3.2 6 290 17 #&gt; Acura RSX Type S 2dr 23820 2.0 4 200 24 #&gt; Acura TL 4dr 33195 3.2 6 270 20 #&gt; hwy_mpg weight wheel length width #&gt; Acura 3.5 RL 4dr 24 3880 115 197 72 #&gt; Acura 3.5 RL w/Navigation 4dr 24 3893 115 197 72 #&gt; Acura MDX 23 4451 106 189 77 #&gt; Acura NSX coupe 2dr manual S 24 3153 100 174 71 #&gt; Acura RSX Type S 2dr 31 2778 101 172 68 #&gt; Acura TL 4dr 28 3575 108 186 72 In this example we take the variable price as the response, and the rest of the columns as input or predictor variables: engine cyl hp city_mpg hw_mpg weight wheel length width The regression model is: \\[ \\texttt{price} = b_0 + b_1 \\texttt{cyl} + b_2 \\texttt{hp} + \\dots + b_9 \\texttt{width} + \\boldsymbol{\\varepsilon} \\] For exploration purposes, let’s examine the matrix of correlations among all variables: #&gt; engine cyl hp city_mpg hwy_mpg weight wheel length width #&gt; price 0.6 0.654 0.836 -0.485 -0.469 0.476 0.204 0.210 0.314 #&gt; engine 0.912 0.778 -0.706 -0.708 0.812 0.631 0.624 0.727 #&gt; cyl 0.792 -0.670 -0.664 0.731 0.553 0.547 0.621 #&gt; hp -0.672 -0.652 0.631 0.396 0.381 0.500 #&gt; city_mpg 0.941 -0.736 -0.481 -0.468 -0.590 #&gt; hwy_mpg -0.789 -0.455 -0.390 -0.585 #&gt; weight 0.751 0.653 0.808 #&gt; wheel 0.867 0.760 #&gt; length 0.752 And let’s also take a look at the circle of correlations, from the output of a PCA on the entire data set: #&gt; Error in text.default(cars_pca$var$coord[1, 1], cars_pca$var$coord[1, : plot.new has not been called yet Computing the OLS solution for the regression model of price onto the other nine predictors we obtain: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 32536.025 17777.488 1.8302 6.802e-02 #&gt; engine -3273.053 1542.595 -2.1218 3.451e-02 #&gt; cyl 2520.927 896.202 2.8129 5.168e-03 #&gt; hp 246.595 13.201 18.6797 1.621e-55 #&gt; city_mpg -229.987 332.824 -0.6910 4.900e-01 #&gt; hwy_mpg 979.967 345.558 2.8359 4.817e-03 #&gt; weight 9.937 2.045 4.8584 1.741e-06 #&gt; wheel -695.392 172.896 -4.0220 6.980e-05 #&gt; length 33.690 89.660 0.3758 7.073e-01 #&gt; width -635.382 306.344 -2.0741 3.875e-02 Out of curiosity, let’s compare the correlations and the coefficients: #&gt; correlation coefficient #&gt; engine 0.5997873 -3273.05304 #&gt; cyl 0.6544123 2520.92691 #&gt; hp 0.8360930 246.59496 #&gt; city_mpg -0.4854130 -229.98735 #&gt; hwy_mpg -0.4694315 979.96656 #&gt; weight 0.4760867 9.93652 #&gt; wheel 0.2035464 -695.39157 #&gt; length 0.2096682 33.69009 #&gt; width 0.3135383 -635.38224 As you can tell from the above output, some correlation signs don’t match the signs of their corresponding regression coefficients. For example, engine is positively correlated with price but it turns out to have a negative regression coefficient. Or look at hwy_mpg which is negatively correlated with price but it has a positive regression coefficient. 13.2 The PCR Model In PCR, we seek principal components \\(\\mathbf{z_1}, \\dots, \\mathbf{z_k}\\), linear combinations of the inputs: \\(\\mathbf{z_k} = \\mathbf{Xv_k}\\). Figure 13.1: PCs as linear combinations of input variables If we retain all principal components, then we know that we can factorize the input matrix \\(\\mathbf{X}\\) as: \\[ \\mathbf{X} = \\mathbf{Z V^\\mathsf{T}} \\] where: \\(\\mathbf{Z}\\) is the matrix of principal components \\(\\mathbf{V}\\) is the matrix of loadings If we only keep a subset of \\(k &lt; p\\) PCs, then we have a decomposition of the data matrix into a signal part captured by \\(k\\) components, and a residual or noise part: \\[ \\underset{n \\times p}{\\mathbf{X}} = \\underset{n \\times k}{\\mathbf{Z}} \\hspace{1mm} \\underset{k \\times p}{\\mathbf{V^\\mathsf{T}}} + \\underset{n \\times p}{\\mathbf{E}} \\] Figure 13.2: Matrix diagram for inputs The idea is to use the components \\(\\mathbf{Z}\\) as predictors of \\(\\mathbf{y}\\). More specifically, the idea is to fit a linear regression in order to find coefficients \\(\\mathbf{b}\\): \\[ \\mathbf{y} = \\mathbf{Zb} + \\mathbf{e} \\] Figure 13.3: Matrix diagram for response Usually, you don’t use all \\(p\\) PCs, but just a few of them. In other words, if we only keep a subset of \\(k &lt; p\\) PCs, then the idea of PCR remains constant: use the \\(k\\) components in \\(\\mathbf{Z}\\) as predictors of \\(\\mathbf{y}\\): \\[ \\mathbf{\\hat{y}} = \\mathbf{Z b} \\] Without loss of generality, suppose the predictors and response are standardized. In Principal Components Regression we regress \\(\\mathbf{y}\\) onto the PC’s: \\[ \\mathbf{\\hat{y}} = b_1 \\mathbf{z_1} + b \\mathbf{z_2} + \\dots + b_p \\mathbf{z_k} \\] The vector of PCR coefficients is obtained via ordinary least squares (OLS): \\[ \\mathbf{b} = \\mathbf{(Z^\\mathsf{T} Z)^{-1} Z^\\mathsf{T} y} \\] Using the cars2004 data set, with the PCA of the inputs, we can run a linear regression of price onto all nine PCs: #&gt; Regression coefficients for all PCs #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; PC1 -4470.761 205.1288 -21.7949 0.0000 #&gt; PC2 7608.419 468.4759 16.2408 0.0000 #&gt; PC3 -9650.324 660.1829 -14.6177 0.0000 #&gt; PC4 -1768.547 980.6487 -1.8034 0.0721 #&gt; PC5 10528.146 1115.0825 9.4416 0.0000 #&gt; PC6 -5593.736 1177.6700 -4.7498 0.0000 #&gt; PC7 -5746.452 1721.5725 -3.3379 0.0009 #&gt; PC8 -7606.196 1926.3769 -3.9484 0.0001 #&gt; PC9 5473.090 2660.3834 2.0573 0.0404 If we only take the first two PCs, then the regression coefficients are: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; PC1 -4470.761 205.1288 -21.7949 0 #&gt; PC2 7608.419 468.4759 16.2408 0 Likewise, if take only the first three PCs, then the regression coefficients are: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; PC1 -4470.761 205.1288 -21.7949 0 #&gt; PC2 7608.419 468.4759 16.2408 0 #&gt; PC3 -9650.324 660.1829 -14.6177 0 Because of uncorrelatedness, the contributions and estimated coefficient of a PC are unaffected by which other PCs are also included in the regression. 13.3 How does PCR work? Start with \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times p}\\), assuming standardized data. Then, perform PCA on \\(\\mathbf{X}\\): either an EVD of a multiple of \\(\\mathbf{X^\\mathsf{T} X}\\) (e.g. \\((n-1)^{-1} \\mathbf{X^\\mathsf{T} X}\\)), or the SVD of \\(\\mathbf{X} = \\mathbf{U D V^\\mathsf{T}}\\). In either case, \\(\\mathbf{X} = \\mathbf{Z V^\\mathsf{T}}\\) (the matrix of PC’s times the transpose of the matrix of loadings). From OLS, we have: \\[ \\mathbf{\\hat{y}} = \\mathbf{X (X^\\mathsf{T} X)^{-1} X^\\mathsf{T} y} = \\mathbf{Xb}_\\text{ols} \\] We can replace \\(\\mathbf{X}\\) by \\(\\mathbf{Z V^\\mathsf{T}}\\): \\[\\begin{align*} \\mathbf{\\hat{y}} &amp;= \\mathbf{X (X^\\mathsf{T} X)^{-1} X^\\mathsf{T} y} \\\\ &amp;= \\mathbf{Z V^\\mathsf{T}} \\left (\\mathbf{(Z V^\\mathsf{T})^\\mathsf{T} Z V^\\mathsf{T}} \\right )^{-1} \\mathbf{(Z V^\\mathsf{T})^\\mathsf{T} y} \\\\ &amp;= \\mathbf{Z V^\\mathsf{T}} \\left (\\mathbf{V Z^\\mathsf{T} Z V^\\mathsf{T}} \\right )^{-1} \\mathbf{V Z^\\mathsf{T} y} \\\\ &amp;= \\mathbf{Z V^\\mathsf{T}} (\\mathbf{V \\Lambda V^\\mathsf{T}})^{-1} \\mathbf{V Z^\\mathsf{T} y} \\\\ &amp;= \\mathbf{Z V^\\mathsf{T}} (\\mathbf{V} \\mathbf{\\Lambda}^{-1} \\mathbf{V^\\mathsf{T}}) \\mathbf{V Z^\\mathsf{T} y} \\\\ &amp;= \\mathbf{Z} \\mathbf{\\Lambda}^{-1} \\mathbf{Z^\\mathsf{T} y} \\\\ &amp;= \\mathbf{Zb}_\\text{pcr} \\end{align*}\\] 13.3.1 Transition Formula In PCR, if \\(k = p\\) (and assuming \\(\\mathbf{X}\\) is of full-rank), which means that if you keep all PC’s, what happens is: \\[\\begin{align*} \\mathbf{\\hat{y}} &amp;= \\mathbf{X} \\mathbf{b}_{\\text{ols}} \\\\ &amp;= \\mathbf{Z V^\\mathsf{T}} \\mathbf{b}_{\\text{ols}} \\\\ &amp;= \\mathbf{Z} \\mathbf{b}_{\\text{pcr}} \\end{align*}\\] We can reexpress the PCR coefficients in terms of the original variables. \\[\\begin{align*} \\mathbf{\\hat{y}} &amp;= \\mathbf{Z} \\mathbf{b}_{\\text{pcr}} \\\\ &amp;= \\mathbf{X V} \\mathbf{b}_{\\text{pcr}} \\\\ &amp;= \\mathbf{X} \\mathbf{b}_{\\text{ols}} \\end{align*}\\] In summary, we can go back and forth between regression coefficients for PC scores, and regression coefficients for original input features: \\(\\mathbf{\\hat{y}} = \\mathbf{X} \\mathbf{b}_\\text{ols} = \\mathbf{Z V^\\mathsf{T}} \\mathbf{b}_\\text{ols} = \\mathbf{Z b}_\\text{pcr}\\) \\(\\mathbf{\\hat{y}} = \\mathbf{Z} \\mathbf{b}_\\text{pcr} = \\mathbf{X V} \\mathbf{b}_\\text{pcr} = \\mathbf{X b}_\\text{ols}\\) The following output shows the regression coefficients of all nine regression equations in terms of the original input variables. For example, the values in the first column (column Z_1:1) are the regression coefficients of the \\(X\\) inputs when using the first PC. The values in the second column (Z_1:2) are the coefficients when using PC1 and PC2; and so on. #&gt; Z_1:1 Z_1:2 Z_1:3 Z_1:4 Z_1:5 Z_1:6 Z_1:7 Z_1:8 Z_1:9 #&gt; engine 1641 2345 5487 5203 1887 1064 2168.5 -3551 -3327 #&gt; cyl 1544 2881 7312 7289 2213 1401 -295.5 3856 3766 #&gt; hp 1377 4148 8402 8755 16016 17348 17585.7 17700 17352 #&gt; city_mpg -1487 -3864 459 -149 -470 999 1891.3 2151 -1213 #&gt; hwy_mpg -1472 -4140 583 516 1633 1468 1477.9 1607 5536 #&gt; weight 1642 1261 -510 -1187 -2266 707 3557.7 5507 7033 #&gt; wheel 1385 -2392 -2860 -2422 -3009 -187 -3315.7 -4832 -4938 #&gt; length 1332 -2634 -2202 -1346 -883 -2778 28.2 1260 447 #&gt; width 1501 -738 -1731 -2811 1464 -904 -2413.8 -1978 -2142 Obviously, if you keep all components, you aren’t changing anything: you’re spending the same amount of money as you were in regular least-squares regression. #&gt; engine cyl hp city_mpg hwy_mpg weight wheel #&gt; -3326.7904 3766.1495 17352.2185 -1213.0142 5535.9355 7032.5383 -4937.8106 #&gt; length width #&gt; 446.9752 -2141.8196 compare with the regression coefficients of OLS: #&gt; Xengine Xcyl Xhp Xcity_mpg Xhwy_mpg Xweight Xwheel #&gt; -3326.7904 3766.1495 17352.2185 -1213.0142 5535.9355 7032.5383 -4937.8106 #&gt; Xlength Xwidth #&gt; 446.9752 -2141.8196 The idea is to keep only a few components. Hence, the goal is to find \\(k\\) principal components (with \\(k \\ll p\\); \\(k\\) is called the tuning parameter or the hyperparameter). How do we determine \\(k\\)? The typical way is to use cross-validation. 13.3.2 Size of Coefficients Let’s look at the evolution of the PCA regression coefficients. This is a very interesting plot that allows us to see how the size of the coefficients grow as we add more and more PCs into the regression equation: 13.4 Selecting Number of PCs The number \\(q\\) of PCs to use in PC Regression is a hyperparameter or tuning parameter. This means that we cannot derive an analytical expression that tells us what the number \\(q\\) of PCs is the optimal to be used. So how do we find \\(q\\)? We find \\(q\\) through resampling methods; the most popular resampling technique that most practioners apply is \\(K\\)-fold cross-validation. Here’s a description of the steps to be carried out: Assumet that we have a training data set consisting of \\(n\\) data points: \\(\\mathcal{D}_{train} = (\\mathbf{x_1}, y_1), \\dots, (\\mathbf{x_n}, y_n)\\). Using \\(K\\)-fold cross-validation, we (randomly) split the data into \\(K\\) folds: \\[ \\mathcal{D}_{train} = \\mathcal{D}_{fold-1} \\cup \\mathcal{D}_{fold-2} \\dots \\cup \\mathcal{D}_{fold-K} \\] Each fold set \\(\\mathcal{D}_{fold-k}\\) will play the role of an evaluation set \\(\\mathcal{D}_{eval-k}\\). Having defined the \\(k\\) fold sets, we form the corresponding \\(K\\) retraining sets: \\(\\mathcal{D}_{train-1} = \\mathcal{D}_{train} \\setminus \\mathcal{D}_{fold-1}\\) \\(\\mathcal{D}_{train-2} = \\mathcal{D}_{train} \\setminus \\mathcal{D}_{fold-2}\\) \\(\\dots\\) \\(\\mathcal{D}_{train-K} = \\mathcal{D}_{train} \\setminus \\mathcal{D}_{fold-K}\\) The cross-validation procedure then repeats the following loops: For \\(q = 1, 2, \\dots, r = rank(\\mathbf{X})\\) For \\(k = 1, \\dots, K\\) fit PCR model \\(h_{q,k}\\) with \\(q\\)-PCs on \\(\\mathcal{D}_{train-k}\\) compute and store \\(E_{eval-k} (h_{q,k})\\) using \\(\\mathcal{D}_{eval-k}\\) end for \\(k\\) compute and store \\(E_{cv_{q}} = \\frac{1}{K} \\sum_k E_{eval-k}(h_{q,k})\\) end for \\(q\\) Compare all cross-validation errors \\(E_{cv_1}, E_{cv_2}, \\dots, E_{cv_r}\\) and choose the smallest of them, say \\(E_{cv_{q^*}}\\) Use \\(q^*\\) PCs to fit the (finalist) PCR model: \\(\\mathbf{\\hat{y}} = b_1 \\mathbf{z_1} + b_2 \\mathbf{z_2} + \\dots + b_q^* \\mathbf{z_q^*} = \\mathbf{Z_{1:q^*}} \\mathbf{b_{q^*}}\\) Remember that we can reexpress the PCR model in terms of the original predictors: \\(\\mathbf{\\hat{y}} = (\\mathbf{XV_{1:q^*}}) \\mathbf{b_{q^*}}\\) Remarks The catch: those PC’s you choose to keep may not be good predictors of \\(\\mathbf{y}\\). That is, there is a chance that some of those PC’s you discarded actually capture a fair amount of the signal. Unfortunately, there is no way of knowing this in a real-life setting. Partial Least Squares was developed as a cure for this, which is the topic of the next chapter. "],
["pls.html", "14 Partial Least Squares Regression 14.1 Motivation Example 14.2 The PLSR Model 14.3 How does PLSR work? 14.4 PLSR Algorithm 14.5 Selecting Number of PLS Components", " 14 Partial Least Squares Regression Another dimension reduction method that we can use to regularize a model is Partial Least Squares Regression (PLSR). Before we dive deep into the nuts and bolts of PLSR, we should let you know that PLS methods form a very big family of methods. While the regression method is probably the most popular PLS technique, it is by no means the only one. And even within PLSR, there’s a wide array of different algorithms that let you obtain the core solution. PLS Regression was mainly developed in the early 1980s by Scandinavian chemometricians Svante Wold and Harald Martens. The theoretical background was based on the PLS Modeling framework of Herman Wold (Svante’s father). PLS Regression was developed as an algorithmic solution, with no optimization criterion explicitly defined. It was introduced in the fields of chemometrics (where almost immediately became a hit). It slowly attracted the attention of curious applied statisticians. It took a couple of years (late 1980s - early 1990s) for applied mathematicians to discover its properties. Nowadays there are several versions (flavors) of the algorithm to compute a PLS regression. 14.1 Motivation Example To introduce PLSR we are using the same data set of the previous chapter: a subset of the “2004 New Car and Truck Data”. The data set consists of 10 variables measured on 385 cars. Here’s what the first six rows look like: #&gt; price engine cyl hp city_mpg #&gt; Acura 3.5 RL 4dr 43755 3.5 6 225 18 #&gt; Acura 3.5 RL w/Navigation 4dr 46100 3.5 6 225 18 #&gt; Acura MDX 36945 3.5 6 265 17 #&gt; Acura NSX coupe 2dr manual S 89765 3.2 6 290 17 #&gt; Acura RSX Type S 2dr 23820 2.0 4 200 24 #&gt; Acura TL 4dr 33195 3.2 6 270 20 #&gt; hwy_mpg weight wheel length width #&gt; Acura 3.5 RL 4dr 24 3880 115 197 72 #&gt; Acura 3.5 RL w/Navigation 4dr 24 3893 115 197 72 #&gt; Acura MDX 23 4451 106 189 77 #&gt; Acura NSX coupe 2dr manual S 24 3153 100 174 71 #&gt; Acura RSX Type S 2dr 31 2778 101 172 68 #&gt; Acura TL 4dr 28 3575 108 186 72 The response variable is price, and there are nine predictors: engine cyl hp city_mpg hw_mpg weight wheel length width The regression model involves predicting price in terms of the nine inputs: \\(\\texttt{price} = b_0 + b_1 \\texttt{cyl} + b_2 \\texttt{hp} + \\dots + b_9 \\texttt{width} + \\boldsymbol{\\varepsilon}\\) Computing the OLS solution for the regression model of price onto the other nine predictors we obtain the following coefficients (see first column): #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 32536.025 17777.488 1.8302 6.802e-02 #&gt; engine -3273.053 1542.595 -2.1218 3.451e-02 #&gt; cyl 2520.927 896.202 2.8129 5.168e-03 #&gt; hp 246.595 13.201 18.6797 1.621e-55 #&gt; city_mpg -229.987 332.824 -0.6910 4.900e-01 #&gt; hwy_mpg 979.967 345.558 2.8359 4.817e-03 #&gt; weight 9.937 2.045 4.8584 1.741e-06 #&gt; wheel -695.392 172.896 -4.0220 6.980e-05 #&gt; length 33.690 89.660 0.3758 7.073e-01 #&gt; width -635.382 306.344 -2.0741 3.875e-02 14.2 The PLSR Model In PLS regression, like in PCR, we seek components \\(\\mathbf{z_1}, \\dots, \\mathbf{z_k}\\), linear combinations of the inputs (e.g. \\(\\mathbf{z_1} = \\mathbf{Xw_1}\\)), such that they are good predictors for both the response \\(\\mathbf{y}\\) as well as the inputs \\(\\mathbf{x_j}\\) for \\(j = 1,\\dots, p\\). Moreover, there is an implicit assumption in the PLS regression model: both \\(\\mathbf{X}\\) and \\(\\mathbf{y}\\) are assumed to be functions of a reduced (\\(k &lt; p\\)) number of components \\(\\mathbf{Z} = [\\mathbf{z_1}, \\dots, \\mathbf{z_k}]\\) that can be used to decompose the inputs and the response: \\[ \\mathbf{X} = \\mathbf{Z V^\\mathsf{T}} + \\mathbf{E} \\] Figure 14.1: Matrix diagram for inputs and \\[ \\mathbf{y} = \\mathbf{Z b} + \\mathbf{e} \\] Figure 14.2: Matrix diagram for response where: \\(\\mathbf{Z}\\) is a matrix of PLS components \\(\\mathbf{V}\\) is a matrix of PLS loadings \\(\\mathbf{E}\\) is a matrix of \\(X\\)-residuals \\(\\mathbf{b}\\) is a vector of PLS regression coefficients \\(\\mathbf{e}\\) is a vector of \\(y\\)-residuals By taking a few number of components \\(\\mathbf{Z} = [\\mathbf{z_1}, \\dots, \\mathbf{z_k}]\\), we can have approximations for the inputs and the response. The model for the \\(X\\)-space is: \\[ \\mathbf{\\hat{X}} = \\mathbf{Z V^{\\mathsf{T}}} \\] In turn, the prediction model for \\(\\mathbf{y}\\) is given by: \\[ \\mathbf{\\hat{y}} = \\mathbf{Z b} \\] To avoid confusing the PLSR coefficients with the OLS coefficients, we can be more proper and rewirte the previous expression as: \\[ \\mathbf{\\hat{y}} = \\mathbf{Z} \\mathbf{b}_{\\text{pls}} \\] to indicate that the obtained \\(b\\)-coefficients are those associated to the PLS components. 14.3 How does PLSR work? So how do we obtain the PLS components, aka PLS scores? We use an iterative algorithm, obtaining one component at a time. Here’s how to get the first PLS component \\(\\mathbf{z_1}\\). Assume that both the inputs and the response are mean-centered (and possibly standardized). We compute the covariances between all the input variables and the response: \\(\\mathbf{\\tilde{w}_1} = (cov(\\mathbf{x_1}, \\mathbf{y}), \\dots, cov(\\mathbf{x_p}, \\mathbf{y}))\\). Figure 14.3: Towards the first PLS component. In vector-matrix notation we have: \\[ \\mathbf{\\tilde{w}_1} = \\mathbf{X^\\mathsf{T}y} \\] For convenience purposes, we normalize the vector of covariances \\(\\mathbf{\\tilde{w}_1}\\), which gives us a unit-vector \\(\\mathbf{w_1}\\): \\[ \\mathbf{w_1} = \\frac{\\mathbf{\\tilde{w}_1}}{\\|\\mathbf{\\tilde{w}_1}\\|} \\] #&gt; first PLS weight #&gt; [,1] #&gt; engine 0.001782118 #&gt; cyl 0.002857956 #&gt; hp 0.171985612 #&gt; city_mpg -0.007484109 #&gt; hwy_mpg -0.007752089 #&gt; weight 0.984987298 #&gt; wheel 0.004225081 #&gt; length 0.008131684 #&gt; width 0.003089621 We use these weights to compute the first component \\(\\mathbf{z_1}\\) as a linear combination of the inputs: \\(\\mathbf{z_1} = w_{11} \\mathbf{x_1} + \\dots + w_{p1} \\mathbf{x_p}\\), Figure 14.4: First PLS component. or in vector-matrix notation: \\[ \\mathbf{z_1} = \\mathbf{Xw_1} \\] Here are the first 10 elements of \\(\\mathbf{z_1}\\) #&gt; first PLS component (10 elements only) #&gt; [,1] #&gt; Acura 3.5 RL 4dr 344.24572 #&gt; Acura 3.5 RL w/Navigation 4dr 357.05055 #&gt; Acura MDX 913.48050 #&gt; Acura NSX coupe 2dr manual S -360.90753 #&gt; Acura RSX Type S 2dr -745.89228 #&gt; Acura TL 4dr 51.39841 #&gt; Acura TSX 4dr -300.53740 #&gt; Audi A4 1.8T 4dr -284.07748 #&gt; Audi A4 3.0 4dr -68.58479 #&gt; Audi A4 3.0 convertible 2dr 278.15085 We use this component to regress both the inputs and the response onto it. The regression coefficient \\(v_{1j}\\) of each simple linear regression between \\(\\mathbf{z_1}\\) and the \\(j\\)-th predictor is called a PLS loading. The entire vector of loadings \\(\\mathbf{v_1}\\) associated to first component is: \\[ \\mathbf{v_1} = \\mathbf{X^\\mathsf{T}z_1} / \\mathbf{z_{1}^\\mathsf{T}z_1} \\] #&gt; first PLS loading #&gt; [,1] #&gt; engine 0.001176718 #&gt; cyl 0.001561745 #&gt; hp 0.064016991 #&gt; city_mpg -0.005536001 #&gt; hwy_mpg -0.006343509 #&gt; weight 1.003819205 #&gt; wheel 0.007551534 #&gt; length 0.012276141 #&gt; width 0.003862309 In turn, the PLS regression coefficient \\(b_1\\) is obtained by regressing the response onto the first component: \\[ b_1 = \\mathbf{y^\\mathsf{T} z_1} / \\mathbf{z_{1}^\\mathsf{T}z_1} \\] In our example, the regression coefficient \\(b_1\\) of the first PLS score \\(\\mathbf{z_1}\\) is: #&gt; first PLS regression coefficient #&gt; [1] 13.61137 We can get a first one-rank approximation \\(\\mathbf{\\hat{X}} = \\mathbf{z_1 v^{\\mathsf{T}}_1}\\), and then obtain a residual matrix \\(\\mathbf{X_1}\\) by removing the variation in the inputs captured by \\(\\mathbf{z_1}\\): \\[ \\mathbf{X_1} = \\mathbf{X} - \\mathbf{\\hat{X}} = \\mathbf{X} - \\mathbf{z_1 v^{\\mathsf{T}}_1} \\qquad \\mathsf{(deflation)} \\] We can also deflate the response: \\[ \\mathbf{y_1} = \\mathbf{y} - b_1 \\mathbf{z_1} \\] We can obtain further PLS components \\(\\mathbf{z_2}\\), \\(\\mathbf{z_3}\\), etc. by repeating the process described above on the residual data matrices \\(\\mathbf{X_1}\\), \\(\\mathbf{X_2}\\), etc., and the residual response vectors \\(\\mathbf{y_1}\\), \\(\\mathbf{y_2}\\), etc. For example, a second PLS score \\(\\mathbf{z_2}\\) will be formed by a linear combination of first \\(X\\)-residuals: Figure 14.5: Second PLS component. 14.4 PLSR Algorithm We will describe what we consider the “standard” or “classic” algorithm. However, keep in mind that there are a handful of slightly different versions that may find in other places. We assume mean-centered and standardized variables \\(\\mathbf{X}\\) and \\(\\mathbf{y}\\). We start by setting \\(\\mathbf{X_0} = \\mathbf{X}\\), and \\(\\mathbf{y_0} = \\mathbf{y}\\). Repeat for \\(h = 1, \\dots, r = \\text{rank}(\\mathbf{X})\\): Start with weights \\(\\mathbf{\\tilde{w}_h} = \\mathbf{X_{h-1}^\\mathsf{T}} \\mathbf{y_{h-1}} / \\mathbf{y_{h-1}}^\\mathsf{T} \\mathbf{y_{h-1}}\\) Normalize weights: \\(\\mathbf{w_h} = \\mathbf{\\tilde{w}_h} / \\| \\mathbf{\\tilde{w}_h} \\|\\) Compute PLS component: \\(\\mathbf{z_h} = \\mathbf{X_{h-1} w_h} / \\mathbf{w_{h}^\\mathsf{T} w_h}\\) Regress \\(\\mathbf{y_h}\\) onto \\(\\mathbf{z_h}\\): \\(b_h = \\mathbf{y_{h}^{\\mathsf{T}} z_h / z_{h}^{\\mathsf{T}} z_h}\\) Regress \\(\\mathbf{x_j}\\) onto \\(\\mathbf{z_h}\\): \\(\\mathbf{v_h} = \\mathbf{X_{h-1}}^{\\mathsf{T}} \\mathbf{z_h} / \\mathbf{z_{h}}^{\\mathsf{T}} \\mathbf{z_h}\\) Deflate (residual) predictors: \\(\\mathbf{X_h} = \\mathbf{X_{h-1}} - \\mathbf{z_h} \\mathbf{v_{h}^{\\mathsf{T}}}\\) Deflate (residual) response: \\(\\mathbf{y_h} = \\mathbf{y_{h-1}} - b_h \\mathbf{z_h}\\) Notice that steps 1, 3, 4, and 5 involve simple OLS regressions. If you think about it, what PLS is doing is calculating all the different ingredients (e.g. \\(\\mathbf{w_h}, \\mathbf{z_h}, \\mathbf{v_h}, b_h\\)) separately, using least squares regressions. Hence the reason for its name partial least squares. By default, we can obtain up to \\(r = rank(\\mathbf{X})\\) different PLS components. In the cars example, we can actually obtain nine scores. The following output shows the regression coefficients of all nine regression equations: #&gt; Z_1:1 Z_1:2 Z_1:3 Z_1:4 Z_1:5 Z_1:6 Z_1:7 Z_1:8 #&gt; engine 0.0243 1.44 -3.34 -15.09 -33.59 -113.70 -284.84 -1148.41 #&gt; cyl 0.0389 3.03 6.87 55.93 166.51 471.47 1056.23 2073.22 #&gt; hp 2.3410 250.04 248.74 262.63 254.81 251.35 243.73 238.81 #&gt; city_mpg -0.1019 -4.69 50.66 368.94 210.79 -69.52 -412.43 -171.42 #&gt; hwy_mpg -0.1055 -3.49 48.59 464.50 528.56 811.07 1177.28 933.19 #&gt; weight 13.4070 -2.27 1.80 6.44 8.21 9.61 9.77 9.08 #&gt; wheel 0.0575 -7.32 -125.75 -387.68 -797.42 -669.88 -680.47 -676.98 #&gt; length 0.1107 -9.00 -196.71 -90.85 83.57 59.30 2.26 17.07 #&gt; width 0.0421 -1.61 -43.73 -181.27 -427.09 -940.70 -729.49 -725.37 #&gt; Z_1:9 #&gt; engine -3273.05 #&gt; cyl 2520.93 #&gt; hp 246.59 #&gt; city_mpg -229.99 #&gt; hwy_mpg 979.97 #&gt; weight 9.94 #&gt; wheel -695.39 #&gt; length 33.69 #&gt; width -635.38 If we keep all components, we get the OLS solution (using standardized data): #&gt; Regression coefficients using all 9 PLS components #&gt; engine cyl hp city_mpg hwy_mpg weight #&gt; -3273.05304 2520.92691 246.59496 -229.98735 979.96656 9.93652 #&gt; wheel length width #&gt; -695.39157 33.69009 -635.38224 Compare with the regression coefficients of OLS: #&gt; Xengine Xcyl Xhp Xcity_mpg Xhwy_mpg Xweight Xwheel #&gt; -3326.7904 3766.1495 17352.2185 -1213.0142 5535.9355 7032.5383 -4937.8106 #&gt; Xlength Xwidth #&gt; 446.9752 -2141.8196 Obviously, reatining all PLS scores does not provide an improvement over the OLS regression. This is because we are not changing anything. Using our metaphor about “shopping for coefficients”, by keeping all PLS scores we are spending the same amount of money that we spend in ordinary least-squares regression. The dimension reduction idea involves keeping only a few components \\(k \\ll r\\), such that the fitted model has a better generalization ability than the full model. The number of components \\(k\\) is called the tuning parameter or hyperparameter. How do we determine \\(k\\)? The typical way is to use cross-validation. 14.4.1 PLS Solution with original variables The PLS regression equation is typically expressed in terms of the original variables \\(\\mathbf{X}\\) instead of using the deflated matrices \\(\\mathbf{X_{h-1}}\\). This re-arrangement is more convenient for prediction purposes. \\[ \\mathbf{y = X b}_{\\text{pls}} + \\mathbf{e} \\] Note that the PLS \\(\\mathbf{b}_{\\text{pls}}\\)-coefficients of the regression equation are not parameters of the PLS regression model. Instead, these are post-hoc calculations for making things more maneagable. Transforming Regression Coefficients PLS components are obtained as linear combinations of residual matrices \\(\\mathbf{X_h}\\), but they can also be expressed in terms of the original variables: \\[\\begin{align*} \\mathbf{z_1} &amp;= \\mathbf{X w_1 = X \\overset{*}{w}_{1}} \\\\ \\mathbf{z_2} &amp;= \\mathbf{X_1 w_2 = X (I - z_1 v_{1}^{\\mathsf{T}}) w_2 = X \\overset{*}{w}_{2}} \\\\ \\mathbf{z_3} &amp;= \\mathbf{X_2 w_3 = X (I - z_2 v_{2}^{\\mathsf{T}}) w_3 = X \\overset{*}{w}_{3}} \\\\ &amp; \\vdots \\\\ \\mathbf{z_h} &amp;= \\mathbf{X_{h-1} w_h = X (I - z_{h-1} v_{h-1}^{\\mathsf{T}}) w_h = X \\overset{*}{w}_{h}} \\\\ \\end{align*}\\] Consequently, \\(\\mathbf{Z_h} = [\\mathbf{z_1}, \\mathbf{z_2}, \\dots, \\mathbf{z_h}] = \\mathbf{X \\overset{*}{W}_{h}}\\) We know that \\(\\mathbf{\\hat{y}} = b_1 \\mathbf{z_1} + \\dots + b_h \\mathbf{z_h} = \\mathbf{Z b}\\). And because the PLS components can be expressed in terms of the original variables, we can conveniently reexpress the solution in terms of the original predictors: \\[ \\mathbf{\\hat{y}} = \\mathbf{Z b} = \\mathbf{X \\overset{*}{W} b} = \\mathbf{X \\overset{*}{b}}_\\text{pls} \\] where \\(\\overset{*}{\\mathbf{b}}_\\text{pls}\\) are the derived PLS-coefficients (not OLS). Note that these PLS star-coefficients of the regression equation are NOT parameters of the PLS regression model. Instead, these are post-hoc calculations for making things more maneagable. Figure 14.6: PLS components in terms of original inputs. Interestingly, if you retain all \\(h = rank(\\mathbf{X})\\) PLS components, the \\(\\overset{*}{\\mathbf{b}}_\\text{pls}\\) coefficients will be equal to the OLS coefficients. 14.4.2 Size of Coefficients Let’s look at the evolution of the PLS regression coefficients. This is a very interesting plot that allows us to see how the size of the coefficients grow as we add more and more PLS components into the regression equation: 14.4.3 Some Properties Some interesting properties of the different elements derived in PLS Regression: \\(\\mathbf{z_h^{\\mathsf{T}} z_l} = 0, \\quad l &gt; h\\) \\(\\mathbf{w_h^{\\mathsf{T}} p_h} = 1\\) \\(\\mathbf{w_h^{\\mathsf{T}} X_{l}^{\\mathsf{T}}} = 0, \\quad l \\geq h\\) \\(\\mathbf{w_h^{\\mathsf{T}} p_l} = 0, \\quad l &gt; h\\) \\(\\mathbf{w_h^{\\mathsf{T}} w_l} = 0, \\quad l &gt; h\\) \\(\\mathbf{z_h^{\\mathsf{T}} X_l} = 0, \\quad l \\geq h\\) \\(\\mathbf{X_h} = \\mathbf{X} \\prod_{j=1}^{p} (\\mathbf{I - w_j v_{j}^{\\mathsf{T}}}), \\quad h \\geq 1\\) 14.5 Selecting Number of PLS Components The number \\(q\\) of PLS components to use in PLS Regression is a hyperparameter or tuning parameter. This means that we cannot derive an analytical expression that tells us what the number \\(q\\) of PCs is the optimal to be used. So how do we find \\(q\\)? We find \\(q\\) through resampling methods; the most popular resampling technique being \\(K\\)-fold cross-validation. Here’s a description of the steps to be carried out: Assumet that we have a training data set consisting of \\(n\\) data points: \\(\\mathcal{D}_{train} = (\\mathbf{x_1}, y_1), \\dots, (\\mathbf{x_n}, y_n)\\). Using \\(K\\)-fold cross-validation, we (randomly) split the data into \\(K\\) folds: \\[ \\mathcal{D}_{train} = \\mathcal{D}_{fold-1} \\cup \\mathcal{D}_{fold-2} \\dots \\cup \\mathcal{D}_{fold-K} \\] Each fold set \\(\\mathcal{D}_{fold-k}\\) will play the role of an evaluation set \\(\\mathcal{D}_{eval-k}\\). Having defined the \\(k\\) fold sets, we form the corresponding \\(K\\) retraining sets: \\(\\mathcal{D}_{train-1} = \\mathcal{D}_{train} \\setminus \\mathcal{D}_{fold-1}\\) \\(\\mathcal{D}_{train-2} = \\mathcal{D}_{train} \\setminus \\mathcal{D}_{fold-2}\\) \\(\\dots\\) \\(\\mathcal{D}_{train-K} = \\mathcal{D}_{train} \\setminus \\mathcal{D}_{fold-K}\\) The cross-validation procedure then repeats the following loops: For \\(q = 1, 2, \\dots, r = rank(\\mathbf{X})\\) For \\(k = 1, \\dots, K\\) fit PLSR model \\(h_{q,k}\\) with \\(q\\) PLS-scores on \\(\\mathcal{D}_{train-k}\\) compute and store \\(E_{eval-k} (h_{q,k})\\) using \\(\\mathcal{D}_{eval-k}\\) end for \\(k\\) compute and store \\(E_{cv_{q}} = \\frac{1}{K} \\sum_k E_{eval-k}(h_{q,k})\\) end for \\(q\\) Compare all cross-validation errors \\(E_{cv_1}, E_{cv_2}, \\dots, E_{cv_r}\\) and choose the smallest of them, say \\(E_{cv_{q^*}}\\) Use \\(q^*\\) PLS scores to fit the (finalist) PLSR model: \\(\\mathbf{\\hat{y}} = b_1 \\mathbf{z_1} + b_2 \\mathbf{z_2} + \\dots + b_q^* \\mathbf{z_q^*} = \\mathbf{Z_{1:q^*}} \\mathbf{b_{q^*}}\\) Remember that we can reexpress the PLSR model in terms of the original predictors: \\(\\mathbf{\\hat{y}} = (\\mathbf{X} \\mathbf{\\overset{*}{W}_{1:q^*}}) \\mathbf{b_{q^*}}\\) Remarks PLS regression is somewhat close to Principal Components regression (PCR). Like PCR, PLSR involves projecting the response onto uncorrelated components (i.e. linear combinations of predictors). Unlike PCR, the way PLS components are extracted is by taking into account the response variable. We can conveniently reexpress the solution in terms of the original predictors. PLSR is not based on any optimization criterion. Rather it is based on an interative algorithm (which converges) Simplicity in its algorithm: no need to invert any matrix, no need to diagonalize any matrix. All you need to do is compute simple regressions. In other words, you just need inner products. Missing data is allowed (but you need to modify the algorithm). Easily extendable to the multivariate case of various responses. Handles cases where we have more predictors than observations (\\(p \\gg n\\)). "],
["ridge.html", "15 Ridge Regression 15.1 A New Minimization Problem 15.2 A New Minimization Solution 15.3 What does RR accomplish?", " 15 Ridge Regression From ordinary least squares regression, we know that the predicted response is given by: \\[ \\mathbf{\\hat{y}} = \\mathbf{X} (\\mathbf{X^\\mathsf{T} X})^{-1} \\mathbf{X^\\mathsf{T} y} \\tag{15.1} \\] (provided the inverse exists). We also know that if \\(\\mathbf{X^\\mathsf{T} X}\\) is near-singular, finding its inverse could lead to trouble. For example, near-perfect multicollinearity corresponds to small eigenvalues of \\(\\mathbf{X^\\mathsf{T} X}\\), which will cause estimated regression coefficients to have large variance. Hence, a natural idea arises: let’s try to modify \\(\\mathbf{X^\\mathsf{T} X}\\) to avoid this danger of near-singularity. Specifically, let us add a constant positive term \\(\\lambda\\) to the diagonal of \\(\\mathbf{X^\\mathsf{T} X}\\), here’s how: \\[ \\mathbf{X^\\mathsf{T} X} + \\lambda \\mathbf{I} \\tag{15.2} \\] Note that \\(\\lambda\\) is not an eigenvalue here; it is simply a positive constant. In what way the addition of \\(\\lambda &gt; 0\\) to the diagonal of \\(\\mathbf{X^\\mathsf{T} X}\\) changes the structure of this matrix? By properties of eigenstructures, it turns out that \\(\\mathbf{X^\\mathsf{T} X} + \\lambda \\mathbf{I}\\) and \\(\\mathbf{X^\\mathsf{T} X}\\) have the same eigenvectors; however, the eigenvalues of the former matrix will never be \\(0\\). Specifically, say \\(\\mathbf{X^\\mathsf{T} X}\\) has eigenvalues \\(\\lambda_1, \\lambda_2, \\dots, \\lambda_p\\). The eigenvalues of \\(\\mathbf{X^\\mathsf{T} X} + \\lambda \\mathbf{I}\\) will therefore be \\(\\overset{*}{\\lambda_1}, \\overset{*}{\\lambda_2}, \\dots, \\overset{*}{\\lambda_p}\\) where \\(\\overset{*}{\\lambda_j} = \\lambda_j + \\lambda\\). Another interesting aspect is that there is actually a minimization problem behind this method (of appending an extra term to the eigenvalues). In Ridge Regression, we have that the predicted output is: \\[ \\mathbf{\\hat{y}}_{RR} = \\mathbf{X} (\\mathbf{X^\\mathsf{T} X} + \\lambda \\mathbf{I} )^{-1} \\mathbf{X^\\mathsf{T} y} \\tag{15.3} \\] To understand where this estimator comes from, let’s consider the geometry of the MSE function in a regression context. 15.1 A New Minimization Problem As you know, in linear regression the overall error function \\(E_{in}\\) that we are interested in minimizing is the mean squared error (\\(\\text{MSE}\\)). Moreover, we can look at the geometry of such error measure from the perspective of the parameters (i.e. the regression coefficients). From this perspective, we denote this error function as \\(E_{in}(\\mathbf{b})\\), making explicit its dependency on the vector of coefficients \\(\\mathbf{b}\\). For illustration purposes, let’s consider two inputs \\(X_1\\) and \\(X_2\\), and their corresponding parameters \\(b_1\\) and \\(b_2\\). The error function \\(E_{in}(\\mathbf{b})\\) will generate a convex error surface with the shape of a bowl, or paraboloid (see figure below). Figure 15.1: Error Surface As usual we can express MSE for \\(E_{in}(\\mathbf{b})\\) as follows: \\[ E_{in}(\\mathbf{b}) = \\frac{1}{n} (\\mathbf{Xb} - \\mathbf{y})^{\\mathsf{T}} (\\mathbf{Xb} - \\mathbf{y}) \\tag{15.4} \\] In OLS, we minimize \\(E(\\mathbf{b})\\) unconditionally; that is, without any type of restriction. The associated solution, is indicated with a blue dot at the center of the elliptical contours of constant error. Figure 15.2: Contour Errors with OLS solution 15.1.1 Constraining Regression Coefficients An alternative approach to minimize \\(E_{in}(\\mathbf{b})\\) unconditionally is to impose a restriction on the squared magnitude of the regression coefficients. In other words, we still want to minimize the mean squared error \\(E_{in}(\\mathbf{b})\\) but now we don’t let \\(\\mathbf{b}\\) take any kind of values. Instead, we are going to require the following condition on \\(b_1, \\dots, b_p\\): \\[ \\textsf{constraining coefficients:} \\quad \\sum_{j=1}^{p} b_{j}^2 \\leq c \\] Here’s an easy way to think about—and remember—this condition: think of \\(c\\) as a budget for how much you can spend on the size of all coefficients \\(b_1, \\dots, b_p\\) (or their squared values to be more precise). Adding this restriction to the MSE, we now have the following constrained minimization of \\(E_{in}(\\mathbf{b})\\): \\[ \\min_{\\mathbf{b}} \\left\\{ \\frac{1}{n} (\\mathbf{Xb} - \\mathbf{y})^{\\mathsf{T}} (\\mathbf{Xb} - \\mathbf{y}) \\right\\} \\quad \\mathrm{s.t.} \\quad \\| \\mathbf{b} \\|_{2}^{2} = \\mathbf{b^\\mathsf{T} b} \\leq c \\tag{15.5} \\] for some “budget” \\(c\\). On the \\(b_1, b_2\\) plane, what is the locus of points such that \\(\\mathbf{b^\\mathsf{T}b} \\leq c\\)? The answer is a disk of radius \\(c\\), centered at the origin. Hence, sketching the level curves (curves of constant errors) as well as this restriction, we obtain the following picture (for some value of \\(c\\)): Figure 15.3: Error Contours with Constraint Depending on the chosen value for \\(c\\), we could end up with a big enough constraint that includes the OLS solution, like in the following picture. As you would expect, in this case the solution with the restriction is not constraining anything whatsoever. Figure 15.4: Error contours with a large budget Of course, we could make our budget stricter by reducing the value of \\(c\\) we choose, something depicted in the next figure. Figure 15.5: Error Contours with small budget 15.2 A New Minimization Solution Let’s consider one generic elliptical contour of constant error, a given budget \\(c\\), and a point \\(\\mathbf{b}\\) satisfying the budget constraint, like in the picture below. Figure 15.6: Generic error contour with candidate point As you can tell from the above diagram, the candidate point \\(\\mathbf{b}\\) is such that \\(\\mathbf{b^\\mathsf{T} b} = c\\). However, this point is not fully minimizing \\(E(\\mathbf{b})\\), given the constraint. Visually speaking, we could find other candidate values for \\(\\mathbf{b}\\) along the red circumference that would give us smaller \\(E(\\mathbf{b})\\). How do we know that the shown candidate point \\(\\mathbf{b}\\) can be improved? To answer this question we should visualize the gradient \\(\\nabla E(\\mathbf{b})\\) which, as we know, will be pointing in the direction orthogonal to the contour ellipse—i.e. direction of largest change of \\(E(\\mathbf{b})\\). In addition, we should also pay attention to the direction of the candidate point, illustrated with the red vector. Notice that this vector is normal (i.e. orthogonal) to the circumference of the constraint. Figure 15.7: Gradient of error surface associated to candidate point Notice also that the angle between the normal vector and the gradient is less than 180 degrees. This is an indication that we can find better \\(\\mathbf{b}\\) points that make the error smaller. But is there an optimal \\(\\mathbf{b^*}\\) that achieves the smallest \\(E(\\mathbf{b})\\) while satisfying the budget constraint? With this visualization, we get the intuition that \\(\\mathbf{b}^{*}\\) must lie on the boundary of the constraint region (i.e., in the 2-D case, on the circle of radius \\(c\\) centered at the origin). It turns out that the optimal vector \\(\\mathbf{b^*}\\) corresponds to the one that is exactly the opposite of the gradient. Figure 15.8: Optimal point opposite to the gradient At this point, the gradient and normal vectors will be exactly antiparallel which, in mathematical terms, means that the normal vector is proportional to the gradient: \\[ \\nabla E_{in}(\\mathbf{b}^*) \\propto - \\mathbf{b}^{*} \\tag{15.6} \\] For convenience purposes, let us choose a proportionality constant of \\(-2 (\\lambda / n)\\). That is, we want the vector \\(\\mathbf{b}^{*}\\) such that \\[ \\nabla E_{in}(\\mathbf{b}^*) = - 2 \\frac{\\lambda}{n} \\mathbf{b}^{*} \\tag{15.7} \\] Rearranging the terms the above equation becomes: \\[ \\nabla E_{in}(\\mathbf{b}^*) + 2 \\frac{\\lambda}{n} \\mathbf{b}^{*} = \\mathbf{0} \\tag{15.8} \\] Now, notice that the above equation looks like the gradient of something… But what is that something? The answer turns out to be: \\[ f(\\mathbf{b}) = E_{in}(\\mathbf{b}) + (\\lambda/n) \\mathbf{b}^\\mathsf{T} \\mathbf{b} \\tag{15.9} \\] You can verify directly that \\(\\nabla f(\\mathbf{b})\\) is precisely the left hand side of equation (15.8): \\[ \\nabla f(\\mathbf{b}^*) = \\nabla E_{in}(\\mathbf{b}^*) + 2 \\frac{\\lambda}{n} \\mathbf{b}^{*} \\tag{15.10} \\] Let us rephrase what we have seen so far. From our initial minimization problem (15.5), we saw that its solution is given by the vector \\(\\mathbf{b}^{*}\\) that solves another minimization problem: \\[ \\min_{\\mathbf{b}} \\left\\{ E_{in}(\\mathbf{b}) + \\frac{\\lambda}{n} \\mathbf{b}^\\mathsf{T} \\mathbf{b} \\right\\} \\tag{15.11} \\] In other words, the minimizations (15.5) and (15.11) are exactly equivalent. From (15.11), we can obtain the solution, namely, the ridge regression estimate of \\(\\mathbf{b}\\): \\[ \\textsf{ridge coefficients:} \\quad \\mathbf{b}_{RR} = (\\mathbf{X}^\\mathsf{T} \\mathbf{X} + \\lambda \\mathbf{I})^{-1} \\mathbf{X}^\\mathsf{T} \\mathbf{y} \\] As you can imagine, the optimal vector \\(\\mathbf{b}\\) that minimizes \\(E_{in}\\) and satisfies the constraint, will be on a given contour of constant error. This situation is illustrated in the image below. Also, this image depicts the general case in which the estimated rigde parameters are biased. That is, the direction of the optimal vector \\(\\mathbf{b}\\) is not pointing in the direction of the OLS solution. Figure 15.9: Error contour intersecting the constraint circle Note that, from the discussion at the beginning of this chapter, we see that the ridge regression estimate always exists, and is always unique (unlike the OLS estimate). 15.3 What does RR accomplish? In ridge regression, the key ingredient is the hyperparameter \\(\\lambda\\). Setting \\(\\lambda = 0\\) causes the ridge solution to be the same as the OLS solution: \\[\\begin{align*} \\mathbf{b}_{RR} &amp;= (\\mathbf{X}^\\mathsf{T} \\mathbf{X} + 0 \\mathbf{I})^{-1} \\mathbf{X}^\\mathsf{T} \\mathbf{y} \\\\ &amp;= (\\mathbf{X}^\\mathsf{T} \\mathbf{X})^{-1} \\mathbf{X}^\\mathsf{T} \\mathbf{y} \\\\ &amp;= \\mathbf{b}_{OLS} \\tag{15.12} \\end{align*}\\] But what about specifying a very large value for \\(\\lambda\\)? Let’s think about this case. If \\(\\lambda\\) becomes larger and larger, the terms on the diagonal of \\((\\mathbf{X}^\\mathsf{T} \\mathbf{X} + \\lambda \\mathbf{I})\\) will be dominated by such \\(\\lambda\\) value. Consequently, the matrix inverse, \\((\\mathbf{X}^\\mathsf{T} \\mathbf{X} + \\lambda \\mathbf{I})^{-1}\\), will basically be dominated by the inverse of the terms on its diagonal: \\[ \\lambda &gt;&gt; 0 \\quad \\Longrightarrow \\quad (\\mathbf{X}^\\mathsf{T} \\mathbf{X} + \\lambda \\mathbf{I})^{-1} \\rightarrow \\frac{1}{\\lambda} \\mathbf{I} \\tag{15.13} \\] Taking this idea to the extreme with very large \\(\\lambda\\), will cause \\(1/\\lambda\\) to be zero, essentially “collapsing” \\((\\mathbf{X}^\\mathsf{T} \\mathbf{X} + \\lambda \\mathbf{I})^{-1}\\) into a \\(p \\times p\\) matrix full of zeros: \\[ \\lambda \\rightarrow \\infty \\quad \\Longrightarrow \\quad (\\mathbf{X}^\\mathsf{T} \\mathbf{X} + \\lambda \\mathbf{I})^{-1} \\rightarrow \\mathbf{0}_{(p,p)} \\tag{15.14} \\] thus, the ridge regression coefficients will also collapse to zero: \\[ \\lambda \\rightarrow \\infty \\quad \\Longrightarrow \\quad \\mathbf{b}_{RR} = \\mathbf{0} \\tag{15.15} \\] Relation between \\(\\lambda\\) and \\(c\\)? What is the relationship between the budget \\(c\\) and the non-negative constant \\(\\lambda\\)? Both of these scalars are intimately related. Imposing a large budget constraint \\(c\\), results in a small \\(\\lambda\\). The smaller the \\(\\lambda\\), the closer \\(\\mathbf{b}_{RR}\\) to the OLS solution. Conversely, imposing a small budget \\(c\\), causes \\(\\lambda\\) to become large. The larger the \\(\\lambda\\), the closer the ridge coefficients to zero. \\[ \\uparrow c \\quad \\Longrightarrow \\quad \\downarrow \\lambda \\\\ \\downarrow c \\quad \\Longrightarrow \\quad \\uparrow \\lambda \\] 15.3.1 How to find \\(\\lambda\\)? In ridge regression, \\(\\lambda\\) is a tuning parameter, and therefore it cannot be found analytically. Instead, you have to implement a trial and error process with various values for \\(\\lambda\\), and determine which seems to be a good one. How? Typically with cross-validation, or other type of resampling approach. Here’s a description of the steps to be carried out. Assumet that we have a training data set consisting of \\(n\\) data points: \\(\\mathcal{D}_{train} = (\\mathbf{x_1}, y_1), \\dots, (\\mathbf{x_n}, y_n)\\). Using \\(K\\)-fold cross-validation, we (randomly) split the data into \\(K\\) folds: \\[ \\mathcal{D}_{train} = \\mathcal{D}_{fold-1} \\cup \\mathcal{D}_{fold-2} \\dots \\cup \\mathcal{D}_{fold-K} \\] Each fold set \\(\\mathcal{D}_{fold-k}\\) will play the role of an evaluation set \\(\\mathcal{D}_{eval-k}\\). Having defined the \\(k\\) fold sets, we form the corresponding \\(K\\) retraining sets: \\(\\mathcal{D}_{train-1} = \\mathcal{D}_{train} \\setminus \\mathcal{D}_{fold-1}\\) \\(\\mathcal{D}_{train-2} = \\mathcal{D}_{train} \\setminus \\mathcal{D}_{fold-2}\\) \\(\\dots\\) \\(\\mathcal{D}_{train-K} = \\mathcal{D}_{train} \\setminus \\mathcal{D}_{fold-K}\\) The cross-validation procedure then repeats the following loops: For \\(\\lambda_b = 0.001, 0.002, \\dots, \\lambda_B\\) For \\(k = 1, \\dots, K\\) fit RR model \\(h_{b,k}\\) with \\(\\lambda_b\\) on \\(\\mathcal{D}_{train-k}\\) compute and store \\(E_{eval-k} (h_{b,k})\\) using \\(\\mathcal{D}_{eval-k}\\) end for \\(k\\) compute and store \\(E_{cv_{b}} = \\frac{1}{K} \\sum_k E_{eval-k}(h_{b,k})\\) end for \\(\\lambda_b\\) Compare all cross-validation errors \\(E_{cv_1}, E_{cv_2}, \\dots, E_{cv_B}\\) and choose the smallest of them, say \\(E_{cv_{b^*}}\\) Use \\(\\lambda^*\\) to fit the (finalist) Ridge Regression model: \\(\\mathbf{\\hat{y}} = (\\mathbf{X}^\\mathsf{T} \\mathbf{X} + \\lambda^* \\mathbf{I})^{-1} \\mathbf{X}^\\mathsf{T} \\mathbf{y}\\) "],
["linear-extensions.html", "16 Beyond Linear Regression 16.1 Introduction 16.2 Expanding the Regression Horizon 16.3 Transforming Features", " 16 Beyond Linear Regression In this part of the book we will talk about some extensions of the linear model, as well as alternative approaches to relax the assumption of linearity. 16.1 Introduction So far, we have talked about regression models in their most basic way: linear regression models. So let’s circle back to the general regression framework in which the main idea is to model an output value \\(y_i\\) in terms of one or more input features \\(\\mathbf{x_i}\\) (\\(p\\)-vector). In addition, we also suppose that there is a noise or error term to indicate the imperfect nature of our model: \\[ y_i = f(\\mathbf{x_i}) + \\varepsilon_i \\tag{16.1} \\] Usually, we assume that the error terms are independent from the input features, and that they have zero-mean: \\(\\mathbb{E}(\\varepsilon_i) = 0\\), and constant variance: \\(var(\\varepsilon_i) = \\sigma^2\\). To formalize things further, we assume that there is a joint distribution \\(P(y, \\mathbf{x})\\), a marginal distribution for the input features \\(P(\\mathbf{x})\\), and a conditional distribution \\(P(y | \\mathbf{x})\\). With all the previous assumptions, the theoretical essence in regression is to model \\(f()\\) as the conditional expectation of \\(y|\\mathbf{x}\\), which is precisely the so-called regression function: \\[ \\text{regression function} \\longrightarrow \\mathbb{E} (y|\\mathbf{x}) = \\hat{f}(\\mathbf{x}) \\tag{16.2} \\] The regression function \\(\\hat{f}()\\) takes in one or more input features, in the form of a vector \\(\\mathbf{x}\\), and returns a predicted output \\(\\hat{y}_i\\). One important thing to highlight is that the theoretical framework of regression says nothing about what the target function \\(f()\\) should or could look like, which is good news because we have a lot of freedom to decide on almost any form for \\(f()\\), and consequently, on any form for \\(\\hat{f}()\\). Again, here the term function is not the classic definition of a mathematical function. Instead, think of the notion of function as a machine that takes input features \\(\\mathbf{x}\\), and returns an output \\(y\\). Up to now, we have been working with the most standard type of form for \\(\\hat{f}()\\) which is a linear model. The estimated regression is simply a linear combination of the \\(p\\) input features, possibly including a constant term \\(b_0\\): \\[\\begin{align*} \\hat{y}_i &amp;= b_0 + b_1 x_{i1} + b_2 x_{i2} + \\dots + b_p x_{ip} \\\\ &amp;= \\mathbf{b^\\mathsf{T} x_i} \\tag{16.3} \\end{align*}\\] In vector-matrix notation, the vector of predictions is expressed as: \\[ \\mathbf{\\hat{y}} = \\mathbf{Xb} \\tag{16.4} \\] which we can graphically represent using a path diagram like the one shown below (assuming a constant input term \\(\\mathbf{x_0} = \\mathbf{1}\\)): Figure 16.1: Linear regression model in diagram form While a linear regression model like the one above can be very useful—not to mention its importance for many other derived methods—and should be part of your machine learning toolbox, it’s far too limiting. Therefore, we need to discuss some of the ways in which the standard linear regression model can be enriched, and be made more flexible. 16.2 Expanding the Regression Horizon There are several ways in which we can enrich and extend a linear regression model. Our objective is to expose you to an assortment of methods, and give you a taste of some interesting notions that are employed in more sophisticated approaches. Having said that, we will describe just a few of these ways. Trying to cover all possible extensions would require us to write a whole separate book. To make the discussion more organized, we have decided to classify the covered approaches into two major classes that we are calling: (1) parametric models, and (2) nonparametric models. At this point, we have two magic words that deserve some clarification: linear and (non)parametric: When people talk about linear regression models, what do they exactly mean by “linear”? When people talk about parametric -vs- nonparametric models, what do they mean by this? What kind of parameters are they referring to? 16.2.1 Linearity The standard linear regression model: \\[ f(\\mathbf{x_i}) = b_0 + b_1 x_{i1} + \\dots + b_p x_{ip} + \\varepsilon_i \\tag{16.5} \\] is linear in two modes. On one hand, it is linear in the input variables \\(X_1, \\dots, X_p\\). On the other hand, it is also linear in the parameters \\(b_0, b_1, \\dots, b_p\\). As we said before, although a model like this is very friendly to work with, its double linearity may be fairly restrictive. In the regression world, the most general type of linearity is the one that applies to the regression coefficients or parameters \\(b_0, b_1, \\dots, b_p\\). Here’s an example of a model that is nonlinear with respect to the predictors, but it is linear with respect to the parameters: \\[ f(\\mathbf{x_i}) = b_0 + b_1 x_{i1} + b_2 x_{i2} + b_3 x_{i1}^2 + b_4 x_{i2}^2 + \\varepsilon_i \\tag{16.6} \\] In contrast, the model below is nonlinear in both the predictors and the parameters: \\[ f(\\mathbf{x_i}) = b_0 + exp(x_{i1}^{b_1}) + \\sqrt{b_2 x_{i2}} + \\varepsilon_i \\tag{16.7} \\] So, when using the term “linear model”, we typically refer to a model that is linear in its coefficients or parameters, not necessarily in the input features. 16.2.2 Parametric and Nonparametric The word “parametric”, and its sibling “nonparametric”, are those kind of terms commonly used in various branches of statistics. For better or worse, they form part of those terms that people assign different meanings to. Depending on who you talk to, some people will give you a definition of the term “nonparametric” in the sense of distribution-free methods, or so-called, nonparametric statistics. This is NOT the meaning that we use in this book for nonparametric. So what do we mean by parametric and nonparametric? By parametric, we refer to a functional form of \\(f()\\) that is fully described by a finite set of parameters, like in the standard linear model \\[ f(\\mathbf{x_i}) = b_0 + b_1 x_{i1} + \\dots + b_p x_{ip} + \\varepsilon_i \\] In this book we use the notion of nonparametric models to imply a more relaxed way to specify a function \\(f()\\) without directly imposing a known functional form. Somewhat contradictory, nonparametric does not necessarily mean that a model has no parameters. As we’ll see, it turns out that nonparametric models do have parameters (or hyperparameters). A common example of a nonparametric method is \\(k\\)-Nearest-Neighbors (KNN). The functional form of \\(f()\\) is way more relaxed, and all we do is to use an average of the response values \\(y_i\\) for the closest \\(k\\) points \\(x_i\\) to a query value \\(x_0\\): \\[ \\hat{y}_0 = \\frac{1}{k} \\sum_{i \\in \\mathcal{N}_k(x_0)} y_i \\tag{16.8} \\] where the notation \\(\\mathcal{N}_K(x_0)\\) indicates the set of \\(K\\) closest neighbors to \\(x_0\\). Notice that KNN does not impose any functional form that specifies how to combine the \\(X\\)-input feature(s). In this sense, we say that this is a nonparametric model. Now that we have provided some clarifications on the meanings of terms such as “linear”, “parametric”, and “nonparametric”, we can continue with our introduction of ways in which we can extend linear models beyond linearity, as well as extensions of models in a more relaxed (nonparametric) sense. 16.3 Transforming Features Let us consider parametric models first. A first option to make a linear model more sophisticated is by means of applying transformations to some or all of the input features. This is not a new idea. In fact, we have alredy used this strategy when we studied Principal Components Regression and Partial Least Squares Regression. Consider a linear model that uses some type of dimension reduction approach. The general recipe is to obtain new variables by using linear combinations of the input features, illustrated in the following diagram. Figure 16.2: PCs as linear combinations of input variables We can think of any given component \\(\\mathbf{z_h}\\) as a transformation applied on all the \\(X\\)-input variables, that is: \\[ \\mathbf{z_q} = v_{1q} \\mathbf{x_1} + \\dots + v_{pq} \\mathbf{x_p} \\quad \\longrightarrow \\quad Z_q = \\phi_q (X_1, \\dots, X_p) \\] We an explicitly think of a function \\(\\phi_h : \\mathbb{R}^p \\rightarrow \\mathbb{R}\\) that transforms the inputs into a new synthetic feature. In the case of PCR and PLSR, the transformation functions \\(\\phi_q\\)() happen to be linear functions. Figure 16.3: Linear combinations of input variables With the matrix \\(\\mathbf{Z}\\) containing the transformed features, we can still use ordinary least squares to obtain the predicted response as: \\[ \\mathbf{\\hat{y}} = \\mathbf{Z} (\\mathbf{Z}^\\mathsf{T} \\mathbf{Z})^{-1} \\mathbf{Z}^\\mathsf{T} \\mathbf{y} \\tag{16.9} \\] This is a first approach that gives us the opportunity to enrich a linear model by transforming the \\(p\\) input features into new \\(r\\) synthetic features. In the next chapter, we will describe a particular type of linear models (i.e. linear in the parameters) that uses what is called basis functions for the transformation functions \\(\\phi_q()\\). "],
["nonparametric.html", "17 Nonparametric Regression 17.1 Conditional Averages 17.2 Looking at the Neighbors", " 17 Nonparametric Regression In this chapter we provide a high level intuition behind nonparametric regression procedures. Full disclaimer: most of what we talk about in the next chapters is going to center on the case when we have one predictor: \\(p = 1\\). We’ll make a couple of comments here and there about extensions to the multivariate case. But most things covered are for univariate regression. Why? Doing nonparametric regression in higher dimensions becomes very quickly statistically and computationally inefficient (e.g. curse of dimensionality). 17.1 Conditional Averages Consider the following example that has to do with NBA players in the 2018 season. Specifically, we are looking at two variables: years of experience, and salary (in millions of dollars). Let’s assume that salary is the response \\(Y\\), and experience is the predictor \\(X\\). The image below depicts the scatterplot between \\(X\\) and \\(Y\\). Figure 17.1: Salary -vs- Experience of NBA players As you can tell, the relationship between Experience and Salary does not seem to be linear. Say we are interested in predicting the salary for a player with four years of experience. One approach to get this prediction involves using a conditional mean: \\[ \\text{Predicted Salary} = Avg(\\text{Salary } | \\text{ Experience = 4 years}) \\] A natural thing to do is to look at the salaries \\(y_i\\) of all players that have 4 years of experience: \\(x_i = 4 \\text{ years}\\), Figure 17.2: Salaries of players with 4 years of Experience and then aggregate such salaries into a single value, for example, using the arithmetic mean: \\[ \\hat{y}_0 = Avg(y_i | x_i = 4) \\] This is an empirical and nonparametric way to compute the conceptal definition of regression, which as we know, is defined in terms of conditional expectation: \\[ \\textsf{regression function:} \\quad \\hat{f}(x_0) = \\mathbb{E}(y | X = x_0) = \\hat{y}_0 \\] We can compute all the average salaries for each value of years of experience, and make another plot with such averages: Figure 17.3: Average Salaries by years of Experience Taking a further step, we can enrich the graph by connecting the dots of average salaries, in order to get a nonparametric regression line, displayed in the following image: Figure 17.4: Average Salaries by years of Experience Notice that this approach is surprisingly simple, and it does match the conceptual idea of regression as a conditional expectation. In this example, we are approximating such expectation with conditional averages: averages of salaries, conditioning by years of experience. But this approach is not perfect. One of its downsides is that the fitted line looks very jagged. Another disadvantage is that we don’t have regression coefficients that helps us in the interpretation of the model. In addition, we may have certain \\(X\\)-values for which there’s a scarcity of data. Consider for example the case of players with 17 years of experience in the NBA. The data from the 2018 season contains only one player with 17 years of experience. With just one data point, the predicted salary will be highly unreliable. Figure 17.5: Only one player with 17 years of experience 17.2 Looking at the Neighbors Because we only have one player with 17 years of experience, we need to find a way of using more of the data points in \\(X\\) to try to improve the prediction. Looking at the scatterplot below, there are four players that are very close to the one with 17 years of experience. So why not use the salaries of all these players to calculate a predicted salary for someone with 17 years of experience? Figure 17.6: Four closest players to 17 years of experience Some common options that we can use to profit from the neighboring data points are: calculate the arithmetic mean of the neighboring \\(y_i\\)’s. calculate a weighted average of the neighboring \\(y_i\\)’s. calculate a linear fit with the neighboring \\((x_i, y_i)\\) points. calculate a polynomial fit with the neighboring \\((x_i, y_i)\\) points. Key Ideas Let’s summarize what we have discussed so far with the following two core ideas: Neighborhood: one of the main ideas has to do with the notion of neighboring points \\(x_i\\) for a given query point \\(x_0\\). Local Fitting Mechanism: the other main idea implies some sort of mechanism to determine a local fit with the \\(y_i\\)’s (of the neighboring points) that we can use as the predicted outcome \\(\\hat{y_0}\\). In the next sections we describe a number of procedures that handle both of the above ideas in different ways, but that can be seen as special cases of what is known as linear smoothers. "],
["knn.html", "18 Nearest Neighbor Estimates 18.1 Introduction 18.2 \\(k\\) Nearest Neighbors (KNN)", " 18 Nearest Neighbor Estimates 18.1 Introduction In the previous chapter we finished our discussion by listing two core ideas that are common to several nonparametric regression methods. On one hand, we typically invoke a notion of “Neighborhood” of nearby points \\(x_i\\) around a query point \\(x_0\\). On the other hand, we also require some sort of mechanism to get a “local” fit using the data points \\((x_i, y_i)\\) in the neighborhood. Let’s now describe one flavor of procedures that is based on a narrow notion of neighborhood, and very simple local fitting mechanisms. 18.2 \\(k\\) Nearest Neighbors (KNN) Perhaps the most popular approach that takes into account neighboring points to make predictions is \\(k\\) Nearest Neighbors, or KNN for short. In KNN, we define a neighborhood, denoted \\(\\mathcal{N}_k(x_0)\\), that consists of the \\(k\\) closest points around a query point \\(x_0\\). This neighborhood will give us a set of \\(k\\) points \\((x_i, y_i)\\), for \\(i \\in \\mathcal{N}_k(x_0)\\), that will be taken into account to compute \\(\\hat{y}_0\\) with some local averaging mechanism. To keep the notation simpler, sometimes we will use \\(\\mathcal{N}_0 = \\mathcal{N}_k(x_0)\\) to denote the neighborhood around \\(x_0\\). The plain vanilla KNN estimator \\(\\hat{f}(x_0)\\) is defined as the arithmetic mean of the \\(y_i\\) values associated to the \\(k\\) closest points \\(x_i\\) to \\(x_0\\): \\[ \\textsf{running mean:} \\quad \\hat{f}(x_0) = \\frac{1}{k} \\sum_{i \\in \\mathcal{N}_0} y_i = \\hat{y}_0 \\] A less common option of local averaging mechanism is the median of the \\(y_i\\) values associated to the \\(k\\) closest points \\(x_i\\) to \\(x_0\\): \\[ \\textsf{running median:} \\quad \\hat{f}(x_0) = \\text{median} (y_i | i \\in \\mathcal{N}_0) \\] A much less common approach is to use a local linear fit, which is to say, estimate a linear regression based in the neighboring points \\((x_i, y_i), i \\in \\mathcal{N}_0\\) \\[ \\textsf{running line:} \\quad \\hat{f}(x_0) = b_{0,x_0} + b_{1,x_0} x_0 \\] where \\(b_{0,x_0}\\) and \\(b_{1,x_0}\\) are the least squares estimates using the data points \\((x_i, y_i)\\), with \\(i \\in \\mathcal{N}_0\\). 18.2.1 Distance Measures One important consideration behind KNN has to do with the question of “How do we measure the closeness between two points \\(x_0\\) and \\(x_i\\)?” In the univariate case, we typically use the absolute distance: \\[ d(x_0, x_i) = |x_0 - x_i| \\] Notice that for the univariate case, there’s no difference between using the absolute distance or using the Euclidean distance. The \\(k\\) nearest neighbors should be the same regardless of the choice of distance. In the multivariate case, common options for distances are Euclidean distance, and Manhattan distance (but many other distances are also available). \\[ \\textsf{Euclidean:} \\quad d(x_0, x_i) = \\sqrt{\\sum_{j=1}^{p} (x_{0j} - x_{ij})^2} \\\\ \\textsf{Manhattan:} \\quad d(x_0, x_i) = \\sum_{j=1}^{p} |x_{0j} - x_{ij}| \\] For instance, say we have a data set of \\(n = 10\\) objects: \\[ \\mathcal{D} = \\{ (x_1, y_1), (x_2, y_2), \\dots, (x_9, y_9), (x_{10}, y_{10}) \\} \\] and suppose that we have a given query point \\(x_0\\) for which we want to compute its predicted response \\(\\hat{y}_0\\). Let’s also assume that we are considering the \\(k = 3\\) closest neighbors to \\(x_0\\). What we have to do is compute all the distances \\(d_i = d(x_0, x_i)\\) between the query point and the \\(n\\) data points: \\[ \\textsf{set of distances:} \\quad (d_1, d_2, d_3, \\dots, d_9, d_{10}) \\] and then select the 3 closest \\(x_i\\) points to \\(x_0\\). For illustrations purposes, let’s pretend that \\(d_5\\), \\(d_2\\), and \\(d_7\\) are the closest distances to \\(x_0\\), in that order. This results in the neighbor indices \\(\\mathcal{N}_3 (x_0) = (2, 5, 7)\\), and the corresponding \\(y_i\\)-values: \\((y_2, y_5, y_7)\\). The predicted value \\(\\hat{y}_0\\) is then computed as: \\[ \\frac{y_2 + y_5 + y_7}{3} = \\hat{y}_0 \\] 18.2.2 KNN Estimator In general, the plain KNN estimator can be seen as an average of \\(y_i\\)’s: \\[ \\hat{f}(x_0) = \\frac{1}{k} \\sum_{i \\in \\mathcal{N}_0} y_i \\quad \\longrightarrow \\quad \\hat{f}(x_0) = \\sum_{i \\in \\mathcal{N}_0} w_i(x_0) \\hspace{1mm} y_i \\] where \\(w_i (x_0)\\) is a weight of the form: \\[ w_i (x_0) = \\begin{cases} \\frac{1}{k} \\quad \\text{if} &amp; i \\in \\mathcal{N}_k (x_0) \\\\ 0 \\quad \\text{else} \\end{cases} \\] Under this formulation, the standard KNN estimator is simply a constant average of the \\(y_i\\) (with \\(i \\in \\mathcal{N}_{k(x_0)}\\)). 18.2.3 How to find \\(k\\)? As we mentioned above, \\(k\\) is a tuning parameter, and therefore it cannot be found analytically. Instead, you have to implement a trial and error process with various values for \\(k\\), and determine which seems to be a good one. How? Typically with cross-validation, or other type of resampling approach. Here’s a description of the steps to be carried out. Assumet that we have a training data set consisting of \\(n\\) data points: \\(\\mathcal{D}_{train} = (\\mathbf{x_1}, y_1), \\dots, (\\mathbf{x_n}, y_n)\\). To avoid confusion between the number of neighbors \\(k\\), and the number of folds \\(K\\), here we are going to use \\(Q = K\\) to indicate the index of folds. Using \\(Q\\)-fold cross-validation, we (randomly) split the data into \\(Q\\) folds: \\[ \\mathcal{D}_{train} = \\mathcal{D}_{fold-1} \\cup \\mathcal{D}_{fold-2} \\dots \\cup \\mathcal{D}_{fold-Q} \\] Each fold set \\(\\mathcal{D}_{fold-q}\\) will play the role of an evaluation set \\(\\mathcal{D}_{eval-q}\\). Having defined the \\(Q\\) fold sets, we form the corresponding \\(Q\\) retraining sets: \\(\\mathcal{D}_{train-1} = \\mathcal{D}_{train} \\setminus \\mathcal{D}_{fold-1}\\) \\(\\mathcal{D}_{train-2} = \\mathcal{D}_{train} \\setminus \\mathcal{D}_{fold-2}\\) \\(\\dots\\) \\(\\mathcal{D}_{train-Q} = \\mathcal{D}_{train} \\setminus \\mathcal{D}_{fold-Q}\\) The cross-validation procedure then repeats the following loops: For \\(k = 1, 3, \\dots, B\\), (number of neighbors) For \\(q = 1, \\dots, Q\\), (fold number) fit KNN \\(h_{k,q}\\) with \\(k_b\\)-neighbors on \\(\\mathcal{D}_{train-q}\\) compute and store \\(E_{eval-q} (h_{k,q})\\) using \\(\\mathcal{D}_{eval-q}\\) end for \\(q\\) compute and store \\(E_{cv_{k}} = \\frac{1}{Q} \\sum_q E_{eval-q}(h_{k,q})\\) end for \\(k\\) Compare all cross-validation errors \\(E_{cv_1}, E_{cv_2}, \\dots, E_{cv_B}\\) and choose the smallest of them, say \\(E_{cv_{b^*}}\\) Use \\(k^*\\) to fit the (finalist) KNN model Properties of KNN estimator Here are some important remarks: Notice that the function that “generates” the weights \\(w_i (x_0)\\) is discontinuous, which explains why the fitted curve is jagged. Notice also that the formulation of \\(\\hat{f}(x_0)\\) as a weighted average of neighboring \\(y_i\\) is linear with respect to the response \\(Y\\). KNN, compared to a parametric model, is very difficult to interpret, since there are no coefficients or analytical parameters. The parameter \\(k\\) is a tuning or hyperparameter. Large values for \\(k\\) result in more inflexible fits (i.e. more bias), but also less variance. Small values for \\(k\\) result in more flexible fits (i.e. less bias), at the expense of increasing the variance. "],
["kernel-smoothers.html", "19 Kernel Smoothers 19.1 Introduction 19.2 Kernel Smoothers 19.3 Local Polynomial Estimators", " 19 Kernel Smoothers 19.1 Introduction In the previous chapter, we described the KNN estimator as the arithmetic mean or average of the \\(y_i\\) values associated to the \\(k\\) closest \\(x_i\\) observations to the query point \\(x_0\\): \\[ \\hat{f}(x_0) = \\frac{1}{k} \\sum_{i \\in \\mathcal{N}_0} y_i \\quad \\longrightarrow \\quad \\hat{f}(x_0) = \\sum_{i \\in \\mathcal{N}_0} w_i(x_0) \\hspace{1mm} y_i \\] As you can tell, such estimate can be expressed in a generic form of a weighted average of \\(y_i\\) (with \\(i \\in \\mathcal{N}_0\\)), in which the weights are all constant \\(w_i(x_0) = 1/k\\). This way of handling the weights \\(w_i(x_0)\\) gives the same importance to the \\(k\\) nearest neighbors of \\(x_0\\), regardless of how close or how far they are from the query point. Which sometimes could be problematic. Consider the figure below. Suppose that we are interested in predicting \\(y_0\\) using \\(k\\) closest neighbors. The local information of \\(x_i\\) is concentrated around the actual observed \\(x_0\\). Figure 19.1: k = 10 Closest Neighbors However, if we consider more neighbors \\(k\\), now some of the \\(x_i\\) values are considerably farther from the query point \\(x_0\\): Figure 19.2: k = 20 Closest Neighbors Instead of giving the same importance to all the neighbors, we could instead give them variable importances depending on their proximity to \\(x_0\\). Those values \\(x_i\\) that are closer to \\(x_0\\) should receive more importance via a bigger weight. Those values \\(x_i\\) that are farther from \\(x_0\\) should receive less importance vis a smaller weight. Figure 19.3: k = 20 Closest Neighbors, Weighted Consequently, we can modify the formula of the KNN estimator that uses a constant average, and replace the constant weights with variable weights. This modification will result in an estimator that computes \\(\\hat{f}(x_0)\\) as a weighted average. 19.2 Kernel Smoothers We are looking for a weighted average estimator \\[ \\hat{f}(x_0) = \\sum_{i \\in N_0} w_i(x_0) \\hspace{1mm} y_i \\] in which the weights \\(w_i(x_0)\\) take into account the proximity of \\(x_i\\) to \\(x_0\\). One very interesting option for determining weights is with Kernel functions. These are functions that take into account the density of \\(X\\)-points. 19.2.1 Kernel Functions What is a kernel? The term “kernel” is somewhat of an umbrella term in the stattistical and machine world. You will often hear people talking about kernel functions, kernel methods, kernel regression, kernel classifiers, just to mention a few of them. With respect to nonparametric regression, and in particular with linear smoothers, a kernel is a function that has special properties. Before looking at those properties, we should say that the type of kernel we are referring to in this chapter, is different (although related) from terms such as Mercer kernels, and the so-called kernel trick. A one-dimensional smoothing kernel is a symmetric function \\(K(u) : \\mathbb{R} \\rightarrow \\mathbb{R}\\) with the following properties: finite support: \\(K(u) = 0\\) for \\(|u| \\geq 1\\) symmetry: \\(K(u) = K(-u)\\) positive values: \\(K(u) &gt; 0\\) for \\(|u| &lt; 1\\) normalization: \\(\\int K(x) dx = 1\\) zero-midpoint: \\(\\int x K(x) dx = 0\\) \\(\\int x^2 K(x) dx &gt; 0\\) Some of the common Kernel functions are: \\[ \\begin{aligned} \\text{Uniform} &amp; \\qquad K(u) = \\frac{1}{2}, \\quad \\text{for } |u| \\leq 1 \\\\ &amp; \\\\ \\text{Gaussian} &amp; \\qquad K(u) = \\frac{1}{\\sqrt{2 \\pi}} exp(-u^2 / 2) \\\\ &amp; \\\\ \\text{Epanechnikov} &amp; \\qquad K(u) = \\frac{3}{4} (1 - u^2), \\quad \\text{for } |u| \\leq 1\\\\ &amp; \\\\ \\text{Biweight} &amp; \\qquad K(u) = \\frac{15}{16} (1 - u^2)^2, \\quad \\text{for } |u| \\leq 1 \\\\ &amp; \\\\ \\text{Triweight} &amp; \\qquad K(u) = \\frac{35}{32} (1 - u^2)^3, \\quad \\text{for } |u| \\leq 1 \\\\ \\end{aligned} \\] Figure 19.4: Various Kernels (source: wikipedia) In case you are curious, Wikipedia’s entry about kernel functions has many more options: https://en.wikipedia.org/wiki/Kernel_(statistics) 19.2.2 Weights from Kernels Choosing any type of one-dimensional kernel, we can construct weights \\[ w_i (x_0) = \\frac{K \\big(\\frac{x_i - x_0}{h} \\big)}{\\sum_{i}^{n} K \\big( \\frac{x_i - x_0}{h} \\big)} \\] Weights obtained in this way have the property that they add up to 1: \\[ \\sum_{i}^{n} w_i (x_0) = 1 \\] 19.2.3 Kernel Estimator The kernel estimator is \\[ \\hat{f}(x_0) = \\frac{\\sum_{i=1}^{n} y_i K \\big(\\frac{x_i - x_0}{h} \\big)}{\\sum_{i=1}^{n} K \\big(\\frac{x_i - x_0}{h} \\big)} = \\sum_{i=1}^{n} w_i(x_0) y_i \\] The kernel estimator minimizes a localized squared error \\[ \\sum_{i=1}^{n} K \\left( \\frac{x_i - x_0}{h} \\right) (y_i - \\theta)^2 \\] It can be shown that the estimator \\(\\theta = \\hat{f}()\\) is given by: \\[ \\hat{f}(x_0) = \\frac{\\sum_{i=1}^{n} y_i K \\big(\\frac{x_i - x_0}{h} \\big)}{\\sum_{i=1}^{n} K \\big(\\frac{x_i - x_0}{h} \\big)} = \\sum_{i=1}^{n} w_i(x_0) y_i \\] This estimator is also known as the Nadaraya-Watson (NW) estimator, and it is a local constant estimator. Why do we say \\(\\hat{f}()\\) is a local constant estimator? Because it is a local weighted average of the \\(y_i\\)’s. If we use the Epanechnikov kernel, it turns out that the NW estimator is optimal. But this is not something to get obsessed about. In practice, you can use any flavor of kernel function, and be able to obtain similar results. 19.3 Local Polynomial Estimators Let’s bring back the kernel estimator which minimizes a localized squared error function: \\[ \\sum_{i=1}^{n} K \\left( \\frac{x_i - x_0}{h} \\right) (y_i - \\theta)^2 \\] Instead of using a local weighted average of \\(y_i\\)’s, which is basically a local constant fit, why not use a more flexible fit such as a local linear fit, or more general, a local polynomial fit? This is precisely what local polynomial estimators are for. If we use a polynomial of degree 1, that is a line, the local minimization problem becomes: \\[ \\sum_{i=1}^{n} K \\left( \\frac{x_i - x_0}{h} \\right) \\big( y_i - b_{0,x_0} - b_{1,x_0} (x_i - x_0) \\big)^2 \\] The estimated \\(\\hat{f}(x_0) \\approx b_{0,x_0} + b_{1,x_0} (x - x_0)\\). If we arrange the terms in an \\(n \\times 2\\) matrix \\(\\mathbf{X_0}\\), and a \\(n \\times n\\) matrix of weights \\(\\mathbf{W}\\) \\[ \\mathbf{X_0} = \\ \\begin{bmatrix} 1 &amp; x_{1} - x_0 \\\\ 1 &amp; x_{2} - x_0 \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; x_{n} - x_0 \\\\ \\end{bmatrix}, \\\\ \\quad \\\\ \\mathbf{W_0} = \\begin{bmatrix} K_h (x_1 - x_0) &amp; 0 &amp; \\dots &amp; 0\\\\ 0 &amp; K_h (x_2 - x_0) &amp; \\dots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; K_h (x_n - x_0) \\\\ \\end{bmatrix} \\] then: \\[ \\mathbf{b}_{x_0} = (\\mathbf{X_0}^\\mathsf{T} \\mathbf{W_0 X_0})^{-1} \\mathbf{X_0}^\\mathsf{T} \\mathbf{W_0 y} \\] "],
["classif.html", "20 Classification 20.1 Introduction 20.2 Bayes Classifier", " 20 Classification In this part of the book we discuss classification methods, that is, methods to predict a qualitative or categorical response: Figure 20.1: Clustering Corner Some of the methods that we discuss in the next chapters are: Logistic Regression Canonical Discriminant Analysis Linear Discriminant Analysis Quadratic Discriminant Analysis Bayes’ Classifier 20.1 Introduction The goal in classification is to take an input vector \\(\\mathbf{x}\\) and to assign it to one of \\(K\\) discrete classes or groups \\(\\mathcal{C_k}\\) where \\(k = 1, \\dots, K\\). In the most common case, the classes are taken to be disjoint, so that each input is assigned to one and only one class. 20.1.1 Credit Score Example Your credit score is a three-digit number that relates to how likely you are to repay debt. This score typically ranges from 300-850. Banks and lenders use it to decide whether they’ll approve you for a credit card or loan. Figure 20.2: Typical display of a Credit Score with a gauge chart It’s not uncommon to have multiple different scores at the same time. For example, an auto lender might use one scoring model, while a mortgage lender uses another. Your scores are typically based on things like On-time payments: This is your track record for paying bills on time. It has the most impact on your score. Credit Usage: This is how much of your credit card limits you’re using. Credit Type: The types of credit you have (credit cards, auto loans, student loans, mortgages, etc.) Average Age of Credit: This is the average age of all your open credit cards and loans. Total Accounts: The more credit cards and loans you’ve had, the more lenders trust you. Credit Inquiries: A “hard” inquiry is when a lender checks your credit report. Derogatory Marks: You’ll get a ding on your report if one or more of your accounts goes into collections or bankruptcy. Credit Scores, at least those calculated in the US, claim that they never factor in personal information like your race, gender, religion, marital status or national origin. 20.1.2 Toy Example Let’s consider a credit application from which \\(p\\) predictors are derived \\(X = [ X_1, \\dots, X_p ]\\). Age Job type (and job seniority) Residential Status Marital Status Loan purpose etc And suppose also that customers are divided in two classes: good and bad. Good customers are those that payed their loan back Bad customers are those that defaulted on their loan Let’s bring our mental map back of the Supervised Learning diagram, and see how its elements get framed from a classification perspective. Figure 20.3: Some elements of the Supervised Learning Diagram From the previous diagram, we have three unknown distributions \\(P()\\): joint distribution of data: \\(P(\\mathbf{x}, y)\\) conditional distribution of target, given inputs: \\(P(y \\mid \\mathbf{x})\\) marginal distribution of inputs: \\(P(\\mathbf{x})\\) The idea in classification problems is the following. Given a customer’s attributes \\(X = \\mathbf{x}\\), to what class \\(y\\) we should assign this customer? Ideally (although not mandatory), we would like to know what is the conditional probability: \\[ P(y \\mid X = \\mathbf{x}) \\] For example, we can think of \\(n\\) individuals in a \\(p\\)-dimensional space. Moreover, assume that each class of customers forms its own cloud: the good customers, and the bad customers. Figure 20.4: Cloud of n points in p-dimensional space Now, assume that there are four individuals \\(\\{a, b, c, d\\}\\) for which we want to predict their classes. Figure 20.5: To which class we assign new individuals? It would be nice to have a mechanism or rule to classifiy observations. For example, we could end up with the following classifications. Customer \\(a\\) could be assigned to class bad. Customer \\(d\\) could also be assigned to class bad with high confidence. In turn, customer \\(c\\) coud be assigned with high confidence to class good. What about customer \\(b\\)? We could be uncertain to which class this individual belongs. Figure 20.6: Some possible classifications Having a classification rule allows us to divide the input space into regions \\(\\mathcal{R}_k\\) called decision regions (one for each class). The boundaries between decision regions would establish decision boundaries or decision surfaces. Figure 20.7: Two possible decision boundaries For example, one decision rule could result in a linear decision boundary, while another decision rule could result in a nonlinear decision surface. 20.1.3 Two-class Problem In our credit application example we have customers belonging to one of two classes: \\(\\mathcal{C}_1 =\\) good, and \\(\\mathcal{C}_2 =\\) bad. Figure 20.8: Cloud of n customers in p-dimensional space Since this is a supervised learning problem, we start with some available evidence (i.e. training data set). A first step may involve studying how \\(X\\) values vary according to a given class \\(\\mathcal{C_k}\\). In other words, we may start exploring the class-conditional distribution of \\[ P(X = \\mathbf{x} \\mid y = k) \\] For instance, we may ask “How does \\(X_j \\mid y = 1\\) compare with \\(X_j \\mid y = 2\\)?” Similarly, “How does \\(X_q \\mid y = 1\\) compare with \\(X_q \\mid y = 2\\)?” Figure 20.9: Exploring conditional distributions Typically we have descriptive information about \\(X \\mid y = k\\). We can calculate summary statistics, and compare visual displays of these distributions. If we found a way to get the class-conditional distribution \\(P(X \\mid y = k)\\), we could compute: \\[ P(X = \\mathbf{x} \\mid \\text{Good}) = \\frac{P(\\text{applicant is Good and has attributes } \\mathbf{x})}{P(\\text{applicant is Good})} \\] and \\[ P(X =\\mathbf{x} \\mid \\text{Bad}) = \\frac{P(\\text{applicant is Bad and has attributes } \\mathbf{x})}{P(\\text{applicant is Bad})} \\] However, the probability that we are interested in at the end of the day is the conditional probability \\(P(y = k \\mid X = \\mathbf{x})\\). This is the unknown target distribution in the supervised learning diargam. \\[ P(\\text{Good} \\mid X = \\mathbf{x}) = \\frac{P(\\text{applicant has attributes } \\mathbf{x} \\text{ and is Good})}{P(\\text{applicant has attributes } \\mathbf{x})} \\] similarly \\[ P(\\text{Bad} \\mid X = \\mathbf{x}) = \\frac{P(\\text{applicant has attributes } \\mathbf{x} \\text{ and is Bad})}{P(\\text{applicant has attributes } \\mathbf{x})} \\] In order to connect all the previous probabilities, we have to review the Bayes’ rule. 20.1.4 Bayes’ Rule Reminder Let’s look at both types of conditional probabilities: \\[ P(X = \\mathbf{x} \\mid y = k) = \\frac{P(y = k , X = \\mathbf{x})}{P(y = k)} \\] and \\[ P(y = k \\mid X = \\mathbf{x}) = \\frac{P(X = \\mathbf{x} , y = k)}{P(X = \\mathbf{x})} \\] And consider the joint distribution \\[\\begin{align*} P(X = \\mathbf{x} , y = k) &amp;= P(y = k \\mid X = \\mathbf{x}) P(X = \\mathbf{x}) \\\\ &amp;= P(X = \\mathbf{x} \\mid y = k) P(y = k) \\end{align*}\\] Solving for the joint distribution \\(P(X = \\mathbf{x} , y = k)\\) we have that: \\[ P(X = \\mathbf{x} \\mid y = k) P(y = k) \\hspace{1mm} = \\hspace{1mm} P(y = k \\mid X = \\mathbf{x}) P(X = \\mathbf{x}) \\] Thus: \\[ P(y = k \\mid X = \\mathbf{x}) = \\frac{P(X = \\mathbf{x} \\mid y = k) P(y = k)}{P(X = \\mathbf{x})} \\] where the marginal probability \\(P(X = \\mathbf{x})\\) is calculated with the total probability formula: \\[ P(X = \\mathbf{x}) = \\sum_{k} P(X = \\mathbf{x} \\mid y = k) P(y = k) \\] We can use Bayes’ Theorem for classification purposes, changing some of the notation, let: \\(P(y = k)\\) = \\(\\pi_k\\), the prior probability for class \\(k\\). \\(P(X = \\mathbf{x} \\mid y = k)\\) = \\(f_k(\\mathbf{x})\\), the class-conditional density for inputs \\(X\\) in class \\(k\\). Thus, the posterior probability (the conditional probability of the response given the inputs) is: \\[ P(y = k \\mid X = \\mathbf{x}) = \\frac{f_k(\\mathbf{x}) \\hspace{1mm} \\pi_k}{\\sum_{k=1}^{K} f_k(\\mathbf{x}) \\pi_k} \\] By using Bayes’ theorem we are essentially modeling the posterior probability \\(P(y=k \\mid X=\\mathbf{x})\\) in terms of likelihood densities \\(f_k(\\mathbf{x})\\) and prior probabilities \\(\\pi_k\\). \\[ \\text{posterior} = \\frac{\\text{likelihood} \\times \\text{prior}}{\\text{evidence}} \\] 20.2 Bayes Classifier As you know, in Supervised Learning our goal is to find a model \\(\\hat{f}()\\) that makes good predictions. In a classification setting this translates into minimizing the probability of assigning an individual \\(\\mathbf{x_i}\\) to the wrong class. Under this mindset, it seems a good idea to classify an object \\(\\mathbf{x_i}\\) to the class \\(k\\) that makes \\(P(y=k \\mid X=\\mathbf{x_i})\\) as large as possible. That is, classify \\(\\mathbf{x_i}\\) to the most likely class, given its predictors. An error occurs when an input vector of class \\(\\mathcal{C}_1\\) is misclassified as belonging to class \\(\\mathcal{C}_2\\), or vice versa. Mathematically, a classification error involves having an input \\(\\mathbf{x}\\) in decision region \\(\\mathcal{R}_1\\) being labeled as \\(\\mathcal{C}_2\\): \\((\\mathbf{x} \\in \\mathcal{R}_1, \\mathcal{C}_2)\\), and vice versa \\((\\mathbf{x} \\in \\mathcal{R}_2, \\mathcal{C}_1)\\). The probability of this classification error is given by: \\[ P(error) = P(\\mathbf{x} \\in \\mathcal{R}_1, \\mathcal{C}_2) + P(\\mathbf{x} \\in \\mathcal{R}_2, \\mathcal{C}_1) \\] In order to minimize \\(P(error)\\) we should find a way to assign \\(\\mathbf{x_i}\\) to whichever class renders this probability to a minimum. Thus, if \\(P(\\mathbf{x}, \\mathcal{C}_1) &gt; P(\\mathbf{x}, \\mathcal{C}_2)\\) for a certain input vector \\(\\mathbf{x}\\), then we label \\(\\mathbf{x}\\) as \\(\\mathcal{C}_1\\). Alternatively, minimizing \\(P(error)\\) is equivalent to maximizing \\(P(correct)\\) \\[ P(correct) = P(\\mathbf{x} \\in \\mathcal{R}_1, \\mathcal{C}_1) + P(\\mathbf{x} \\in \\mathcal{R}_2, \\mathcal{C}_2) \\] In general, for \\(K\\) classes the probability of correct classification is: \\[ P(correct) = \\sum_{k=1}^{K} P(\\mathbf{x} \\in \\mathcal{R}_k, \\mathcal{C}_k) \\] which is maximized by choosing decision regions \\(\\mathcal{R}_k\\) such that each \\(\\mathbf{x}\\) is assigned to the class for which the joint distribution \\(P(\\mathbf{x}, \\mathcal{C}_k)\\) is largest. Using the product rule, we get that this joint probability is: \\[ P(\\mathbf{x}, \\mathcal{C}_k) = P(\\mathcal{C}_k \\mid \\mathbf{x}) P(\\mathbf{x}) \\] We should assign \\(\\mathbf{x}\\) to the most likely class, given its input features \\(P(\\mathcal{C}_k \\mid \\mathbf{x})\\). This is the so-called Bayes Classifier. Note that the Bayes formula \\[ P(y = k | X = \\mathbf{x}) = \\frac{ \\pi_k f_k(\\mathbf{x})}{\\sum_{k=1}^{K} \\pi_k f_k(\\mathbf{x})} \\] does NOT tell us: how to calculate priors \\(\\pi_k\\) what form should we use for densities \\(f_k(\\mathbf{x})\\) which means there is plenty of room to play with \\(\\pi_k\\) and \\(f_k(\\mathbf{x})\\) So it’s up to us to decide how to estimate priors \\(\\pi_k\\), as well as what class-conditional densities \\(f_k(\\mathbf{x})\\) to use: Normal distribution(s)? Mixture of Normal distributions? Non-parametric estimates (e.g. kernel densities)? Assume predictors are independet (Naive Bayes)? Keep in mind that a Bayes Classfier theoretically works as long as the terms in \\(P(y=k \\mid X=\\mathbf{x})\\) are all correctly specified. Interestingly, we can also try to directly specify the posterior \\(P(y = k \\mid X = \\mathbf{x})\\) with a semi-parametric approach, for instance: \\[ P(y = k | X = \\mathbf{x}) = \\frac{e^{\\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p}}{1 + e^{\\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p}} \\] "],
["logistic.html", "21 Logistic Regression 21.1 Motivation 21.2 Logistic Regression Model", " 21 Logistic Regression So far we’ve been dealing with the prediction of a quantitative response, using mostly linear models. But what about predicting a qualitative or categorical response? We now turn our attention to predicting a discrete (aka categorical) response. To have a gentle transition from regression models into classification models, we’ll start with the famous logistic regression. 21.1 Motivation Let’s consider a classic example: predicting heart attack. The data consists of a number of individuals (patients) with some medical variables. The following (very famous) example is based on data found in Hosmer and Lemeshow (edition from 2000) in which we wish to predict coronary heart disease (chd). The data is relatively small (\\(n = 100\\) patients), and we start by considering only one predictor: age. The following display shows a few data points in the data set. The predictor variable age is expressed in whole years. The response chd is a binary variable with 0 indicating absence of chd, and 1 indicating presence of chd. #&gt; age chd #&gt; 1 20 0 #&gt; 2 23 0 #&gt; 3 24 0 #&gt; 4 25 0 #&gt; 5 25 1 #&gt; 6 26 0 If we graph a scatterplot we get the following image. We have added a little bit of jitter effect to the dots, in order to better visualize their spread. With respect to the x-axis (age), we have values ranging from small x’s to large x’s. In contrast, the response is a binary response, so there are only 0’s or 1’s. As you can tell, the distribution of points does not seem uniform along the x-axis. Moreover, for a given response value, say \\(y_i = 0\\) there are more small values \\(x\\) than large ones. And viceversa, for \\(y_i = 1\\), there are more large values \\(x\\) than small ones. In other words, we can see that (in general) younger people are less likely to have chd than older people. Hence, there seems to be some information about chd in age. The goal is to fit a model that predicts chd from age: \\[ \\textsf{chd} = f(\\textsf{age}) + \\varepsilon \\tag{21.1} \\] 21.1.1 First Approach: Fitting a Line As a naive first approach, we could try to fit a linear model: \\[ \\mathbf{\\hat{y}} = b_0 + b_1 \\mathbf{x} = \\mathbf{Xb} \\tag{21.2} \\] where: \\[ \\mathbf{X} = \\begin{pmatrix} 1 &amp; x_1 \\\\ 1 &amp; x_2 \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; x_n \\\\ \\end{pmatrix}; \\hspace{5mm} \\mathbf{b} = \\begin{pmatrix} b_0 \\\\ b_1 \\\\ \\end{pmatrix} \\] So let’s see what happens if we use least squares to fit a line for this model. # fit line via OLS reg = lm(chd ~ age, data = dat) summary(reg) #&gt; #&gt; Call: #&gt; lm(formula = chd ~ age, data = dat) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -0.85793 -0.33992 -0.07274 0.31656 0.99269 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) -0.537960 0.168809 -3.187 0.00193 ** #&gt; age 0.021811 0.003679 5.929 4.57e-08 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 0.429 on 98 degrees of freedom #&gt; Multiple R-squared: 0.264, Adjusted R-squared: 0.2565 #&gt; F-statistic: 35.15 on 1 and 98 DF, p-value: 4.575e-08 Looking at the estimated coefficients, in particular the coefficient of age, there seems to be some sort of positive relation between age and chd. Which, in this example, it does make sense and matches what we previously stated about younger patients are less likely to have chd (\\(y = 0\\)), whereas older patients are more likely to have chd (\\(y = 1\\)). To better see this, let’s take a look at the scatterplot, and see how the fitted line looks like: This model yields a very awkward fit, with a couple of issues going on. For one thing, the regression line, and consequently the predicted values \\(\\hat{y}_i\\), extend beyond the range \\([0,1]\\). Think about it: we could obtain fitted values \\(\\hat{y}_i\\) taking any number between 0 and 1 (which, in this context, makes no sense). We could also get negative predicted values, or even predicted values greater than 1! (which also does not make sense). On the other hand, if we examine the residuals, then things don’t look great for the linear regression. So, we need a way to fix these problems. 21.1.2 Second Approach: Harsh Thresholding One idea to try to solve the issues from the regression line would be to set some threshold \\(c\\) (for example, \\(c = 0.5\\)) and look at how the predict \\(\\hat{y}_i\\) compares to it: \\[ \\hat{y}_i = b_0 + b_1 x_i \\quad -vs- \\quad c \\] If we think of the linear expression \\(b_0 + b_1 x_i\\) as the signal, we are basically comparing such signal against a specific threshold \\(c\\). Which in turn will allow us to create a decision rule. That is, for a given age value \\(x_0\\), compare its predicted value to the threshold. If \\(\\hat{y}_0 \\geq 0.5\\), classify it as “1,” otherwise classify it as “0.” \\[ \\hat{y}_i = \\begin{cases} 1 \\quad \\text{if} &amp; b_0 + b_1 x_i \\geq c \\\\ &amp; \\\\ 0 \\quad \\text{if} &amp; b_0 + b_1 x_i &lt; c \\end{cases} \\tag{21.3} \\] We can further arrange the terms above to get the following expression: \\[ \\hat{f}(x_i) = b_0 + b_1 x_i - c = \\hat{y}_i \\tag{21.4} \\] and then merge the two constant terms \\(b_0\\) and \\(c\\) into a single constant denoted as \\(b_{0}^{&#39;}\\): \\[\\begin{align*} \\hat{f}(x_i) &amp;= b_0 + b_1 x_i - c \\\\ &amp;= (b_0 - c) +b_1 x_i \\\\ &amp;= b_{0}^{&#39;} + b_1 x_i \\tag{21.5} \\end{align*}\\] In vector notation, the above equation—the so-called signal—becomes: \\[ \\hat{y} = \\mathbf{b^\\mathsf{T}x} \\tag{21.6} \\] By paying attention to the sign of the signal, we can transform our fitted model into: \\[ \\hat{y} = \\text{sign}(\\mathbf{b^\\mathsf{T}x}) \\tag{21.7} \\] that is: \\[ \\hat{y}_i = \\begin{cases} 1 \\quad \\text{if} &amp; \\text{sign}(b_{0}^{&#39;} + b_1 x_i) \\geq 0 \\\\ &amp; \\\\ 0 \\quad \\text{if} &amp; \\text{sign}(b_{0}^{&#39;} + b_1 x_i) &lt; 0 \\end{cases} \\tag{21.8} \\] This transformation imposes a harsh threshold on the signal. Notice that the signal is still linear but we apply a non-linear transformation to it: \\[ \\phi(x) = \\text{sign}(x) \\tag{21.9} \\] Think of this transformation as a quick fix. It’s definitely not something to be proud of, but we could use it to get the job done—although in a quick-dirty fashion. 21.1.3 Third Approach: Conditional Means Using a sign-transformation allows us to overcome some of the limitations of the linear regression model, but it’s far from ideal. An alternative approach involves calculating conditional means. How does that work? The idea is very simple and clever. Say you are looking at patients \\(x = 24\\) years old, and you count the relative frequency of chd cases. In other words, you count the proportion of chd cases among individuals 24 years old. This is nothing else than computing the conditional mean: \\[ avg(y_i | x_i = 24) \\tag{21.10} \\] Following this idea, we could compute all conditional means for all age values: \\[ (\\bar{y}|x_i = 25), \\quad (\\bar{y}|x_i = 26), \\quad \\dots, \\quad (\\bar{y}|x_i = 69) \\] Sometimes, however, we may not have data points for a specific \\(x\\)-value. So instead we can use groups of ages. For example, say we define a first group of ages to be 20 - 29 years. And then calculate the proportion of chd cases in this group. The corresponding average will be: \\[ avg(y_i | x_i \\in \\{ 20 - 29 \\text{ years}\\}) \\tag{21.11} \\] In general, for a given group of age, we calculate the proportion of chd cases as: \\[ avg(y_i | x_i = \\text{age group}) \\tag{21.12} \\] Following our example, we can specify the following age groups: #&gt; age_group #&gt; 20-29 30-34 35-39 40-44 45-49 50-54 55-59 60-69 #&gt; 10 15 12 15 13 8 17 10 Now that we have age by groups, we can get the proportion of coronary heart disease cases in each age group. The results are displayed in the table below. #&gt; # A tibble: 8 x 3 #&gt; age_group count_chd prop_chd #&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 20-29 10 0.1 #&gt; 2 30-34 15 0.133 #&gt; 3 35-39 12 0.25 #&gt; 4 40-44 15 0.333 #&gt; 5 45-49 13 0.462 #&gt; 6 50-54 8 0.625 #&gt; 7 55-59 17 0.765 #&gt; 8 60-69 10 0.8 Likewise, we can graph these averages on a scatterplot: Theoretically, we are modeling the conditional expectations: \\(\\mathbb{E}(y|x)\\). Which is exactly the regression function. By connecting the averages, we get an interesting sigmoid pattern This pattern can be approximated by some mathematical functions, the most popular being the so-called logistic function: \\[ \\text{logistic function:} \\qquad f(s) = \\frac{e^{s}}{1 + e^{s}} \\tag{21.13} \\] Sometimes you may also find the logistic equation in an alternative form: \\[ f(s) = \\frac{e^{s}}{1 + e^{s}} \\quad \\longleftrightarrow \\quad f(s) = \\frac{1}{1 + e^{-s}} \\] Replacing the the signal \\(s\\) by a linear model \\(\\beta_0 + \\beta_1 x\\), we have: \\[\\begin{align*} f(x) &amp;= \\frac{e^{\\beta_0 + \\beta_1 x}}{1 + e^{\\beta_0 + \\beta_1 x}} \\\\ &amp;= \\frac{1}{\\frac{1 + e^{\\beta_0 + \\beta_1 x}}{e^{\\beta_0 + \\beta_1 x}}} \\\\ &amp;= \\frac{1}{\\frac{1}{e^{\\beta_0 + \\beta_1 x}} + 1} \\\\ &amp;= \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x})} \\tag{21.14} \\end{align*}\\] The following figure shows different logistic functions for different \\(\\beta\\) values: Since probability values range inside \\([0,1]\\), instead of using a line to try to approximate these values, we should use a more adequate curve. This is the reason why sigmoid-like curves, such as the logistic function, are preferred for this purpose. 21.2 Logistic Regression Model We consider the following model: \\[ Prob(y \\mid \\mathbf{x}; \\mathbf{b}) = f(\\mathbf{x}) \\tag{21.15} \\] We don’t get to observe the true probability; rather, we observe the noisy target \\(y_i\\), that is generated (or affected) by the probability \\(f(x)\\). How will we model the probability? We would ideally like to use a mathematical function that looks like the sigmoid shape from the toy example above; that is, use a sigmoid function. The most famous function—and the function we will use—is the logistic function: \\[ \\text{logistic function:} \\qquad f(s) = \\frac{e^{s}}{1 + e^{s}} \\tag{21.16} \\] In other words, we have the following model: \\[ Prob(y _i \\mid \\mathbf{x} ; \\mathbf{b} ) = \\frac{e^{\\mathbf{b^\\mathsf{T} x_i}}}{1 + e^{\\mathbf{b^\\mathsf{T} x_i}}} \\tag{21.17} \\] where here \\(\\mathbf{x_i}\\) represents the vector of features for individual \\(i\\). Using the same kind of diagrams depicted for various regression models, we can put logistic regression in this visual format as follows: Figure 21.1: Regression Model Diagram Starting with the input features, we form a linear combination, the so-called linear signal \\(\\mathbf{s} = \\mathbf{Xb}\\), that gets passed to a logistic function \\(\\phi()\\). This is a nonlinear transformation of the linear signal, which allows us to interpret the response as a true probability value. Hence we have that the probabilities are given by: \\[ Prob(y_i \\mid \\mathbf{x_i}, \\mathbf{b}) = \\begin{cases} h(\\mathbf{x_i})&amp; \\textsf{for } y_i = 1 \\\\ &amp; \\\\ 1 - h(\\mathbf{x_i}) &amp; \\textsf{for } y_i = 0 \\\\ \\end{cases} \\tag{21.18} \\] where \\(h()\\) denotes the logistic function \\(\\phi()\\): \\[ h(\\mathbf{x}) = \\phi(\\mathbf{b^\\mathsf{T} x}) \\] 21.2.1 The Criterion Being Optimized We will not be using Mean Squared Error (MSE) as our error measure (since doing so would make no sense in this particular context). Instead, we will use an “error” based on Maximum Likelihood Estimation. In order to do so, we must assume that our model is true; that is: \\[ \\textsf{assuming} \\quad h(x) = f(x) \\] we ask “how likely is it that we observe the data we already observed (i.e. \\(y_i\\))?” We start with the likelihood function \\(L(\\mathbf{b})\\). Note that \\(L\\) implicitly also depends on all \\(X\\)-inputs, and also compute the log-likelihood \\(\\ell(\\mathbf{b})\\): \\[\\begin{align*} Prob(\\mathbf{y} \\mid x_1, x_2, \\dots, x_p; \\mathbf{b}) &amp; = \\prod_{i=1}^{n} P(y_i \\mid \\mathbf{b^\\mathsf{T} x_i} ) \\\\ &amp; = \\prod_{i=1}^{n} h(\\mathbf{b^\\mathsf{T} x_i})^{y_i} \\left[ 1 - h(\\mathbf{b^\\mathsf{T} x_i} ) \\right]^{1 - y_i} \\end{align*}\\] Taking the logarithm (log-likelihood): \\[\\begin{align*} \\ell(\\mathbf{b}) &amp;= \\ln[L(\\mathbf{b}) ] \\\\ &amp;= \\sum_{i=1}^{n} \\ln\\left[ P(y_i \\mid \\mathbf{b^\\mathsf{T} x_i} ) \\right] \\\\ &amp; = \\sum_{i=1}^{n} \\left\\{ y_i \\ln[h(\\mathbf{b^\\mathsf{T} x_i})] + (1 - y_i) \\ln[ 1 - h(\\mathbf{b^\\mathsf{T} x_i} ) ] \\right\\} \\\\ &amp; = \\sum_{i=1}^{n} \\left[ y_i \\ln\\left( \\frac{e^{\\mathbf{b^\\mathsf{T} x_i}} }{1 + e^{\\mathbf{b^\\mathsf{T} x_i} } } \\right) + (1 - y_i) \\ln \\left( 1 - \\frac{e^{\\mathbf{b^\\mathsf{T} x_i}} }{1 + e^{\\mathbf{b^\\mathsf{T} x_i} } } \\right) \\right] \\\\ &amp; = \\sum_{i=1}^{n} \\left[ y_i \\mathbf{b^\\mathsf{T} x_i} - \\ln \\left( 1 + e ^{\\mathbf{b^\\mathsf{T} x_i} } \\right) \\right] \\tag{21.19} \\end{align*}\\] Now, here is the not so good news: differentiating and setting equal to 0 yields and equation for which no closed-form solution exists. To find the maximum likelihood estimate of \\(\\mathbf{b}\\), we require an iterative method: for example, the Newton-Raphson method, or gradient ascent. \\[\\begin{align*} \\nabla \\ell(\\mathbf{b}) &amp; = \\sum_{i=1}^{n} \\left[ y_i \\mathbf{x_i} - \\left( \\frac{e^{\\mathbf{b^\\mathsf{T} x_i}} }{1 + e^{\\mathbf{b^\\mathsf{T} x_i} } } \\right) \\mathbf{x_i} \\right] \\\\ &amp; = \\sum_{i=1}^{n} \\left[ y_i \\mathbf{x_i} - \\phi(\\mathbf{b^\\mathsf{T} x_i} ) \\mathbf{x_i} \\right] \\\\ &amp; = \\sum_{i=1}^{n} \\left[ y_i - \\phi(\\mathbf{b^\\mathsf{T} x_i} ) \\right] \\mathbf{x_i} \\tag{21.20} \\end{align*}\\] Hence, in gradient ascent, we would use: \\[ \\mathbf{b}^{(s + 1)} = \\mathbf{b}^{(s)} + \\alpha \\nabla \\ell(\\mathbf{b}^{(s)}) \\tag{21.21} \\] 21.2.2 Another Way to Solve Logistic Regression The overall picture still stands: Now, however, we will change our probability expression a bit. How? By recoding the response variable \\(y_i\\). Specifically, what it used to be \\(y_i = 0\\) let’s now encode it as \\(y_i = -1\\). \\[ Prob(y_i \\mid \\mathbf{x_i}, \\mathbf{b}) = \\begin{cases} h(\\mathbf{x_i}) &amp; \\textsf{for } y_i = 1 \\\\ &amp; \\\\ 1 - h(\\mathbf{x_i}) &amp; \\textsf{for } y_i = -1 \\\\ \\end{cases} \\tag{21.22} \\] Recall that \\(h()\\) is the logistic function \\(\\phi()\\): \\[ h(\\mathbf{x}) = \\phi(\\mathbf{b^\\mathsf{T} x}) \\] They key observation is to note the following property of the logistic function: \\[ \\phi(-s) = 1 - \\phi(s) \\tag{21.23} \\] Taking into account this property, we can update the expression of the conditional probability: \\[ Prob(y_i \\mid \\mathbf{x_i}, \\mathbf{b}) = \\begin{cases} h(\\mathbf{x_i}) = \\phi(y_i \\mathbf{b^\\mathsf{T} x}) &amp; \\textsf{for } y_i = 1 \\\\ &amp; \\\\ 1 - h(\\mathbf{x_i}) = \\phi(y_i \\mathbf{b^\\mathsf{T} x}) &amp; \\textsf{for } y_i = -1 \\\\ \\end{cases} \\tag{21.24} \\] Notice that this allows us to simplify things in a very convenient way. This implies that we only need one case regardless of the value of \\(y_i\\): \\[ Prob(y_i \\mid \\mathbf{x_i}, \\mathbf{b}) = \\phi(y_i \\mathbf{b^\\mathsf{T} x}) \\tag{21.25} \\] With this, we begin computing the likelihood as we previously did: \\[\\begin{align*} Prob(\\mathbf{y} \\mid x_1, x_2, \\dots, x_p; \\mathbf{b}) &amp; = \\prod_{i=1}^{n} P(y_i \\mid \\mathbf{b^\\mathsf{T} x_i} ) \\\\ &amp; = \\prod_{i=1}^{n} \\phi\\left( y_i \\mathbf{b^\\mathsf{T} x_i} \\right) \\\\ \\tag{21.26} \\end{align*}\\] and then the log-likelihood: \\[\\begin{align*} \\ell(\\mathbf{b}) : = \\ln[L(\\mathbf{\\beta})] &amp; = \\sum_{i=1}^{n} \\ln\\left[ \\phi\\left( y_i \\mathbf{b^\\mathsf{T} x_i} \\right) \\right] \\\\ &amp; \\Rightarrow \\frac{1}{n} \\sum_{i=1}^{n} \\underbrace{ \\ln\\left[ \\phi\\left( y_i \\mathbf{b^\\mathsf{T} x_i} \\right) \\right] }_{\\text{logit}} \\tag{21.27} \\end{align*}\\] Notice that we are adding the constant \\(1/n\\). The inclusion of this term does not modify the maximization of the loglikelihood. But it will help us in the next step to have a mathematically convenient expression. Here comes an important trick. Instead of maximizing the log-likelihood, we can minimize the negative of the loglikelihood. In other words, we can perform the following minimization problem: \\[ \\min_{\\mathbf{b}} \\left\\{ - \\frac{1}{n} \\sum_{i=1}^{n} \\ln\\left[ \\phi\\left( y_i \\mathbf{b^\\mathsf{T} x_i} \\right) \\right] \\right\\} \\tag{21.28} \\] Doing some algebra, it can be proved that the above criterion is equivalent to the right hand side of the expression below: \\[\\begin{align*} &amp; \\min_{\\mathbf{b}} \\left\\{ - \\frac{1}{n} \\sum_{i=1}^{n} \\ln\\left[ \\phi\\left( y_i \\mathbf{b^\\mathsf{T} x_i} \\right) \\right] \\right\\} \\ \\Leftrightarrow \\ \\min_{\\mathbf{b}} \\left\\{ \\underbrace{ \\frac{1}{n} \\sum_{i=1}^{n} \\underbrace{ \\ln\\left( 1 + e^{-y_i \\mathbf{b^\\mathsf{T} x_i} } \\right) }_{\\text{pointwise error}} }_{E_{in}(\\mathbf{b})} \\right\\} \\end{align*}\\] Focus on the product term between the response and the linear signal: \\[ y_i \\mathbf{b^\\mathsf{T} x_i} =: y_i \\mathbf{s} \\tag{21.29} \\] where \\(\\mathbf{s}\\) represents “signal”. To better understand this term let us use the following table: We can think of the signal term \\(\\mathbf{b^\\mathsf{T} x_i}\\) as a numeric value or “score” ranging from “very small” to “very large”. A small signal means that the probability \\(\\phi(\\mathbf{b^\\mathsf{T} x_i})\\) will be small. Conversely, a large signal means that \\(\\phi(\\mathbf{b^\\mathsf{T} x_i})\\) will be large. In turn, the term \\(y_i\\) can be either \\(-1\\) or \\(+1\\). If we have a correct prediction, we would expect a small probability \\(\\phi(\\mathbf{b^\\mathsf{T} x_i})\\) to agree with \\(y_i = -1\\). Likewise, we would also expect a large probability \\(\\phi(\\mathbf{b^\\mathsf{T} x_i})\\) to agree with \\(y_i = +1\\). In turn, if we have an incorrect prediction, we would expect a small probability \\(\\phi(\\mathbf{b^\\mathsf{T} x_i})\\) to mismatch \\(y_i = +1\\). Likewise, we would also expect a large probability \\(\\phi(\\mathbf{b^\\mathsf{T} x_i})\\) to disagree with an observed \\(y_i = -1\\). With agreements, \\(e^{-y_i \\mathbf{b^\\mathsf{T} x_i} }\\) will be small, and will consequently give you a small error. With disagreements, \\(e^{-y_i \\mathbf{b^\\mathsf{T} x_i} }\\) will be large, and will consequently give you a large error. We will also refer to our \\(E_{in}(\\mathbf{b})\\) quantity as the mean cross-entropy error. Technically it isn’t a “cross-entropy” measure in the classification sense, however we will ignore that technicality for now. How do we find \\(\\mathbf{b}\\) that minimizes the criterion: \\[ \\min_{\\mathbf{b}} \\left\\{ \\underbrace{ \\frac{1}{n} \\sum_{i=1}^{n} \\underbrace{ \\ln\\left( 1 + e^{-y_i \\mathbf{b^\\mathsf{T} x_i} } \\right) }_{\\text{pointwise error}} }_{E_{in}(\\mathbf{b})} \\right\\} \\tag{21.30} \\] This expression does not have a closed analytical solution. So in order to find the minimum, we need to apply iterative optimization methods. For example, gradient descent. Consequently, calculating the gradient of the above expression, we find: \\[ \\nabla E_{in}(\\mathbf{b}) = - \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\frac{1}{1 + e^{-y_i \\mathbf{b^\\mathsf{T} x_i} } } \\right) y_i \\mathbf{x_i} \\tag{21.31} \\] and we would use gradient descent to compute the minimum. "],
["discrim.html", "22 Preamble for Discriminant Analysis 22.1 Motivation 22.2 Derived Ratios from Sum-of-Squares 22.3 Geometric Perspective", " 22 Preamble for Discriminant Analysis We now turn our attention to a different kind of classification methods that belong to a general framework known as Dicriminant Analysis. In this chapter, we introduce some preliminary concepts that should allow you to have a better understanding of some underlying ideas behind discriminant analysis methods. The starting point involves discussing some aspects that are typically studied in Analysis of Variance (ANOVA). Overall, we focus on certain notions and formulas to measure variation (or dispersion) within classes and between classes. Now, keep in mind that we won’t describe any inferential aspects that are commonly used in statistical tests for comparing means of classes. 22.1 Motivation Let’s consider the famous Iris data set collected by Edgar Anderson (1935), and used by Ronald Fisher (1936) in his seminal paper about Discriminant Analysis: The use of multiple measurements in taxonomic problems. This data consists of 5 variables measured on \\(n = 150\\) iris flowers. There are \\(p\\) = 4 predictors, and one response. The four variables are: Sepal.Length Sepal.Width Petal.Length Petal.Width The response is a categorical (i.e. qualitative) variables that indicates the species of iris flower with three categories: setosa versicolor virginica We should say that the iris data set is a classic textbook example. It is: clean data tidy data classess are fairly well separated size of data (small) good for learning and teaching purposes Keep in mind that most real data sets won’t be like the iris data. #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; 1 5.1 3.5 1.4 0.2 setosa #&gt; 2 4.9 3.0 1.4 0.2 setosa #&gt; 3 4.7 3.2 1.3 0.2 setosa #&gt; 4 4.6 3.1 1.5 0.2 setosa #&gt; 5 5.0 3.6 1.4 0.2 setosa #&gt; 6 5.4 3.9 1.7 0.4 setosa Some summary statistics: #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width #&gt; Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 #&gt; 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 #&gt; Median :5.800 Median :3.000 Median :4.350 Median :1.300 #&gt; Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 #&gt; 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 #&gt; Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 #&gt; Species #&gt; setosa :50 #&gt; versicolor:50 #&gt; virginica :50 #&gt; #&gt; #&gt; Let’s look at the distribution of the four input variables without making distinction between species of iris flowers: As you can tell from the above conditional boxplots, all predictors have different ranges, as well as different types of distributions. Now let’s take into account the group structure according to the species: You should be able to observe that the boxplot of some predictors are fairly different between iris species. For example, take a look at the boxplots of Petal.Length, and the boxplots of Petal.Width. In contrast, predictors like Sepal.Length and Sepal.Width have boxplots that are not as different as those of Petal.Length. The same differences can be seen if we take a look at density curves: 22.1.1 Distinguishing Species Let us consider the following question: Which predictor provides the “best” distinction between Species? In classification problems, the response variable \\(Y\\) provides a group or class structure to the data. We expect that the predictors will help us to differentiate (i.e. discriminate) between one class and the others. The general idea is to look for systematic differences among classes. But how? A “natural” way to look for differences is paying attention to class means. Let’s begin with a single predictor \\(X\\) and a categorical response \\(Y\\) measured on \\(n\\) individuals. Let’s take into account the class structure conveyed by \\(Y\\) Assume there are \\(K\\) classes (or categories) Let \\(\\mathcal{C_k}\\) represent the \\(k\\)-th class in \\(Y\\) Let \\(n_k\\) be the number of observations in class \\(\\mathcal{C_k}\\), Then: \\[ n = n_1 + n_2 + \\dots + n_K = \\sum_{k=1}^{K} n_k \\tag{22.1} \\] The (global) mean value of \\(X\\) is: \\[ \\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i \\tag{22.2} \\] Each class \\(k\\) will have its mean \\(\\bar{x}_k\\): \\[ \\bar{x}_k = \\frac{1}{n_k} \\sum_{i \\in \\mathcal{C_k}} x_{ik} \\tag{22.3} \\] #&gt; overall mean of Sepal.Length #&gt; [1] 5.843333 Recall that a measure of (global) dispersion in \\(X\\) is given by the total sum of squares (\\(\\text{tss}\\)): \\[ \\text{tss} = \\sum_{i=1}^{n} (x_i - \\bar{x})^2 \\tag{22.4} \\] #&gt; total sums-of-squares of Sepal.Length #&gt; [1] 102.1683 Each class \\(k\\) will also have its own sum-of-squares \\(\\text{ss}_k\\) \\[ \\text{ss}_k = \\sum_{i \\in \\mathcal{C_k}} (x_{ik} - \\bar{x}_k)^2 \\tag{22.5} \\] One way to look for systematic differences between the classes is to compare their means. If there’s no group difference in \\(X\\), then the group means \\(\\bar{x}_{k}\\) should be similar. If there is really a difference, it is likely that one or more of the mean values will differ. A useful measure to compare differences among the \\(k\\) means is the deviation from the overall mean: \\[ \\bar{x}_{k} - \\bar{x} \\] An effective summary of these deviations is the so-called between-group sum of squares (\\(\\text{bss}\\)) given by: \\[ \\text{bss} = \\sum_{k=1}^{K} n_k (\\bar{x}_{k} - \\bar{x})^2 \\tag{22.6} \\] #&gt; between sums-of-squares of Sepal.Length #&gt; [1] 63.21213 To assess the relative magnitude of the between sum of squares (\\(\\text{bss}\\)), we need to compare it to a measure of the “background” variation. Such a measure of background variation can be formed by combining the group variances into a pooled-estimate called within-group sum of squares (\\(\\text{wss}\\)): \\[ \\text{wss} = \\sum_{k=1}^{K} \\sum_{i \\in \\mathcal{C_k}} (x_{ik} - \\bar{x}_k)^2 = \\text{ss}_1 + \\dots + \\text{ss}_K \\tag{22.7} \\] #&gt; within sums-of-squares of Sepal.Length #&gt; [1] 38.9562 So far we have three types of sums of squares: \\[\\begin{align*} \\textsf{total} \\quad \\text{tss} &amp;= \\sum_{i=1}^{n} (x_i - \\bar{x})^2\\\\ \\textsf{between} \\quad \\text{bss} &amp;= \\sum_{k=1}^{K} n_k (\\bar{x}_{k} - \\bar{x})^2 \\\\ \\textsf{within} \\quad \\text{wss} &amp;= \\sum_{k=1}^{K} \\sum_{i \\in \\mathcal{C_k}} (x_{ik} - \\bar{x}_k)^2 \\tag{22.8} \\end{align*}\\] 22.1.2 Sum of Squares Decomposition An important aspect has to do with looking at the squared deviations, \\((x_{i} - \\bar{x})^2\\), in terms of the class structure. A useful trick is to rewrite the deviation terms \\(x_{i} - \\bar{x}\\) as: \\[\\begin{align*} x_{i} - \\bar{x} &amp;= x_{i} - (\\bar{x}_{k} - \\bar{x}_{k}) - \\bar{x} \\\\ &amp;= (x_{i} - \\bar{x}_{k}) + (\\bar{x}_{k} - \\bar{x}) \\tag{22.9} \\end{align*}\\] We can decompose \\(\\text{tss}\\) in terms of \\(\\text{bss}\\) and \\(\\text{wss}\\) as follows: \\[ \\underbrace{\\sum_{k=1}^{K} \\sum_{i \\in \\mathcal{C_k}} (x_{ik} - \\bar{x})^2}_{\\text{tss}} = \\underbrace{\\sum_{k=1}^{K} n_k (\\bar{x}_k - \\bar{x})^2}_{\\text{bss}} + \\underbrace{\\sum_{k=1}^{K} \\sum_{i \\in \\mathcal{C_k}} (x_{ik} - \\bar{x}_k)^2}_{\\text{wss}} \\tag{22.10} \\] In summary: \\[ \\text{tss} = \\text{bss} + \\text{wss} \\tag{22.11} \\] 22.2 Derived Ratios from Sum-of-Squares We now present two ratios derived from these sums of squares: Correlation ratio F-ratio 22.2.1 Correlation Ratio The correlation ratio is a measure of the relationship between the dispersion within groups and the dispersion across all individuals. Correlation ratio \\(\\eta^2\\) (originally proposed by Karl Pearson) is given by: \\[ \\eta^2(X,Y) = \\frac{\\text{bss}}{\\text{tss}} \\tag{22.12} \\] \\(\\eta^2\\) takes vaues between 0 and 1. \\(\\eta^2 = 0\\) represents the special case of no dispersion among the means of the different groups. \\(\\eta^2 = 1\\) refers to no dispersion within the respective groups. #&gt; correlation ratio of Sepal Length and Species #&gt; [1] 0.6187057 22.2.2 F-Ratio With \\(\\text{tss} = \\text{bss} + \\text{wss}\\), we can also calculate the \\(F\\)-ratio (proposed by R.A. Fisher): \\[ F = \\frac{\\text{bss} / (K-1)}{\\text{wss} / (n-K)} \\tag{22.13} \\] The larger the value of both ratios, the more variability is there between groups than within groups. #&gt; F-ratio of Sepal Length and Species #&gt; [1] 119.2645 The \\(F\\)-ratio can be used for hypothesis testing purposes. More formally, a null hypothesis postulates that the population means do not differ (\\(H_0: \\mu_1 = \\mu_2 = \\dots = \\mu_K = \\mu)\\) versus the alternative hypothesis \\(H_1\\) that one or more population means differ among the \\(K\\) normally distributed populations. Assuming or knowing that the variances of each sampled population are the same \\(\\sigma^2\\), a test statistic to assess the null hypothesis is: \\[ F = \\frac{\\text{bss} / (K-1)}{\\text{wss} / (n-K)} \\tag{22.14} \\] which has an \\(F\\)-distribution with \\(K-1\\) and \\(n-K\\) degrees of freedom under the null hypothesis. Example with Iris data Let’s compute the dispersion decompositions for all predictors, and obtain the correlation ratios and \\(F\\)-ratios #&gt; correlation ratios #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width #&gt; 0.6187057 0.4007828 0.9413717 0.9288829 #&gt; F-ratios #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width #&gt; 119.26450 49.16004 1180.16118 960.00715 22.3 Geometric Perspective As we’ve been doing throughout most chapters in the book, let’s provide a geometric perspective of what’s going on with the data, and the classification setting behind discriminant analysis. As usual, assume that the objects form a cloud of points in \\(p\\)-dimensional space. Figure 22.1: Cloud of n points in p-dimensional space One of the things that we can do is to look at the average individual, denoted by \\(\\mathbf{g}\\), also known as the global centroid (i.e. the center of gravity of the cloud of points). Figure 22.2: The centroid of the cloud of points The global centroid \\(\\mathbf{g}\\) is the point of averages which consists of the point formed with all the variable means: \\[ \\mathbf{g} = [\\bar{x}_1, \\bar{x}_2, \\dots, \\bar{x}_p] \\tag{22.15} \\] where: \\[ \\bar{x}_j = \\frac{1}{n} \\sum_{i=1}^{n} x_{ij} \\tag{22.16} \\] If all variables are mean-centered, the centroid is the origin \\[ \\mathbf{g} = \\underbrace{[0, 0, \\dots, 0]}_{p \\text{ times}} \\tag{22.17} \\] Taking the global centroid as a point of reference, we can look at the amount of spread or dispersion in the data. Assuming centered features, a matrix of total dispersion is given by the Total Sums of Squares (\\(\\text{TSS}\\)): \\[ \\text{TSS} = \\mathbf{X^\\mathsf{T} X} \\tag{22.18} \\] Alternatively, we can get the (sample) variance-covariance matrix \\(\\mathbf{V}\\): \\[ \\mathbf{V} = \\frac{1}{n-1} \\mathbf{X^\\mathsf{T} X} \\tag{22.19} \\] 22.3.1 Clouds from Class Structure Here’s some notation that we’ll be using while covering some discriminant methods: Let \\(n_k\\) be the number of observations in the \\(k\\)-th class Let \\(x_{ijk}\\) represent the \\(i\\)-th observation, of the \\(j\\)-th variable, in the \\(k\\)-th class Let \\(x_{ik}\\) represent \\(i\\)-th observation in class \\(k\\) Let \\(x_{jk}\\) represent \\(j\\)-th variable in class \\(k\\) Let \\(n_k\\) be the number of observations in the \\(k\\)-th class \\(\\mathcal{C_k}\\) The number of individuals: \\(n = n_1 + n_2 + \\dots + n_K = \\sum_{k=1}^{K} n_k\\) Let’s now take into the account the class structure given by the response variable. Figure 22.3: The objects are divided into classes or groups Each class is denoted by \\(\\mathcal{C_k}\\), and it is supposed to be formed by \\(n_k\\) individuals. Figure 22.4: Each class forms its own sub-cloud We can also look at the local or class centroids (one per class) Figure 22.5: Each class has its own centroid The class centroid \\(\\mathbf{g_k}\\) is the point of averages for those observations in class \\(k\\): \\[ \\mathbf{g_k} = [\\bar{x}_{1k}, \\bar{x}_{2k}, \\dots, \\bar{x}_{pk}] \\tag{22.20} \\] where: \\[ \\bar{x}_{jk} = \\frac{1}{n_k} \\sum_{i \\in \\mathcal{C_k}} x_{ij} \\tag{22.21} \\] We can focus on the dispersion within the clouds Figure 22.6: Dispersion within the sub-clouds Each group will have an associated spread or dispersion matrix given by a Class Sums of Squares (\\(\\text{CSS}\\)): \\[ \\text{CSS}_k = \\mathbf{X_{k}^{\\mathsf{T}} X_k} \\tag{22.22} \\] Equivalently, there is an associated variance matrix \\(\\mathbf{W_k}\\) for each class \\[ \\mathbf{W_k} = \\frac{1}{n_k - 1} \\mathbf{X_{k}^{\\mathsf{T}} X_k} \\tag{22.23} \\] where \\(\\mathbf{X_k}\\) is the data matrix of the \\(k\\)-th class We can combine the class dispersion to obtain a Within-class Sums of Squares (\\(\\text{WSS}\\)) matrix: \\[\\begin{align*} \\text{WSS} &amp;= \\sum_{k=1}^{K} \\mathbf{X_{k}^{\\mathsf{T}} X_k} \\\\ &amp;= \\sum_{k=1}^{K} \\text{CSS}_k \\tag{22.24} \\end{align*}\\] Likewise, we can combine the class variances \\(\\mathbf{W_k}\\) as a weighted average to get the Within-class variance matrix \\(\\mathbf{W}\\): \\[ \\mathbf{W} = \\sum_{k=1}^{K} \\frac{n_k - 1}{n - 1} \\mathbf{W_k} \\tag{22.25} \\] What if we focus on just the centroids? Figure 22.7: Global and Group Centroids Note that the global centroid \\(\\mathbf{g}\\) can be expressed as a weighted average of the group centroids: \\[\\begin{align*} \\mathbf{g} &amp;= \\frac{n_1}{n} \\mathbf{g_1} + \\frac{n_2}{n} \\mathbf{g_2} + \\dots + \\frac{n_K}{n} \\mathbf{g_K} \\\\ &amp;= \\sum_{k=1}^{K} \\left ( \\frac{n_k}{n} \\right ) \\mathbf{g_k} \\tag{22.26} \\end{align*}\\] Focusing on just the centroids, we can get its corresponding matrix of dispersion given by the Between Sums of Squares (\\(\\text{BSS}\\)): \\[ \\text{BSS} = \\sum_{k=1}^{K} n_k (\\mathbf{g_k - g})(\\mathbf{g_k - g})^\\mathsf{T} \\tag{22.27} \\] Equivalently, there is an associated Between-class variance matrix \\(\\mathbf{B}\\) \\[ \\mathbf{B} = \\sum_{k=1}^{K} \\frac{n_k}{n - 1} (\\mathbf{g_k - g})(\\mathbf{g_k - g})^\\mathsf{T} \\tag{22.28} \\] Three types of Dispersions Let’s recap. We have three types of sums-of-squares matrices: \\(\\text{TSS}\\): Total Sums of Squares \\(\\text{WSS}\\): Within-class Sums of Squares \\(\\text{BSS}\\): Between-class Sums of Squares Alternatively, we also have three types of variance matrices: \\(\\mathbf{V}\\): Total variance \\(\\mathbf{W}\\): Within-class variance \\(\\mathbf{B}\\): Between-class variance 22.3.2 Dispersion Decomposition It can be shown (Huygens theorem) for both, sums-of-squares and variances, that the total dispersion (TSS or \\(\\mathbf{V}\\)) can be decomposed as: \\(\\text{TSS} = \\text{BSS} + \\text{WSS}\\) \\(\\mathbf{V} = \\mathbf{B} + \\mathbf{W}\\) Let \\(\\mathbf{X}\\) be the \\(n \\times p\\) mean-centered matrix of predictors, and \\(\\mathbf{Y}\\) be the \\(n \\times K\\) dummy matrix of classes: \\(\\text{TSS} = \\mathbf{X^\\mathsf{T} X}\\) \\(\\text{BSS} = \\mathbf{X^\\mathsf{T} Y (Y^\\mathsf{T} Y)^{-1} Y^\\mathsf{T} X}\\) \\(\\text{WSS} = \\mathbf{X^\\mathsf{T} (I - Y (Y^\\mathsf{T} Y)^{-1} Y^\\mathsf{T}) X}\\) Figure 22.8: Dispersion Decomposition "],
["cda.html", "23 Canonical Discriminant Analysis 23.1 CDA: Semi-Supervised Aspect 23.2 Looking for a discriminant axis 23.3 Looking for a Compromise Criterion 23.4 CDA: Supervised Aspect", " 23 Canonical Discriminant Analysis In this chapter we talk about Canonical Discriminant Analysis (CDA), which is a special case of Linear Discriminant Analysis (LDA). The main reason why we introduce CDA separately, is because this method has a somewhat hybrid learning nature with two aspects: Semi-supervised: On one hand, CDA has an unsupervised or descriptive aspect that aims to tackle the following question. How to find a representation of the objects which provides the best separation between classes? Supervised: On the other hand, CDA also has a decisively supervised aspect that tackles the question: How to find the rules for assigning a class to a given object? We begin with the semi-supervised part of CDA, approaching its exploratory nature from a purely geometric approach. This discussion covers most of the material in this chapter. And then at the end, we present its supervised aspect, describing how to use CDA for classification purposes. 23.1 CDA: Semi-Supervised Aspect We can formulate the classfication problem behind CDA in a geometric way. For sake of illustration, let’s consider three classes in a 2-dimensional space, like depicted in the figure below. Figure 23.1: Three classes in 2-dim space From an exploratory/descriptive perspective, we are looking for a good low dimensional representation that separates the three classes. One option is to consider the axis associated with the predictor \\(X_1\\), that is, the horizontal axis in the next figure: Figure 23.2: Axis \\(X_1\\) separates class 1 from groups 2 and 3 If we project the individuals onto the \\(X_1\\) axis, class 1 is well separated from classes 2 and 3. However, class 2 is largely confounded with class 3. Another option is to consider the axis associated with the predictor \\(X_2\\), that is, the vertical axis in the figure below: Figure 23.3: Axis \\(X_1\\) separates class 3 from groups 1 and 2 As you can tell, if we project the individuals onto this axis \\(X_2\\), class 3 is well separated from classes 1 and 2. However, class 1 is largely confounded with class 2. Is there an axis or dimension that “best” separates the three clouds? We can try to look for an optimal representation in the sense of finding an axis that best separates the clouds, like in the following diagram: Figure 23.4: An axis the best separates all three classes To summarize, the exploratory aspect of canonical discriminant analysis has to do with seeking a low dimensional representation in which the class of objects are well separated. 23.2 Looking for a discriminant axis How do we find a low dimensional representation of the objects which provides the best separation between classes? To answer this question, we can look for an axis \\(\\Delta_u\\), spanned by some vector \\(\\mathbf{u}\\), such that the linear combination \\[ Z = u_1 X_1 + u_2 X_2 \\] separates all three groups in an adequate way. Figure 23.5: Looking for a discriminant axis Algebraically, the idea is to look for a linear combination of the predictors: \\[ \\mathbf{z} = \\mathbf{Xu} \\] that ideally could achieve the following two goals: Minimize within-class dispersion (\\(\\text{wss}\\)): \\(\\quad min \\{ \\mathbf{u^\\mathsf{T} W u \\}}\\) Maximize between-class dispersion (\\(\\text{bss}\\)): \\(\\quad max \\{ \\mathbf{u^\\mathsf{T} B u \\}}\\) On one hand, it would be nice to have \\(\\mathbf{u}\\), such that the between-class dispersion is maximized. This corresponds to a situation in which the class centroids are well separated: Figure 23.6: Maximize between-class dispersion On the other hand, it would also make sense to have \\(\\mathbf{u}\\), such that the within-class dispersion is minimized. This implies having classes in which, on average, the “inner” variation is small (i.e. concentrated local dispersion) Figure 23.7: Minimize within-class dispersion Can we find such a mythical vector \\(\\mathbf{u}\\)? We have not so good news. It is generally impossible to find an axis \\(\\Delta_u\\), spanned by \\(\\mathbf{u}\\), which simoultaneously: maximizes the between-groups variance minimizes the within-groups variance Let’s see a cartoon picture of this issue. Say we have two classes, each one with its centroid. Figure 23.8: Minimize within-class dispersion Maximizing the between-class separation, involves choosing \\(\\mathbf{u}_a\\) parallel to the segment linking the centroids. In other words, the direction of \\(\\mathbf{u}\\) is the one that crosses the centroids: Figure 23.9: Maximize between-class dispersion Minimizing the within-class separation, involves finding \\(\\mathbf{u}_b\\) perpendicular to the principal axis of the ellipses. This type of \\(\\mathbf{u}\\) does not necessarily crosses through the centroids: Figure 23.10: Minimize within-class dispersion In general, we end up with two possibilities \\(\\mathbf{u}_a \\neq \\mathbf{u}_b\\): Figure 23.11: Double goal of discriminant analysis … generally impossible Let’s summarize the geometric motivation behind canonical discriminant analysis. We seek to find the linear combination of the predictors such that the between-class variance is maximized relative to the within-class variance. In other words, we want to find the combination of the predictors that gives maximum separation between the centroids of the data while at the same time minimizing the variation within each class of data. In general, accomplishing these two goals in a simultaneous way is impossible. 23.3 Looking for a Compromise Criterion So far we have an impossible simultaneity involving a minimization criterion, as well as a maximization criterion: \\[ min \\{ \\mathbf{u^\\mathsf{T} W u} \\} \\Longrightarrow \\mathbf{W u} = \\alpha \\mathbf{u} \\\\ \\textsf{and} \\\\ max \\{ \\mathbf{u^\\mathsf{T} B u} \\} \\Longrightarrow \\mathbf{B u} = \\beta \\mathbf{u} \\] What can we do about this simultaneous impossibility? Well, we need to look for a compromise. This is where the variance decomposition discussed in the previous chapter comes handy: \\[ \\mathbf{V} = \\mathbf{W} + \\mathbf{B} \\] Doing some algebra, it can be shown that the quadratic form \\(\\mathbf{u^\\mathsf{T} V u}\\) can be decomposed as: \\[ \\mathbf{u^\\mathsf{T} V u} = \\mathbf{u^\\mathsf{T} W u} + \\mathbf{u^\\mathsf{T} B u} \\] Again, we are pursuing a dual goal that is, in general, hard to accomplish: \\[ \\mathbf{u^\\mathsf{T} V u} = \\underbrace{\\mathbf{u^\\mathsf{T} W u}}_{\\text{minimize}} + \\underbrace{\\mathbf{u^\\mathsf{T} B u}}_{\\text{maximize}} \\] We have two options for the compromise: \\[ max \\left \\{ \\frac{\\mathbf{u^\\mathsf{T} B u}}{\\mathbf{u^\\mathsf{T} V u}} \\right \\} \\quad \\textsf{OR} \\quad max \\left \\{ \\frac{\\mathbf{u^\\mathsf{T} B u}}{\\mathbf{u^\\mathsf{T} W u}} \\right \\} \\] which are actually associated to the two ratios described in the previous chapter: correlation ratio \\(\\eta^2\\), and \\(F\\)-ratio: \\[ \\eta^2 \\leftrightarrow \\frac{\\mathbf{u^\\mathsf{T} B u}}{\\mathbf{u^\\mathsf{T} V u}}, \\qquad F \\leftrightarrow\\frac{\\mathbf{u^\\mathsf{T} B u}}{\\mathbf{u^\\mathsf{T} W u}} \\] Which option to choose? The good news is that both options are equivalent, so using any one of them should give you similar—although not exactly identical—results. 23.3.1 Correlation Ratio Criterion Suppose we decide to work with the first criterion. We look for \\(\\mathbf{u}\\) such that: \\[ max \\left \\{ \\frac{\\mathbf{u^\\mathsf{T} B u}}{\\mathbf{u^\\mathsf{T} V u}} \\right \\} \\] This criterion is scale invariant, meaning that we use any scale variation of \\(\\mathbf{u}\\): i.e. \\(\\alpha \\mathbf{u}\\). For convenience, we can impose a normalizing restriction: \\(\\mathbf{u^\\mathsf{T} V u} = 1\\). Consequently: \\[ max \\left \\{ \\frac{\\mathbf{u^\\mathsf{T} B u}}{\\mathbf{u^\\mathsf{T} V u}} \\right \\} \\Longleftrightarrow max \\{\\mathbf{u^\\mathsf{T} B u}\\} \\quad \\text{s.t.} \\quad \\mathbf{u^\\mathsf{T} V u} = 1 \\] Using the method of Lagrangian multiplier: \\[ L(\\mathbf{u}) = \\mathbf{u^\\mathsf{T} B u} - \\lambda (\\mathbf{u^\\mathsf{T} V u} - 1) \\] Deriving w.r.t \\(\\mathbf{u}\\) and equating to zero: \\[ \\frac{\\partial L(\\mathbf{u})}{\\partial \\mathbf{u}} = 2 \\mathbf{Bu} - 2 \\lambda \\mathbf{Vu} = \\mathbf{0} \\] The optimal vector \\(\\mathbf{u}\\) is such that: \\[ \\mathbf{Bu} = \\lambda \\mathbf{Vu} \\] If the matrix \\(\\mathbf{V}\\) is inversible (which it is in general) then: \\[ \\mathbf{V^{-1}Bu} = \\lambda \\mathbf{u} \\] that is, the optimal vector \\(\\mathbf{u}\\) is eigenvector of \\(\\mathbf{V^{-1}B}\\). Keep in mind that, in general, \\(\\mathbf{V^{-1}B}\\) is not symmetric. 23.3.2 F-ratio Criterion If we decide to work with the criterion associated to the \\(F\\)-ratio, then the criterion to be maximized is: \\[ max \\left \\{ \\frac{\\mathbf{u^\\mathsf{T} B u}}{\\mathbf{u^\\mathsf{T} W u}} \\right \\} \\Longleftrightarrow max \\{\\mathbf{u^\\mathsf{T} B u}\\} \\quad \\text{s.t.} \\quad \\mathbf{u^\\mathsf{T} W u} = 1 \\] Applying the same Lagrangian procedure, with a multiplier \\(\\rho\\), we have that \\(\\mathbf{u}\\) is such a vector that: \\[ \\mathbf{Bu} = \\rho \\mathbf{Wu} \\] and if \\(\\mathbf{W}\\) is invertible (which it is in most cases), then it can be shown that \\(\\mathbf{u}\\) is also eigenvector of \\(\\mathbf{W^{-1}B}\\), associated to eigenvalue \\(\\rho\\): \\[ \\mathbf{W^{-1}Bu} = \\rho \\mathbf{u} \\] Likewise, keep in mind that, in general, \\(\\mathbf{W^{-1}B}\\) is not symmetric. The relationship between the eigenvalues \\(\\lambda\\) and \\(\\rho\\) is: \\[ \\rho = \\frac{\\lambda}{1 - \\lambda} \\] Why is it important to keep in mind that, in general, both matrices \\(\\mathbf{V^{-1}B}\\) and \\(\\mathbf{W^{-1}B}\\) are not symmetric? Because most software routines that diagonilize a matrix (to find its eigenvalue decomposition) use the Jacobi method, which works with symmetric matrices. 23.3.3 A Special PCA What do \\(\\mathbf{u}\\) and \\(\\mathbf{W^{-1}}\\) correspond in geometric terms? The vector \\(\\mathbf{u}\\) is the axis from the PCA on the cloud of centroids \\(\\mathbf{g_1}, \\mathbf{g_2}, \\dots, \\mathbf{g_K}\\), but it is an axis on which the points are projected obliquely, not orthogonally. Figure 23.12: Oblique Projection Without this obliqueness, corresponding to the equivalent metrics \\(\\mathbf{V^{-1}}\\) and \\(\\mathbf{W^{-1}}\\), this would be a simple PCA performed on the centroids, in which the classes would be less well separated, because of an orthogonal projection. Figure 23.13: Orthogonal Projection The implication of using a metric matrix such as \\(\\mathbf{W^{-1}}\\) (or \\(\\mathbf{V^{-1}}\\)), is that the separation of two points depends not only on a Euclidean measurement, but also on the variance and correlation of the variables. So, in summary: \\(\\mathbf{u}\\) is the vector associated to the so-called canonical axis When the first canonical axis has been determined, we search for a 2nd one The second axis should be the most discriminant and uncorrelated with the first one This procedure is repeated until the number of axis reaches the minimum of: \\(K - 1\\) and \\(p\\) In fact, it is not the canonical axes that are manipulated directly, but the canonical variables or vectors associated to the canonical axes. 23.4 CDA: Supervised Aspect The supervised learning aspect of CDA has to do with the question: how do we use it for classification purposes? This involves establishing a decision rule that let us predict the class of an object. CDA proposes a geometric rule of classification. As we will see, such rule has a linear form. This is the reason why we consider CDA to be a special member of the family of Linear Discriminant Analysis. 23.4.1 Distance behind CDA The first thing we need to discuss is the notion of distance used in CDA. For this, consider two vectors \\(\\mathbf{a}, \\mathbf{b} \\in \\mathbb{R}^{p}\\). Recall that the inner product of \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\) is defined to be: \\[ \\langle \\mathbf{a}, \\mathbf{b} \\rangle = \\mathbf{a}^\\mathsf{T} \\mathbf{b} \\] Note that we can equivalently write the inner product as: \\[ \\langle \\mathbf{a}, \\mathbf{b} \\rangle = \\mathbf{a}^\\mathsf{T} \\mathbf{I_p} \\mathbf{b} \\] where \\(\\mathbf{I_p}\\) denotes the \\(p \\times p\\) identity matrix. This notation allows us to generalize inner products: for a symmetric matrix \\(\\mathbf{M}\\) (called the metric matrix) of conformable dimensions, we define the inner product of \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\), under metric \\(\\mathbf{M}\\), to be \\[ \\langle \\mathbf{a}, \\mathbf{b} \\rangle_{\\mathbf{M}} = \\mathbf{a}^\\mathsf{T} \\mathbf{M} \\mathbf{b} \\] With this new notion of an inner product, we can also expand our definition of distance. Recall that the squared Euclidean distance between two vectors \\(\\mathbf{x_i}\\) and \\(\\mathbf{x_\\ell}\\) is defined as: \\[\\begin{align*} d^2(i, \\ell) &amp;= \\langle \\mathbf{x_i} - \\mathbf{x_\\ell}, \\mathbf{x_i} - \\mathbf{x_\\ell} \\rangle \\\\ &amp;= (\\mathbf{x_i} - \\mathbf{x_\\ell})^{\\mathsf{T}} (\\mathbf{x_i} - \\mathbf{x_\\ell}) \\\\ &amp;= (\\mathbf{x_i} - \\mathbf{x_\\ell})^{\\mathsf{T}} \\mathbf{I_p} (\\mathbf{x_i} - \\mathbf{x_\\ell}) \\end{align*}\\] Now, we can play the same game as we did for the inner product and replace \\(\\mathbf{I_p}\\) with any metric matrix \\(\\mathbf{M}\\) to obtain a generalized distance metric: \\[ d^2_{\\mathbf{M}} (i, \\ell) = (\\mathbf{x_i} - \\mathbf{x_\\ell})^{\\mathsf{T}} \\mathbf{M} (\\mathbf{x_i} - \\mathbf{x_\\ell}) \\] 23.4.2 Predictive Idea The classification rule used in CDA consists of assigning each individual \\(\\mathbf{x_i}\\) to the class \\(\\mathcal{C_k}\\) for which the distance to the centroid \\(\\mathbf{g_k}\\) is minimal. \\[ \\text{assign object } i \\text{ to the class for which } d^2(\\mathbf{x_i},\\mathbf{g_k}) \\text{ is minimal} \\] But here we don’t use the typical Euclidean distance. Instead, in CDA we use a different distance: the Mahalanobis distance. This other type of distance is based on the Mahalanobis metric matrix \\(\\mathbf{W^{-1}}\\). Consequently, the formula of the (squared) distance is given by: \\[ \\textsf{Mahalanobis:} \\quad d^2(\\mathbf{x_i, g_k}) = (\\mathbf{x_i} - \\mathbf{g_k})^\\mathsf{T} \\mathbf{W}^{-1} (\\mathbf{x_i} - \\mathbf{g_k}) \\] What does this distance do? The Mahalanobis distance measures the (squared) distance of a point \\(\\mathbf{x_i}\\) to a centroid \\(\\mathbf{g_k}\\) by taking into account the correlational structure of the variables. The following figure illustrates what this distance is doing compared to the Euclidean distance. Suppose we have two points \\(\\mathbf{x_i}\\) and \\(\\mathbf{x_\\ell}\\) lying on the same density ellipsoid. Figure 23.14: Euclidean -vs- Mahalanobis distances Without taking into account the correlational structure of the features, the points \\(\\mathbf{x_i}\\) and \\(\\mathbf{x_\\ell}\\) are at different distances from the centroid \\(\\mathbf{g_k}\\). This is the case of the Euclidean distance which does not take into account such correlational structure. However, if we use a metric such as the Mahalanobis metric based on \\(\\mathbf{W}^{-1}\\) (or an equivalent metric \\(\\mathbf{V}^{-1}\\)), the distance of a particular point \\(\\mathbf{x_i}\\) to the centroid \\(\\mathbf{g_k}\\) depends on how spread out is the cloud of points in class \\(k\\). Moreover, if two points \\(i\\) and \\(\\ell\\) are on the same density ellipsoid, then they are equidistant to the centroid: \\[ d^{2}_{\\mathbf{W}^{-1}} (\\mathbf{x_i}, \\mathbf{g_k}) = d^{2}_{\\mathbf{W}^{-1}} (\\mathbf{x_\\ell}, \\mathbf{g_k}) \\] 23.4.3 CDA Classifier As we said before, the classification rule behind CDA says that we should assign an object \\(\\mathbf{x_i}\\) to the class it is nearest to, using the \\(\\mathbf{W^{-1}}\\) metric to calculate the distance of the object from the centroid of the group. Expanding the Mahalanobis distance of \\(\\mathbf{x_i}\\) to centroid \\(\\mathbf{g_k}\\) we get: \\[\\begin{align*} d^2(\\mathbf{x_i, g_k}) &amp;= (\\mathbf{x_i} - \\mathbf{g_k})^\\mathsf{T} \\mathbf{W}^{-1} (\\mathbf{x_i} - \\mathbf{g_k}) \\\\ &amp;= \\mathbf{x_i}^\\mathsf{T} \\mathbf{W}^{-1} \\mathbf{x_i} - 2 \\mathbf{g_k}^\\mathsf{T} \\mathbf{W}^{-1} \\mathbf{x_i} + \\mathbf{g_k}^\\mathsf{T} \\mathbf{W}^{-1} \\mathbf{g_k} \\end{align*}\\] As you can tell, there are three terms: the first one does not depend on \\(k\\), but the second and third terms do depend on \\(k\\): \\[ d^2(\\mathbf{x_i, g_k}) = \\underbrace{\\mathbf{x_i}^\\mathsf{T} \\mathbf{W}^{-1} \\mathbf{x_i}}_{constant} - \\underbrace{2 \\mathbf{g_k}^\\mathsf{T} \\mathbf{W}^{-1} \\mathbf{x_i}}_{\\text{depends on } k} + \\underbrace{ \\mathbf{g_k}^\\mathsf{T} \\mathbf{W}^{-1} \\mathbf{g_k} }_{\\text{depends on } k} \\] Note that minimizing \\(d^2(\\mathbf{x_i, g_k})\\) is equivalent to maximizing the negative of those terms that depend on \\(k\\): \\[ 2 \\mathbf{g_k}^\\mathsf{T} \\mathbf{W}^{-1} \\mathbf{x_i} - \\mathbf{g_k}^\\mathsf{T} \\mathbf{W}^{-1} \\mathbf{g_k} \\] Let’s go back to our hypothetical example at the beginning of the chapter. Now suppose that we have three unclassified individuals A, B, and C. In canonical discriminant analysis we compute the Mahalanobis distances of the unclassified objects to the three centroids. Figure 23.15: Mahalanobis distance of an object to the class centroids For instance, with individual A, we would have to compute: \\[ d^2(\\mathbf{x_A}, \\mathbf{g_1}), \\quad d^2(\\mathbf{x_A}, \\mathbf{g_2}), \\quad and \\quad d^2(\\mathbf{x_A}, \\mathbf{g_3}) \\] and then select the class, in this case class \\(C_2\\), for which the Mahalanobis distance is minimal. An equivalent approach is to look for the maximum of: \\(2 \\mathbf{g_1}^\\mathsf{T} \\mathbf{W}^{-1} \\mathbf{x_A} - \\mathbf{g_1}^\\mathsf{T} \\mathbf{W}^{-1} \\mathbf{g_1}\\) \\(2 \\mathbf{g_2}^\\mathsf{T} \\mathbf{W}^{-1} \\mathbf{x_A} - \\mathbf{g_2}^\\mathsf{T} \\mathbf{W}^{-1} \\mathbf{g_2}\\) \\(2 \\mathbf{g_3}^\\mathsf{T} \\mathbf{W}^{-1} \\mathbf{x_A} - \\mathbf{g_3}^\\mathsf{T} \\mathbf{W}^{-1} \\mathbf{g_3}\\) Figure 23.16: Assign class for which Mahalanobis distance is minimal 23.4.4 Limitations of CDA classifier There is an underlying assumption behind the geometric rule of CDA. This rule should not be used if the two classes have different a priori probabilities or variances (we discuss this in the next chapter). Figure 23.17: To which class should we assign the new object? In the next chapter we shift gears by introducing a more generic framework for discriminant analysis based on a probabilistic approach, as opposed to the geometric distance-based approach of CDA. "],
["discanalysis.html", "24 Discriminant Analysis 24.1 Probabilistic DA 24.2 Discriminant Functions 24.3 Quadratic Discriminant Analysis (QDA) 24.4 Linear Discriminant Analysis 24.5 Comparing the Cases", " 24 Discriminant Analysis So far we have considered Discriminant Analysis (DA) from a largely conceptual standpoint. In this lecture, we start to formalize our notions into a mathematical framework in what we will call Probabilistic Discriminant Analysis. 24.1 Probabilistic DA We start with the notion of the Bayes Classifier, which is explained in more detail in the introductory chapter about Classification. Suppose we have a response with \\(K\\) classes \\((\\mathcal{C}_1, \\dots, \\mathcal{C}_K)\\), and further suppose we have \\(p\\) predictors. The Bayes Classifier gives us the following classification rule: \\[ \\text{assign } \\mathbf{x_i} \\text{ to the class for which } P( y_i = k \\mid \\mathbf{x_i}) \\text{ is the largest} \\] This rule is optimal in the sense that it minimizes the misclassification error. The formula for the conditional probability is given by: \\[ \\underbrace{ P(y_i = k \\mid \\mathbf{x}) }_{\\text{posterior}} = \\frac{ \\overbrace{ P(\\mathbf{x_i} \\mid y_i = k) }^{\\text{likelihood}} \\overbrace{ P(y_i = k) }^{\\text{prior}} }{P(\\mathbf{x_i}) } \\] where the denominator is obtained as: \\[ P(\\mathbf{x_i}) = \\sum_{k=1}^{K} P(y_i = k) P(\\mathbf{x_i} \\mid y_i = k) \\] Changing some of the notation, let: \\(P(y = k)\\) = \\(\\pi_k\\), the prior probability for class \\(k\\). \\(P(X = \\mathbf{x} \\mid y = k)\\) = \\(f_k(\\mathbf{x})\\), the class-conditional density for inputs \\(X\\) in class \\(k\\). Thus, the posterior probability (the conditional probability of the response given the inputs) is: \\[ P(y = k \\mid X = \\mathbf{x}) = \\frac{f_k(\\mathbf{x}) \\hspace{1mm} \\pi_k}{\\sum_{k=1}^{K} f_k(\\mathbf{x}) \\pi_k} \\] Note that the numerator in the above equation is the same for all classes \\(k = 1, \\dots, K\\). 24.1.1 Normal Distributions Now, here comes one of the key assumptions of Discriminant Analysis: we assume that our class-conditional probabilities follow a Gaussian distribution. Univariate Data (\\(\\boldsymbol{p = 1}\\)) In the case of univariate data with just one input \\(x\\), we have that the class-conditional density has a Normal distribution: \\(x|\\mathcal{C_k} \\sim N(\\mu_k, \\sigma_k)\\). \\[ f_k(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma_k^2}} \\hspace{1mm} \\exp\\left\\{ -\\frac{1}{2} \\left( \\frac{x - \\mu_k}{\\sigma_k} \\right)^2 \\right\\} \\] Multivariate Data (\\(\\boldsymbol{p &gt; 1}\\)) In the case of multivariate data, we use a multivariate normal distribution for our class-conditional density: \\[ \\mathbf{x} \\mid \\mathcal{C}_k \\sim \\textit{MVN}(\\boldsymbol{\\mu_k}, \\mathbf{\\Sigma_k}) \\] Thus, the density function is given by: \\[ f_k(\\mathbf{x}) = \\frac{1}{(2 \\pi)^{p/2} | \\mathbf{\\Sigma_k} |^{1/2} } \\exp\\left\\{ - \\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu_k})^{\\mathsf{T}} \\mathbf{\\Sigma_k}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu_k}) \\right\\} \\] where \\(\\mathbf{\\Sigma_k}\\) denotes the variance-covariance matrix associated with \\(\\mathcal{C_k}\\), and \\(\\boldsymbol{\\mu_k}\\) denotes the centroid of class \\(\\mathcal{C_k}\\). Note that the exponent of the density is simply the Mahalanobis distance between \\(\\mathbf{x}\\) and \\(\\boldsymbol{\\mu_k}\\): \\[ d^2(\\mathbf{x}, \\boldsymbol{\\mu_k}) = (\\mathbf{x} - \\boldsymbol{\\mu_k})^{\\mathsf{T}} \\mathbf{\\Sigma_k}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu_k}) \\] The following interactive plot shows an example of a (pseudo) bivariate normal distribution: 24.1.2 Estimating Parameters of Normal Distributions Let’s parse through our equation from above. Clearly, in order for this to be of any practical use, we need to estimate some of these quantities: prior probabilities: \\(\\pi_k\\) mean-vectors: \\(\\boldsymbol{\\hat{\\mu}_k}\\) variance-covariance matrices: \\(\\mathbf{\\widehat{\\Sigma}_k}\\) Priors Estimating \\(\\pi_k\\) is relatively intuitive: \\[ \\hat{\\pi}_k = \\frac{n_k}{n} \\] where \\(n_k = |\\mathcal{C}_k|\\) denotes the size of class \\(k\\) and \\(n\\) denotes the total number of data points. Mean Vectors For \\(\\boldsymbol{\\hat{\\mu}_k}\\), we can use the centroid of \\(\\mathcal{C}_k\\); i.e. the average individual of class \\(k\\). \\[ \\boldsymbol{\\hat{\\mu}_k} = \\mathbf{g_k} \\] Variance-Covariance Matrices For \\(\\mathbf{\\widehat{\\Sigma}_k}\\), we can use the within-variance matrix: \\[ \\mathbf{\\widehat{\\Sigma}_k} = \\frac{1}{n-1} \\mathbf{X_k}^\\mathsf{T} \\mathbf{X_k} \\] where \\(\\mathbf{X_k}\\) is the mean-centered data matrix for objects of class \\(\\mathcal{C_k}\\). 24.2 Discriminant Functions Given all of our above estimations, we can now find an estimate for the posterior probability \\(P(y_i = k \\mid \\mathbf{x_i})\\): \\[ \\widehat{P(y_i = k \\mid \\mathbf{x} )} = \\frac{\\hat{\\pi}_k \\hspace{1mm} \\hat{f_k} (\\mathbf{x}) }{\\sum_{k=1}^{K} \\hat{\\pi}_k \\hat{f_k} (\\mathbf{x}) } \\] Now, note that the denominator remains constant across classes. Furthermore, we have a Gaussian expression in our numerator; hence, it makes sense to take logarithms: \\[ \\ln\\left[ P(y_i = k \\mid \\mathbf{x} ) \\right] \\propto \\ln( \\hat{\\pi}_k) + \\ln\\left[ \\hat{f_k} (\\mathbf{x}) \\right] \\] We now substitute the Multivariate Normal p.d.f. into our expression above, to see: \\[\\begin{align*} \\ln\\left[ \\hat{f_k}(x) \\right] &amp; = \\ln \\left[ (2\\pi)^{-p/2} | \\mathbf{\\hat{\\Sigma}_k} |^{-1/2} \\exp\\left\\{ - \\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\hat{\\mu}_k})^{\\mathsf{T}} \\mathbf{\\hat{\\Sigma}_k}^{-1} (\\mathbf{x} - \\boldsymbol{\\hat{\\mu}_k}) \\right\\} \\right] \\\\ &amp; \\to \\ln\\left( | \\mathbf{\\hat{\\Sigma}_k} |^{-1/2} \\right) - \\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\hat{\\mu}_k})^{\\mathsf{T}} \\mathbf{\\hat{\\Sigma}_k}^{-1} (\\mathbf{x} - \\boldsymbol{\\hat{\\mu}_k}) \\\\ &amp; = -\\frac{1}{2} \\ln\\left( | \\mathbf{\\hat{\\Sigma}_k} | \\right) - \\frac{1}{2} \\left[ \\mathbf{x}^\\mathsf{T} \\mathbf{\\hat{\\Sigma}_k^{-1}} \\mathbf{x} - 2 \\mathbf{x}^\\mathsf{T} \\mathbf{\\hat{\\Sigma}_k^{-1}} \\boldsymbol{\\hat{\\mu}_k} + \\boldsymbol{\\hat{\\mu}_k}^{\\mathsf{T}} \\mathbf{\\hat{\\Sigma}_k^{-1}} \\boldsymbol{\\hat{\\mu}_k} \\right] \\end{align*}\\] Substituting this back into our expression for \\(\\ln[P(y_i = k \\mid \\mathbf{x})]\\) leads us to the so-called discriminant functions: \\[ \\delta_k(\\mathbf{x}) = -\\frac{1}{2} \\ln\\left( | \\mathbf{\\hat{\\Sigma}_k} | \\right) - \\frac{1}{2} \\left[ \\mathbf{x}^\\mathsf{T} \\mathbf{\\hat{\\Sigma}_k^{-1}} \\mathbf{x}^\\mathsf{T} - 2 \\mathbf{x}^\\mathsf{T} \\mathbf{\\hat{\\Sigma}_k^{-1}} \\boldsymbol{\\hat{\\mu}_k} + \\boldsymbol{\\hat{\\mu}_k}^{\\mathsf{T}} \\mathbf{\\hat{\\Sigma}_k^{-1}} \\boldsymbol{\\hat{\\mu}_k} \\right] + \\ln\\left( \\hat{\\pi}_k \\right) \\] In general, our classification rule is as follows: \\[ \\text{assign } \\mathbf{x} \\text{ to the class for which } \\delta_k(\\mathbf{x}) \\text{ is the largest} \\] 24.3 Quadratic Discriminant Analysis (QDA) Let us classify the order of each term in \\(\\delta_K(\\mathbf{x})\\), w.r.t. \\(\\mathbf{x}\\): \\[ \\delta_k(\\mathbf{x}) = -\\frac{1}{2} \\underbrace{ \\ln\\left( | \\mathbf{\\hat{\\Sigma}_k} | \\right) }_{\\text{constant}}- \\frac{1}{2} \\left[ \\underbrace{ \\mathbf{x}^\\mathsf{T} \\mathbf{\\hat{\\Sigma}_k^{-1}} \\mathbf{x} }_{\\text{quadratic}} - \\underbrace{ 2 \\mathbf{x}^\\mathsf{T} \\mathbf{\\hat{\\Sigma}_k^{-1}} \\boldsymbol{\\hat{\\mu}_k} }_{\\text{linear}} + \\underbrace{ \\boldsymbol{\\hat{\\mu}_k}^{\\mathsf{T}} \\mathbf{\\hat{\\Sigma}_k^{-1}} \\boldsymbol{\\hat{\\mu}_k} }_{\\text{constant}} \\right] + \\underbrace{ \\ln\\left( \\hat{\\pi}_k \\right) }_{\\text{constant}} \\] Of course, we could group terms to obtain an expression of the form \\(\\text{const} + \\text{linear} + \\text{quadratic}\\). The important thing is that we obtain a quadratic function of \\(\\mathbf{x}\\); this leads us to Quadratic Discriminant Analysis (QDA). Having a quadratic discriminant function causes the decision boundaries in QDA to be quadratic surfaces. 24.4 Linear Discriminant Analysis What if, in our expression for \\(\\delta_K(\\mathbf{x})\\), we have that all covariance matrices are the same: \\[ \\mathbf{\\hat{\\Sigma}_1} = \\mathbf{\\hat{\\Sigma}_2} = \\dots = \\mathbf{\\hat{\\Sigma}_K} = \\mathbf{\\hat{\\Sigma}} \\] In this case, the discriminant function becomes \\[ \\delta_k(\\mathbf{x}) = \\underbrace{ \\ln\\left( \\hat{\\pi}_k \\right) }_{\\text{constant}} - \\underbrace{ \\frac{1}{2} \\ln\\left( | \\mathbf{\\hat{\\Sigma}} | \\right) }_{\\text{no k dependency}} - \\frac{1}{2} \\left[ \\underbrace{ \\mathbf{x}^\\mathsf{T} \\mathbf{\\hat{\\Sigma}^{-1}} \\mathbf{x} }_{\\text{no k dependency}} - \\underbrace{ 2 \\mathbf{x}^\\mathsf{T} \\mathbf{\\hat{\\Sigma}^{-1}} \\boldsymbol{\\hat{\\mu}_k} }_{\\text{linear}} + \\underbrace{ \\boldsymbol{\\hat{\\mu}_k}^{\\mathsf{T}} \\mathbf{\\hat{\\Sigma}^{-1}} \\boldsymbol{\\hat{\\mu}_k} }_{\\text{constant}} \\right] \\] In other words, ignoring terms that do not depend on \\(k\\), we obtain (again, w.r.t. \\(\\mathbf{x}\\)) \\[ \\delta_k(\\mathbf{x}) = \\underbrace{ \\ln\\left( \\hat{\\pi}_k \\right) }_{\\text{constant}} - \\frac{1}{2} \\left[ - \\underbrace{ 2 \\mathbf{x}^\\mathsf{T} \\mathbf{\\hat{\\Sigma}^{-1}} \\boldsymbol{\\hat{\\mu}_k} }_{\\text{linear}} + \\underbrace{ \\boldsymbol{\\hat{\\mu}_k}^{\\mathsf{T}} \\mathbf{\\hat{\\Sigma}^{-1}} \\boldsymbol{\\hat{\\mu}_k} }_{\\text{constant}} \\right] \\] That is, our discriminant function is now linear w.r.t. \\(\\mathbf{x}\\); hence, we end up with Linear Discriminant Analysis (LDA) in its most general form. 24.4.1 Canonical Discriminant Analysis Let us continue adding more assumptions. We again assume that all covariance matrices are the same across classes, as well as the prior probabilities: \\[ \\mathbf{\\hat{\\Sigma}_1} = \\mathbf{\\hat{\\Sigma}_2} = \\dots = \\mathbf{\\hat{\\Sigma}_K} = \\mathbf{\\hat{\\Sigma}} \\quad \\text{and} \\quad \\hat{\\pi}_1 = \\dots = \\hat{\\pi}_k = \\hat{\\pi} \\] Then, the discriminant functions \\(\\delta_k(\\mathbf{x})\\) become: \\[ \\delta_k(\\mathbf{x}) = \\underbrace{ \\ln\\left( \\hat{\\pi} \\right) }_{\\text{no k dependency}} + \\underbrace{ \\mathbf{x}^\\mathsf{T} \\mathbf{\\hat{\\Sigma}^{-1}} \\boldsymbol{\\hat{\\mu}_k} }_{\\text{linear}} - \\frac{1}{2} \\underbrace{ \\boldsymbol{\\hat{\\mu}_k}^{\\mathsf{T}} \\mathbf{\\hat{\\Sigma}^{-1}} \\boldsymbol{\\hat{\\mu}_k} }_{\\text{constant}} \\] which becomes, after ignoring \\(k\\)-independent terms, \\[ \\delta_k(\\mathbf{x}) = \\underbrace{ \\mathbf{x}^\\mathsf{T} \\mathbf{\\hat{\\Sigma}^{-1}} \\boldsymbol{\\hat{\\mu}_k} }_{\\text{linear}} - \\frac{1}{2} \\underbrace{ \\boldsymbol{\\hat{\\mu}_k}^{\\mathsf{T}} \\mathbf{\\hat{\\Sigma}^{-1}} \\boldsymbol{\\hat{\\mu}_k} }_{\\text{constant}} \\] Now, what is going in with this first term? It is simply the distance between \\(\\mathbf{x}\\) and the centroid of \\(\\mathcal{C_k}\\), using the within-class variance matrix as a metric matrix. In other words, the above expression is our good old friend CDA! 24.4.2 Naive Bayes Let’s add one more supposition to the list of assumptions considered so far. Equal covariance matrices, equal priors, and now also assume that \\(\\mathbf{\\hat{\\Sigma}}\\) is diagonal which means that the predictors are uncorrelated: \\[ \\mathbf{\\hat{\\Sigma}} = \\begin{pmatrix} Var(X_1) &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; Var(X_2) &amp; \\dots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; Var(X_p) \\\\ \\end{pmatrix} \\] So we have the following assumptions: \\[ \\mathbf{\\hat{\\Sigma}_1} = \\mathbf{\\hat{\\Sigma}_2} = \\dots = \\mathbf{\\hat{\\Sigma}_K} = \\mathbf{\\hat{\\Sigma}}, \\qquad \\hat{\\pi}_1 = \\dots = \\hat{\\pi}_k = \\hat{\\pi} \\\\ \\quad \\text{and} \\quad \\text{diagonal } \\mathbf{\\hat{\\Sigma}} \\] Then, the discriminant function becomes (with diagonal \\(\\mathbf{\\hat{\\Sigma}}\\)): \\[ \\delta_k(\\mathbf{x}) = \\underbrace{ \\mathbf{x}^\\mathsf{T} \\mathbf{\\hat{\\Sigma}^{-1}} \\boldsymbol{\\hat{\\mu}_k} }_{\\text{linear}} - \\frac{1}{2} \\underbrace{ \\boldsymbol{\\hat{\\mu}_k}^{\\mathsf{T}} \\mathbf{\\hat{\\Sigma}^{-1}} \\boldsymbol{\\hat{\\mu}_k} }_{\\text{constant}} \\] This leads to what is known as Naive Bayes. 24.4.3 Fifth Case Again assume that \\(\\mathbf{\\hat{\\Sigma}_1} = \\mathbf{\\hat{\\Sigma}_2} = \\dots = \\mathbf{\\hat{\\Sigma}_K} = \\mathbf{\\hat{\\Sigma}}\\) as well as \\(\\hat{\\pi}_1 = \\dots = \\hat{\\pi}_k =: \\hat{\\pi}\\). Also assume that \\(\\mathbf{\\hat{\\Sigma}} = \\mathbf{I_p}\\), the \\(p \\times p\\) identity matrix. In other words, our Mahalanobis distance collapses to the Euclidean distance. 24.5 Comparing the Cases Here’s a summary table of the five cases discussed above. All cases assume that the class-conditional distributions \\(f_k(\\mathbf{x})\\) follow a Normal or Multivariate-Normal Distribution, depending on the number of input features \\(p\\). Cov Matrix Priors Method Bayes Rule Flexibility Bias Unequal across classes Unequal across classes QDA quadratic +++++ + Equal across classes Unequal across classes LDA linear ++++ ++ Equal across classes Equal across classes CDA linear +++ +++ Equal across classes, diagonal Equal across classes Naive Bayes linear ++ ++++ Equal across classes, identity Equal across classes Euclid. Dist. linear + +++++ Notice that the cases are listed from more flexible to less flexible. Discriminant methods such as Naive Bayes imposes fairly restrictive assumptions. Also, notice that you can have discriminant methods with linear decision boundaries of different degrees of flexibility. "],
["classperformance.html", "25 Performance of Classifiers 25.1 Classification Error Measures 25.2 Confusion Matrices 25.3 Binary Response Example 25.4 Decision Rules and Errors 25.5 ROC Curves", " 25 Performance of Classifiers Now that we have described several classification methods, it is time to discuss the evaluation of their predictive performance. Specifically, the pertaining question is: How do we measure the performance of a classifier? 25.1 Classification Error Measures Recall that, in supervised learning, we are interested in obtaining models that give accurate predictions. At the conceptual level we need some mechanism to quantify how different the fitted model is from the target function \\(f\\): \\[ \\widehat{f} \\text{ -vs- } f \\] This means that we need a function that summarizes the total amount of error, also referred to as the Overall Measure of Error: \\[ \\text{Overall Measure of Error:} \\quad E(\\widehat{f},f) \\] The typical way in which an overall measure of error is defined is in terms of individual or pointwise errors \\(err_i(\\hat{y}_i, y_i)\\) that quantify the difference between an observed value \\(y_i\\) and its predicted value \\(\\hat{y}_i\\). Usually, most overall errors aggregate pointwise errors into a single measure: \\[ E(\\widehat{f},f) = \\text{measure} \\left( \\sum err_i(\\hat{y}_i, y_i) \\right ) \\] In a classification setting we still have response observations \\(y_i\\) and predicted values \\(\\hat{y}_i\\); we still want a way to compare \\(y_i\\) and \\(\\hat{y}_i\\); and we still need an overall measure of error. 25.1.1 Errors for Binary Response The starting point involves finding a pointwise error function. Assuming that we have a binary response, that is \\(y_i \\in \\{0, 1\\}\\), and assuming also that the predicted values are binary as well, \\(\\hat{y}_i \\in \\{0, 1\\}\\), then we could use the absolute error as a pointwise error measure: \\[ \\left| y_i - \\hat{y}_i \\right| = \\begin{cases} 0 &amp; \\text{if } y_i = \\hat{y}_i \\\\ &amp; \\\\ 1 &amp; \\text{if } y_i \\neq \\hat{y}_i \\\\ \\end{cases} \\] or we could also use squared error: \\[ (y_i - \\hat{y}_i)^2 = \\begin{cases} 0 &amp; \\text{if agree} \\\\ &amp; \\\\ 1 &amp; \\text{if disagree} \\\\ \\end{cases} \\] Again, keep in mind that the above error functions are valid as long as the predictions are binary, \\(\\hat{y}_i \\in \\{0, 1\\}\\), which is not always the case for all classifiers. Now, suppose we still had binary data but instead we encoded our data so that \\(y_i \\in \\{-1, 1\\}\\). What kind of pointwise error measure should we use? If we used squared error, we would get the following: \\[ (y_i - \\hat{y}_i)^2 = \\begin{cases} 0 &amp; \\text{if agree} \\\\ &amp; \\\\ 4 &amp; \\text{if disagree} \\\\ \\end{cases} \\] Alternatively, under the same assumption that \\(y_i \\in \\{-1, 1\\}\\), we could consider the product between the observed response and its predicted value: \\[ y_i \\times \\hat{y}_i = \\begin{cases} +1 &amp; \\text{if agree} \\\\ &amp; \\\\ -1 &amp; \\text{if disagree} \\\\ \\end{cases} \\] The point that we are trying to make is that there are many different choices for pointwise error measures. Perhaps the most common option of pointwise error is the binary error, defined to be: \\[ [\\![ \\hat{y}_i \\neq y_i ]\\!] \\to \\begin{cases} 0 \\ (\\text{or FALSE}) &amp; \\text{if agree} \\\\ &amp; \\\\ 1 \\ (\\text{or TRUE}) &amp; \\text{if disagree} \\\\ \\end{cases} \\] This type of error function is adequate for classification purposes, and it works well for the case in which we have a binary response, as wel as when we have a response with more than two categories. Moreover, averaging these binary errors over our observations yields the misclassification error rate: \\[ \\frac{1}{n} \\sum_{i=1}^{n} [\\![ \\hat{y}_i \\neq y_i ]\\!] \\] 25.1.2 Error for Categorical Response Now suppose we have a categorical response with more than two categories. For example, \\(y_i \\in \\{a, b, c, d\\}\\). For illustrative purposes, consider the following data with hypothetical predictions: \\[ \\begin{array}{c|ccccc} y &amp; a &amp; b &amp; c &amp; d &amp; b \\\\ \\hline \\hat{y} &amp; a &amp; a &amp; c &amp; d &amp; c \\\\ \\hline &amp; \\checkmark &amp; \\times &amp; \\checkmark &amp; \\checkmark &amp; \\times \\\\ \\end{array} \\] here the symbol \\(\\checkmark\\) denotes agreement between the observed values and the predicted values. In this case, it makes sense to use the binary error as our pointwise error measure previously defined: \\[ [\\![ \\hat{y}_i \\neq y_i ]\\!] \\to \\begin{cases} 0 \\ (\\text{or FALSE}) &amp; \\text{if agree} \\\\ &amp; \\\\ 1 \\ (\\text{or TRUE}) &amp; \\text{if disagree} \\\\ \\end{cases} \\] As in the binary-response case discussed in the previous section, averaging these pointwise errors over our observations yields the misclassification error rate: \\[ \\frac{1}{n} \\sum_{i=1}^{n} [\\![ \\hat{y}_i \\neq y_i ]\\!] \\] This overall measure of error is the one that we’ll use in this book, unless mentioned otherwise. 25.2 Confusion Matrices Regardless of the pointwise error measure to be used, it is customary to construct a confusion matrix. This matrix is simply a crosstable representation of the number of agreements and disagreements between the observed response values \\(y_i\\) and the corresponding predicted values \\(\\hat{y}_i\\). In the binary-response case with \\(0-1\\) encoding, \\(y_i \\in \\{0, 1\\}\\), the confusion matrix has the following format: Any entry in this matrix contains information about how our predicted classifications compare with the observed classifications. For example, the entry in row-1 and column-1 represents the number of observations that are correctly classified as being of class \\(0\\). In contrast, the entry in the row-2 and column-1 indicates the number of observations that are classified as \\(1\\) when in fact they are actually in class \\(0\\). As such, the off-diagonal entries represent observations that are misclassified. In general, for a response with more than 2 categories, for example \\(K=4\\), the confusion matrix will take the form: The way we read this crosstable is the same as in the binary-response case. For example, the number in the entry of row-1 and column-2 represents the number of observations that are classified as being of class \\(a\\) when in fact they are actually in class \\(b\\). Likewise, the off-diagonal entries represent observations that are misclassified. We can calculate the misclassification error from the confusion matrix using the following formula: \\[\\begin{align*} \\text{Misclassification Error} &amp;= 1 - \\frac{\\sum (\\text{all cells in diagonal}) }{n} \\\\ &amp;= \\frac{\\sum (\\text{all cells off-diagonal}) }{n} \\\\ &amp;= \\frac{1}{n} \\sum_{i=1}^{n} [\\![ \\hat{y}_i \\neq y_i ]\\!] \\end{align*}\\] In this way, the “ideal” confusion matrix is one that is completely diagonal, with all off-diagonal entries equal to \\(0\\). The higher the off-diagonal entries, the worse our model is performing. 25.3 Binary Response Example Consider an application process for a potential customer that is interested in some financial product. Our hypothetical customer approaches a certain bank and goes through the process to get approval for a financial product like: Savings or Checking Accounts Credit Card Car Loan Mortgage Loan Certificate of Deposit (CD) Money Market Account Retirement Account (e.g. IRA) Insurance policy etc For each of these applications, there will be some series of questions the customer needs to answer. Here’s a screenshot of a typical online application form for a credit card (btw: as of this writing, one of the book authors owns such credit card, but he’s not affiliated with the brand): Using some or all of the descriptors for a customer, we want a model \\(f\\) that takes the answers to these questions \\(\\mathbf{x_i}\\), and spits out either \\(\\text{accept}\\) or \\(\\text{reject}\\). Pictorially: \\[ \\mathbf{x_i} \\to \\boxed{\\hat{f}} \\to \\hat{y}_i = \\begin{cases} \\text{accept} \\\\ &amp; \\\\ \\text{reject} \\\\ \\end{cases} \\] Now, given past data on acceptances and rejections of previous customers, the confusion matrix will have the following four types of entries: The entries in the digonal are correct or “good” predictions, whereas the off-diagonal entries correspond to classification errors. Notice that we have two flavors of misclassifications. One of them is “false reject” when a good customer is incorrectly rejected. The other one is “false accept” when a bad customer is incorrectly accepted. In an ideal situation, we would simultaneously minimize the false acceptance and false rejection rates. However, as with everything else in Machine Learning, there is a tradeoff; in general, minimizing false acceptance rates increases false rejection rates, and vice-versa. We discuss this issue in the last section of this chapter. One interesting aspect in classification has to do with the importance of classification errors. In our hypothetical example: Are both types of errors, “false accept” and “false reject”, equally important? As you can imagine, there may be situations in which a false acceptance is “worse” (in some way) than a false rejection, and vice versa. We explore these considerations with a couple of examples in the next sections. 25.3.1 Application for Checking Account Let’s further investigate the different types of errors that may occur in a classification scenario. For instance, let’s consider an application process for a financial product like a checking (or a savings) account. To make things more interesting, suppose that one bank has a promotion of a cash bonus when someone opens a new checking account with them. Here’s a screenshot of a typical online application form for a checking account: As we mention above, using some or all of the descriptors for a customer, we want a model \\(f\\) that takes the answers to the questions in the application form, denoted as \\(\\mathbf{x_i}\\), and produces an output: \\[ \\mathbf{x_i} \\to \\boxed{\\hat{f}} \\to \\hat{y}_i = \\begin{cases} \\text{accept} \\\\ &amp; \\\\ \\text{reject} \\\\ \\end{cases} \\] The format of the confusion matrix will be something like this: As you can tell, we have two types of errors. The first kind of error is a false rejection. This occurs when a customer fills out an application for a checking account, and despite the fact that they are theoretically eligible to open such account, this customer is rejected. The second kind of error is a false acceptance; that is, a customer who is (theoretically) an unsuited customer for the bank, still gets approved to open an account. Which type of error is worse from the bank’s point of view? The answer is: a false rejection! This is because the “good” customer would likely be able to go across the street and open up an account at the rival bank; that is, the initial bank has lost a potential customer. As for the false accept, this is a type of error that the bank can better tolerate, and the lucky customer will receive the cash bonus. Hence, when constructing the confusion matrix, we might want to assign different penalties to the different errors. For example we can give a penalty of 5 for a false reject, and a penalty of 1 for a false accept. Note that the values 1 and 5 were somewhat arbitrary; the actual values are less important as the relative magnitude of them (for example we could have just as easily used \\(20\\) and \\(50\\), respectively, instead). 25.3.2 Application for Loan Now, suppose that the application process in question is for a loan (e.g. mortgage loan, or car loan). The two types of errors still stand: false rejection and false acceptance. Compared to the previous example about the checking account, now the two errors have different interpretations and consequences: False Rejection: The bank loses money that they could have gathered through interest from the customer. False Acceptance: The bank loses money because the customer will ultimately default and not pay back the loan. Same question: which error is more costly (from the bank’s point of view)? It shouldn’t be too difficult to notice that a false acceptance is worse than a false rejection. If the bank grants a loan to someone who won’t be able to pay it back in the long run, the bank will lose money (likely more money than they would have gained through interest from somebody who could have paid their loan). Hence, we might assign the following penalties: We should mention that the penalties assigned to each type of error is not an analytic question but rather an application domain question. What penalty we use depends on a case by case. Likewise, there is no inherent merrit in choosing one error penalty over another. In summary, the combination of the error measure and the penalties, it’s something that should be specified by the user. 25.4 Decision Rules and Errors Another aspect that we need to discuss involves dealing with classifiers that produce a class assignment proabability, or a score that is used to make a decision. A typical example is logistic regression. Recall that in logistic regression we model the posterior probabilities of the response given the data as: \\[ Prob(y_i \\mid \\mathbf{x_i}, \\mathbf{b}) = \\begin{cases} h(\\mathbf{x_i})&amp; \\textsf{for } y_i = 1 \\\\ &amp; \\\\ 1 - h(\\mathbf{x_i}) &amp; \\textsf{for } y_i = 0 \\\\ \\end{cases} \\] where \\(h()\\) denotes the logistic function \\(\\phi()\\): \\[ h(\\mathbf{x}) = \\phi(\\mathbf{b^\\mathsf{T} x}) \\] In this case, logistic regression provides both ways of handling the output, with the probability \\(P(y \\mid \\mathbf{x})\\) or with a score given by the linear signal \\(s = \\mathbf{b^\\mathsf{T} x}\\). Suppose that \\(y = 1\\) corresponds to \\(\\text{accept}\\) and \\(y = 0\\) corresponds to \\(\\text{reject}\\). If we look at the graph of \\(P(y)\\) and the logistic function of the linear signal, we can get a picture like this: \\[ P(y = 1) = \\frac{e^{\\mathbf{b^\\mathsf{T} x}} }{1 + e^{\\mathbf{b^\\mathsf{T} x}} } \\] Now, suppose we have three individuals \\(A\\), \\(B\\) and \\(C\\) with the following acceptance probabilities: \\[ P(\\text{accept}_A) = 0.9; \\quad P(\\text{accept}_B) = 0.1; \\quad P(\\text{accept}_C) = 0.5 \\] To classify these, we would need to introduce some sort of cutoff \\(\\tau\\) so that our decision rule is \\[ \\text{decision rule} = \\begin{cases} \\text{accept} &amp; \\text{if } P(\\text{accept}) &gt; \\tau \\\\ &amp; \\\\ \\text{reject} &amp; \\text{otherwise} \\\\ \\end{cases} \\] Where should we put \\(\\tau\\)? The default options is to set it at \\(0.5\\): Going back to the example of a loan application, we know that a false accept is a more dangerous type of error than a false reject. Setting \\(\\tau = 0.5\\) doesn’t really make too much sense from the bank’s perspective, if what we want is to prevent accepting customers that will eventually default on their loans. It would make more sense to set \\(\\tau\\) to a value much higher than 0.5; for example, 0.7: Think of this a “raising the bar” for customer applications. The bank will be more strict in approving customers applications, safeguarding against the “worst” type of error (in this case, false acceptances). For comparison purposes, let’s momentarilly see what could happen if we set \\(\\tau = 0.3\\). A threshold with this value would increase the number of false acceptance. The bank would be “lowering the bar”, being less strict with the profile of accepted customers. Consequently, the bank would be running a higher risk by giving loans to customers that may be more likely to default. Note that the confusion matrix will change depending on the value of \\(\\tau\\) we pick. 25.5 ROC Curves So, then, what value of \\(\\tau\\) should we pick? To answer this question, we will use a ROC curve which stands for Receiving Operating Characteristic curve. Keep in mind that ROC curves are only applicable to binary responses. We introduce the following notations in a confusion matrix: The general form of the confusion matrix is the same. The entries on the diagonal have to do with observations correctly classified. In turn, the off-diagonal entries have to do observations incorrectly classified. The difference is the way in which categories are encoded: \\(\\text{accept}\\) is now encoded as positive, while \\(\\text{reject}\\) is encoded as negative. By the way, this encoding is arbitrary, and you can choose to label \\(\\text{accept}\\) as negative, and \\(\\text{reject}\\) as positive. A more common notation involves further abbreviating each of the names in the cells of a confusion matrix as follows: where \\(\\text{TP}\\) stands for “true positive,” \\(\\text{FP}\\) stands for “false positive,” \\(\\text{FN}\\) stands for “false negative,” and \\(\\text{TN}\\) stands for “true negative.” With these values, we can obtain the following rates. We define the True Positive Rate (\\(\\text{TPR}\\); sometimes called the sensitivity) as \\[ \\textsf{sensitivity:} \\quad \\mathrm{TPR} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} \\] Notice that TPR can be regarded as a conditional probability: \\[ \\text{TPR} \\quad \\longleftrightarrow \\quad P(+\\hat{y} \\mid +y) \\] Similarly, we define the True Negative Rate (\\(\\text{TNR}\\); sometimes called the specificity) as \\[ \\textsf{specificity:} \\quad \\mathrm{TNR} = \\frac{\\text{TN}}{\\text{TN} + \\text{FP}} \\] As with TPR, we can also regard TNR as a conditional probability: \\[ \\text{TNR} \\quad \\longleftrightarrow \\quad P(-\\hat{y} \\mid -y) \\] Additionally, we can also get the corresponding False Negative Rate (FNR) defined as one minus sensitivity: \\[ \\mathrm{FNR} = 1 - \\text{TPR} = \\frac{\\text{FN}}{\\text{TP} + \\text{FN}} \\] as well as the False Positive Rate (FPR) defined as one minus specificity: \\[ \\mathrm{FPR} = 1 - \\text{TNR} = \\frac{\\text{FP}}{\\text{TN} + \\text{FP}} \\] Ideally, we would like to have a classifier or decision rule that simultaneously minimizes both the false acceptance and false rejection rates. However, there is a tradeoff; in general, minimizing false acceptance rates increases false rejection rates, and vice-versa. Why? Because increasing the sensitivity of a classifier results in more observations being predicted as positive. Returning to the loan application example, and assuming again that \\(\\text{acceptance}\\) is the positive level, sensitivity measures how often the accepted customers in the test set were actually classified as accepted. 25.5.1 Graphing ROC curves Now that we have introduced the required notation and terminology commonly employed with Receiver Operating Characteristic curves, let’s properly describe this graphic tool. The ROC curve is one visual tool used to measure the performance of a two-class classifier that uses prediction scores or discriminant functions. The ingredients to plot the ROC curve are True Positive Rate (sensitivity) and False Positive Rate (1 - specificity), for a set of thresholds \\(\\tau\\) used to convert classification scores into predicted responses. To be more precise, a ROC curve is used to show in a visual way the tradeoff between sensitivity and specificity for every possible cut-off or threshould value \\(\\tau\\), for a binary classification score. Figure 25.1: Example of a ROC Curve In addition, the area under the ROC curve gives an idea about the benefit of using the decision rule under consideration. Example The following figure shows the ROC curves for two discriminant analysis classifiers applied to a simulated data set with two predictors and a two-class response. One of the classifiers is a linear discriminant model (LDA), while the other one is a quadratic discriminant model (QDA). Given a candidate threshold value \\(\\tau\\), the TPR and FPR are graphed against each other. With a large set of thresholds, graphed pairs of TPR and FPR result in a ROC curve. In the figure above, the ROC curve associated to the QDA indicates that this method has better sensitivity and specificity performance than LDA. One of the purposes of the plot is to give us guidance for selecting a threshold that maximizes the tradeoff between sensitivity and specificity. The best threshold is the one that has the highest sensitivity (TPR) together with the lowest FPR. The following diagram depicts three possible ROC curve profiles, with a data set of \\(n=100\\) individuals belonging to two classes of equal size. The left frame shows a ROC cruve from a classifier that produces perfect classifications. This is the best possible ideal scenario. The frame in the middle shows a ROC curve that coincides with the diagonal (dashed line) indicating that the classifier has no discriminant capacity, and is no better than classifying observations at random (e.g. assign classes flipping a coin). Finally, the frame on the right illustrates another extreme case in which the classifier has perfect misclassification performance; this is the worst possible scenario for a classifier. "],
["clustering.html", "26 Clustering 26.1 About Clustering 26.2 Dispersion Measures 26.3 Complexity in Clustering", " 26 Clustering In this part of the book, we go back to unsupervised learning methods. In particular, the so-called clustering methods. Figure 26.1: Clustering Corner Clustering refers to a very broad set of techniques for finding groups or clusters of objects, in a data set. Here are some examples that have become famous in the clustering literature: Marketing: discover groups of customers and used them for targeted marketing. Medical Field: discover groups of patients suitable for particular treatment protocols. Astronomy: find groups of similar stars and galaxies. Genomics: find groups of genes with similar expressions. The main idea of clustering (or cluster analysis) is to find groups of objects in data. When we say “objects”, it can be groups of individuals, but also groups of variables. So, more formally, clustering refers to the statistical process of forming groups of objects (individuals or variables) into a limited number of groups known as clusters. Notice from the above diagram that both clustering and classification involve dealing with a variable of interest that is categorical. However, the difference between classification and clustering has to to do with whether that feature of interest is observed or not. In classification we actually observe the class of the individuals through the response variable \\(Y\\). In contrast, we don’t have an explicit response in clustering; we want to (in a sense) find a “hidden” variable that exposes group structure in the data. The groups are not defined in advanced. In other words, there is no observed variable that imposes a group structure. Figure 26.2: Unobserved variable of interest 26.1 About Clustering Clustering has its roots in what used to be called numerical taxonomy, later referred to as numerical ecology, which originated in ancient Greece (with the work of Aristotle and his disciples). In a more mathematical (numeric) way, the first formal approaches can be traced back to the late 1950’s and early 1960’s. The main reason: the appearance of computers during this time. Before computers, how did people find groups? Well, typically people would generate a scatterplot and visually ascertain whether or not there are groups. In other words, the humans were the machine learners! Luckily, the human brain is quite good at seeing patterns, so this approach isn’t actually that bad. But, of course, computers made this quite a bit easier (not to mention, for more than 3 variables, visualizing a scatterplot is pretty much impossible). 26.1.1 Types of Clustering We can divide clustering approaches into two main types: Hard (aka crisp), and Soft (aka fuzzy). In this book, we will only focus on hard clustering (for more about soft clustering, see ESL). Within hard/crisp clustering, there are two primary approaches. The first is direct partitioning, e.g. through \\(k\\)-means and \\(k\\)-medoids. The other is hierarchical partitions or embedded partitions, that is, in which partitions may have sub-partitions within them. Direct Partitioning: Clusters are always separated, and their number is defined a priori. Hierarchical Partitions: clusters are either separate or embedded, defining a hierarchy of partitions. You may have what are referred to as “natural” groups; that is, groups that are somewhat obvious from the data at first glance. Of course, there will be cases where there are no natural groups; in such a case, we might say we have “artificial” groups. Figure 26.3: Some Typical Clustering Structures 26.1.2 Hard Clustering In hard clustering, we want objects in a particular cluster to be relatively similar to one another (of course, we will need to define what “similar” means). Additionally, we want there to be some notion of separation/spread; that is, we want each cluster to be relatively distinguishable from the other cluster(s). More specifically, we want objects in different clusters to be less similar than objects within clusters. We give names to these concepts: Within-cluster Homogeneity: Objects in a cluster should be similar enough; each group is as much homogeneous as possible. Between-cluster Heterogeneity: Objects in different clusters should be less similar than object within each cluster; groups are as distinct as possible among them. In other words: we want high within-cluster homogeneity and between-cluster heterogeneity. In summary: We seek a partition of the data into distinct groups. We want the observations within each group to be quite similar to each other. We must define what it means for two or more observations to be similar or different. This is often a domain-specific consideration that must be made based on knowledge of the data being studied. Of course, to quantify “similar” and “different,” we need some sort of mathematical notion of closeness; that is, we need a distance metric. As usual, we will assume that the rows of the data matrix correspond to the individuals to be clustered (although you could also cluster variables). 26.2 Dispersion Measures Let’s assume that the individuals are embeded in a Euclidean space (e.g. quantitative variables, or output of dimension reduction method). As usual, suppose that the individuals form a cloud of points in a \\(p\\)-dimensional space, and that we also have the centroid or average individual, like in the following figure: Figure 26.4: Cloud of n points in p-dim space We can look at the amount of spread or dispersion in the data with respect to the global centroid. One way to do this is by getting the sum of all squared distances from the centroid given by: \\[ \\text{TSS} = \\sum_{i=1}^{n} d^2(\\mathbf{x_i}, \\mathbf{g}) \\tag{26.1} \\] Now suppose that we have some group structure in which objects are divided into several clusters \\(\\mathcal{C}_1, \\mathcal{C}_2, \\dots, \\mathcal{C}_K\\). Figure 26.5: Each cluster forms its own cloud Because each cluster \\(\\mathcal{C}_k\\) will form its own cloud of points, we can make the visual representation more pattent by using different shapes for the points. Likewise, we suppose that each cluster has its own centroid \\(\\mathbf{g_k}\\): Figure 26.6: Each cluster has a certain within dispersion Within Cluster Dispersion Each cluster will have an associated amount of spread from its centroid, that is a specific Cluster Sum of Squared distances (\\(\\text{CSSD}\\)): \\[ \\text{CSSD} = \\sum_{i \\in \\mathcal{C}_k} d^2(\\mathbf{x_i}, \\mathbf{g_k}) \\tag{26.2} \\] Figure 26.7: Dispersion within each cluster We can combine the cluster sum-of-squared distances to obtain the Within-clusters sum of squared distances \\(\\text{WSSD}\\): \\[ \\text{WSSD} = \\sum_{k=1}^{K} \\text{CSSD}_k = \\sum_{k=1}^{K} \\sum_{i \\in \\mathcal{C}_k} d^2(\\mathbf{x_i}, \\mathbf{g_k}) \\tag{26.3} \\] Between Cluster Dispersion Let’s now turn our attention to the centroids: Figure 26.8: Focusing on the centroids Focusing on just the centroids, and taking into acount the size \\(n_k\\) of each cluster, we could examine the distances between the centroids of the groups and the overall centroid. Let us call this between-cluster dispersion the \\(\\text{BSSD}\\) given by: \\[ \\text{BSSD} = \\sum_{k=1}^{K} n_k d^2(\\mathbf{g_k}, \\mathbf{g}) \\tag{26.4} \\] Figure 26.9: Dispersion between the centroids Dispersion Decomposition Let’s recap. We have three types of sums-of-squared distances: \\(\\text{TSSD}\\): Total sum of squared distances \\(\\text{WSSD}\\): Within-clusters sum of squared distances \\(\\text{BSSD}\\): Between-clusters sum of squared distances It can be shown (Huygens’ theorem) that \\[ \\text{TSS} = \\text{BSS} + \\text{WSS} \\] that is: \\[ \\sum_{i=1}^{n} d^2(\\mathbf{x_i}, \\mathbf{g}) = \\sum_{k=1}^{K} n_k d^2(\\mathbf{g_k}, \\mathbf{g}) + \\sum_{k=1}^{K} \\sum_{i \\in \\mathcal{C}_k} d^2(\\mathbf{x_i}, \\mathbf{g_k}) \\tag{26.5} \\] There are two criteria for “correct” clustering: \\(\\text{BSSD}\\) should be large and \\(\\text{WSSD}\\) should be small. From the decomposition formula, for a fixed number of clusters \\(K\\), minimizing \\(\\text{WSSD}\\) or maximizing \\(\\text{BSSD}\\) are equivalent. Notice that in discriminant analysis, we cared a lot about \\(\\text{BSSD}\\). Now, in clustering, we will want to minimize \\(\\text{WSSD}\\). Cluster analysis aims to minimize the within-clusters sum of squared distances \\(\\text{WSSD}\\), to a fixed number of clusters \\(K\\). A cluster becomes more homogenous as its sum of squared distances decreases, and the clustering of the data set becomes better as \\(\\text{WSSD}\\) decreases. Also, as \\(\\text{BSSD}\\) increases, the separation between clusters also increases, indicating satisfactory clustering. 26.3 Complexity in Clustering Consider the following (fictitious) dataset with \\(n = 5\\) observations and 2 continuous variables: x y 1 0.7 0.0 2 1.0 0.4 3 0.0 0.7 4 0.3 0.6 5 0.4 1.0 and their corresponding scatterplot Figure 26.10: Toy data set To measure the distance between observations, we compute the squared Euclidean distance. Recall that this distance between objects \\(i\\) and \\(\\ell\\) (i.e. \\(\\mathbf{x_i}\\) and \\(\\mathbf{x_{\\ell}}\\)) is given by: \\[ d^{2}_{i\\ell} = d^2(\\mathbf{x_i}, \\mathbf{x_{\\ell}}) = \\sum_{j=1}^{p} (x_{ij} - x_{\\ell j})^2 = \\| \\mathbf{x_i} - \\mathbf{x_{\\ell}} \\|^{2}_{2} \\tag{26.6} \\] If we compute all distances between pairs of objects, we obtain a distance matrix given below (we take into account just the diagonal and the entries below the diagonal) 1 2 3 4 5 1 0.00 2 0.25 0.00 3 0.98 1.09 0.00 4 0.52 0.53 0.10 0.00 5 1.09 0.72 0.25 0.17 0.00 We could consider two possible clustering partitions or configurations. One possible partition is the blue configuration that produces \\(K = 2\\) clusters. The other partition is the red configuration which produces another set of \\(K = 2\\) clusters. Figure 26.11: Two possible clustering configurations We could compute the centroids of each group and obtain exact values for \\(\\mathrm{CSSD}_1\\) and \\(\\mathrm{CSSD}_2\\) (using the squared-Euclidian distance as our metric), combine them, and obtain \\(\\mathrm{WSSD}\\). We would then select the partition that gives the smallest \\(\\mathrm{WSSD}\\). Red clustering: \\[ \\text{W}_{red} = \\frac{0.25 + 0.53 + 0.52}{3} + \\frac{0.25}{2} = 0.56 \\] Blue clustering: \\[ \\text{W}_{blue} = \\frac{0.25}{2} + \\frac{0.10 + 0.17 + 0.25}{3} = 0.30 \\] Smaller \\(\\text{W}\\) is better. This suggests that, to find the partition with smallest within-cluster sum of squares \\(\\text{W}\\), we would require trying all possible assignments of the \\(n\\) objects into \\(K\\) groups. So why don’t we just directly find the clustering partition that minimizes \\(\\text{W}\\)? The problem is that this would only work for a small number of individuals, and a small number of partitions. Unfortunately, the computation cost increases drastically as we add more clusters and more data points. To see this, consider four objects \\(a, b, c\\) and \\(d\\). Suppose we are interested in partitions that produce clusters of different sizes. The possible (non-overlapping) partitions for \\(n = 4\\) objects are: 1 partition with 1 cluster: \\(\\{abcd\\}\\) 7 partitions with 2 clusters: \\(\\{ab,cd\\}, \\hspace{1mm} \\{ac,bd\\}, \\hspace{1mm} \\{ad,bc\\}, \\hspace{1mm} \\{a,bcd\\}, \\hspace{1mm} \\{b,acd\\}, \\hspace{1mm} \\{c,abd\\}, \\hspace{1mm} \\{d,abc\\}\\) 6 partitions with 3 clusters: \\(\\{a,b,cd\\}, \\hspace{1mm} \\{a,c,bd\\}, \\hspace{1mm} \\{a,d,bc\\}, \\hspace{1mm} \\{b,c,ad\\}, \\hspace{1mm} \\{b,d,ac\\}, \\hspace{1mm} \\{c,d,ab\\}\\) 1 partition with 4 clusters: \\(\\{a,b,c,d\\}\\) If we wanted only one cluster, we have only \\(\\{abcd\\}\\). If we wanted 4 clusters, we would have \\(\\{a, b, c, d\\}\\). In general, how many possible assignments of \\(n\\) objects into \\(K\\) groups? The number of possible assignments of \\(n\\) objects into \\(K\\) groups is given by: \\[ A(n, K) = \\frac{1}{K!} \\sum_{k=1}^{K} (-1)^{(K-k)} \\binom{K}{k} k^n \\tag{26.7} \\] With four objects, we could form clusters of sizes 1, 2, 3, and 4. Their corresponding number of assignments are: \\(A(4, 1) = 1\\) \\(A(4, 2) = 7\\) \\(A(4, 3) = 6\\) \\(A(4, 4) = 1\\) Hence, with our toy data set of four objects \\(a, b, c\\) and \\(d\\), the different number of (non-overlapping) partitions for different cluster sizes is: \\[ A(4, 1) + A(4, 2) + A(4, 3) + A(4, 4) = 15 \\] Note that: \\(A(10, 4) = 34105\\), and \\(A(25, 4) \\approx 5 \\times 10^{13} \\dots\\) huge!. It turns out that the number of (non-overlapping) partitions for \\(n\\) objects is the so-called Bell number \\(B_n\\): \\[ B_n = \\frac{1}{e} \\sum_{k=1}^{\\infty} \\frac{k^n}{k!} \\tag{26.8} \\] For \\(n = 4\\) objects, we have \\(B_4 = 15\\). This is for only 4 objects! Imagine if we had \\(n = 30\\) objects (which is still relatively small), we find \\[ B_{30} = 8.47 \\times 10^{23} = \\dots \\] a huge number greater than Avogadro’s number (\\(6.022 \\times 10^{23}\\)), which is the number of molecules in one mole of any gas! As a general rule \\[ B_n &gt; \\text{exp}(n) \\] which means this is very computationally intractable! Because of the complexity of clustering problems, we’ll have to settle for an approximation. Typically, this means that instead of finding the global minimum we will have to settle for finding a local minimum. Methodological Considerations Before moving to the next chapters that cover direct partitioning, and hierarchical partitions, we want to provide a list of general considerations that is worth keeping in mind when performing any clustering endevour. The definition of natural clusters is a tricky matter. Clusters are not always as obvious as presented in textbooks. Cluster configurations differ according to the employed algorithm. The determination of the “real” number of clusters is emperical as much as theoretical. Practical matters must be taken into account when choosing the number of clusters. Clusters are often required to be readable and interpretable, rather than theoretically optimal. e.g. the data are naturally clustered into three groups, but four clusters are requested. In practice, sometimes analysts mix/combine different clustering methods to obtain consolidations. e.g. Perform hierarchical clustering on a sample, determine number of clusters, and then apply direct partitioning on the entire set. "],
["kmeans.html", "27 K-Means 27.1 Toy Example 27.2 What does K-means do? 27.3 K-Means Algorithms", " 27 K-Means With direct partitioning methods, two clusters are always separated; and the number of clusters is generally defined a priori. Some common methods are: \\(K\\)-means \\(K\\)-medoids \\(K\\)-prototypes Methods based on a concept of density Kohonen networks (or Kohonen maps) 27.1 Toy Example Imagine we are in \\(\\mathbb{R}^p\\) (or \\(\\mathbb{R}^n\\), depending on if we are looking at things from the individuals perspective or variables perspective). For illustrative purposes, let’s consider two clusters, that is \\(K = 2\\). Figure 27.1: Toy data set We start by initializing two centroids \\(\\mathbf{g_1}\\) and \\(\\mathbf{g_2}\\). Typically, the way centroids are initialized is by randomly selecting two objects. Sometimes, however, a more “educated” initialization may be possible when we have additional information about where the clusters may be located. Figure 27.2: Arbitrary centroids Given a new data point, we compute the distances from this point to the two centroids, and classify the point to the cluster to which the closest centroid belongs. Figure 27.3: Distance of an object to both centroids The distances of all objects to the centroids are computed as well, and the objects get assigned to the group of the closest centroid. Figure 27.4: Assigning objects to closest centroid Now, with this new grouped data, we have to recompute the centroids: Figure 27.5: Update centroids We repeat this procedure, that is, calculate distances of all objects to the centroids, and re-assigning objects to clusters. Figure 27.6: Reassigning objects Likewise, after assigning objects to new clusters, new centroids are updated. Figure 27.7: Recalculate centroids After enough iterations, the centroids will not move too much between each iteration. At this point we will say the algorithm has converged, and we stop (and the resulting clusters will be the ones we use). Figure 27.8: Rinse and repeat until convergence Another question that may arise pertains to efficiency, and convergence times. One benefit of the K-Means Clustering algorithm is that it converges quite quickly; the complexity is of order \\(O(n \\cdot K)\\)). 27.2 What does K-means do? Consider the transition from the \\(s\\)-th step to the \\(s+1\\) step of the algorithm: \\[ \\textbf{step s}, \\quad \\textbf{s} \\rightarrow \\textbf{s+1}, \\quad \\textbf{then s+1} \\] Specifically, let us focus on the intermediate step, when step \\(s\\) goes to step \\(s+1\\). In other words, the centroids have moved but we have not reassigned points. Figure 27.9: From step s to step s+1 At step \\(s\\) we would compute \\(\\mathrm{WSSD}_1\\) and \\(\\mathrm{WSSD}_2\\) which, if we are using the squared Euclidean norm, becomes \\[ \\mathrm{WSSD}^{(s)} = \\sum_{i \\in \\mathcal{C}_1^{(s)}} d^2 \\big(\\mathbf{x_i}, \\mathbf{g_1}^{(s)} \\big) + \\sum_{i \\in \\mathcal{C}_2^{(s)}} d^2 \\big(\\mathbf{x_i}, \\mathbf{g_2}^{(s)} \\big) \\tag{27.1} \\] At the end of the \\(s\\)-th step, we update centroids, obtaining \\(\\mathbf{g_1}^{(s + 1)}\\) and \\(\\mathbf{g_2}^{(s + 1)}\\). These are the centroids used during the assignment of step \\(s+1\\). Notice that there is an intermediate step, with new centroids, but objects clustered according to \\(\\mathcal{C}^{(s)}_1\\) and \\(\\mathcal{C}^{(s)}_2\\). Technically, we can compute within-group dispersions during this intermediate step, given by: \\[ \\mathrm{WSSD}^{(s \\to s+1)} = \\sum_{i \\in \\mathcal{C}_1^{(s)}} d^2 \\big(\\mathbf{x_i}, \\mathbf{g_1}^{(s + 1)} \\big) + \\sum_{i \\in \\mathcal{C}_2^{(s)}} d^2 \\big(\\mathbf{x_i}, \\mathbf{g_1}^{(s + 1)} \\big) \\tag{27.2} \\] At the end of step \\(s+1\\), objects are clustered under new configurations \\(C^{(s+1)}_1\\) and \\(C^{(s+1)}_2\\). Likewise, the within-group dispersions at the end of step \\(s+1\\) become: \\[ \\mathrm{WSSD}^{(s+1)} = \\sum_{i \\in \\mathcal{C}_1^{(s + 1)}} d^2 \\big(\\mathbf{x_i}, \\mathbf{g_1}^{(s + 1)} \\big) + \\sum_{i \\in \\mathcal{C}_2^{(s + 1)}} d^2 \\big(\\mathbf{x_i}, \\mathbf{g_1}^{(s + 1)} \\big) \\tag{27.3} \\] It can be proved that these three dispersions are in order of smallest to largest (i.e. the intermediate is bigger than the \\(s\\)-th step, the \\(s+1\\)-th step is larger than the intermediate). In other words, \\[ \\mathrm{WSSD}^{(s)} \\geq \\mathrm{WSSD}^{(s \\to s+1)} \\geq \\mathrm{WSSD}^{(s+1)} \\tag{27.4} \\] 27.3 K-Means Algorithms In \\(K\\)-means clustering, we must first define the number of clusters we want (for example, \\(K = 3\\) clusters). 27.3.1 Classic Version The original \\(K\\)-means algorithm was proposed by MacQueen in 1967, and is as follows: Define \\(K\\), the number of clusters we want. Start with \\(K\\) random centroids. Start examining a particular individual object, and assign it to the group of the closest centroid. Recompute the centroid for this assigned cluster. Assign the next object to the group of the closest centroid. Recompute the centroid for this assigned cluster. Repeat this procedure until all objects have been assigned. Note that this algorithm proceeds object-by-object, re-computing centroids after each object has been assigned. That is, if we have \\(n\\) objects, we will recompute our centroid \\(n\\) times. 27.3.2 Moving Centers Algorithm This version of \\(K\\)-means was proposed by Forgy in 1965, and is as follows: Define \\(K\\), the number of clusters we want. Start with \\(K\\) random centroids. Assign each object to the nearest centroid, without updating the centroids until all objects have been assigned. Now, recalculate the centroids. Repeat steps (2) and (3) until convergence. Of course, this algorithm requires some criterion to define “convergence.” Some stopping mechanisms could be: Stop when \\(\\mathrm{WSSD}^{(s)}\\) is equal (or roughly equal to) \\(\\mathrm{WSSD}^{(s + 1)}\\). Assign a maximum number of iterations (e.g. we stop the algorithm after 10 iterations). This algorithm is the most popular out of the other two discussed. 27.3.3 Dynamic Clouds This algorithm was proposed by Diday in 1971, and is as follows: Instead of centroids, we work with “kernels” (i.e. a group of objects; that is, we no longer consider individual points (centroids) but rather groups of objects). Repeat the steps from the other algorithms. 27.3.4 Choosing \\(K\\) How many groups should be pick? Well, the answer is highly dependent on our initial centroids. The algorithm will always converge at local minima, but there is no guarantee it will arrive at the global minimum. One idea is to run \\(K\\)-means multiple times, to get a sense of how stable groups are. Note that cross-validation cannot be used to determine \\(K^*\\), the optimal number of clusters. For an explanation of why this is, see pages 518-9 in ESL. There is another proposal on how to choose a “good” number of clusters, but it involves hierarchical clustering, the topic of the next chapter. 27.3.5 Comments The main advantage of most flavors of K-means is their complexity which in general is linear, in the order \\(O(n \\cdot K)\\). In other words, their computational complexity is proportional to the number of observations \\(n\\), because we compute \\(n \\cdot K\\) distances at each step. This is the reason why K-means procedures are favored when dealing with large data sets. The main disadvantage of direct partitioning methods is that the clustering configuration depends on the initial centroids. Which means we don’t obtain the global optimum, but just a local optima. An attempt to overcome this limitation is to run the algorithm several times for different values of \\(k\\), and then compare the obtained configurations in order to detect strong or stable groups. Another suggestion is to run K-means on various random samples of observations, and then use the centroids of a good configuration as the initial centroids for the algorithm applied on the entire data set. "],
["hclus.html", "28 Hierarchical Clustering 28.1 Agglomerative Methods 28.2 Example: Single Linkage 28.3 Example: Complete Linkage", " 28 Hierarchical Clustering The idea behind hierarchical clustering is to find nested partitions. There are two approaches: Agglomerative Methods: this is the method we will discuss below. Divisive Methods: we will not discus this method, however one can think of it as the opposite of agglomerative methods. 28.1 Agglomerative Methods Agglomerative Hierarchical Clustering (AHC) produce sequences of nested partitions of increasing within-cluster heterogeneity. Distance and Dissimilarity As with \\(K\\)-means, we still need a notion of distance between two observations: this notion could be a distance metric, or a measure of dissimilarity or proximity. Metric distances satisfy the following properties: \\(d(\\mathbf{x_i}, \\mathbf{x_\\ell}) \\geq 0 \\quad \\textsf{(non-negativity)}\\) \\(d(\\mathbf{x_i}, \\mathbf{x_i}) = 0 \\quad \\textsf{(reflexivity)}\\) \\(d(\\mathbf{x_i}, \\mathbf{x_\\ell}) = d(\\mathbf{x_\\ell}, \\mathbf{x_i}) \\quad \\textsf{(symmetry)}\\) \\(d(\\mathbf{x_i}, \\mathbf{x_\\ell}) \\leq d(\\mathbf{x_i}, \\mathbf{x_k}) + d(\\mathbf{x_k}, \\mathbf{x_\\ell}) \\quad \\textsf{(triangle inequality)}\\) In turn, dissimilarities satisfy the following three properties: \\(d(\\mathbf{x_i}, \\mathbf{x_\\ell}) \\geq 0 \\quad \\textsf{(non-negativity)}\\) \\(d(\\mathbf{x_i}, \\mathbf{x_i}) = 0 \\quad \\textsf{(reflexivity)}\\) \\(d(\\mathbf{x_i}, \\mathbf{x_\\ell}) = d(\\mathbf{x_\\ell}, \\mathbf{x_i}) \\quad \\textsf{(symmetry)}\\) Distance of clusters We also need another type of distance: this one will be used to measure how different two clusters are. Minimum Distance: Also known as single linkage or nearest neighbor. Sensitive to the ``chain effect’’ (or chaining): if two widely separated clusters are linked by a chain of individuals, they be grouped together. Maximum distance: Also known as complete linkage or farthest-neighbor technique. Tends to generate clusters of equal diameter. Sensitive to outliers. Mean Distance: Also known as average linkage. Intermediate between the minimum distance and the maximum distance methods. Tends to produce clusters having similar variance. Ward method: Matches the purpose of clustering most closely. Ward distance defined as the reduction in between-cluster sum of squares. The following diagram illustrates two clusters with several individuals and their corresponding centroids; listing the common distances between clusters: Figure 28.1: Aggregation Criteria Agglomerative Algorithms The general form of an agglomerative algorithm is as follows: The initial clusters are the observations. The distances between clusters are calculated. The two clusters which are closest together are merged and replaced with a single cluster. We start again at step 2 until there is only one cluster, which contains all the observations. The sequence of partitions is presented in what is known as a tree diagram also known as dendrogram. This tree can be cut at a greater or lesser height to obtain a smaller or larger number of clusters. The number of clusters can be chosen by optimizing certain statistical quality criteria. The main criterion is the loss of between-cluster sum of squares. 28.2 Example: Single Linkage Let’s see a toy example. Suppose we have 5 individuals: \\(\\textsf{a}, \\textsf{b}, \\textsf{c}, \\textsf{d},\\) and \\(\\textsf{e}\\), and two variables \\(x\\) and \\(y\\), as follows: x y a 1 4 b 5 4 c 1 3 d 1 1 e 4 3 If we used the squared-euclidean distance as our distance measure, we obtain a matrix of distances like: a b c d e a 0 16 1 9 10 b 16 0 17 25 2 c 1 17 0 4 9 d 9 25 4 0 13 e 10 2 9 13 0 Here’s a scatterplot of the data set, and the unique distances between individuals: Figure 28.2: Five objects and their distances We first look for the pair of points with the closest distance; from our matrix, we see this is the pair \\((\\textsf{a}, \\textsf{c})\\). In other words, the first aggregation occurs with objects \\(\\textsf{a}\\) and \\(\\textsf{c}\\). We now treat this pair as a single point, and recompute distances: Figure 28.3: The two closest objects We have now four objects: a cluster \\(\\textsf{ac}\\), and three individuals \\(\\textsf{b}\\), \\(\\textsf{d}\\), and \\(\\textsf{e}\\). And here is the interesting part. We know that the distance between \\(\\textsf{b}\\) and \\(\\textsf{d}\\) is 25. Also, the distance between \\(\\textsf{b}\\) and \\(\\textsf{e}\\) is 2. But what about the distance between cluster \\(\\textsf{ac}\\) and \\(\\textsf{b}\\)? How do we determine the distance between a cluster and a point? Well, this is where the two approaches of hierarchical clustering come in. Again, we will consider only agglomerative methods. Figure 28.4: Distance between a cluster and other objects? We need an aggregation method to define the distance between a cluster and another object. In single linkage we define the distance between the \\((\\textsf{ac})\\) and \\(\\textsf{b}\\) to be: \\[ d^2(\\textsf{ac}, \\textsf{b}) = \\min \\{ d^2(\\textsf{a},\\textsf{b}), \\hspace{2mm} d^2(\\textsf{c},\\textsf{b}) \\} \\] For example, in our toy dataset we have: \\(d^2(\\textsf{ac}, \\textsf{b}) = \\min\\{16, 17\\} = 16\\) \\(d^2(\\textsf{ac}, \\textsf{d}) = \\min\\{9, 4\\} = 4\\) \\(d^2(\\textsf{ac}, \\textsf{e}) = \\min\\{10, 9\\} = 9\\) Figure 28.5: Aggregation Criteria Once we have the distances, we look for the next pair of objects that are closest to each other. In this case, those objects are \\(\\textsf{b}\\) and \\(\\textsf{e}\\). Figure 28.6: Second cluster So now we have two clusters \\(\\textsf{ac}\\) and \\(\\textsf{be}\\), and a third individual object \\(\\textsf{d}\\). As previously done, we compute distances between objects: \\(d^2(\\textsf{ac}, \\textsf{d}) = \\min \\{ d^2(\\textsf{a},\\textsf{d}), \\hspace{2mm} d^2(\\textsf{c},\\textsf{d}) \\} = \\min\\{9, 4\\} = 4\\) \\(d^2(\\textsf{be}, \\textsf{d}) = \\min \\{ d^2(\\textsf{b},\\textsf{d}), \\hspace{2mm} d^2(\\textsf{e},\\textsf{d}) \\} = \\min\\{25, 13\\} = 13\\) \\(d^2(\\textsf{ac}, \\textsf{be}) = \\min\\{16, 9\\} = 9\\) The matrix of distances is updated as follows: Figure 28.7: Updating distances between clusters and objects We then select the smallest distance which corresponds to the distance between cluster \\(\\textsf{ac}\\) and individual \\(\\textsf{d}\\): Figure 28.8: Third cluster At this step, we have two clusters left: \\(\\textsf{acd}\\) and \\(\\textsf{be}\\). These two groups have a distance of 9: Figure 28.9: Three partitions The last step involves merging both clusters into a single overall cluster: Figure 28.10: Single final cluster with all objects 28.2.1 Dendrogram The process of forming clusters can be visualized with a dendrogram. In the image above, the horizontal axis is used to locate the objects, placing them in the way they are being clustered. In turn, the vertical axis shows the distance index. As you can tell, the first cluster \\(\\textsf{ac}\\) is has a distance of 1. The cluster \\(\\textsf{be}\\) has a distance of 2, and so on and so forth. 28.3 Example: Complete Linkage In complete linkage, we look at the pair of objects that are farthest apart. We start again with the set of \\(n=5\\) objects, and the matrix of squared Euclidean distances: Figure 28.11: Five objects and their distances The first step is the same as in single linkage, that is, we look for the pair of points with the closest distance which we is the pair \\((\\textsf{a}, \\textsf{c})\\). We now treat this pair as a single point, and recompute distances: Figure 28.12: The two closest objects We have now four objects: a cluster \\(\\textsf{ac}\\), and three individuals \\(\\textsf{b}\\), \\(\\textsf{d}\\), and \\(\\textsf{e}\\). We know that the distance between \\(\\textsf{b}\\) and \\(\\textsf{d}\\) is 25. Also, the distance between \\(\\textsf{b}\\) and \\(\\textsf{e}\\) is 2. But what about the distance between cluster \\(\\textsf{ac}\\) and \\(\\textsf{b}\\)? How do we determine the distance between a cluster and a point? Figure 28.13: Distance between a cluster and other objects? In complete linkage we define the distance between the cluster \\((\\textsf{ac})\\) and individual \\(\\textsf{b}\\) to be: \\[ d^2(\\textsf{ac}, \\textsf{b}) = max \\{ d^2(\\textsf{a},\\textsf{b}), \\hspace{2mm} d^2(\\textsf{c},\\textsf{b}) \\} \\] For example, in our toy dataset we have: \\(d^2(\\textsf{ac}, \\textsf{b}) = \\max\\{16, 17\\} = 17\\) \\(d^2(\\textsf{ac}, \\textsf{d}) = \\max\\{9, 4\\} = 9\\) \\(d^2(\\textsf{ac}, \\textsf{e}) = \\max\\{10, 9\\} = 10\\) Figure 28.14: Aggregation Criteria in Complete Linkage Once we have the distances, we look for the next pair of objects that are closest to each other. In this case, those objects are \\(\\textsf{b}\\) and \\(\\textsf{e}\\). Figure 28.15: Second cluster So now we have two clusters \\(\\textsf{ac}\\) and \\(\\textsf{be}\\), and a third individual object \\(\\textsf{d}\\). The matrix of distances is updated as follows: Figure 28.16: Updating distances between clusters and objects We then select the smallest distance which corresponds to the distance between cluster \\(\\textsf{ac}\\) and individual \\(\\textsf{d}\\): Figure 28.17: Third cluster And finally, after picking \\(\\textsf{acd}\\), we get the last partition that groups all individuals into a unique cluster: Figure 28.18: Three partitions So, in this case our final grouping did not differ from that obtained using single linkage; the only difference will be our dendrogram: 28.3.1 Cutting Dendograms Depending on where we cut our dendrogram, we get different number of groups. For example, in our second dendrogram above, if we cut it at the value 20 we would obtain two clusters: \\((\\textsf{a}, \\textsf{c}, \\textsf{d})\\) and \\((\\textsf{b}, \\textsf{e})\\). Note that in this way, we can interpret the vertical distance on a dendrogram as a sort of measure for how stable our clusters are. For illustrative purposes, consider the dendrogram from complete linkage: after grouping \\((\\textsf{ac})\\) with \\(\\textsf{d}\\) (at a distance of 9), the algorithm did not produce any more groups until we reached a distance of 20. In this way, we might suspect that \\(K = 3\\) is a good guess for the true number of groups. What Could Go Wrong For example, what could go wrong with complete linkage? Well, outliers. In the presence of outliers, complete linkage will favor these points and this could mess with our results severely. Note that both \\(k-\\)means and hierarchical clustering require notions of distances, even though those notions may be drastically different. 28.3.2 Pros and COons Agglomerative methods: Have quadratic cost The dendrogram provides information about the whole process of aggregation A dendrogram also gives clue about the number of clusters Direct partitioning methods: Have linear cost Number of groups must be predetermined Local optimal partition We can take advantage of both approaches: Perform hierarchical clustering Determine the number of groups Calculate the corresponding centroids Perform K-means using as seeds the centroids previously calculated What about clustering very large data sets? Perform \\(m\\) = 2 or 3 times a K-means algorithm (with \\(K\\) = 10) Form the crosstable of the \\(m\\) obtained partitions Calculate the centroids of the non-empty cells of the crosstable Perform hierarchical clustering of the centroids, weighed by the number of individuals per cell Determine the number of clusters Consolidate the clustering configuration "],
["trees.html", "29 Intro to Decision Trees 29.1 Introduction 29.2 Some Terminology 29.3 Space Partitions", " 29 Intro to Decision Trees In this chapter we introduce decision trees, which are one the most visually atractive and intuitive supervised learning methods. In particular, we will focus our discussion around one kind of trees, the CART-style binary decision trees from the methodology developed in the early 1980s by Leo Breiman, Jerome Friedman, Charles Stone, and Richard Olshen. On the downside, we should say that single trees tend to have high variance, and suffer from overfitting. However, when combined in an ensemble of trees (e.g. bagging, random forests)—using clever sampling strategies—they become powerful predicting tools. 29.1 Introduction Trees are used in classification and regression tasks by detecting a set of rules allowing the analyst to assign the individuals in data into a class or assign it a predicted value. Here’s an example with iris data set. This data consists of 5 variables measured on \\(n = 150\\) iris flowers. There are \\(p\\) = 4 predictors, and one response. The four variables are: Sepal.Length Sepal.Width Petal.Length Petal.Width The response is a categorical (i.e. qualitative) variables that indicates the species of iris flower with three categories: setosa versicolor virginica Here’s a few rows of iris data: Sepal.Length Sepal.Width Petal.Length Petal.Width Species 1 5.1 3.5 1.4 0.2 setosa 2 4.9 3.0 1.4 0.2 setosa 3 4.7 3.2 1.3 0.2 setosa 4 4.6 3.1 1.5 0.2 setosa 5 5.0 3.6 1.4 0.2 setosa 6 5.4 3.9 1.7 0.4 setosa If we use a decision tree to classify the species of iris flowers, we obtain a set of rules can be organized into a hierarchical tree structure. Moreover, this structure can be visually displayed as an inverted tree diagram (although the tree is generally displayed upside down) like in the image below: Software programs that compute trees, will also return some kind of numeric output like this: n= 150 node), split, n, loss, yval, (yprob) * denotes terminal node 1) root 150 100 setosa (0.33333333 0.33333333 0.33333333) 2) Petal.Length&lt; 2.45 50 0 setosa (1.00000000 0.00000000 0.00000000) * 3) Petal.Length&gt;=2.45 100 50 versicolor (0.00000000 0.50000000 0.50000000) 6) Petal.Width&lt; 1.75 54 5 versicolor (0.00000000 0.90740741 0.09259259) * 7) Petal.Width&gt;=1.75 46 1 virginica (0.00000000 0.02173913 0.97826087) * 29.2 Some Terminology Before looking at how decision trees are built, let’s review some of the terminology commonly employed around them. Decision trees have three types of nodes: root, internal, and leaf nodes. These are depicted in the diagram below. Figure 29.1: Diagram of a tree structure There is exactly one root node that contains the entire set of elements in the sample; the root node has no incoming branches, and it may have zero, two, or more outgoing branches. Internal nodes, each of which is hoped to contain as many elements as possible belonging to one class; each internal node has one incoming branch and two (or more) outgoing branches. Leaf or terminal nodes which, unlike internal nodes, have no outgoing branches. The root node can be regarded as the starting node. The branches are the lines connecting the nodes. Every node that is preceded by a higher level node is called a “child” node. A node that gives origin to two (or more) child nodes is called a “parent” node. Terminal nodes are nodes without children. The most common way of visually representing a tree is by means of the classical node-link diagram that connects nodes together with line segments. Figure 29.2: Common elements of a tree diagram 29.2.1 Binary Trees One special kind of decision trees are binary trees. They are called binary because the nodes are only partitioned in two (see diagram below). Figure 29.3: Example of a binary tree diagram 29.3 Space Partitions What do partitions of decision trees look like from a geometric standpoint? It turns out that each (binary) split results in a given partition of the feature space. To be more precise, a partition that involves feature \\(X_j\\) will result in dividing the dimension of \\(X_j\\) into two regions, depending on the split condition. In the iris data set example, the first partition is produced by the condition Petal.Length &lt; 2.45. This condition splits the dimension of Petal.Length in two regions: a region with petal length values less than 2.45 inches, and another region with petal length values greater than or equal to 2.45 inches. Likewise, the second split is based on the condition Petal.Width &lt; 1.75, which results in two regions along this dimension. Toy Abstract Example Suppose we have two quantitative predictors \\(X_1\\), and \\(X_2\\), like in the image below: Figure 29.4: Two-dimensional feature space Generally, we create partitions by iteratively splitting one of the predictors into two regions. For example, a first split could be based on \\(X_1 = t_1\\). Figure 29.5: First partition A second split could be produced on a partition for \\(X_2\\). If \\(X_1 \\leq t_1\\), split on \\(X_2 = t_2\\). This results in two regions: \\(R_1\\) and \\(R_2\\). Figure 29.6: Second partition A third split may be detected on the preliminary region \\(B\\). If \\(X_1 &gt; t_1\\), split on \\(X_3 = t_3\\). This results in two regions: \\(C\\) and \\(D\\). Figure 29.7: Third partition Then, another partition on region \\(D\\), if \\(X_2 = t_4\\) Figure 29.8: Fourth partition At the end, with a total number of four splits, the feature space is divided in five regions: Figure 29.9: Regions of feature space 29.3.1 The Process of Building a Tree Let us begin with the general idea behind binary trees. The main idea is to build a tree structure by successively (i.e. recursively) dividing the set of objects with the help of features \\(X_1, \\dots, X_p\\), in order to obtain the terminal nodes (i.e. final segments) that are as homogeneous as possible (with respect to the response). The tree building process requires three main components Establish the set of admissible divisions for each node, that is, compute all possible binary splits. Define a criterion to find the “best” node split, which requires some criterion to be optimized. Determine a rule to declare a given node as either internal node, or leaf node. Establishing the set of admissible splits for each node is related with the nature of the segmentation variables in the data. The number and type of possible binary splits will vary depending on the scale (e.g., binary, nominal, ordinal, and continuous) of the predictor variables. Further discussion is given in the next section. The second aspect to build a segmentation tree is the definition of a node splitting rule. This rule is necessary to find at each internal node a criterion for splitting the data into subgroups. Since finding a split involves finding the predictor variable that is the most discriminating, the splitting crietroin helps to rank variables according to their discriminating power. The last component of the tree building process has to do with the definition of a node termination rule. In essence, there are two different ways to deal with this issue: 1) pre-pruning and 2) post-pruning. Pre-pruning implies that the decision of when to stop the growth of a tree is made prospectively. Post-pruning refers to reducing the size of a fully expanded tree by pruning some of its branches retrospectively. "],
["tree-impurities.html", "30 Binary Splits and Impurity 30.1 Binary Partitions 30.2 Measures of Impurity", " 30 Binary Splits and Impurity As we mentioned in the last part of the previous chapter, the building process of a binary decision tree requires three main components: Establish the set of admissible divisions for each node: compute all possible binary splits. Define a criterion to find the “best” node split: some optimization criterion to be optimized. This in turn, requires determining some measure of heterogeneity (within a given node). Determine a rule to declare a given node as either internal node, or leaf node. In this chapter we’ll review these components in more detail. 30.1 Binary Partitions In binary segmentation trees, the set of admissible splits for each node depends on the nature of the predictor featuers in the data. Predictor variables can be of different type either nature but also continuous. We can divide them in four classes: Binary variables Ordinal variables Nominal variables Continuous variables The number of possible binary splits for each type of segmentation variable is described below. 30.1.1 Splits of Binary variables Because binary variables have two values, they only generate one binary partition. For example, say we have a variable Gender with two values, female and male. This feature can only de divided in one possible way: Figure 30.1: Example of a binary partition for a binary variable 30.1.2 Splits of Nominal Variables A nominal variable may have many categories and the number of binary splits depends on the number of distinct categories for the corresponding variable. The total number of possible binary partitions for a nominal variable with \\(q\\) categories is \\[ 2^{q-1} - 1 \\] For example, suppose a variable Color with three categories: blue, white and red. The number of binary splits for this nominal feature is: \\[ 2^{3-1} - 1 = 3 \\] Figure 30.2: Example of binary splits for a nominal variable 30.1.3 Splits of Ordinal Variables Binary splits for ordinal variables must respect the order property of the categories, that is, the grouping of categories must preserve the order among the values. The total number of binary partitions for an ordinal variable with \\(q\\) categories is \\[ q-1 \\] For example, the Apparel size may have four categories: small (S), medium (M), large (L) and extra-large (XL). In this case, the number of binary splits will be \\[ 4-1 = 3 \\] Figure 30.3: Example of binary splits for an ordinal variable 30.1.4 Splits Continuous variables The treatment for continuous variables depends on the number of different values the variable takes. Suppose a continuous variable with \\(q\\) different values. If we consider that \\(q\\) is “adequate” in some sense, we can treat the continuous variable like an ordinal variable. If we consider \\(q\\) to be large we may group its values and reduced their number in \\(q^*\\) (\\(q^*&lt;q\\)). In both cases, the variable is treated as an ordinal variable and the number of binary splits will be \\[ q-1 \\quad \\text{or} \\quad q^*-1 \\] For example, imagine the continuous variable Age (measured in years) with five values 5, 7, 8, 9 and 10. The number of binary splits is \\[ 5 - 1 = 4 \\] Figure 30.4: Example of binary splits for a continuous variable The following table shows the number of binary splits depending on the type of scale for a given feature. Scale Order matters? Binary splits Binary (\\(q=2\\) categories) No 1 Nominal (\\(q&gt;2\\) categories) No \\(2^{q-1} - 1\\) Ordinal (\\(q&gt;2\\) categories) Yes \\(q - 1\\) Continuous (\\(q\\) values) Yes \\(q - 1\\) 30.2 Measures of Impurity The second aspect behind any binary tree building process has to do with determining a criterion to find the “best” node split. This in turn, requires some sort of optimization criterion. The general idea is to find the node partition that gives the largest reduction in the amount of heterogeneity (or variability) of a parent node. For illustrative purposes, let us focus on a single node with \\(n = 14\\) objects, divided in \\(2\\) classes; one class denoted as \\(\\bigcirc\\), the other denoted as \\(\\Delta\\); both classes having \\(7\\) objects. Suppose this node splits in the following manner: Figure 30.5: Hypothetical split of a node We have an intuitive feeling that the parent node has more heterogeneity than its child nodes. In fact, this is a general result: a split cannot result in child nodes with more heterogeneity than the parent node. Stated differently, you can never do worse than the parent node. In the parent node of the previous figure, the proportion of \\(\\bigcirc\\) equals the proportion of \\(\\Delta\\). In other words, there is an equal probability of being a \\(\\bigcirc\\) as there is to be a \\(\\Delta\\). However, in the left node, there is a higher chance of being a \\(\\bigcirc\\) than \\(\\Delta\\) (the reverse is true of the right node). Most measures of heterogeneity (and therefore homogeneity) relate to proportions of classes in a parent node. Let’s spitball some ideas as to how to do this: \\(\\mathrm{prop}(\\bigcirc) - \\mathrm{prop}(\\Delta)\\); you would need to assign some sort of interpretation to the value \\(0\\) in this case. Entropy (information, as viewed from a computer science standpoint). Could also refer to this as a measure of “disorder” within a node. Impurity (how mixed, diverse) 30.2.1 Entropy The first type of measure of heterogeneity (or impurity) that we’ll study is the so-called Entropy. Let’s start with an (abstract) toy example: say we have a box containing 12 objects, all of the same class (say, \\(\\bigcirc\\)). Figure 30.6: Set of 12 objects of same type If we randomly select an object from this box, we have zero uncertainty about its class: in other words, there is zero entropy associated with this situation. Now, suppose three of those \\(\\bigcirc\\)’s were replaced with \\(\\square\\)’s. Figure 30.7: Set with two different types of objects If we were to randomly draw an object from this box, there would be some amount of uncertainty as to its class; that is, there is some entropy. Probabilistically speaking, there’s a chance of 9/12 that the selected object is \\(\\bigcirc\\), and a chance of 3/12 that the drawn object is \\(\\square\\). Finally, in the most extreme case, say we have 12 objects, all of different classes, like in the diagram below: Figure 30.8: Set with 12 different types of objects We are quite uncertain as to the classes of objects drawn from this box; that is, we have maximum entropy. In Short: the intuition behind entropy relates to how “mixed” a box is, with respect to the classes contained inside. If a box is “pure” (i.e. has only one class), it has zero entropy where as if a box is “highly impure” (i.e. has many different classes), it has some nonzero amount of entropy. Figure 30.9: Set with 12 different types of objects Now, let’s see how we can link our idea of entropy with the idea of probability. From the examples above, we can see the following relationship when randomly slecting an object from the box: \\[ \\begin{array}{lcl} \\text{large probability of guessing class} &amp; \\Rightarrow &amp; \\text{small entropy value} \\\\ \\text{small probability of guessing class} &amp; \\Rightarrow &amp; \\text{large entropy value} \\\\ \\end{array} \\] Hence, it makes sense to try and model entropy as a function of \\(P\\), the probability of selecting an object of a certain class. From our discussion above, we see that our entropy function should be small when \\(P\\) is around 1, and large when \\(P\\) is around \\(0\\). There are quite a few functions that display this behavior. The one we will use is the logarithm, of any base \\(b\\). Note that \\[ \\log_b(P) : [0, 1] \\to (-\\infty, 0] \\] That is, since our probability values are restricted to lie in the unit interval, the entropy will lie somewhere on the negative real axis. 30.2.2 The Math Behind Entropy Letting \\(H(\\text{node})\\) denote the entropy of a particular node, we use the following definition: \\[ H(\\text{node}) = - \\sum_{k=1}^{K} p_k \\log_2(p_k) \\] where \\(p_k\\) denotes the probability of randomly selecting an object that comes from class \\(k\\). Note the negative sign in the above formula; the reason for this negative sign is to have positive values from logarithms of probabilities. By convention, we use a log of base 2. The rationale behind picking \\(2\\) comes from information theory, as base-2 logs can be interpreted in terms of bits. Additionally, if \\(p_j = 0\\) for some class \\(j \\in \\{1, \\dots, K\\}\\), we set \\(p_j \\log_b(p_j) = 0\\) because \\[ \\lim_{p_j \\to 0} \\left[ p_j \\log_b(p_j) \\right] = 0 \\] In this way, we never have to directly evaluate \\(\\log_b(0)\\). Let’s see if this definition of entropy is consistent with our intuition. For example, let us consider a pure node; that is, a node in which all objects share the same class. In this case \\(p_k = 1\\), and we have only one class so \\[\\begin{align*} H(\\text{node}) &amp;= - p_1 \\cdot log_2 (p_1) \\\\ &amp;= -(1) \\cdot \\log_2(1) \\\\ &amp;= -1 \\cdot 0 = 0 \\end{align*}\\] Now, for our second example above, we have \\(K = 2\\) classes with \\(p_1 = 9/12\\) and \\(p_2 = 3/12\\), so that \\[\\begin{align*} H(\\text{node}) &amp;= - p_1 \\log_2 (p1) - p_2 \\log_2 (p_2) \\\\ &amp;= - \\frac{9}{12} \\log_2\\left( \\frac{9}{12} \\right) - \\frac{3}{12} \\log_2\\left( \\frac{3}{12} \\right) \\approx 0.8112 \\end{align*}\\] In a node where there is a 50-50 split, we have \\[\\begin{align*} H(\\text{node}) &amp;= - p_1 \\log_2 (p1) - p_2 \\log_2 (p_2) \\\\ &amp;= - \\frac{1}{2} \\log_2\\left( \\frac{1}{2} \\right) - \\frac{1}{2} \\log_2\\left( \\frac{1}{2} \\right) = 1 \\end{align*}\\] The following figure depicts several examples of sets and their entropy values. Figure 30.10: Different sets with their entropy values 30.2.3 Gini Impurity The second type of measure of heterogeneity (or impurity) that we’ll describe is the so-called Gini impurity, also refer to as Gini index. Let us return to the second box example from the previous section. There are 9 \\(\\bigcirc\\)’s and 3 \\(\\square\\)’s. Figure 30.11: Set with two different types of objects Let’s suppose I randomly select a ball, and note what class it belongs to. I then ask you (without giving you any information about the box, other than the fact that there are \\(\\bigcirc\\)’s and \\(\\square\\)’s), to guess what class my selected object belongs to. There are 4 possible outcomes to this situation: I select \\(\\bigcirc\\), you guess \\(\\bigcirc\\). I select \\(\\bigcirc\\), you guess \\(\\square\\). I select \\(\\square\\), you guess \\(\\bigcirc\\). I select \\(\\square\\), you guess \\(\\square\\). The Gini index pays attention to the misclassifications; that is, the situations in which your guess was wrong. For instance, in our example above, there is a total of 2 possible misclassifications: I select \\(\\bigcirc\\), you guess \\(\\square\\). I select \\(\\square\\), you guess \\(\\bigcirc\\). The Gini Index involves the probability of misclassification of an object randomly selected from the box. It is computed as follows: \\[\\begin{align*} \\text{Gini Index} &amp; = \\underbrace{ Prob(\\bigcirc) }_{\\text{me}} \\underbrace{ Prob(\\square) }_{\\text{you}} + \\underbrace{ Prob(\\square) }_{\\text{me}} \\underbrace{ Prob(\\bigcirc) }_{\\text{you}} \\\\ &amp; = \\left( \\frac{9}{12} \\right) \\left( \\frac{3}{12} \\right) + \\left( \\frac{3}{12} \\right) \\left( \\frac{9}{12} \\right) = \\frac{1}{8} = 0.375 \\end{align*}\\] For a given node containing objects of \\(K\\) classes, the Gini impurity is given by: \\[ Gini(\\text{node}) = \\sum_{k=1}^{K} p_k (1 - p_k) \\] Doing a bit of algebra, it is possible to find alternative formulas for the gini impurity: \\[\\begin{align*} Gini(\\text{node}) &amp;= \\sum_{k=1}^{K} p_k (1 - p_k) \\\\ &amp;= 1 - \\sum_{k=1}^{K} p^{2}_{k} \\\\ &amp;= \\sum_{k \\neq \\ell} p_k p_{\\ell} \\end{align*}\\] Some Examples For illustration purposes, let’s compute gini impurity of a node containing 12 objects, all of the same class: Figure 30.12: Set of 12 objects of same type \\[\\begin{align*} Gini(\\text{node}) &amp;= \\sum_{k=1}^{1} p_k (1 - p_k) \\\\ &amp;= \\left( \\frac{12}{12} \\right) \\left( \\frac{0}{12} \\right) = 0 \\end{align*}\\] Now consider a balanced node with objects of two classes Figure 30.13: Two-class balanced set the gini impurity in this case is: \\[\\begin{align*} Gini(\\text{node}) &amp;= \\sum_{k=1}^{2} p_k (1 - p_k) \\\\ &amp;= \\left( \\frac{6}{12} \\right) \\left( \\frac{6}{12} \\right) + \\left( \\frac{6}{12} \\right) \\left( \\frac{6}{12} \\right) = 0.5 \\end{align*}\\] The following figure depicts several examples of sets and their gini impurities. Figure 30.14: Different sets with their gini impurities 30.2.4 Variance-based Impurity So far we’ve been considering measures of impurity when we have a categorical response. But what about a quantitative response? What measure of impurity can we use in this case? The typical way in which we measure the amount of impurity of a node, with respect to a quantitative response \\(Y\\), is by computing the sum of squared deviations from the mean (or alternatively the variance). For instance, consider a parent node denoted by \\(N\\). The sum of squared deviations from the mean is given by: \\[ \\text{rss} = \\sum_{i \\in N} (y_i - \\bar{y})^2 \\] where \\(\\bar{y}\\) is the mean response for the individuals in node \\(N\\). Now, suppose we are looking the best binary split among a set of \\(X_1, \\dots, X_m\\) features. Say we take predictor \\(X_j\\), and we are evaluating a splitting condition given by the cutoff value \\(s\\). This means that \\(s\\) splits the feature space in two regions \\(\\mathcal{R_1}\\) and \\(\\mathcal{R_2}\\): \\[ \\mathcal{R_1}(j,s) = \\{ X | X_j &lt; s\\} \\quad \\mathrm{and} \\quad \\mathcal{R_2}(j,s) = \\{ X | X_j \\geq s \\} \\] The goal is to find the feature \\(j\\), and the value \\(s\\) that minimize the following equation: \\[ \\sum_{i \\in \\mathcal{R_1}} (y_i - \\bar{y}_{\\mathcal{R_1}})^2 + \\sum_{i \\in \\mathcal{R_2}} (y_i - \\bar{y}_{\\mathcal{R_2}})^2 \\] where \\(\\bar{y}_{\\mathcal{R_1}}\\) is the mean response for the individuals in region \\(\\mathcal{R_1}(j,s)\\), and \\(\\bar{y}_{\\mathcal{R_2}}\\) is the mean response for the individuals in region \\(\\mathcal{R_2}(j,s)\\). When \\(j\\) and the value \\(s\\) have been identified, the predicted response \\(\\hat{y}_0\\) of an observation is calculated as the mean of the in-sample individuals in the region to which \\(\\mathbf{x_0}\\) belongs to. "],
["tree-splits.html", "31 Splitting Nodes 31.1 Entropy-based Splits 31.2 Gini-index based Splits 31.3 Looking for the best split", " 31 Splitting Nodes Having introduced the common measures of impurity in the previous chapter, let’s now review a fairly simple example by applying entropy and gini impurity indices on a two-class response classification problem. Consider a simple credit data example, with 8 individuals described by three features (gender, job, region), and one response (status) with two categories: \\(\\text{good}\\) and \\(\\text{bad}\\) customers. gender job region status a male yes A good b female yes B good c male no C good d female yes B good e male no A bad f female no B bad g male yes C bad h female no C bad 31.1 Entropy-based Splits Let us encode \\(\\text{good}\\) as a circle \\(\\bigcirc\\), and let us encode \\(\\text{bad}\\) as a square \\(\\square\\). Our root node then contains 8 objects, with a 50-50 split in credit status: Figure 31.1: Root node Recall that the entropy of a particular node, \\(H(\\text{node})\\), containing objects of \\(k = 1, \\dots, K\\) classes, is given by: \\[ H(\\text{node}) = - \\sum_{k=1}^{K} p_k \\log_2(p_k) \\] The entropy of the root node would therefore be \\[ H(\\text{root}) = - \\underbrace{ \\frac{1}{2} \\log_2\\left( \\frac{1}{2} \\right) }_{\\text{good}} - \\underbrace{ \\frac{1}{2} \\log_2\\left( \\frac{1}{2} \\right) }_{\\text{bad}} = 1 \\] Partition by Gender Now, say we choose to partition by gender. Our parent node would then split as follows: Figure 31.2: Split by gender Each of these child nodes have the same entropy: \\[ H(\\text{left}) = - \\underbrace{ \\frac{1}{2} \\log_2\\left( \\frac{1}{2} \\right) }_{\\text{good}} - \\underbrace{ \\frac{1}{2} \\log_2\\left( \\frac{1}{2} \\right) }_{\\text{bad}} = 1 = H(\\text{right}) \\] Now, we want to compare the combined entropies of these two child nodes and the entropy of the root node. That is, we compare \\[ H(\\text{root}) \\quad \\text{-vs-} \\quad \\left( \\frac{4}{8} \\right) H(\\text{left}) + \\left( \\frac{4}{8} \\right) H(\\text{right}) \\] Hence, let us define the information gain by the quantity \\[\\begin{align*} \\Delta I &amp; = H(\\text{root}) - \\left[ \\left( \\frac{n_{\\text{left}}}{n} \\right) H(\\text{left}) + \\left( \\frac{n_{\\text{right}}}{n} \\right) H(\\text{right}) \\right] \\\\ &amp;= H(\\text{root}) - \\left[ \\left( \\frac{4}{8} \\right) H(\\text{left}) + \\left( \\frac{4}{8} \\right) H(\\text{right}) \\right] \\\\ &amp;= 1 - \\left[ \\frac{4}{8} + \\frac{4}{8} \\right] = 1 - 1 = 0 \\end{align*}\\] In this way, we see that gender is not a good predictor to base our partitions on. Why? Because we obtain child nodes that have the same amount of entropy (the same amount of disorder) as the parent node. Partition by Job If we decide to partition by job, the parent node would then split as: Figure 31.3: Split by job The left node has an entropy of: \\[ H(\\text{left}) = -\\frac{3}{4} \\log_2\\left( \\frac{3}{4} \\right) -\\frac{1}{4} \\log_2\\left( \\frac{1}{4} \\right) \\approx 0.8112 \\] whereas the right node has an entropy of: \\[ H(\\text{right}) = -\\frac{1}{4} \\log_2\\left( \\frac{1}{4} \\right) -\\frac{3}{4} \\log_2\\left( \\frac{3}{4} \\right) \\approx 0.8112 \\] The information gain (i.e. reduction in impurity) \\(\\Delta I\\) is thus: \\[\\begin{align*} \\Delta I &amp;= H(\\text{root}) - \\frac{1}{2} H(\\text{left}) - \\frac{1}{2} H(\\text{right}) \\\\ &amp;= 1 - \\frac{1}{2} (0.8112) - \\frac{1}{2} (0.8112) \\\\ &amp;\\approx 0.1888 \\end{align*}\\] Partition by Region What about splitting the root node based on the categories of region? Note that region is a nominal qualitative variable with three categories, which means that there will be three possible binary partitions. Figure 31.4: Splits by region Region A -vs- Regions B,C \\[\\begin{align*} \\Delta I &amp;= H(\\text{root}) - \\frac{2}{8} H(\\text{left}) - \\frac{6}{8} H(\\text{right}) \\\\ &amp;= 1 - \\frac{2}{8} (1) - \\frac{6}{8} (1) \\\\ &amp;= 0 \\end{align*}\\] Region B -vs- Regions A,C \\[\\begin{align*} \\Delta I &amp;= H(\\text{root}) - \\frac{3}{8} H(\\text{left}) - \\frac{5}{8} H(\\text{right}) \\\\ &amp;= 1 - \\frac{3}{8} (0.9183) - \\frac{5}{8} (0.9709) \\\\ &amp;\\approx 0.0488 \\end{align*}\\] Region C -vs- Regions A,B \\[\\begin{align*} \\Delta I &amp;= H(\\text{root}) - \\frac{3}{8} H(\\text{left}) - \\frac{5}{8} H(\\text{right}) \\\\ &amp;= 1 - \\frac{3}{8} (0.9183) - \\frac{5}{8} (0.9709) \\\\ &amp;\\approx 0.04888 \\end{align*}\\] Comparing the splits As a summary, here are the following splits and the associated information gains: We therefore see that splitting based on \\(\\text{Job}\\) yields the best split (i.e. the split with the highest information gain). In practice, this is the procedure that you would have to carry out. Try splitting based on all of the variables and compute the associated information gains, and visually ascertain which split results in the highest information gain. If there are ties in \\(\\Delta I\\) values, it is customary to randomly select one to have a higher ranking than the other. Entropy-based Classification Tree The resulting classfication tree using entropy for binary splits is: n= 8 node), split, n, loss, yval, (yprob) * denotes terminal node 1) root 8 4 bad (0.5000000 0.5000000) 2) job= no 4 1 bad (0.7500000 0.2500000) 4) gender=female 2 0 bad (1.0000000 0.0000000) * 5) gender=male 2 1 bad (0.5000000 0.5000000) 10) region=A 1 0 bad (1.0000000 0.0000000) * 11) region=C 1 0 good (0.0000000 1.0000000) * 3) job=yes 4 1 good (0.2500000 0.7500000) 6) gender= male 1 0 bad (1.0000000 0.0000000) * 7) gender=female,male 3 0 good (0.0000000 1.0000000) * with the tree diagram as follows: 31.2 Gini-index based Splits Let’s repeat the same process but now calculating Gini impurities instead of entropies. Recall that Gini impurity of a particular node, \\(H(\\text{node})\\), containing objects of \\(k = 1, \\dots, K\\) classes, is given by: \\[ Gini(\\text{node}) = \\sum_{k=1}^{K} p_k (1 - p_k) \\] Consider the root node: Figure 31.5: Root node The Gini impurity of the root node would therefore be: \\[\\begin{align*} Gini(\\text{root}) &amp;= \\sum_{k=1}^{2} p_k (1 - p_k) = \\\\ &amp;= p_1(1-p_1) + p_2 (1 - p_2) \\\\ &amp;= \\frac{1}{2} \\left(1 - \\frac{1}{2} \\right) + \\frac{1}{2} \\left( 1 - \\frac{1}{2} \\right) \\\\ &amp;= 0.5 \\end{align*}\\] Partition by Gender We begin partitioning by gender. Our parent node would then split as follows: Figure 31.6: Split by gender Each of these child nodes have the same impurity: \\[\\begin{align*} Gini(\\text{left}) &amp;= \\sum_{k=1}^{2} p_k (1 - p_k) \\\\ &amp;= \\frac{1}{2} \\left(1 - \\frac{1}{2} \\right) + \\frac{1}{2} \\left(1 - \\frac{1}{2} \\right) \\\\ &amp;= \\frac{1}{2} \\cdot \\frac{1}{2} + \\frac{1}{2} \\cdot \\frac{1}{2} \\\\ &amp;= 0.5 \\\\ &amp;= Gini(\\text{right}) \\end{align*}\\] Now, we want to compare the combined entropies of these two child nodes and the entropy of the root node. That is, we compare \\[ Gini(\\text{root}) \\quad \\text{-vs-} \\quad \\left( \\frac{4}{8} \\right) Gini(\\text{left}) + \\left( \\frac{4}{8} \\right) Gini(\\text{right}) \\] Again, we use information gain defined by the quantity \\[\\begin{align*} \\Delta I &amp; = Gini(\\text{root}) - \\left[ \\left( \\frac{n_{\\text{left}}}{n} \\right) Gini(\\text{left}) + \\left( \\frac{n_{\\text{right}}}{n} \\right) Gini(\\text{right}) \\right] \\\\ &amp;= Gini(\\text{root}) - \\left[ \\left( \\frac{4}{8} \\right) Gini(\\text{left}) + \\left( \\frac{4}{8} \\right) Gini(\\text{right}) \\right] \\\\ &amp;= 0.5 - \\left[ \\frac{4}{8} \\cdot \\frac{1}{2} + \\frac{4}{8} \\cdot \\frac{1}{2} \\right] = 0.5 - 0.5 = 0 \\end{align*}\\] As you can tell, using Gini index, we obtain the same information \\(\\Delta I\\) value as when using entropy on the gender-based split. Consequently, this feature is not a helpful predictor to base our partitions on. Partition by Job If we decide to partition by job, the parent node would then split as: Figure 31.7: Split by job The left node has a gini impurity of: \\[ Gini(\\text{left}) = \\frac{3}{4} \\left(1- \\frac{3}{4} \\right) + \\frac{1}{4} \\left( 1 - \\frac{1}{4} \\right) = 0.375 \\] whereas the right node has a gini impurity of: \\[ Gini(\\text{right}) = \\frac{1}{4} \\left( 1 - \\frac{1}{4} \\right) + \\frac{3}{4} \\left( 1 - \\frac{3}{4} \\right) = 0.375 \\] The information gain (i.e. reduction in impurity) \\(\\Delta I\\) is thus: \\[\\begin{align*} \\Delta I &amp;= Gini(\\text{root}) - \\frac{1}{2} Gini(\\text{left}) - \\frac{1}{2} Gini(\\text{right}) \\\\ &amp;= 0.5 - \\frac{1}{2} (0.375) - \\frac{1}{2} (0.375) \\\\ &amp;= 0.125 \\end{align*}\\] Partition by Region Finally, let’s split the root node based on the categories of region. Figure 31.8: Splits by region Region A -vs- Regions B,C \\[\\begin{align*} \\Delta I &amp;= Gini(\\text{root}) - \\frac{2}{8} Gini(\\text{left}) - \\frac{6}{8} Gini(\\text{right}) \\\\ &amp;= 0.5 - \\frac{2}{8} (0.5) - \\frac{6}{8} (0.5) \\\\ &amp;= 0 \\end{align*}\\] Region B -vs- Regions A,C \\[\\begin{align*} \\Delta I &amp;= Gini(\\text{root}) - \\frac{3}{8} Gini(\\text{left}) - \\frac{5}{8} Gini(\\text{right}) \\\\ &amp;\\approx 0.5 - \\frac{3}{8} (0.4444) - \\frac{5}{8} (0.48) \\\\ &amp;\\approx 0.0333 \\end{align*}\\] Region C -vs- Regions A,B \\[\\begin{align*} \\Delta I &amp;= Gini(\\text{root}) - \\frac{3}{8} Gini(\\text{left}) - \\frac{5}{8} Gini(\\text{right}) \\\\ &amp;\\approx 0.5 - \\frac{3}{8} (0.4444) - \\frac{5}{8} (0.48) \\\\ &amp;\\approx 0.0333 \\end{align*}\\] Comparing the splits As a summary, here are the following splits and the associated information gains: We therefore see that splitting based on \\(\\text{Job}\\) yields the best split (i.e. the split with the highest information gain). Gini-based Classification Tree The resulting classfication tree using Gini-impurity for binary splits is: n= 8 node), split, n, loss, yval, (yprob) * denotes terminal node 1) root 8 4 bad (0.5000000 0.5000000) 2) job= no 4 1 bad (0.7500000 0.2500000) 4) gender=female 2 0 bad (1.0000000 0.0000000) * 5) gender=male 2 1 bad (0.5000000 0.5000000) 10) region=A 1 0 bad (1.0000000 0.0000000) * 11) region=C 1 0 good (0.0000000 1.0000000) * 3) job=yes 4 1 good (0.2500000 0.7500000) 6) gender= male 1 0 bad (1.0000000 0.0000000) * 7) gender=female,male 3 0 good (0.0000000 1.0000000) * with the tree diagram as follows: 31.3 Looking for the best split Let’s quickly review the steps for splitting a node, which involve the first two aspects listed above. The main idea to grow a tree is to successively (i.e. recursively) divide the set of objects with the help of features \\(X_1, \\dots, X_p\\), in order to obtain nodes that are as homogeneous as possible (with respect to the response). Here’s what we have to do in a given (parent) node: Compute impurity of the (parent) node For each variable feature \\(X_j\\) Determine the number of binary splits For each binary split: compute impurity for left node compute impurity for right node calculate and store reduction in impurity (information gain) Find the best splitting feature \\(X_j\\) that has the maximum reduction in impurity. Once you find the optimal split, perform the split, and repeat the above procedure for the new child nodes. "],
["tree-basics.html", "32 Building Binary Trees 32.1 Node-Splitting Stopping Criteria 32.2 Issues with Trees 32.3 Pruning a Tree 32.4 Pros and Cons of Trees", " 32 Building Binary Trees As previously mentioned, the building process of a binary tree requires three main components: Establish the set of all possible binary splits. Define a criterion to find the “best” node split, which requires some criterion to be optimized based on measures of impurity. Determine a rule to declare a given node as either internal node, or leaf node. So far, we have described the first two aspects. But we still need to talk about how to stop the tree-growing process. 32.1 Node-Splitting Stopping Criteria To grow a tree, we need to recursively apply the steps described in the previous chapter, to all child nodes. But at some point we need to decide whether to stop splitting a given node. In no particular order of importance, here are a few criteria for stopping splitting a node. We stop splitting a node when/if: The node is pure (i.e. a node contains objects of only one class). We run out of features (that is, there are no more features to split on). A pre-specified minimum number of objects in a node is reached. For example, stop splitting when a node contains at most 10 elements. In rpart(), this can be specified using the minsplit parameter. Size of resulting nodes would be “too small” (e.g. 10, 15, 20, etc.). This is closely related to the previous rule, but a bit different. For example, we can have a node with enough objects greater than the minimum required. However, all resulting binary splits are deemed too small. In rpart(), this can be specified using the parameter minbucket. A certain depth level is reached. This is another condition in which a pre-determined level of depth is chosen, in order to prevent “large” overfitting trees. In rpart(), this can be specified using the maxdepth parameter. 32.2 Issues with Trees How good are trees? Do they predict well? Yes and No. A tree can achieve high prediction performance … of the training data. The larger the tree, the better its in-sample performance. In some cases, a (very large) tree can even accomplish zero (or near-zero) error rates. But as you can imagine, this optential for having good in-sample predictions comes with a price: large test error. Why? Because the tree structure is highly dependent on early splits. Put it otherwise, trees are complex models. To illustrate the complexity of trees, let’s use the Heart data set from An Introduction to Statistical Learning (ISL, by James te al). This data contains 14 variables observed on 303 individuals. The response variable is AHD. We have randomly split the data into training (90%) and testing test (10%), using the training set to fit the response. For illustration purposes, we repeat this operation four times, obtaining four different trees, displayed in the following figure. Figure 32.1: Different Tress Notice how different the trees are. This is why we say that trees tend to have high complexity (i.e. high flexibility). Recall that the way in which splits are chosen is by looking for the best split at a given step, without paying attention to what subsequent splits may look like. It can be the case that a less good split leads to a better tree later on. But this is not how the algorithm proceeds. In other words, trees can become very complex “too soon”, because splits are obtained with a “short-term view”, (best split now with instant gratification), instead of a \"long-term view (less good split now with delayed gratification). Up to now, we are using the notion of complexity in a vague manner. But it is important to think about this for a minute. How do we define complexity in decision trees? Well, there is no unique answer to this question. Perhaps the most natural way to think about a tree’s complexity is by looking at its depth (how tall, or small, a tree is). The taller the tree (that is, the deeper it is), the more complexity. Knowing that larger trees (large depth level) tend to result in poor out-of-sample prediction, it feels intuitive that we should look for smaller, less complex, trees. We may be willing to sacrifice some of the in-sample error in exchange of increasing the out-of-sample performance. So far, so good. But things are not that really simple. To see this, we need to talk about the bias-variance tradeoff of decision trees. 32.2.1 Bias-Variance of Trees In general, decision trees have a bias and a variance problem. With small depth levels, they have a bias problem. With large depth levels, they have a variance problem. The following diagram shows typical error curves of decision trees: Figure 32.2: Typical error curves of decision trees Small trees (i.e. small depth) tend to have a bias problem (underfitting). Conversely, large trees (i.e. large depth) tend to have a variance problem (overfitting). Figure 32.3: Bias-Variance problem of trees Because the depth of trees is measured in integer steps (not in a continuous scale), it is very difficult to find the “sweet spot” for the right depth level. Figure 32.4: Bias-Variance problem of trees Compared to penalized criteria like those used in penalized methods (e.g. ridge regression and lasso), using tree depth to measure complexity, does not provide the same “fine granularity” of a penalty tuning parameter. Why? Because the depth level of a tree is an integer number. You either have a tree of depth 2 or 3 or 4, but you cannot have a tree of depth level 2.78 or 3.125. Therefore, finding the sweet spot for level of depth that minimizes the test-error does not allow us to have a fine grain sensitive analysis of what the best tradeoff is between bias and variance. And this is, to a large extent, the downfall of decision trees. This is why in practice they don’t tend to work as expected (in terms of generalization power). The main strategy to overcome this limitation is not by changing the depth of the tree, but by focusing on building large trees, and tackling the high variance issue with resampling techniques, and aggregating several versions of individual trees. These are the topics of the next chapters. 32.3 Pruning a Tree Now that we have described all the main aspects to grow a tree, as well as some of the main drawbacks that a tree suffers from, we can present the formal algorithm of CART-trees, based on cost-complexity pruning. The building process of CART-style binary trees is to grow a “large” tree and then prune off branches (from the bottom up) until we get an “okay size” tree. Finding the pruned tree of “okay size” implies finding a subtree of the large tree that is “optimal” in some predictive-power sense. This, in turn, requires to handle a tree’s complexity in a different manner: not in terms of a tree’s depth, but in terms of its number of terminal nodes. Here are the main stages: Grow a large tree, denoted by \\(T_{\\max}\\), typically obtained by getting nodes of a pre-determined minimum size. Calculate \\(err(\\tau)\\), the error at each terminal node \\(\\tau \\in T_{\\max}\\). Aggregating these errors \\(err(\\tau)\\) gives us an overall measure of error for the tree: \\(E(T_{\\max})\\) Prune \\(T_{\\max}\\), from the bottom up, by getting rid of terminal nodes that don’t provide “enough” information gain (i.e. useless leaf nodes). This is done by including a tuning parameter that penalizes the “complexity” of the subtrees (complexity in terms of number of terminal nodes). To introduce the cost-complexity criterion, we need to first introduce some terminology and notation. We denote \\(err(\\tau)\\), the error measure of terminal node \\(\\tau\\). The type of error function will depend on the type of response: \\(err()\\) will be a misclassification rate when dealing with a categorical response. \\(err()\\) will be a regression error, e.g. squared error, when dealing with a quantitative response. In order to use a penalized criterion, we define the penalized error in terminal \\(\\tau\\) with \\(\\alpha\\) complexity parameter as: \\[ err_{\\alpha} (\\tau) = err(\\tau) + \\alpha \\] where \\(\\alpha\\) is defined on the interval \\([0, \\infty)\\)]. The total error of a (sub)tree \\(T\\), also referred to as the cost of a tree, is then calculated as the sum of penalized errors of all terminal nodes: \\[ E_{\\alpha} (T) = \\sum_{\\ell}^{L} err_{\\alpha}(\\tau_{\\ell}) = E(T) + \\alpha |\\tilde{T}| \\] where \\(L = |\\tilde{T}|\\) is the number of terminal nodes in subtree \\(T\\) of \\(T_{\\max}\\). The penalty parameter \\(\\alpha\\) determines the tree size. When \\(\\alpha\\) is very small, the penalty will be small, and so the size of the subtree \\(T(\\alpha)\\) will be large. Setting \\(\\alpha = 0\\), and the obtained tree will be \\(T_{\\max}\\), the largest possible tree. As we increase \\(\\alpha\\), the minimizing subtrees \\(T(\\alpha)\\) will have fewer and fewer terminal nodes. Taking things to the extreme, with a very large \\(\\alpha\\), all nodes will be pruned, resulting in \\(T_{\\max}\\) being the root node. The idea is to choose several values of \\(\\alpha\\), in an increasing order, and then look for the subtree \\(T_m = T(\\alpha_m)\\) that minimizes the penalized error. It can be shown that the sequence of penalty values \\[ 0 = \\alpha_0 &lt; \\alpha_1 &lt; \\alpha_2 &lt; \\dots &lt; \\alpha_M \\] corresponds to a finte sequence of nested subtrees of \\(T_{\\max}\\) \\[ T_{\\max} = T_0 \\supseteq T_1 \\supseteq T_2 \\supseteq \\dots \\supseteq T_M \\] Because the complexity (i.e. penalty) parameter is a tuning parameter, we typically use cross-validation to find the subtree \\(T&#39; \\subseteq T\\) that results in the smallest cost. In the R package \"rpart\", the function rpart() allows you to specify the argument cp which controls the complexity parameter. 32.4 Pros and Cons of Trees Let’s review some of the advantages and disadvantages of trees. 32.4.1 Advantages of Trees They work for both types of responses: 1) categorical and 2) quantitative. When dealing with a categorical response we talk about classification tree; when dealing with a quantitative response we talk about regression tree. They work with any type of predictors: binary, nominal, ordinal, continuous, even with missing values. They can detect interactions among the predictors. Another advantage is that there’s almost no need to preprocess data for a decision tree. This is very rare among statistical methods. Likewise, decision trees make no stochastic assumptions: e.g. no normality assumptions, or other specific distributions about the data. In general, as long as the resulting trees are not too small, they tend to have low bias. The graphical display of a tree tends to be easy to read and interpret (at least the main output; there are some summary statistics that might require further background knowledge). 32.4.2 Disadvantages of Trees Trees are highly dependent on the training set. That is, given a different training set, there is a high chance that you will obtain a different tree. Which means they are prone to overfitting. Because of this dependency, they suffer from high variance (this is perhaps the main “Achilles’ heel” of decision trees). Training a decision tree requires a reasonably large number of observations: as a rule of thumb there should be at least 30-50 observations per node. Recall that decision trees produce splits which involve rectangular regions in the feature space which may not necessarily match the distribution of the observations. Finding the optimal sweetspot in bias-variance tradeoff is difficult. "],
["bagging.html", "33 Bagging 33.1 Introduction 33.2 Why Bother Bagging?", " 33 Bagging 33.1 Introduction Bagging is the acronym for “bootstrap aggregating”, a method developped by Leo Breiman in the early 1990s, and officially published in his paper Bagging Predictors, in the Machine Learning journal. A preliminary report is still available in Leo’s archived academic website: https://www.stat.berkeley.edu/~breiman/bagging.pdf We will not discuss bagging in detail, rather we will only focus on its conceptual essence, following Leo’s description: “a method for generating multiple versions of a predictor and using these to get an aggregated predictor”. 33.1.1 Idea of Bagging Suppose that, instead of having an initial dataset \\(\\mathcal{D}\\), we are able to obtain \\(M\\) new datasets \\(\\mathcal{D}_1, \\dots, \\mathcal{D}_M\\). From each set \\(\\mathcal{D_m}\\), we then obtain a model \\(h_m\\). The type of model can be a single decision tree, or a generic regression model (e.g. ridge regression), or a generic classification model (e.g. logistic regression). Figure 33.1: Several data sets and fitted models Having the set of \\(m\\) models, we can compute the (emperical) average model \\(\\bar{h}(x)\\) as: \\[ \\text{empirical} \\quad \\bar{h}(x) = \\frac{1}{M} \\sum_{m=1}^{M} h_m(x) \\] which we can include in our previous diagram and get an updated figure: Figure 33.2: Empirical average model As the number of data sets increases, i.e. a very large number \\(M \\to \\infty\\), the empirical average model will approach the theoretical average hypothesis: \\[\\begin{align*} \\text{empirical} \\quad \\bar{h}(x) &amp; \\to \\text{theoretical} \\quad \\bar{h}(x) \\\\ \\text{as} \\hspace{5mm} M &amp; \\to \\infty \\end{align*}\\] where the theoretical average hypothesis \\(\\bar{h}\\) is given by the expectation over all learning data sets: \\[ \\text{theoretical} \\quad \\bar{h}(x) = \\mathbb{E}_{\\mathcal{D}} [h^{\\mathcal{D}} (x)] \\] Now, there’s a catch. In order for the approximation to be valid, that is, to approximate the theoretical average hypothesis with the empirical one, the data sets \\(\\mathcal{D}_1, \\dots, \\mathcal{D}_M\\) have to be independent. The problem is that, in practice, we don’t have \\(M\\) independent datasets. However, what we can do is to resample from \\(\\mathcal{D}\\), sampling with replacement. If \\(|\\mathcal{D}| = n\\), we wish to produce a set of data sets \\(\\{\\mathcal{D_m} \\}_{m=1}^{M}\\) with \\(|\\mathcal{D_m}|=n\\); that is, these are bootstrap samples. Given these \\(M\\) bootstrap samples, we proceed as we did in our theoretical experiment above: generate the set of individual models \\(\\{h_m \\}_{m=1}^{M}\\), and define compute the empirical average hypothesis: \\[ \\bar{h}(x) = \\frac{1}{M} \\sum_{m=1}^{M} h_m(x) \\] Now, we do not have the weak law of large numbers, because these \\(\\mathcal{D_m}\\)’s are not independent. However, it turns out that despite this, the empirical \\(\\bar{h}(x)\\) gets quite close to the theoretical average hypothesis. This process of taking bootstrap samples, averaging, and using this to approximate the average hypothesis is known as bagging, which is a portmanteau of bootstrap aggregating. To summarize, we generate a series of data sets \\(\\mathcal{D}_1, \\dots, \\mathcal{D}_M\\) by drawing bootstrap samples from the original learning set \\(\\mathcal{D}\\); we fit the corresponding models \\(h_m(x)\\) for each bootstrap set \\(\\mathcal{D}_m\\), and then we aggregate them (averaging them) to obtain the final model: Figure 33.3: Bagging Scheme The way we obtain a prediction \\(\\hat{y}_0\\) for a query point \\(\\mathbf{x_0}\\) is by taking the average of all \\(M\\) predictions, that is: \\[ \\hat{y}_0 \\rightarrow \\bar{h}(\\mathbf{x_0}) = \\frac{1}{M} \\sum_{m=1}^{M} h_m(\\mathbf{x_0}) \\] 33.2 Why Bother Bagging? Recall the bias-variance decomposition. Given a learning data set \\(\\mathcal{D}\\) of \\(n\\) points, and a hypothesis \\(h(x)\\), the expectation of the Squared Error for a given out-of-sample point \\(x_0\\), over all possible learning sets, is expressed as: \\[ \\mathbb{E}_{\\mathcal{D}} \\left [ \\left( h^{(\\mathcal{D})}(x_0) - f(x_0) \\right)^2 \\right ] \\] When we have a noisy target \\(y = f(x) + \\epsilon\\), with \\(\\epsilon\\) a zero-mean noise random variable with variance \\(\\sigma^2\\), the bias-variance decomposition becomes: \\[ \\mathbb{E}_{\\mathcal{D}} \\left [ \\left (h^{(\\mathcal{D})}(x_0) - y_0 \\right)^2 \\right ] = \\text{var} + \\text{bias}^2 + \\sigma^2 \\] which can be expanded as: \\[\\begin{align*} \\mathbb{E}_{\\mathcal{D}} \\left [ \\left( h^{(\\mathcal{D})}(x_0) - f(x_0) + \\epsilon \\right)^2 \\right ] &amp;= \\underbrace{\\mathbb{E}_{\\mathcal{D}} \\left [ \\left (h^{(\\mathcal{D})}(x_0) - \\bar{h}(x_0) \\right)^2 \\right ]}_{\\text{variance}} \\\\ &amp;+ \\underbrace{ \\big( \\bar{h}(x_0) - f(x_0) \\big)^2 }_{\\text{bias}^2} \\\\ &amp;+ \\underbrace{\\sigma^2}_{\\text{noise}} \\end{align*}\\] Suppose we only use our training set to fit one model; if this is a decision tree, we know it will tend to suffer from high variance and hence there is a strong chance this model will be wildly off from the true model. Using bagging, however, we are able to reduce variance. Now, will bagging increase bias? We know that, in general, reducing variance comes at the cost of increasing bias. However, after closer examination of bias: \\[ \\text{Bias}^2 (h(x)) = \\big( \\bar{h}(x_0) - f(x_0) \\big)^2 \\] you should be able to note that bias does not depend on our individual datasets directly. Sure, there is an implicit dependency through the average model, however there is no \\(\\mathcal{D}\\) in the bias formula bove. Hence, in bagging, we can decrease variance while not sacrificing bias. Figure 33.4: Ensemble of models, and bagged model Surely there is still a cost, somewhere? Well, there is. On one hand, we lose interpretability. If we apply bagging with decision trees, we then obtain a bagged forest in which there is not a “global tree” that we can visualize and follow its partitions. On the other hand, there is a higher computational cost associated with bagging (especially in the resample procedure). Luckily, there is a “cheat” for this: parallel processing. This resampling procedure is highly parallelizable; we could run different parts on different computers and combine them in the end. Hence, if we have enough computational resources, this should not be a huge problem. "],
["forest.html", "34 Random Forests 34.1 Introduction 34.2 Algorithm", " 34 Random Forests 34.1 Introduction Random Forests are another creation of Leo Breiman, co-developped with Adele Cutler, in the late 1990s. Like bagging, documentation on random forests is available in Leo’s archived academic website: https://www.stat.berkeley.edu/~breiman/RandomForests/ As the name indicates, a random forest entails a set of trees. The building blocks are still single trees, but instead of fitting just one tree, we grow many of them. Depending on the type of analyzed response variable, you will end up with a forest of regression trees or a forest of classification trees. Conceptually, growing many trees is something that can also be done with bagging, in which case we talk about a bagged forest. Figure 34.1: Several data sets and fitted models The starting point is the same as in bagging: use of bootstrap samples to generate multiple learning data sets \\(\\mathcal{D}_1, \\dots, \\mathcal{D}_B\\), which in turn will be used to grow several trees, like in the following diagram: Figure 34.2: Conceptual diagram of a Random Forest So far, so good. Now, what exactly makes a random forest different from a bagged forest? The answer to this question has to do with the way in which single trees are grown in a random forests. We are going to introduce a twist in the tree-building algorithm. More specifically, we are going to modify the node splitting procedure. Assuming we have \\(p\\) features, we predetermine a number \\(k \\ll p\\) such that at each node, \\(k\\) features are randomly selected (from the set of \\(p\\) predictors), to find the best split. The fundamental reason to introduce another random sampling operation, which is repeated for every node partition, is to attempt reducing the number of correlated predictors that could potentially be present in the construction of a single tree. This seemingly awkward operation turns out to return high dividends when a forest is deployed in practice. 34.2 Algorithm Say we decide to grow \\(B\\) trees (e.g. \\(B\\) = 100). Here is how we would grow our forest with trees \\(T_1, T_2, \\dots, T_B\\). Draw a bootstrap sample—that is, with replacement—of size \\(n\\) from the learning set \\(\\mathcal{D}\\), where \\(n\\) is the size of the learning set. Specify a number \\(k &lt;&lt; p\\) of features which will be used in each node-splitting operation. Grow a tree \\(T_b\\) using the bootstrapped data \\(\\mathcal{D}_b\\), by recursively repeating the following steps: For each child node: Randomly select \\(k\\) features from the \\(p\\) possible features. Pick the feature \\(X_{\\ell}\\), among the \\(k\\) available features, that produces the best split. Split the node into two child nodes according to the rule you obtained from step (ii). Output the set of trees, that is, the forest: \\(\\{T_1, T_2, \\dots, T_B\\}\\) (also known as the ensemble of trees). Here, \\(k\\) is a tuning parameter. The good news, however, is that we typically set \\(k = \\lceil \\sqrt{p} \\rceil\\) (take square root of \\(p\\) and round up, although some authors prefer to round down, \\(k = \\lfloor \\sqrt{p} \\rfloor\\)). Why is this good news? Because using the previous formula to select \\(k\\), we don’t really need to tune anything. There is a theoretical justification for the fact that \\[ k = \\lceil \\sqrt{p} \\rceil \\] This gives us a good chance that our subsamples are very nearly independent. 34.2.1 Two Sources of Randomness Randomness in the data points Each tree gets to see a different data set (that is, no tree gets to train on the entire dataset) Behind this, we have the notion of bagging (recall, this is a portmanteau of Bootstrap and Aggregating) Randomness in the features/predictors Each tree gets to see a different set of features (that is, no tree gets to train on the entire set of features) This is the powerful feature of random forests. Specifically, it is in this step that trees become de-correlated. Note that this source of randomness is not present in bagging. 34.2.2 Regressions and Classification Forests Once you grow a random forests, how does it get used to make predictions? Let’s consider the type of prediction based on the kind of response variable. Forest of Regression Trees Suppose we are in a regression setting. Given a test a point \\((\\mathbf{x_0}, y_0)\\), we pass this query point to all the trees. We would obtain \\(B\\) predictions for \\(y_0\\): denote these by \\(\\hat{y}_0^{(1)}, \\hat{y}_0^{(2)}, \\dots, \\hat{y}_0^{(B)}\\). Figure 34.3: Random forest of regression trees To get the predicted value \\(\\hat{y}_0\\), we would then average these single-tree predictions: \\[ \\hat{y}_0 = \\frac{1}{B} \\sum_{b=1}^{B} \\hat{y}_0^{(b)} \\] Forest of Classification Trees Suppose now, that we are in a classification setting with a binary response with 2 classes, say class “red” and class “yellow”. Given a test a point \\((\\mathbf{x_0}, y_0)\\), we pass this query point to all the trees: Figure 34.4: Random forest of classification trees The single-tree predictions \\(\\hat{y}_0^{(b)}\\) would be obtained by examining the distribution of each class within each terminal node. In this hypothetical example (see above diagram), \\(\\hat{y}_0^{(1)} = \\verb|red|\\), \\(\\hat{y}_0^{(2)} = \\verb|yellow|\\), \\(\\hat{y}_0^{(3)} = \\verb|red|\\), and \\(\\hat{y}_0^{(B)} = \\verb|red|\\). We would then assign \\(\\hat{y}_0\\) the mode of all this single-tree predictions: \\[ \\hat{y}_0 = \\mathrm{mode} \\left\\{ T_b(x_0) \\right\\} \\] 34.2.3 Key Advantage of Random Forests Let’s first start by recapping what we typically do with most techniques covered in this book. We start with a dataset \\(\\mathcal{D}\\), and then split it into some number of sub-datasets: \\[ \\mathcal{D} \\to \\begin{cases} \\mathcal{D}_{\\mathrm{training}} \\\\ \\mathcal{D}_{\\text{testing}} \\\\ \\end{cases} \\hspace{5mm} \\text{|or|} \\hspace{5mm} \\mathcal{D} \\to \\begin{cases} \\mathcal{D}_{\\mathrm{training}} \\\\ \\mathcal{D}_{\\text{validation}} \\\\ \\mathcal{D}_{\\text{testing}} \\\\ \\end{cases} \\] In bagging, as well as in random forests, we do NOT take this approach. Rather, we end up with the so-called Out-of-Bag Error (or OOB, for short). For example, suppose we have a forest of 10 trees \\(T_1, T_2, \\dots, T_{10}\\) Figure 34.5: Out-of-Bag points in Random Forest Some points \\((\\mathbf{x}_i, y_i)\\) are not used by all the trees. For instance, say we have a data point \\((\\mathbf{x}_i, y_i)\\) that is never used by trees \\(T_2, T_3, T_5,\\) and \\(T_8\\). This means that point \\((\\mathbf{x}_i, y_i)\\) is really a test point for these trees. Hence, we could plug \\((\\mathbf{x}_i, y_i)\\) into these trees to obtain predictors \\(\\hat{y}_i^{(2)} , \\hat{y}_i^{(3)} , \\hat{y}_i^{(5)},\\) and \\(\\hat{y}_i^{(8)}\\). We would then compute the OOB in the following manner: \\[ \\mathrm{OOB}_i = \\frac{ (\\hat{y}_i^{(2)} - y_i)^2 + (\\hat{y}_i^{(3)} - y_i)^2 + (\\hat{y}_i^{(5)} - y_i)^2 + (\\hat{y}_i^{(8)} - y_i)^2 }{4} \\] More generally, we define the overall OOB error to be \\[ E_{\\mathrm{OOB}} = \\frac{1}{n} \\sum_{i=1}^{n} \\mathrm{OOB}_i \\] where the OOB\\(_i\\)’s are computed as above. Here is the key observation: it turns out that \\(E_{\\mathrm{OOB}}\\) is an unbiased estimate of \\(E_{out}\\). Errors Last but not least, we want to highlight the extremely atractive property of ensemble methods: we get to reduce variance without increasing bias. And with random forests, we get an unbiased estimate of \\(E_{out}\\) (almost for free!). Figure 34.6: Typical error curves of random forests "]
]
