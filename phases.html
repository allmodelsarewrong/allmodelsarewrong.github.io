<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>10 Learning Phases | All Models Are Wrong</title>
  <meta name="description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  <meta name="generator" content="bookdown 0.16 and GitBook 2.6.7" />

  <meta property="og:title" content="10 Learning Phases | All Models Are Wrong" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://allmodelsarewrong.github.io" />
  
  <meta property="og:description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  <meta name="github-repo" content="allmodelsarewrong/allmodelsarewrong.github.io" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="10 Learning Phases | All Models Are Wrong" />
  
  <meta name="twitter:description" content="This is a text about the fundamental concepts of Statistical Learning Methods." />
  

<meta name="author" content="by Gaston Sanchez, and Ethan Marzban" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="biasvar.html"/>
<link rel="next" href="resampling.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><b>All Models Are Wrong (CSL)</b><br><small>by G. Sanchez & E. Marzban</small></a></li>

<li class="divider"></li>
<li class="part"><span><b>I Welcome</b></span></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="about.html"><a href="about.html"><i class="fa fa-check"></i><b>1</b> About this book</a><ul>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#prerequisites"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="part"><span><b>II Intro</b></span></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#basic-notation"><i class="fa fa-check"></i><b>2.1</b> Basic Notation</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="duality.html"><a href="duality.html"><i class="fa fa-check"></i><b>3</b> Geometric Duality</a><ul>
<li class="chapter" data-level="3.1" data-path="duality.html"><a href="duality.html#rows-space"><i class="fa fa-check"></i><b>3.1</b> Rows Space</a></li>
<li class="chapter" data-level="3.2" data-path="duality.html"><a href="duality.html#columns-space"><i class="fa fa-check"></i><b>3.2</b> Columns Space</a></li>
<li class="chapter" data-level="3.3" data-path="duality.html"><a href="duality.html#cloud-of-individuals"><i class="fa fa-check"></i><b>3.3</b> Cloud of Individuals</a><ul>
<li class="chapter" data-level="3.3.1" data-path="duality.html"><a href="duality.html#average-individual"><i class="fa fa-check"></i><b>3.3.1</b> Average Individual</a></li>
<li class="chapter" data-level="3.3.2" data-path="duality.html"><a href="duality.html#centered-data"><i class="fa fa-check"></i><b>3.3.2</b> Centered Data</a></li>
<li class="chapter" data-level="3.3.3" data-path="duality.html"><a href="duality.html#distance-between-individuals"><i class="fa fa-check"></i><b>3.3.3</b> Distance between individuals</a></li>
<li class="chapter" data-level="3.3.4" data-path="duality.html"><a href="duality.html#distance-to-the-centroid"><i class="fa fa-check"></i><b>3.3.4</b> Distance to the centroid</a></li>
<li class="chapter" data-level="3.3.5" data-path="duality.html"><a href="duality.html#measures-of-dispersion"><i class="fa fa-check"></i><b>3.3.5</b> Measures of Dispersion</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="duality.html"><a href="duality.html#cloud-of-variables"><i class="fa fa-check"></i><b>3.4</b> Cloud of Variables</a><ul>
<li class="chapter" data-level="3.4.1" data-path="duality.html"><a href="duality.html#mean-of-a-variable"><i class="fa fa-check"></i><b>3.4.1</b> Mean of a Variable</a></li>
<li class="chapter" data-level="3.4.2" data-path="duality.html"><a href="duality.html#variance-of-a-variable"><i class="fa fa-check"></i><b>3.4.2</b> Variance of a Variable</a></li>
<li class="chapter" data-level="3.4.3" data-path="duality.html"><a href="duality.html#variance-with-vector-notation"><i class="fa fa-check"></i><b>3.4.3</b> Variance with Vector Notation</a></li>
<li class="chapter" data-level="3.4.4" data-path="duality.html"><a href="duality.html#standard-deviation-as-a-norm"><i class="fa fa-check"></i><b>3.4.4</b> Standard Deviation as a Norm</a></li>
<li class="chapter" data-level="3.4.5" data-path="duality.html"><a href="duality.html#covariance"><i class="fa fa-check"></i><b>3.4.5</b> Covariance</a></li>
<li class="chapter" data-level="3.4.6" data-path="duality.html"><a href="duality.html#correlation"><i class="fa fa-check"></i><b>3.4.6</b> Correlation</a></li>
<li class="chapter" data-level="3.4.7" data-path="duality.html"><a href="duality.html#geometry-of-correlation"><i class="fa fa-check"></i><b>3.4.7</b> Geometry of Correlation</a></li>
<li class="chapter" data-level="3.4.8" data-path="duality.html"><a href="duality.html#orthogonal-projections"><i class="fa fa-check"></i><b>3.4.8</b> Orthogonal Projections</a></li>
<li class="chapter" data-level="3.4.9" data-path="duality.html"><a href="duality.html#the-mean-as-an-orthogonal-projection"><i class="fa fa-check"></i><b>3.4.9</b> The mean as an orthogonal projection</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Unsupervised I: PCA</b></span></li>
<li class="chapter" data-level="4" data-path="pca.html"><a href="pca.html"><i class="fa fa-check"></i><b>4</b> Principal Components Analysis</a><ul>
<li class="chapter" data-level="4.1" data-path="pca.html"><a href="pca.html#low-dimensional-representations"><i class="fa fa-check"></i><b>4.1</b> Low-dimensional Representations</a></li>
<li class="chapter" data-level="4.2" data-path="pca.html"><a href="pca.html#projections"><i class="fa fa-check"></i><b>4.2</b> Projections</a><ul>
<li class="chapter" data-level="4.2.1" data-path="pca.html"><a href="pca.html#vector-and-scalar-projections"><i class="fa fa-check"></i><b>4.2.1</b> Vector and Scalar Projections</a></li>
<li class="chapter" data-level="4.2.2" data-path="pca.html"><a href="pca.html#projected-inertia"><i class="fa fa-check"></i><b>4.2.2</b> Projected Inertia</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="pca.html"><a href="pca.html#maximization-problem"><i class="fa fa-check"></i><b>4.3</b> Maximization Problem</a><ul>
<li class="chapter" data-level="4.3.1" data-path="pca.html"><a href="pca.html#eigenvectors-of-mathbfs"><i class="fa fa-check"></i><b>4.3.1</b> Eigenvectors of <span class="math inline">\(\mathbf{S}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="pca.html"><a href="pca.html#another-perspective-of-pca"><i class="fa fa-check"></i><b>4.4</b> Another Perspective of PCA</a><ul>
<li class="chapter" data-level="4.4.1" data-path="pca.html"><a href="pca.html#finding-principal-components"><i class="fa fa-check"></i><b>4.4.1</b> Finding Principal Components</a></li>
<li class="chapter" data-level="4.4.2" data-path="pca.html"><a href="pca.html#finding-the-first-pc"><i class="fa fa-check"></i><b>4.4.2</b> Finding the first PC</a></li>
<li class="chapter" data-level="4.4.3" data-path="pca.html"><a href="pca.html#finding-the-second-pc"><i class="fa fa-check"></i><b>4.4.3</b> Finding the second PC</a></li>
<li class="chapter" data-level="4.4.4" data-path="pca.html"><a href="pca.html#finding-all-pcs"><i class="fa fa-check"></i><b>4.4.4</b> Finding all PCs</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="pca.html"><a href="pca.html#data-decomposition-model"><i class="fa fa-check"></i><b>4.5</b> Data Decomposition Model</a><ul>
<li class="chapter" data-level="4.5.1" data-path="pca.html"><a href="pca.html#alternative-approaches"><i class="fa fa-check"></i><b>4.5.1</b> Alternative Approaches</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Linear Regression</b></span></li>
<li class="chapter" data-level="5" data-path="ols.html"><a href="ols.html"><i class="fa fa-check"></i><b>5</b> Linear Regression</a><ul>
<li class="chapter" data-level="5.1" data-path="ols.html"><a href="ols.html#motivation"><i class="fa fa-check"></i><b>5.1</b> Motivation</a></li>
<li class="chapter" data-level="5.2" data-path="ols.html"><a href="ols.html#the-ideaintuition-of-regression"><i class="fa fa-check"></i><b>5.2</b> The Idea/Intuition of Regression</a></li>
<li class="chapter" data-level="5.3" data-path="ols.html"><a href="ols.html#the-linear-regression-model"><i class="fa fa-check"></i><b>5.3</b> The Linear Regression Model</a></li>
<li class="chapter" data-level="5.4" data-path="ols.html"><a href="ols.html#the-error-measure"><i class="fa fa-check"></i><b>5.4</b> The Error Measure</a></li>
<li class="chapter" data-level="5.5" data-path="ols.html"><a href="ols.html#the-least-squares-algorithm"><i class="fa fa-check"></i><b>5.5</b> The Least Squares Algorithm</a></li>
<li class="chapter" data-level="5.6" data-path="ols.html"><a href="ols.html#geometries-of-ols"><i class="fa fa-check"></i><b>5.6</b> Geometries of OLS</a><ul>
<li class="chapter" data-level="5.6.1" data-path="ols.html"><a href="ols.html#rows-perspective"><i class="fa fa-check"></i><b>5.6.1</b> Rows Perspective</a></li>
<li class="chapter" data-level="5.6.2" data-path="ols.html"><a href="ols.html#columns-perspective"><i class="fa fa-check"></i><b>5.6.2</b> Columns Perspective</a></li>
<li class="chapter" data-level="5.6.3" data-path="ols.html"><a href="ols.html#parameters-perspective"><i class="fa fa-check"></i><b>5.6.3</b> Parameters Perspective</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="gradient.html"><a href="gradient.html"><i class="fa fa-check"></i><b>6</b> Gradient Descent</a><ul>
<li class="chapter" data-level="6.1" data-path="gradient.html"><a href="gradient.html#error-surface"><i class="fa fa-check"></i><b>6.1</b> Error Surface</a></li>
<li class="chapter" data-level="6.2" data-path="gradient.html"><a href="gradient.html#idea-of-gradient-descent"><i class="fa fa-check"></i><b>6.2</b> Idea of Gradient Descent</a></li>
<li class="chapter" data-level="6.3" data-path="gradient.html"><a href="gradient.html#moving-down-an-error-surface"><i class="fa fa-check"></i><b>6.3</b> Moving Down an Error Surface</a><ul>
<li class="chapter" data-level="6.3.1" data-path="gradient.html"><a href="gradient.html#the-direction-of-mathbfv"><i class="fa fa-check"></i><b>6.3.1</b> The direction of <span class="math inline">\(\mathbf{v}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="gradient.html"><a href="gradient.html#gradient-descent-and-our-model"><i class="fa fa-check"></i><b>6.4</b> Gradient Descent and our Model</a></li>
</ul></li>
<li class="part"><span><b>V Learning Concepts</b></span></li>
<li class="chapter" data-level="7" data-path="learning.html"><a href="learning.html"><i class="fa fa-check"></i><b>7</b> Theoretical Framework</a><ul>
<li class="chapter" data-level="7.1" data-path="learning.html"><a href="learning.html#mental-map"><i class="fa fa-check"></i><b>7.1</b> Mental Map</a></li>
<li class="chapter" data-level="7.2" data-path="learning.html"><a href="learning.html#kinds-of-predictions"><i class="fa fa-check"></i><b>7.2</b> Kinds of Predictions</a><ul>
<li class="chapter" data-level="7.2.1" data-path="learning.html"><a href="learning.html#two-types-of-data"><i class="fa fa-check"></i><b>7.2.1</b> Two Types of Data</a></li>
<li class="chapter" data-level="7.2.2" data-path="learning.html"><a href="learning.html#two-types-of-predictions"><i class="fa fa-check"></i><b>7.2.2</b> Two Types of Predictions</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="learning.html"><a href="learning.html#two-types-of-errors"><i class="fa fa-check"></i><b>7.3</b> Two Types of Errors</a><ul>
<li class="chapter" data-level="7.3.1" data-path="learning.html"><a href="learning.html#individual-errors"><i class="fa fa-check"></i><b>7.3.1</b> Individual Errors</a></li>
<li class="chapter" data-level="7.3.2" data-path="learning.html"><a href="learning.html#overall-errors"><i class="fa fa-check"></i><b>7.3.2</b> Overall Errors</a></li>
<li class="chapter" data-level="7.3.3" data-path="learning.html"><a href="learning.html#auxiliary-technicality"><i class="fa fa-check"></i><b>7.3.3</b> Auxiliary Technicality</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="learning.html"><a href="learning.html#noisy-targets"><i class="fa fa-check"></i><b>7.4</b> Noisy Targets</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="mse.html"><a href="mse.html"><i class="fa fa-check"></i><b>8</b> MSE of Estimator</a><ul>
<li class="chapter" data-level="8.1" data-path="mse.html"><a href="mse.html#mse-of-an-estimator"><i class="fa fa-check"></i><b>8.1</b> MSE of an Estimator</a><ul>
<li class="chapter" data-level="8.1.1" data-path="mse.html"><a href="mse.html#prototypical-cases-of-bias-and-variance"><i class="fa fa-check"></i><b>8.1.1</b> Prototypical Cases of Bias and Variance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="biasvar.html"><a href="biasvar.html"><i class="fa fa-check"></i><b>9</b> Bias-Variance Tradeoff</a><ul>
<li class="chapter" data-level="9.1" data-path="biasvar.html"><a href="biasvar.html#introduction"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="biasvar.html"><a href="biasvar.html#motivation-example"><i class="fa fa-check"></i><b>9.2</b> Motivation Example</a><ul>
<li class="chapter" data-level="9.2.1" data-path="biasvar.html"><a href="biasvar.html#two-hypotheses"><i class="fa fa-check"></i><b>9.2.1</b> Two Hypotheses</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="biasvar.html"><a href="biasvar.html#learning-from-two-points"><i class="fa fa-check"></i><b>9.3</b> Learning from two points</a></li>
<li class="chapter" data-level="9.4" data-path="biasvar.html"><a href="biasvar.html#bias-variance-derivation"><i class="fa fa-check"></i><b>9.4</b> Bias-Variance Derivation</a><ul>
<li class="chapter" data-level="9.4.1" data-path="biasvar.html"><a href="biasvar.html#out-of-sample-predictions"><i class="fa fa-check"></i><b>9.4.1</b> Out-of-Sample Predictions</a></li>
<li class="chapter" data-level="9.4.2" data-path="biasvar.html"><a href="biasvar.html#noisy-target"><i class="fa fa-check"></i><b>9.4.2</b> Noisy Target</a></li>
<li class="chapter" data-level="9.4.3" data-path="biasvar.html"><a href="biasvar.html#types-of-theoretical-mses"><i class="fa fa-check"></i><b>9.4.3</b> Types of Theoretical MSEs</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="biasvar.html"><a href="biasvar.html#the-tradeoff"><i class="fa fa-check"></i><b>9.5</b> The Tradeoff</a><ul>
<li class="chapter" data-level="9.5.1" data-path="biasvar.html"><a href="biasvar.html#bias-variance-tradeoff-picture"><i class="fa fa-check"></i><b>9.5.1</b> Bias-Variance Tradeoff Picture</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="phases.html"><a href="phases.html"><i class="fa fa-check"></i><b>10</b> Learning Phases</a><ul>
<li class="chapter" data-level="10.1" data-path="phases.html"><a href="phases.html#introduction-1"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="phases.html"><a href="phases.html#model-assessment"><i class="fa fa-check"></i><b>10.2</b> Model Assessment</a><ul>
<li class="chapter" data-level="10.2.1" data-path="phases.html"><a href="phases.html#holdout-test-set"><i class="fa fa-check"></i><b>10.2.1</b> Holdout Test Set</a></li>
<li class="chapter" data-level="10.2.2" data-path="phases.html"><a href="phases.html#why-does-a-test-set-work"><i class="fa fa-check"></i><b>10.2.2</b> Why does a test set work?</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="phases.html"><a href="phases.html#model-selection"><i class="fa fa-check"></i><b>10.3</b> Model Selection</a><ul>
<li class="chapter" data-level="10.3.1" data-path="phases.html"><a href="phases.html#three-way-holdout-method"><i class="fa fa-check"></i><b>10.3.1</b> Three-way Holdout Method</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="phases.html"><a href="phases.html#model-training"><i class="fa fa-check"></i><b>10.4</b> Model Training</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="resampling.html"><a href="resampling.html"><i class="fa fa-check"></i><b>11</b> Resample Approaches</a><ul>
<li class="chapter" data-level="11.1" data-path="resampling.html"><a href="resampling.html#general-sampling-blueprint"><i class="fa fa-check"></i><b>11.1</b> General Sampling Blueprint</a></li>
<li class="chapter" data-level="11.2" data-path="resampling.html"><a href="resampling.html#monte-carlo-cross-validation"><i class="fa fa-check"></i><b>11.2</b> Monte Carlo Cross Validation</a></li>
<li class="chapter" data-level="11.3" data-path="resampling.html"><a href="resampling.html#bootstrap-method"><i class="fa fa-check"></i><b>11.3</b> Bootstrap Method</a></li>
<li class="chapter" data-level="11.4" data-path="resampling.html"><a href="resampling.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>11.4</b> <span class="math inline">\(K\)</span>-Fold Cross-Validation</a><ul>
<li class="chapter" data-level="11.4.1" data-path="resampling.html"><a href="resampling.html#leave-one-out-cross-validation-loocv"><i class="fa fa-check"></i><b>11.4.1</b> Leave-One-Out Cross Validation (LOOCV)</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>VI Regularization</b></span></li>
<li class="chapter" data-level="12" data-path="regular.html"><a href="regular.html"><i class="fa fa-check"></i><b>12</b> Regularization Techniques</a><ul>
<li class="chapter" data-level="12.1" data-path="regular.html"><a href="regular.html#multicollinearity-issues"><i class="fa fa-check"></i><b>12.1</b> Multicollinearity Issues</a><ul>
<li class="chapter" data-level="12.1.1" data-path="regular.html"><a href="regular.html#toy-example"><i class="fa fa-check"></i><b>12.1.1</b> Toy Example</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="regular.html"><a href="regular.html#irregular-coefficients"><i class="fa fa-check"></i><b>12.2</b> Irregular Coefficients</a></li>
<li class="chapter" data-level="12.3" data-path="regular.html"><a href="regular.html#connection-to-regularization"><i class="fa fa-check"></i><b>12.3</b> Connection to Regularization</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="pcr.html"><a href="pcr.html"><i class="fa fa-check"></i><b>13</b> Principal Components Regression</a><ul>
<li class="chapter" data-level="13.1" data-path="pcr.html"><a href="pcr.html#motivation-example-1"><i class="fa fa-check"></i><b>13.1</b> Motivation Example</a></li>
<li class="chapter" data-level="13.2" data-path="pcr.html"><a href="pcr.html#the-pcr-model"><i class="fa fa-check"></i><b>13.2</b> The PCR Model</a></li>
<li class="chapter" data-level="13.3" data-path="pcr.html"><a href="pcr.html#how-does-pcr-work"><i class="fa fa-check"></i><b>13.3</b> How does PCR work?</a><ul>
<li class="chapter" data-level="13.3.1" data-path="pcr.html"><a href="pcr.html#transition-formula"><i class="fa fa-check"></i><b>13.3.1</b> Transition Formula</a></li>
<li class="chapter" data-level="13.3.2" data-path="pcr.html"><a href="pcr.html#size-of-coefficients"><i class="fa fa-check"></i><b>13.3.2</b> Size of Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="pcr.html"><a href="pcr.html#selecting-number-of-pcs"><i class="fa fa-check"></i><b>13.4</b> Selecting Number of PCs</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="pls.html"><a href="pls.html"><i class="fa fa-check"></i><b>14</b> Partial Least Squares Regression</a><ul>
<li class="chapter" data-level="14.1" data-path="pls.html"><a href="pls.html#motivation-example-2"><i class="fa fa-check"></i><b>14.1</b> Motivation Example</a></li>
<li class="chapter" data-level="14.2" data-path="pls.html"><a href="pls.html#the-plsr-model"><i class="fa fa-check"></i><b>14.2</b> The PLSR Model</a></li>
<li class="chapter" data-level="14.3" data-path="pls.html"><a href="pls.html#how-does-plsr-work"><i class="fa fa-check"></i><b>14.3</b> How does PLSR work?</a></li>
<li class="chapter" data-level="14.4" data-path="pls.html"><a href="pls.html#plsr-algorithm"><i class="fa fa-check"></i><b>14.4</b> PLSR Algorithm</a><ul>
<li class="chapter" data-level="14.4.1" data-path="pls.html"><a href="pls.html#pls-solution-with-original-variables"><i class="fa fa-check"></i><b>14.4.1</b> PLS Solution with original variables</a></li>
<li class="chapter" data-level="14.4.2" data-path="pls.html"><a href="pls.html#size-of-coefficients-1"><i class="fa fa-check"></i><b>14.4.2</b> Size of Coefficients</a></li>
<li class="chapter" data-level="14.4.3" data-path="pls.html"><a href="pls.html#some-properties"><i class="fa fa-check"></i><b>14.4.3</b> Some Properties</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="pls.html"><a href="pls.html#selecting-number-of-pls-components"><i class="fa fa-check"></i><b>14.5</b> Selecting Number of PLS Components</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="ridge.html"><a href="ridge.html"><i class="fa fa-check"></i><b>15</b> Ridge Regression</a><ul>
<li class="chapter" data-level="15.1" data-path="ridge.html"><a href="ridge.html#a-new-minimization-problem"><i class="fa fa-check"></i><b>15.1</b> A New Minimization Problem</a><ul>
<li class="chapter" data-level="15.1.1" data-path="ridge.html"><a href="ridge.html#constraining-regression-coefficients"><i class="fa fa-check"></i><b>15.1.1</b> Constraining Regression Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="ridge.html"><a href="ridge.html#a-new-minimization-solution"><i class="fa fa-check"></i><b>15.2</b> A New Minimization Solution</a></li>
<li class="chapter" data-level="15.3" data-path="ridge.html"><a href="ridge.html#what-does-rr-accomplish"><i class="fa fa-check"></i><b>15.3</b> What does RR accomplish?</a><ul>
<li class="chapter" data-level="15.3.1" data-path="ridge.html"><a href="ridge.html#how-to-find-lambda"><i class="fa fa-check"></i><b>15.3.1</b> How to find <span class="math inline">\(\lambda\)</span>?</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>VII Extending Linear Regression</b></span></li>
<li class="chapter" data-level="16" data-path="linear-extensions.html"><a href="linear-extensions.html"><i class="fa fa-check"></i><b>16</b> Beyond Linear Regression</a><ul>
<li class="chapter" data-level="16.1" data-path="linear-extensions.html"><a href="linear-extensions.html#introduction-2"><i class="fa fa-check"></i><b>16.1</b> Introduction</a></li>
<li class="chapter" data-level="16.2" data-path="linear-extensions.html"><a href="linear-extensions.html#expanding-the-regression-horizon"><i class="fa fa-check"></i><b>16.2</b> Expanding the Regression Horizon</a><ul>
<li class="chapter" data-level="16.2.1" data-path="linear-extensions.html"><a href="linear-extensions.html#linearity"><i class="fa fa-check"></i><b>16.2.1</b> Linearity</a></li>
<li class="chapter" data-level="16.2.2" data-path="linear-extensions.html"><a href="linear-extensions.html#parametric-and-nonparametric"><i class="fa fa-check"></i><b>16.2.2</b> Parametric and Nonparametric</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="linear-extensions.html"><a href="linear-extensions.html#transforming-features"><i class="fa fa-check"></i><b>16.3</b> Transforming Features</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="nonparametric.html"><a href="nonparametric.html"><i class="fa fa-check"></i><b>17</b> Nonparametric Regression</a><ul>
<li class="chapter" data-level="17.1" data-path="nonparametric.html"><a href="nonparametric.html#conditional-averages"><i class="fa fa-check"></i><b>17.1</b> Conditional Averages</a></li>
<li class="chapter" data-level="17.2" data-path="nonparametric.html"><a href="nonparametric.html#looking-at-the-neighbors"><i class="fa fa-check"></i><b>17.2</b> Looking at the Neighbors</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="knn.html"><a href="knn.html"><i class="fa fa-check"></i><b>18</b> Nearest Neighbor Estimates</a><ul>
<li class="chapter" data-level="18.1" data-path="knn.html"><a href="knn.html#introduction-3"><i class="fa fa-check"></i><b>18.1</b> Introduction</a></li>
<li class="chapter" data-level="18.2" data-path="knn.html"><a href="knn.html#k-nearest-neighbors-knn"><i class="fa fa-check"></i><b>18.2</b> <span class="math inline">\(k\)</span> Nearest Neighbors (KNN)</a><ul>
<li class="chapter" data-level="18.2.1" data-path="knn.html"><a href="knn.html#distance-measures"><i class="fa fa-check"></i><b>18.2.1</b> Distance Measures</a></li>
<li class="chapter" data-level="18.2.2" data-path="knn.html"><a href="knn.html#knn-estimator"><i class="fa fa-check"></i><b>18.2.2</b> KNN Estimator</a></li>
<li class="chapter" data-level="18.2.3" data-path="knn.html"><a href="knn.html#how-to-find-k"><i class="fa fa-check"></i><b>18.2.3</b> How to find <span class="math inline">\(k\)</span>?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html"><i class="fa fa-check"></i><b>19</b> Kernel Smoothers</a><ul>
<li class="chapter" data-level="19.1" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#introduction-4"><i class="fa fa-check"></i><b>19.1</b> Introduction</a></li>
<li class="chapter" data-level="19.2" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#kernel-smoothers-1"><i class="fa fa-check"></i><b>19.2</b> Kernel Smoothers</a><ul>
<li class="chapter" data-level="19.2.1" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#kernel-functions"><i class="fa fa-check"></i><b>19.2.1</b> Kernel Functions</a></li>
<li class="chapter" data-level="19.2.2" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#weights-from-kernels"><i class="fa fa-check"></i><b>19.2.2</b> Weights from Kernels</a></li>
<li class="chapter" data-level="19.2.3" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#kernel-estimator"><i class="fa fa-check"></i><b>19.2.3</b> Kernel Estimator</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="kernel-smoothers.html"><a href="kernel-smoothers.html#local-polynomial-estimators"><i class="fa fa-check"></i><b>19.3</b> Local Polynomial Estimators</a></li>
</ul></li>
<li class="part"><span><b>VIII Classification</b></span></li>
<li class="chapter" data-level="20" data-path="classif.html"><a href="classif.html"><i class="fa fa-check"></i><b>20</b> Classification Methods</a></li>
<li class="chapter" data-level="21" data-path="logistic.html"><a href="logistic.html"><i class="fa fa-check"></i><b>21</b> Logistic Regression</a><ul>
<li class="chapter" data-level="21.1" data-path="logistic.html"><a href="logistic.html#motivation-1"><i class="fa fa-check"></i><b>21.1</b> Motivation</a><ul>
<li class="chapter" data-level="21.1.1" data-path="logistic.html"><a href="logistic.html#first-approach-fitting-a-line"><i class="fa fa-check"></i><b>21.1.1</b> First Approach: Fitting a Line</a></li>
<li class="chapter" data-level="21.1.2" data-path="logistic.html"><a href="logistic.html#secodn-approach-harsh-thresholding"><i class="fa fa-check"></i><b>21.1.2</b> Secodn Approach: Harsh Thresholding</a></li>
<li class="chapter" data-level="21.1.3" data-path="logistic.html"><a href="logistic.html#third-approach-conditional-means"><i class="fa fa-check"></i><b>21.1.3</b> Third Approach: Conditional Means</a></li>
</ul></li>
<li class="chapter" data-level="21.2" data-path="logistic.html"><a href="logistic.html#logistic-regression-model"><i class="fa fa-check"></i><b>21.2</b> Logistic Regression Model</a><ul>
<li class="chapter" data-level="21.2.1" data-path="logistic.html"><a href="logistic.html#the-criterion-being-optimized"><i class="fa fa-check"></i><b>21.2.1</b> The Criterion Being Optimized</a></li>
<li class="chapter" data-level="21.2.2" data-path="logistic.html"><a href="logistic.html#another-way-to-solve-logistic-regression"><i class="fa fa-check"></i><b>21.2.2</b> Another Way to Solve Logistic Regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="22" data-path="discrim.html"><a href="discrim.html"><i class="fa fa-check"></i><b>22</b> Preamble for Discriminant Analysis</a><ul>
<li class="chapter" data-level="22.1" data-path="discrim.html"><a href="discrim.html#motivation-2"><i class="fa fa-check"></i><b>22.1</b> Motivation</a><ul>
<li class="chapter" data-level="22.1.1" data-path="discrim.html"><a href="discrim.html#distinguishing-species"><i class="fa fa-check"></i><b>22.1.1</b> Distinguishing Species</a></li>
</ul></li>
<li class="chapter" data-level="22.2" data-path="discrim.html"><a href="discrim.html#derived-ratios-from-sum-of-squares"><i class="fa fa-check"></i><b>22.2</b> Derived Ratios from Sum-of-Squares</a><ul>
<li class="chapter" data-level="22.2.1" data-path="discrim.html"><a href="discrim.html#correlation-ratio"><i class="fa fa-check"></i><b>22.2.1</b> Correlation Ratio</a></li>
<li class="chapter" data-level="22.2.2" data-path="discrim.html"><a href="discrim.html#f-ratio"><i class="fa fa-check"></i><b>22.2.2</b> F-Ratio</a></li>
<li class="chapter" data-level="22.2.3" data-path="discrim.html"><a href="discrim.html#example-with-iris-data"><i class="fa fa-check"></i><b>22.2.3</b> Example with Iris data</a></li>
</ul></li>
<li class="chapter" data-level="22.3" data-path="discrim.html"><a href="discrim.html#geometric-perspective"><i class="fa fa-check"></i><b>22.3</b> Geometric Perspective</a><ul>
<li class="chapter" data-level="22.3.1" data-path="discrim.html"><a href="discrim.html#clouds-from-class-structure"><i class="fa fa-check"></i><b>22.3.1</b> Clouds from Class Structure</a></li>
<li class="chapter" data-level="22.3.2" data-path="discrim.html"><a href="discrim.html#dispersion-decomposition"><i class="fa fa-check"></i><b>22.3.2</b> Dispersion Decomposition</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="23" data-path="cda.html"><a href="cda.html"><i class="fa fa-check"></i><b>23</b> Canonical Discriminant Analysis</a><ul>
<li class="chapter" data-level="23.0.1" data-path="cda.html"><a href="cda.html#looking-for-a-discriminant-axis"><i class="fa fa-check"></i><b>23.0.1</b> Looking for a discriminant axis</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="discanalysis.html"><a href="discanalysis.html"><i class="fa fa-check"></i><b>24</b> Discriminant Analysis</a><ul>
<li class="chapter" data-level="24.1" data-path="discanalysis.html"><a href="discanalysis.html#probabilistic-da"><i class="fa fa-check"></i><b>24.1</b> Probabilistic DA</a></li>
<li class="chapter" data-level="24.2" data-path="discanalysis.html"><a href="discanalysis.html#estimations"><i class="fa fa-check"></i><b>24.2</b> Estimations</a><ul>
<li class="chapter" data-level="24.2.1" data-path="discanalysis.html"><a href="discanalysis.html#univariate-data-boldsymbolp-1"><i class="fa fa-check"></i><b>24.2.1</b> Univariate Data (<span class="math inline">\(\boldsymbol{p = 1}\)</span>)</a></li>
<li class="chapter" data-level="24.2.2" data-path="discanalysis.html"><a href="discanalysis.html#multivariate-data-boldsymbolp-1"><i class="fa fa-check"></i><b>24.2.2</b> Multivariate Data (<span class="math inline">\(\boldsymbol{p &gt; 1}\)</span>)</a></li>
<li class="chapter" data-level="24.2.3" data-path="discanalysis.html"><a href="discanalysis.html#multivariate-estimations"><i class="fa fa-check"></i><b>24.2.3</b> Multivariate Estimations</a></li>
</ul></li>
<li class="chapter" data-level="24.3" data-path="discanalysis.html"><a href="discanalysis.html#discriminant-functions"><i class="fa fa-check"></i><b>24.3</b> Discriminant Functions</a></li>
<li class="chapter" data-level="24.4" data-path="discanalysis.html"><a href="discanalysis.html#quadratic-discriminant-analysis-qda"><i class="fa fa-check"></i><b>24.4</b> Quadratic Discriminant Analysis (QDA)</a></li>
<li class="chapter" data-level="24.5" data-path="discanalysis.html"><a href="discanalysis.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>24.5</b> Linear Discriminant Analysis</a></li>
<li class="chapter" data-level="24.6" data-path="discanalysis.html"><a href="discanalysis.html#canonical-discriminant-analysis"><i class="fa fa-check"></i><b>24.6</b> Canonical Discriminant Analysis</a></li>
<li class="chapter" data-level="24.7" data-path="discanalysis.html"><a href="discanalysis.html#naive-bayes"><i class="fa fa-check"></i><b>24.7</b> Naive Bayes</a></li>
<li class="chapter" data-level="24.8" data-path="discanalysis.html"><a href="discanalysis.html#fifth-case"><i class="fa fa-check"></i><b>24.8</b> Fifth Case</a></li>
<li class="chapter" data-level="24.9" data-path="discanalysis.html"><a href="discanalysis.html#comparing-the-cases"><i class="fa fa-check"></i><b>24.9</b> Comparing the Cases</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="classperformance.html"><a href="classperformance.html"><i class="fa fa-check"></i><b>25</b> Performance of Classifiers</a><ul>
<li class="chapter" data-level="25.1" data-path="classperformance.html"><a href="classperformance.html#error-for-binary-response"><i class="fa fa-check"></i><b>25.1</b> Error for Binary Response</a></li>
<li class="chapter" data-level="25.2" data-path="classperformance.html"><a href="classperformance.html#categorical-response"><i class="fa fa-check"></i><b>25.2</b> Categorical Response</a></li>
<li class="chapter" data-level="25.3" data-path="classperformance.html"><a href="classperformance.html#confusion-matrices"><i class="fa fa-check"></i><b>25.3</b> Confusion Matrices</a></li>
<li class="chapter" data-level="25.4" data-path="classperformance.html"><a href="classperformance.html#binary-response-example"><i class="fa fa-check"></i><b>25.4</b> Binary Response: Example</a><ul>
<li class="chapter" data-level="25.4.1" data-path="classperformance.html"><a href="classperformance.html#types-of-errors-application-for-savings-account"><i class="fa fa-check"></i><b>25.4.1</b> Types of Errors: Application for Savings Account</a></li>
<li class="chapter" data-level="25.4.2" data-path="classperformance.html"><a href="classperformance.html#another-example-loans"><i class="fa fa-check"></i><b>25.4.2</b> Another Example: Loans</a></li>
<li class="chapter" data-level="25.4.3" data-path="classperformance.html"><a href="classperformance.html#the-ideal-situation"><i class="fa fa-check"></i><b>25.4.3</b> The Ideal Situation</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IX Unsupervised II: Clustering</b></span></li>
<li class="chapter" data-level="26" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>26</b> Clustering</a><ul>
<li class="chapter" data-level="26.1" data-path="clustering.html"><a href="clustering.html#about-clustering"><i class="fa fa-check"></i><b>26.1</b> About Clustering</a><ul>
<li class="chapter" data-level="26.1.1" data-path="clustering.html"><a href="clustering.html#types-of-clustering"><i class="fa fa-check"></i><b>26.1.1</b> Types of Clustering</a></li>
<li class="chapter" data-level="26.1.2" data-path="clustering.html"><a href="clustering.html#hard-clustering"><i class="fa fa-check"></i><b>26.1.2</b> Hard Clustering</a></li>
</ul></li>
<li class="chapter" data-level="26.2" data-path="clustering.html"><a href="clustering.html#dispersion-measures"><i class="fa fa-check"></i><b>26.2</b> Dispersion Measures</a></li>
<li class="chapter" data-level="26.3" data-path="clustering.html"><a href="clustering.html#complexity-in-clustering"><i class="fa fa-check"></i><b>26.3</b> Complexity in Clustering</a></li>
</ul></li>
<li class="chapter" data-level="27" data-path="kmeans.html"><a href="kmeans.html"><i class="fa fa-check"></i><b>27</b> K-Means</a><ul>
<li class="chapter" data-level="27.1" data-path="kmeans.html"><a href="kmeans.html#toy-example-1"><i class="fa fa-check"></i><b>27.1</b> Toy Example</a></li>
<li class="chapter" data-level="27.2" data-path="kmeans.html"><a href="kmeans.html#what-does-k-means-do"><i class="fa fa-check"></i><b>27.2</b> What does K-means do?</a></li>
<li class="chapter" data-level="27.3" data-path="kmeans.html"><a href="kmeans.html#k-means-algorithms"><i class="fa fa-check"></i><b>27.3</b> K-Means Algorithms</a><ul>
<li class="chapter" data-level="27.3.1" data-path="kmeans.html"><a href="kmeans.html#classic-version"><i class="fa fa-check"></i><b>27.3.1</b> Classic Version</a></li>
<li class="chapter" data-level="27.3.2" data-path="kmeans.html"><a href="kmeans.html#moving-centers-algorithm"><i class="fa fa-check"></i><b>27.3.2</b> Moving Centers Algorithm</a></li>
<li class="chapter" data-level="27.3.3" data-path="kmeans.html"><a href="kmeans.html#dynamic-clouds"><i class="fa fa-check"></i><b>27.3.3</b> Dynamic Clouds</a></li>
<li class="chapter" data-level="27.3.4" data-path="kmeans.html"><a href="kmeans.html#choosing-k"><i class="fa fa-check"></i><b>27.3.4</b> Choosing <span class="math inline">\(k\)</span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="28" data-path="hclus.html"><a href="hclus.html"><i class="fa fa-check"></i><b>28</b> Hierarchical Clustering</a><ul>
<li class="chapter" data-level="28.1" data-path="hclus.html"><a href="hclus.html#agglomerative-methods"><i class="fa fa-check"></i><b>28.1</b> Agglomerative Methods</a></li>
<li class="chapter" data-level="28.2" data-path="hclus.html"><a href="hclus.html#example-single-linkage"><i class="fa fa-check"></i><b>28.2</b> Example: Single Linkage</a><ul>
<li class="chapter" data-level="28.2.1" data-path="hclus.html"><a href="hclus.html#dendrogram"><i class="fa fa-check"></i><b>28.2.1</b> Dendrogram</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>X Tree-based Methods</b></span></li>
<li class="chapter" data-level="29" data-path="trees.html"><a href="trees.html"><i class="fa fa-check"></i><b>29</b> Intro to Decision Trees</a><ul>
<li class="chapter" data-level="29.1" data-path="trees.html"><a href="trees.html#introduction-5"><i class="fa fa-check"></i><b>29.1</b> Introduction</a></li>
<li class="chapter" data-level="29.2" data-path="trees.html"><a href="trees.html#binary-trees"><i class="fa fa-check"></i><b>29.2</b> Binary Trees</a><ul>
<li class="chapter" data-level="29.2.1" data-path="trees.html"><a href="trees.html#the-process-of-building-a-tree"><i class="fa fa-check"></i><b>29.2.1</b> The Process of Building a Tree</a></li>
<li class="chapter" data-level="29.2.2" data-path="trees.html"><a href="trees.html#binary-partitions"><i class="fa fa-check"></i><b>29.2.2</b> Binary Partitions</a></li>
</ul></li>
<li class="chapter" data-level="29.3" data-path="trees.html"><a href="trees.html#measures-of-heterogeneity"><i class="fa fa-check"></i><b>29.3</b> Measures of Heterogeneity</a><ul>
<li class="chapter" data-level="29.3.1" data-path="trees.html"><a href="trees.html#entropy"><i class="fa fa-check"></i><b>29.3.1</b> Entropy</a></li>
<li class="chapter" data-level="29.3.2" data-path="trees.html"><a href="trees.html#the-math-behind-entropy"><i class="fa fa-check"></i><b>29.3.2</b> The Math Behind Entropy</a></li>
<li class="chapter" data-level="29.3.3" data-path="trees.html"><a href="trees.html#gini-impurity"><i class="fa fa-check"></i><b>29.3.3</b> Gini Impurity</a></li>
<li class="chapter" data-level="29.3.4" data-path="trees.html"><a href="trees.html#toy-example-2"><i class="fa fa-check"></i><b>29.3.4</b> Toy Example</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Made with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">All Models Are Wrong</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="phases" class="section level1">
<h1><span class="header-section-number">10</span> Learning Phases</h1>
<p>In this chapter we further discuss more theoretical elements of the
supervised learning framework. In particular, we take a deep dive into
some of the activities to be performed in every learning process, namely,
model training, model selection, and model assessment.</p>
<p>A word of caution needs to be said about the terminology that we use here.
If you look at other books or resources about statistical/machine learning,
you will find that there is no consistent use of words such as training,
testing, validation, evaluation, assessment, and other similar terms.
We have decided to use specific words—for certain concepts—that other
authors or practitioners may handle differently.</p>
<p>Also, bear in mind that many of the notions and ideas described in this chapter
tend to have a decisive theoretical flavor. As we move on, we will provide
more details and explanations on how to make such ideas more concrete, and
how to execute them in practice.</p>
<div id="introduction-1" class="section level2">
<h2><span class="header-section-number">10.1</span> Introduction</h2>
<p>Let’s bring back a simplified version of the supervised learning diagram
depicted in the figure below. We know that there are more elements in the
full-fledged diagram, but we want to focus on three main stages that are
present in all supervised learning contexts.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-89"></span>
<img src="images/validation/learning-diag-simplified1.svg" alt="Simplified Supervised Learning Diagram" width="85%" />
<p class="caption">
Figure 10.1: Simplified Supervised Learning Diagram
</p>
</div>
<p>In every learning situation, there is a handful of tasks we need to carry out.
First, we need to fit several models, typically involving working with different
classes of hypothesis. For example, we can consider four types of regression
methods: principal components regression (<span class="math inline">\(\mathcal{H_1}\)</span>), partial least
squares regression (<span class="math inline">\(\mathcal{H_2}\)</span>), ridge regression (<span class="math inline">\(\mathcal{H_3}\)</span>),
and lasso (<span class="math inline">\(\mathcal{H_4}\)</span>). All of these types of regression models have
tuning parameters that cannot be derived analytically from the data, but have
to be determined through trial-error steps.</p>
<p>For each hypothesis family, we need to find the optimal tuning parameter,
which involves choosing a finalist model: the best principal components
regression, the best partial least regression, etc. Then, we need to select the
best model among the finalist models. This will be <em>the</em> final model to be
delivered. And finally, we need to measure the predicting performance of the
final model: measure how the model will behave with out-of-sample points.</p>
<p>We can formalize these tasks into three major phases encountered in every
supervised learning system: 1) model training, 2) model selection, and
3) model assessment.</p>
<ul>
<li><p><strong>Training</strong>: Given some data, and a certain modeling hypothesis
<span class="math inline">\(\mathcal{H}_m\)</span>, how can we fit/estimate a model <span class="math inline">\(h_m\)</span>? Often, we are also
interested in telling something about its predicting performance (in a limited sense).</p></li>
<li><p><strong>Selection</strong>: Choosing a model from a set of candidate models. Typically,
this has to do with two types of selection:</p>
<ul>
<li><p><strong>Pre-Selection</strong>: choosing a finalist model from a set of models belonging
to a certain class of hypothesis.</p></li>
<li><p><strong>Final-Selection</strong>: choosing a final model among a set of finalist models;
that is, choosing <em>the</em> very final model.</p></li>
</ul></li>
<li><p><strong>Assessment</strong>: Given a final <span class="math inline">\(\widehat{f}\)</span>, how can we
provide an estimate <span class="math inline">\(\widehat{E}_{out}(\hat{f})\)</span> of the out-of-sample error?
In other words, how to measure the performance of the final model in order to
provide an estimation of its predictive behavior out-of-sample.</p></li>
</ul>
<p>To expand our mental map, let’s place the learning phases in the supervised
learning diagram (see image below). As you can tell, now we
are taking a peek inside the learning algorithm section. This is where the
model training, the pre-selection, and the final-selection tasks occur.
In turn, the model assessment part has to do with estimating the out-of-sample
performance of the final model.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-90"></span>
<img src="images/validation/learning-diag-simplified2.svg" alt="Schematic of Learning Phases" width="90%" />
<p class="caption">
Figure 10.2: Schematic of Learning Phases
</p>
</div>
<p>Having identified the main learning tasks, and knowing how they show up inside
the supervised larning diagram, the next thing to consider is:
<strong>What data should we use to perform each task?</strong>
We will answer this question in the following sections, starting first with the
model assessment phase, then the selection of the final model, then the
pre-selection of finalist, and finally the training of candidate models.</p>
</div>
<div id="model-assessment" class="section level2">
<h2><span class="header-section-number">10.2</span> Model Assessment</h2>
<p>Let us consider a situation in which we have a final model <span class="math inline">\(h\)</span>.
The next logical step should involve measuring its prediction quality. In other
words, we want to see how good (or how bad) the predictions of <span class="math inline">\(h\)</span> are.
The general idea is fairly simple—at least conceptually. Given an input data <span class="math inline">\(\mathbf{x_i}\)</span>, we need to assess the discrepancy between the predicted value
<span class="math inline">\(\hat{y}_i = h(\mathbf{x_i})\)</span> and the observed value <span class="math inline">\(y_i\)</span>.</p>
<p>Simple, right? … Well, not really.</p>
<p>As you may recall from the chapter about the
<a href="learning.html#learning">theoretical framework of supervised learning</a>, there are two major
types of <em>predictions</em>. On one hand, we have predictions
<span class="math inline">\(\hat{y}_i = h(\mathbf{x_i})\)</span> of in-sample points
<span class="math inline">\((\mathbf{x_i}, y_i) \in \mathcal{D}_{in}\)</span>.
In-sample points are those data points that we used to fit a given model.
On the other hand, we have predictions <span class="math inline">\(\hat{y}_0 = h(\mathbf{x_0})\)</span> of
out-of-sample points <span class="math inline">\((\mathbf{x_0}, y_0) \in \mathcal{D}_{out}\)</span>.
Out-of-sample points are data points not used to fit a model.</p>
<p>Because of this distinction between in-sample data <span class="math inline">\(\mathcal{D}_{in}\)</span>, and
out-of-sample data <span class="math inline">\(\mathcal{D}_{out}\)</span>, we can
measure the predictive quality of a model from these two perspectives. This
obviously implies having predictions, and errors, of two different natures.
Aggregating all the discrepancies between the predicted in-sample points
<span class="math inline">\(\hat{y}_i = h(\mathbf{x_i})\)</span> and their observed values <span class="math inline">\(y_i\)</span>, allows us
to quantify the in-sample error <span class="math inline">\(E_{in}\)</span>, which inform us
about the <strong>resubstitution power</strong> of the model <span class="math inline">\(h\)</span> (i.e. how well the model
fits the learning data).</p>
<p>The more interesting part comes with the out-of-sample points. If we had
access to all points in <span class="math inline">\(\mathcal{D}_{out}\)</span>, measuring the
discrepancies between the predicted out-of-sample points
<span class="math inline">\(\hat{y}_0 = h(\mathbf{x_0})\)</span> and their observed values <span class="math inline">\(y_0\)</span>, would allow us
to quantify the out-of-sample error <span class="math inline">\(E_{out}\)</span>, truly measuring the
<strong>generalization power</strong> of the model <span class="math inline">\(h\)</span>. In practice, unfortunately, we won’t
have access to the entire out-of-sample data <span class="math inline">\(\mathcal{D}_{out}\)</span>.</p>
<p>Knowing that the whole out-of-sample data set is not within our reach, the
second best thing that we can do is to find a proxy set for <span class="math inline">\(\mathcal{D}_{out}\)</span>.
If we can obtain a data set <span class="math inline">\(\mathcal{D}_{proxy}\)</span> that is a representative
subset of <span class="math inline">\(\mathcal{D}_{out}\)</span>, then we can compute <span class="math inline">\(E_{proxy}\)</span> and use it to
approximate <span class="math inline">\(E_{out}\)</span>, thus having a fair estimation of the generalization power
of <span class="math inline">\(h\)</span>. This is precisely the idea of the so-called term <strong>Model Assessment</strong>:
How can we estimate <span class="math inline">\(E_{out}(\widehat{f})\)</span> of a final model? Which basically
reduces to: how can we find an unbiased sample <span class="math inline">\(\mathcal{D}_{proxy} \subset \mathcal{D}_{out}\)</span> in order to get <span class="math inline">\(\hat{E}_{out}\)</span>?</p>
<div id="holdout-test-set" class="section level3">
<h3><span class="header-section-number">10.2.1</span> Holdout Test Set</h3>
<p>In order to answer the question “How can we estimate <span class="math inline">\(E_{out}\)</span>?”, let us
discuss the theoretical rationale behind the so-called <strong>Holdout Method</strong>.</p>
<p>In practice, we always have some available data
<span class="math inline">\(\mathcal{D} = (\mathbf{x_1}, y_1), \dots, (\mathbf{x_n}, y_n)\)</span>. If we used
all the <span class="math inline">\(n\)</span> data points to train/fit a model, we could perfectly measure <span class="math inline">\(E_{in}\)</span>
but then we wouldn’t have any out-of-sample points to get an honest
approximation <span class="math inline">\(\widehat{E}_{out}\)</span> of <span class="math inline">\(E_{out}\)</span>. Perhaps you are thinking:
“Why not use <span class="math inline">\(E_{in}\)</span> as an estimate <span class="math inline">\(\widehat{E}_{out}\)</span> of <span class="math inline">\(E_{out}\)</span>?” Because it
won’t be a realiable estimate. We may have a model <span class="math inline">\(h\)</span> that produces a very
small in-sample error, <span class="math inline">\(E_{in} \approx 0\)</span>, but that doesn’t necessarily mean
that it has good generalization performance out-of-sample.</p>
<p>Given that the only available data set is <span class="math inline">\(\mathcal{D}\)</span>, the Holdout Method
proposes to split <span class="math inline">\(\mathcal{D}\)</span> into two subsets: (1) a subset
<span class="math inline">\(\mathcal{D}_{train}\)</span>, called the <strong>training set</strong>, used to train/fit the model,
and (2) another subset <span class="math inline">\(\mathcal{D}_{test}\)</span>, called the <strong>test set</strong>, to be used
as a proxy of <span class="math inline">\(\mathcal{D}_{out}\)</span> for testing/assessment purposes.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-91"></span>
<img src="images/validation/training-test-sets.svg" alt="Data split into training and test sets" width="60%" />
<p class="caption">
Figure 10.3: Data split into training and test sets
</p>
</div>
<p>How do we actually obtain these two sets? Usually by taking random
samples of size <span class="math inline">\(a\)</span>, without replacement, from <span class="math inline">\(\mathcal{D}\)</span>
(although there are exceptions).</p>
<p><span class="math display">\[
\mathcal{D} \to \begin{cases} 
\text{training } \mathcal{D}_{train} &amp; \to \text{size } n - a \\ \text{test } \mathcal{D}_{test} &amp;  \to \text{size } a \\ 
\end{cases}
\]</span></p>
<p>We then fit a particular model using <span class="math inline">\(\mathcal{D}_{train}\)</span>; obtaining a model
that we’ll call <span class="math inline">\(h^{-}(x)\)</span>, “<span class="math inline">\(h\)</span>-minus”, because it is a model fitted with the
training set <span class="math inline">\(\mathcal{D}_{train}\)</span>, which is a subset of the available data
<span class="math inline">\(\mathcal{D}\)</span>. With the remainder points in <span class="math inline">\(\mathcal{D}_{test}\)</span>, we can measure
the performance of the model <span class="math inline">\(h^{-}(x)\)</span> as:</p>
<p><span class="math display">\[
E_{test}(h^{-}) = \frac{1}{a} \sum_{\ell=1}^{a} err\left( h^{-}(\mathbf{x_\ell}) , y_\ell \right) ; \hspace{5mm} (\mathbf{x_\ell}, y_\ell) \in \mathcal{D}_{test}
\]</span></p>
<p>As long as <span class="math inline">\(\mathcal{D}_{test}\)</span> is a representative sample of <span class="math inline">\(\mathcal{D}_{out}\)</span>, <span class="math inline">\(E_{test}\)</span> should give a good estimate of <span class="math inline">\(E_{out}\)</span>. Let’s see why?</p>
</div>
<div id="why-does-a-test-set-work" class="section level3">
<h3><span class="header-section-number">10.2.2</span> Why does a test set work?</h3>
<p>Consider an out-of-sample point <span class="math inline">\((\mathbf{x_0}, y_0)\)</span> that is part of the test
set: <span class="math inline">\((\mathbf{x_0}, y_0) \in \mathcal{D}_{test}\)</span>. Given a pointwise error
function <span class="math inline">\(err()\)</span>, we can measure the error: <span class="math inline">\(err(h(\mathbf{x_0}), y_0)\)</span>.
Moreover, we can treat it as a point estimate of <span class="math inline">\(E_{out}(h)\)</span>.</p>
<p>Here’s a relevant question: Is <span class="math inline">\(err(h(\mathbf{x_0}), y_0)\)</span> an unbiased
estimate of <span class="math inline">\(E_{out}(h)\)</span>?</p>
<p>To see whether the point estimate <span class="math inline">\(err(h(\mathbf{x_0}), y_0)\)</span> is an unbiased
estimate of <span class="math inline">\(E_{out}(h)\)</span>, let’s determine its expectation over the input space:</p>
<p><span class="math display">\[
\mathbb{E}_{\mathcal{X}} [ err(h(\mathbf{x_0}), y_0) ]
\]</span></p>
<p>Remember what the above expression represent? Yes, it is precisely the
out-of-sample error <span class="math inline">\(E_{out}(h)\)</span>! Therefore, <span class="math inline">\(err(h(\mathbf{x_0}), y_0)\)</span> is
an unbiased point estimate of the out-of-sample error.</p>
<p>What about the variance: <span class="math inline">\(Var[err(h(\mathbf{x_0}), y_0)]\)</span>? For the sake of
simplicity let’s assume that this variance is constant:</p>
<p><span class="math display">\[
Var[err(h(\mathbf{x_0}), y_0)] = s^2
\]</span></p>
<p>Obviously this variance could be large (or small). So having just one test
point <span class="math inline">\((\mathbf{x_0}, y_0)\)</span>, even though is an unbiased estimate of <span class="math inline">\(E_{out}\)</span>,
does not allow us to have an idea of how reliable that estimate is.</p>
<p>Well, let’s consider a set <span class="math inline">\(D_{test} = \{ (\mathbf{x_1}, y_1), \dots, (\mathbf{x_a}, y_a) \}\)</span> containing <span class="math inline">\(a &gt; 1\)</span> points. We can average their pointwise errors to
get <span class="math inline">\(E_{test}\)</span></p>
<p><span class="math display">\[
E_{test}(h) = \frac{1}{a} \sum_{\ell=1}^{a} err[ h^{-}(\mathbf{x_\ell}), y_\ell ]
\]</span></p>
<p>The question becomes: is <span class="math inline">\(E_{test}(h)\)</span> an unbiased estimate of <span class="math inline">\(E_{out}(h)\)</span>?</p>
<p>Let’s find out:</p>
<p><span class="math display">\[\begin{align*}
\mathbb{E}_{\mathcal{X}} [E_{test}(h)] &amp;= \mathbb{E}_{\mathcal{X}} \left [ \frac{1}{a} \sum_{\ell=1}^{a} err[ h^{-}(\mathbf{x_\ell}), y_\ell ] \right ] \\
&amp;= \frac{1}{a} \sum_{k=1}^{a} \mathbb{E}_{\mathcal{X}} \left [ err \left ( h^{-}(\mathbf{x_\ell}), y_\ell \right) \right ] \\
&amp;= \frac{1}{a} \sum_{\ell=1}^{a} E_{out}(h) \\
&amp;= E_{out}(h)
\end{align*}\]</span></p>
<p>Yes, it turns out that <span class="math inline">\(E_{test}(h)\)</span> is an unbiased estimate of <span class="math inline">\(E_{out}(h)\)</span>.
But what about the variance? Let’s assume that the errors across points are
independent (this may not be the case in practice, but we make this assumption
to ease computation): It can be shown that the variance of <span class="math inline">\(E_{test}(h)\)</span>
is given by:</p>
<p><span class="math display">\[
Var[E_{test}(h)] = \frac{1}{a^2} \sum_{\ell=1}^{a} Var[ err(h(\mathbf{x_0}), y_0) ] = \frac{s^2}{a}
\]</span></p>
<p>The above equation tells us that, as we increase the number <span class="math inline">\(a\)</span> of test points,
the variance of the estimator <span class="math inline">\(E_{test}(h)\)</span> will decrease. Simply put, the
more test points we use, the more reliably <span class="math inline">\(E_{test}(h)\)</span> estimates <span class="math inline">\(E_{out}(h)\)</span>.
Of course, <span class="math inline">\(a\)</span> is not freely selectable; the larger <span class="math inline">\(a\)</span> is, the smaller our
training dataset will be.</p>
<p>In any case, the important thing is that reserving some points
<span class="math inline">\(\mathcal{D}_{test}\)</span> from a learning set <span class="math inline">\(\mathcal{D}\)</span> to use them for testing
purposes is a very wise idea. It definitely allows us to have a war for
estimating the performance of a model <span class="math inline">\(h\)</span>, when applied to out-of-sample points.</p>
<p>Again, the holdout method is simply a conceptual starting point. Also, depending
on how you form your training and test sests, you may end up with a split that
may not be truly representative of the studied phenomenon. So instead of using
just one split, some authors propose to use several splits of training-test sets
obtained through resampling methods. We talk about this topic with more detail
in the next chapter.</p>
<div id="holdout-algorithm" class="section level4 unnumbered">
<h4>Holdout Algorithm</h4>
<p>Here’s the conceptual algorithm behind the holdout method.</p>
<ol style="list-style-type: decimal">
<li><p>Compile the available data into a set <span class="math inline">\(\mathcal{D} = \{(\mathbf{x_1}, y_1), \dots, (\mathbf{x_n}, y_n) \}\)</span>.</p></li>
<li><p>Choose <span class="math inline">\(a \in \mathbb{Z}^{+}\)</span> elements from <span class="math inline">\(\mathcal{D}\)</span> to comprise a test
set <span class="math inline">\(\mathcal{D}_{test}\)</span>, and place the remaining <span class="math inline">\(n - a\)</span> points into the
training set <span class="math inline">\(\mathcal{D}_{train}\)</span>.</p></li>
<li><p>Use <span class="math inline">\(\mathcal{D}_{train}\)</span> to fit a particular model <span class="math inline">\(h^{-}(x)\)</span>.</p></li>
<li><p>Measure the performance of <span class="math inline">\(h^{-}\)</span> using <span class="math inline">\(\mathcal{D}_{test}\)</span>;
specifically, compute</p></li>
</ol>
<p><span class="math display">\[
E_{test}(h^{-}) = \frac{1}{a} \sum_{\ell=1}^{a} err_\ell[ h^{-}(\mathbf{x_\ell}), y_\ell ]
\]</span></p>
<p>where <span class="math inline">\((\mathbf{x_\ell}, y_\ell) \in \mathcal{D}_{test}\)</span>, and <span class="math inline">\(err_\ell\)</span> is
some measure of pointwise error.</p>
<ol start="5" style="list-style-type: decimal">
<li>Generate the final model <span class="math inline">\(\widehat{h}\)</span> by refitting <span class="math inline">\(h^{-}\)</span> to the entire
dataset <span class="math inline">\(\mathcal{D}\)</span>.</li>
</ol>
<p>There are many different conventions as to how to pick <span class="math inline">\(a\)</span>: one common
rule-of-thumb is to assign 80% of your data to the training set, and the
remaining 20% to the test set.</p>
<p>Also, keep in mind that the model that you ultimately deliver will not be
<span class="math inline">\(h^{-}\)</span>; rather, you need to refit <span class="math inline">\(h^{-}\)</span> using the <em>entire</em> data
<span class="math inline">\(\mathcal{D}\)</span>. This yields the final hypothesis model <span class="math inline">\(\widehat{h}\)</span>.</p>
</div>
</div>
</div>
<div id="model-selection" class="section level2">
<h2><span class="header-section-number">10.3</span> Model Selection</h2>
<p>Now that we’ve talked about how to measure the generalization performance of a
given final model, the next thing to discuss is how to compare different models,
in order to select the best one.</p>
<p>Say we have <span class="math inline">\(M\)</span> models to choose from. Consider, for example, the following
three cases (each with <span class="math inline">\(M = 3\)</span>):</p>
<p>We could have three different types of hypotheses:</p>
<ul>
<li><span class="math inline">\(h_1:\)</span> linear model</li>
<li><span class="math inline">\(h_2:\)</span> neural network</li>
<li><span class="math inline">\(h_3:\)</span> regression tree</li>
</ul>
<p>Or we could also have a particular type of model, e.g. polynomials, with three
different degrees:</p>
<ul>
<li><span class="math inline">\(h_1:\)</span> quadratic model</li>
<li><span class="math inline">\(h_2:\)</span> cubic model</li>
<li><span class="math inline">\(h_3:\)</span> 15th-degree model</li>
</ul>
<p>Or maybe a principal components regression with three options for the number of
components to retain:</p>
<ul>
<li><span class="math inline">\(h_1:\)</span> PCR <span class="math inline">\((c = 1)\)</span></li>
<li><span class="math inline">\(h_2:\)</span> PCR <span class="math inline">\((c = 2)\)</span></li>
<li><span class="math inline">\(h_3:\)</span> PCR <span class="math inline">\((c = 3)\)</span></li>
</ul>
<p>How do we select the best model from a set of candidate models?</p>
<div id="three-way-holdout-method" class="section level3">
<h3><span class="header-section-number">10.3.1</span> Three-way Holdout Method</h3>
<p>We can extend the idea behind the holdout method, to go from one holdout
set to two holdout sets.
Namely, instead of splitting <span class="math inline">\(\mathcal{D}\)</span> into two sets (training and test),
we split it into three sets that we will call: training <span class="math inline">\(\mathcal{D}_{train}\)</span>,
validation <span class="math inline">\(\mathcal{D}_{val}\)</span>, and test <span class="math inline">\(\mathcal{D}_{test}\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-92"></span>
<img src="images/validation/training-validation-test-sets.svg" alt="Data split into training, validation, and test sets" width="60%" />
<p class="caption">
Figure 10.4: Data split into training, validation, and test sets
</p>
</div>
<p>The test set <span class="math inline">\(\mathcal{D}_{test}\)</span> is the set that will be used for assessing
the performance of a final model. This means that once we have created
<span class="math inline">\(\mathcal{D}_{test}\)</span>, the only time we use this set is at end of the
learning process. We only use it to quantify the generalization error of the
final model. And that’s it. We don’t use this set to make any learning
decision.</p>
<p>What about the validation set <span class="math inline">\(\mathcal{D}_{val}\)</span>? What do we use it for?</p>
<p>We recommend using the validation set for selecting the final model from a
set of finalist models. However, keep in mind that other authors may
recommend other uses for <span class="math inline">\(\mathcal{D}_{val}\)</span>.</p>
<p>Here’s our ideal suggestion on how to use the validation set.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-93"></span>
<img src="images/validation/repeated-3way-holdout.svg" alt="Using a validation set for final-selection" width="70%" />
<p class="caption">
Figure 10.5: Using a validation set for final-selection
</p>
</div>
<p><span class="math inline">\(\mathcal{H}_m\)</span> represents the <span class="math inline">\(m\)</span>-th hypothesis, and <span class="math inline">\(h^{-}_m\)</span> represents the
best <span class="math inline">\(m\)</span>-th fit to the <span class="math inline">\(m\)</span>-th hypothesis using <span class="math inline">\(\mathcal{D}_{train}\)</span>. In other
words, <span class="math inline">\(h^{-}_m\)</span> is the finalist model from class <span class="math inline">\(\mathcal{H}_m\)</span>.</p>
<p>After a finalist model <span class="math inline">\(h^{-}_m\)</span> has been pre-selected for each class of
hypothesis, then we use <span class="math inline">\(\mathcal{D}_{val}\)</span> to compute validation errors
<span class="math inline">\(E^{m}_{val}\)</span>. The model with the smallest validation error is then selected
as the final model.</p>
<p>After the best model <span class="math inline">\(h^{-}_{m}\)</span> has been selected (with the smallest <span class="math inline">\(E_{val}\)</span>),
a model <span class="math inline">\(h_{\overset{*}m}\)</span> is fitted using <span class="math inline">\(\mathcal{D}_{train} \cap \mathcal{D}_{val}\)</span>.
The performance of this model is assessed by using <span class="math inline">\(\mathcal{D}_{test}\)</span> to
obtain <span class="math inline">\(E_{test}(h_{m}^{*})\)</span>. Finally, the “official” model is the model fitted
on the entire data set <span class="math inline">\(\mathcal{D}\)</span>, but the reported performance is
<span class="math inline">\(E^{m}_{test}\)</span>.</p>
<p>One important thing to notice is that <span class="math inline">\(E^{m}_{test}\)</span> is an unbiased estimate of
the out-of-sample performance of <span class="math inline">\(h_{\overset{*}m}\)</span>, even though the actual
model <span class="math inline">\(h_m\)</span> that is delivered is the fitted on the entire data.</p>
<p>Why are we calling <span class="math inline">\(\mathcal{D}_{val}\)</span> a “validation” set, when it appears to
be serving the same role as a “test” set? Because choosing the “best” model
(i.e. the model with the smallest <span class="math inline">\(E_{val}\)</span>) is a <strong>learning decision</strong>.
Had we stopped our procedure before making this choice
(i.e. if we had stopped after considering <span class="math inline">\(E_{val}^m\)</span> for <span class="math inline">\(m = 1, 2, ..., M\)</span>),
we could have plausibly called these errors “test errors.” However, we went
one step further and as a result obtained a <em>biased</em> estimate of <span class="math inline">\(E_{out}\)</span>;
namely, <span class="math inline">\(E_{val}^{m}\)</span> for the chosen model <span class="math inline">\(m\)</span>.</p>
<p>Of course, there is still a tradeoff when working with a 3-way split, which
is precisely the fact that we need to split our data into <em>three</em> sets. The
larger our test and validations sets are, the more reliable the estimates of
the out-of-sample performance will be. At the same time, however, the mode
data points for validation and testing, the smaller our training set will be.
Which will very likely produce far from optimal finalist models, as well as
the very final model.</p>
</div>
</div>
<div id="model-training" class="section level2">
<h2><span class="header-section-number">10.4</span> Model Training</h2>
<p>In order to perform the pre-selection of finalist models, the selection of the
final model, and its corresponding assessment, we need to be able to fit
all candidate models belonging to different classes of hypotheses.</p>
<p>The data set that we use to fit/train such models is, surprise-surprise,
the training set <span class="math inline">\(\mathcal{D}_{train}\)</span>. This will also be the set that we’ll
use to pre-select the finalist models from each class.
For example, say the class <span class="math inline">\(\mathcal{H}_{m}\)</span> is principal components
regression (pcr), and we need to train several models
<span class="math inline">\(h_{1,m}, h_{2,m}, \dots, h_{q,m}\)</span> with different number of principal
components (i.e. the tuning parameter).</p>
<p>Obviously, if we just simply use <span class="math inline">\(\mathcal{D}_{train}\)</span> to fit all possible
pcr models, and also to choose the one with smallest error <span class="math inline">\(E_{train}\)</span>, then
we run the risk of overfitting. Likewise, we know that <span class="math inline">\(E_{train}\)</span> is an
optimistic—unreliable—biased estimate of <span class="math inline">\(E_{out}\)</span>. One may ask,
“Why don’t we use <span class="math inline">\(\mathcal{D}_{val}\)</span> to compute validation errors, and
select the model with smallest validation error?” Quick answer: you could.
But then you are going to run out of fresh points for the final-selection
phase, also running the risk of choosing a model that overfits the validation
data.</p>
<p>It looks like we are heading into a dead-end road. On one hand, we cannot
use all the training points for both model training, and pre-selection
of finalists. On the other hand, if we use the validation points for the
pre-selection phase, then we are left with no fresh points to choose the
final model, unless we exhaust the points in the test set.</p>
<p>In theory, it seems that we should use different data sets for each phase,
something like a <span class="math inline">\(\mathcal{D}_{train}\)</span> for training candidate models,
<span class="math inline">\(\mathcal{D}_{pre}\)</span> for pre-selecting finalist models,
<span class="math inline">\(\mathcal{D}_{final}\)</span> for selecting the very final model, and
<span class="math inline">\(\mathcal{D}_{assess}\)</span> for assessing the performance of the final model.
We’ve seen what theory says about reserving some out-of-sample points
for both 1) making learning decisions (e.g. choosing a finalist), and for 2)
assessing the performance of a model. It is a wise idea to have fresh
unseen points to be spent as we move on into a new learning phase.
The problem is that theory doesn’t tell us exactly how many holdout sets we
should have, or how big they should be.</p>
<p>The main limitation is that we only have so much data. Can we really afford
splitting our limited data resources into four different sets? Are we doomed …?</p>
<p>Resampling methods to the rescue!</p>
<p>Fortunately, we can use (re)sampling methods that will allow us to make the
most out of—at least—the training set. Because of its practical relevance,
we discuss this topic in the next chapter.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="biasvar.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="resampling.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["allmodelsarewrong.pdf", "allmodelsarewrong.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
